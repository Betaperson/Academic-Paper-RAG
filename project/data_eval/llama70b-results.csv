query,pid,orig_ans,llm_ans
How do the models learned by PULSE facilitate musical interpretation?,6c996f7a,"[{'abstract': '  Predictive models for music are studied by researchers of algorithmic\ncomposition, the cognitive sciences and machine learning. They serve as base\nmodels for composition, can simulate human prediction and provide a\nmultidisciplinary application domain for learning algorithms. A particularly\nwell established and constantly advanced subtask is the prediction of\nmonophonic melodies. As melodies typically involve non-Markovian dependencies\ntheir prediction requires a capable learning algorithm. In this thesis, I apply\nthe recent feature discovery and learning method PULSE to the realm of symbolic\nmusic modeling. PULSE is comprised of a feature generating operation and\nL1-regularized optimization. These are used to iteratively expand and cull the\nfeature set, effectively exploring feature spaces that are too large for common\nfeature selection approaches. I design a general Python framework for PULSE,\npropose task-optimized feature generating operations and various\nmusic-theoretically motivated features that are evaluated on a standard corpus\nof monophonic folk and chorale melodies. The proposed method significantly\noutperforms comparable state-of-the-art models. I further discuss the free\nparameters of the learning algorithm and analyze the feature composition of the\nlearned models. The models learned by PULSE afford an easy inspection and are\nmusicologically interpreted for the first time.\n', 'title': 'Learning a Predictive Model for Music Using PULSE'}]","Title: Learning a Predictive Model for Music Using PULSE

Abstract: Predictive models for music are studied by researchers of algorithmic composition, the cognitive sciences and machine learning. They serve as base models for composition, can simulate human prediction and provide a multidisciplinary application domain for learning algorithms. A particularly well established and constantly advanced subtask is the prediction of monophonic melodies. As melodies typically involve non-Markovian dependencies their prediction requires a capable learning algorithm. In this thesis, I apply the recent feature discovery and learning method PULSE to the realm of symbolic music modeling. PULSE is comprised of a feature generating operation and L1-regularized optimization. These are used to iteratively expand and cull the feature set, effectively exploring feature spaces that are too large for common feature selection approaches. I design a general Python framework for PULSE, propose task-optimized feature generating operations and various music-theoretically motivated features that are evaluated on a standard corpus of monophonic folk and chorale melodies. The proposed method significantly outperforms comparable state-of-the-art models. I further discuss the free parameters of the learning algorithm and analyze the feature composition of the learned models. The models learned by PULSE afford an easy inspection and are musicologically interpreted for the first time.

According to this abstract, the models learned by PULSE facilitate musical interpretation because they ""afford an easy inspection"" and are ""musicologically interpreted for the first time""."
What is the primary discrepancy between measurements of the Hubble constant from different cosmological methods?,ed3bbe2a,"[{'abstract': ""  Estimates of the Hubble constant, $H_0$, from the distance ladder and the\ncosmic microwave background (CMB) differ at the $\\sim$3-$\\sigma$ level,\nindicating a potential issue with the standard $\\Lambda$CDM cosmology.\nInterpreting this tension correctly requires a model comparison calculation\ndepending on not only the traditional `$n$-$\\sigma$' mismatch but also the\ntails of the likelihoods. Determining the form of the tails of the local $H_0$\nlikelihood is impossible with the standard Gaussian least-squares\napproximation, as it requires using non-Gaussian distributions to faithfully\nrepresent anchor likelihoods and model outliers in the Cepheid and supernova\n(SN) populations, and simultaneous fitting of the full distance-ladder dataset\nto correctly propagate uncertainties. We have developed a Bayesian hierarchical\nmodel that describes the full distance ladder, from nearby geometric anchors\nthrough Cepheids to Hubble-Flow SNe. This model does not rely on any\ndistributions being Gaussian, allowing outliers to be modeled and obviating the\nneed for arbitrary data cuts. Sampling from the $\\sim$3000-parameter joint\nposterior using Hamiltonian Monte Carlo, we find $H_0$ = (72.72 $\\pm$ 1.67)\n${\\rm km\\,s^{-1}\\,Mpc^{-1}}$ when applied to the outlier-cleaned Riess et al.\n(2016) data, and ($73.15 \\pm 1.78$) ${\\rm km\\,s^{-1}\\,Mpc^{-1}}$ with SN\noutliers reintroduced. Our high-fidelity sampling of the low-$H_0$ tail of the\ndistance-ladder likelihood allows us to apply Bayesian model comparison to\nassess the evidence for deviation from $\\Lambda$CDM. We set up this comparison\nto yield a lower limit on the odds of the underlying model being $\\Lambda$CDM\ngiven the distance-ladder and Planck XIII (2016) CMB data. The odds against\n$\\Lambda$CDM are at worst 10:1 or 7:1, depending on whether the SNe outliers\nare cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016)\nlikelihood is used.\n"", 'title': 'Clarifying the Hubble constant tension with a Bayesian hierarchical model of the local distance ladder'}]","The primary discrepancy between measurements of the Hubble constant from different cosmological methods is that estimates from the distance ladder and the cosmic microwave background (CMB) differ at the ~3-σ level, indicating a potential issue with the standard ΛCDM cosmology.

Title: Clarifying the Hubble constant tension with a Bayesian hierarchical model of the local distance ladder

Abstract: Estimates of the Hubble constant, H₀, from the distance ladder and the cosmic microwave background (CMB) differ at the ~3-σ level, indicating a potential issue with the standard ΛCDM cosmology. Interpreting this tension correctly requires a model comparison calculation depending on not only the traditional 'n-σ' mismatch but also the tails of the likelihoods. Determining the form of the tails of the local H₀ likelihood is impossible with the standard Gaussian least-squares approximation, as it requires using non-Gaussian distributions to faithfully represent anchor likelihoods and model outliers in the Cepheid and supernova (SN) populations, and simultaneous fitting of the full distance-ladder dataset to correctly propagate uncertainties. We have developed a Bayesian hierarchical model that describes the full distance ladder, from nearby geometric anchors through Cepheids to Hubble-Flow SNe. This model does not rely on any distributions being Gaussian, allowing outliers to be modeled and obviating the need for arbitrary data cuts. Sampling from the ~3000-parameter joint posterior using Hamiltonian Monte Carlo, we find H₀ = (72.72 ± 1.67) km s⁻¹ Mpc⁻¹ when applied to the outlier-cleaned Riess et al. (2016) data, and (73.15 ± 1.78) km s⁻¹ Mpc⁻¹ with SN outliers reintroduced. Our high-fidelity sampling of the low-H₀ tail of the distance-ladder likelihood allows us to apply Bayesian model comparison to assess the evidence for deviation from ΛCDM. We set up this comparison to yield a lower limit on the odds of the underlying model being ΛCDM given the distance-ladder and Planck XIII (2016) CMB data. The odds against ΛCDM are at worst 10:1 or 7:1, depending on whether the SNe outliers are cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016) likelihood is used."
How is the Euclidean stochastic variance reduced gradient algorithm (R-SVRG) adapted for manifolds?,b06c0299,"[{'abstract': '  Stochastic variance reduction algorithms have recently become popular for\nminimizing the average of a large but finite number of loss functions. In this\npaper, we propose a novel Riemannian extension of the Euclidean stochastic\nvariance reduced gradient algorithm (R-SVRG) to a manifold search space. The\nkey challenges of averaging, adding, and subtracting multiple gradients are\naddressed with retraction and vector transport. We present a global convergence\nanalysis of the proposed algorithm with a decay step size and a local\nconvergence rate analysis under a fixed step size under some natural\nassumptions. The proposed algorithm is applied to problems on the Grassmann\nmanifold, such as principal component analysis, low-rank matrix completion, and\ncomputation of the Karcher mean of subspaces, and outperforms the standard\nRiemannian stochastic gradient descent algorithm in each case.\n', 'title': 'Riemannian stochastic variance reduced gradient'}]","Title: Riemannian stochastic variance reduced gradient

Abstract: Stochastic variance reduction algorithms have recently become popular for
minimizing the average of a large but finite number of loss functions. In this
paper, we propose a novel Riemannian extension of the Euclidean stochastic
variance reduced gradient algorithm (R-SVRG) to a manifold search space. The
key challenges of averaging, adding, and subtracting multiple gradients are
addressed with retraction and vector transport. We present a global convergence
analysis of the proposed algorithm with a decay step size and a local
convergence rate analysis under a fixed step size under some natural
assumptions. The proposed algorithm is applied to problems on the Grassmann
manifold, such as principal component analysis, low-rank matrix completion, and
computation of the Karcher mean of subspaces, and outperforms the standard
Riemannian stochastic gradient descent algorithm in each case."
How was the dynamical matrix of the network model in this paper constructed?,a747a15d,"[{'abstract': ""  A model in which a three-dimensional elastic medium is represented by a\nnetwork of identical masses connected by springs of random strengths and\nallowed to vibrate only along a selected axis of the reference frame, exhibits\nan Anderson localization transition. To study this transition, we assume that\nthe dynamical matrix of the network is given by a product of a sparse random\nmatrix with real, independent, Gaussian-distributed non-zero entries and its\ntranspose. A finite-time scaling analysis of system's response to an initial\nexcitation allows us to estimate the critical parameters of the localization\ntransition. The critical exponent is found to be $\\nu = 1.57 \\pm 0.02$ in\nagreement with previous studies of Anderson transition belonging to the\nthree-dimensional orthogonal universality class.\n"", 'title': 'Finite-time scaling at the Anderson transition for vibrations in solids'}]","I apologize, but after reviewing the provided abstracts, I couldn't find any information about the construction of a dynamical matrix of a network model. The papers seem to discuss different topics such as adaptive diffusion processes, topology optimization, network-theoretic approach to sparsified discrete vortex dynamics, graph convolutional networks, and convolutional generative networks for next item recommendation.

If you could provide more context or clarify which paper you are referring to, I would be happy to help you further."
What are the advantages of using FLMC over LMC in machine learning tasks?,057c4857,"[{'abstract': '  Recent studies on diffusion-based sampling methods have shown that Langevin\nMonte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and\nrigorous theoretical guarantees have been proven for both asymptotic and\nfinite-time regimes. Algorithmically, LMC-based algorithms resemble the\nwell-known gradient descent (GD) algorithm, where the GD recursion is perturbed\nby an additive Gaussian noise whose variance has a particular form. Fractional\nLangevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the\nGaussian noise is replaced by a heavy-tailed {\\alpha}-stable noise. As opposed\nto its Gaussian counterpart, these heavy-tailed perturbations can incur large\njumps and it has been empirically demonstrated that the choice of\n{\\alpha}-stable noise can provide several advantages in modern machine learning\nproblems, both in optimization and sampling contexts. However, as opposed to\nLMC, only asymptotic convergence properties of FLMC have been yet established.\nIn this study, we analyze the non-asymptotic behavior of FLMC for non-convex\noptimization and prove finite-time bounds for its expected suboptimality. Our\nresults show that the weak-error of FLMC increases faster than LMC, which\nsuggests using smaller step-sizes in FLMC. We finally extend our results to the\ncase where the exact gradients are replaced by stochastic gradients and show\nthat similar results hold in this setting as well.\n', 'title': 'Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization'}]","Unfortunately, based on the provided abstracts, I couldn't find a direct answer to the question ""What are the advantages of using FLMC over LMC in machine learning tasks?""

However, the abstract of ""Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization"" mentions that ""it has been empirically demonstrated that the choice of α-stable noise can provide several advantages in modern machine learning problems, both in optimization and sampling contexts."" This suggests that FLMC may have some advantages over LMC in certain machine learning tasks, but the specific advantages are not explicitly stated.

If you're interested in exploring this topic further, I can try to help you find additional resources or papers that might provide more insight into the advantages of using FLMC over LMC.

Title: Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization

Abstract: Recent studies on diffusion-based sampling methods have shown that Langevin Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and rigorous theoretical guarantees have been proven for both asymptotic and finite-time regimes. Algorithmically, LMC-based algorithms resemble the well-known gradient descent (GD) algorithm, where the GD recursion is perturbed by an additive Gaussian noise whose variance has a particular form. Fractional Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the Gaussian noise is replaced by a heavy-tailed α-stable noise. As opposed to its Gaussian counterpart, these heavy-tailed perturbations can incur large jumps and it has been empirically demonstrated that the choice of α-stable noise can provide several advantages in modern machine learning problems, both in optimization and sampling contexts. However, as opposed to LMC, only asymptotic convergence properties of FLMC have been yet established. In this study, we analyze the non-asymptotic behavior of FLMC for non-convex optimization and prove finite-time bounds for its expected suboptimality. Our results show that the weak-error of FLMC increases faster than LMC, which suggests using smaller step-sizes in FLMC. We finally extend our results to the case where the exact gradients are replaced by stochastic gradients and show that similar results hold in this setting as well."
Which linear model families have had their screening conditions explicitly derived?,7470dc5f,"[{'abstract': '  Statistical inference can be computationally prohibitive in\nultrahigh-dimensional linear models. Correlation-based variable screening, in\nwhich one leverages marginal correlations for removal of irrelevant variables\nfrom the model prior to statistical inference, can be used to overcome this\nchallenge. Prior works on correlation-based variable screening either impose\nstrong statistical priors on the linear model or assume specific post-screening\ninference methods. This paper first extends the analysis of correlation-based\nvariable screening to arbitrary linear models and post-screening inference\ntechniques. In particular, ($i$) it shows that a condition---termed the\nscreening condition---is sufficient for successful correlation-based screening\nof linear models, and ($ii$) it provides insights into the dependence of\nmarginal correlation-based screening on different problem parameters. Numerical\nexperiments confirm that these insights are not mere artifacts of analysis;\nrather, they are reflective of the challenges associated with marginal\ncorrelation-based variable screening. Second, the paper explicitly derives the\nscreening condition for two families of linear models, namely, sub-Gaussian\nlinear models and arbitrary (random or deterministic) linear models. In the\nprocess, it establishes that---under appropriate conditions---it is possible to\nreduce the dimension of an ultrahigh-dimensional, arbitrary linear model to\nalmost the sample size even when the number of active variables scales almost\nlinearly with the sample size.\n', 'title': 'ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional Linear Models'}]","Here is the answer to the question:

Title: ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional Linear Models

Abstract: Statistical inference can be computationally prohibitive in ultrahigh-dimensional linear models. Correlation-based variable screening, in which one leverages marginal correlations for removal of irrelevant variables from the model prior to statistical inference, can be used to overcome this challenge. Prior works on correlation-based variable screening either impose strong statistical priors on the linear model or assume specific post-screening inference methods. This paper first extends the analysis of correlation-based variable screening to arbitrary linear models and post-screening inference techniques. In particular, ($i$) it shows that a condition---termed the screening condition---is sufficient for successful correlation-based screening of linear models, and ($ii$) it provides insights into the dependence of marginal correlation-based screening on different problem parameters. Numerical experiments confirm that these insights are not mere artifacts of analysis; rather, they are reflective of the challenges associated with marginal correlation-based variable screening. Second, the paper explicitly derives the screening condition for two families of linear models, namely, sub-Gaussian linear models and arbitrary (random or deterministic) linear models. In the process, it establishes that---under appropriate conditions---it is possible to reduce the dimension of an ultrahigh-dimensional, arbitrary linear model to almost the sample size even when the number of active variables scales almost linearly with the sample size."
What is the impact of setting individual margins for training vectors in a Support Vector Machine?,dc660c88,"[{'abstract': '  This work proposes a new algorithm for training a re-weighted L2 Support\nVector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès\net al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In\nparticular, the margin required for each training vector is set independently,\ndefining a new weighted SVM model. These weights are selected to be binary, and\nthey are automatically adapted during the training of the model, resulting in a\nvariation of the Frank-Wolfe optimization algorithm with essentially the same\ncomputational complexity as the original algorithm. As shown experimentally,\nthis algorithm is computationally cheaper to apply since it requires less\niterations to converge, and it produces models with a sparser representation in\nterms of support vectors and which are more stable with respect to the\nselection of the regularization hyper-parameter.\n', 'title': 'Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers'}]","Title: Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers

Abstract:   This work proposes a new algorithm for training a re-weighted L2 Support Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In particular, the margin required for each training vector is set independently, defining a new weighted SVM model. These weights are selected to be binary, and they are automatically adapted during the training of the model, resulting in a variation of the Frank-Wolfe optimization algorithm with essentially the same computational complexity as the original algorithm. As shown experimentally, this algorithm is computationally cheaper to apply since it requires less iterations to converge, and it produces models with a sparser representation in terms of support vectors and which are more stable with respect to the selection of the regularization hyper-parameter."
What methods were used to characterize potential contamination on the OSIRIS-REx spacecraft?,a0374db8,"[{'abstract': '  OSIRIS-REx will return pristine samples of carbonaceous asteroid Bennu. This\narticle describes how pristine was defined based on expectations of Bennu and\non a realistic understanding of what is achievable with a constrained schedule\nand budget, and how that definition flowed to requirements and implementation.\nTo return a pristine sample, the OSIRIS- REx spacecraft sampling hardware was\nmaintained at level 100 A/2 and <180 ng/cm2 of amino acids and hydrazine on the\nsampler head through precision cleaning, control of materials, and vigilance.\nContamination is further characterized via witness material exposed to the\nspacecraft assembly and testing environment as well as in space. This\ncharacterization provided knowledge of the expected background and will be used\nin conjunction with archived spacecraft components for comparison with the\nsamples when they are delivered to Earth for analysis. Most of all, the\ncleanliness of the OSIRIS-REx spacecraft was achieved through communication\namong scientists, engineers, managers, and technicians.\n', 'title': 'OSIRIS-REx Contamination Control Strategy and Implementation'}]","Title: OSIRIS-REx Contamination Control Strategy and Implementation

Abstract:   OSIRIS-REx will return pristine samples of carbonaceous asteroid Bennu. This
article describes how pristine was defined based on expectations of Bennu and
on a realistic understanding of what is achievable with a constrained schedule
and budget, and how that definition flowed to requirements and implementation.
To return a pristine sample, the OSIRIS- REx spacecraft sampling hardware was
maintained at level 100 A/2 and <180 ng/cm2 of amino acids and hydrazine on the
sampler head through precision cleaning, control of materials, and vigilance.
Contamination is further characterized via witness material exposed to the
spacecraft assembly and testing environment as well as in space. This
characterization provided knowledge of the expected background and will be used
in conjunction with archived spacecraft components for comparison with the
samples when they are delivered to Earth for analysis. Most of all, the
cleanliness of the OSIRIS-REx spacecraft was achieved through communication
among scientists, engineers, managers, and technicians.

The methods used to characterize potential contamination on the OSIRIS-REx spacecraft were:
1. Precision cleaning
2. Control of materials
3. Vigilance
4. Witness material exposed to the spacecraft assembly and testing environment
5. Witness material exposed to space"
How can improving the first stage prediction in a two-stage least-squares model reduce bias?,681e4606,"[{'abstract': '  The two-stage least-squares (2SLS) estimator is known to be biased when its\nfirst-stage fit is poor. I show that better first-stage prediction can\nalleviate this bias. In a two-stage linear regression model with Normal noise,\nI consider shrinkage in the estimation of the first-stage instrumental variable\ncoefficients. For at least four instrumental variables and a single endogenous\nregressor, I establish that the standard 2SLS estimator is dominated with\nrespect to bias. The dominating IV estimator applies James-Stein type shrinkage\nin a first-stage high-dimensional Normal-means problem followed by a\ncontrol-function approach in the second stage. It preserves invariances of the\nstructural instrumental variable equations.\n', 'title': 'Bias Reduction in Instrumental Variable Estimation through First-Stage Shrinkage'}]","Title: Bias Reduction in Instrumental Variable Estimation through First-Stage Shrinkage

Abstract:   The two-stage least-squares (2SLS) estimator is known to be biased when its
first-stage fit is poor. I show that better first-stage prediction can
alleviate this bias. In a two-stage linear regression model with Normal noise,
I consider shrinkage in the estimation of the first-stage instrumental variable
coefficients. For at least four instrumental variables and a single endogenous
regressor, I establish that the standard 2SLS estimator is dominated with
respect to bias. The dominating IV estimator applies James-Stein type shrinkage
in a first-stage high-dimensional Normal-means problem followed by a
control-function approach in the second stage. It preserves invariances of the
structural instrumental variable equations."
Can neural networks or Gaussian processes be used to represent functions on sets?,5384ba59,"[{'abstract': '  Recent work on the representation of functions on sets has considered the use\nof summation in a latent space to enforce permutation invariance. In\nparticular, it has been conjectured that the dimension of this latent space may\nremain fixed as the cardinality of the sets under consideration increases.\nHowever, we demonstrate that the analysis leading to this conjecture requires\nmappings which are highly discontinuous and argue that this is only of limited\npractical use. Motivated by this observation, we prove that an implementation\nof this model via continuous mappings (as provided by e.g. neural networks or\nGaussian processes) actually imposes a constraint on the dimensionality of the\nlatent space. Practical universal function representation for set inputs can\nonly be achieved with a latent dimension at least the size of the maximum\nnumber of input elements.\n', 'title': 'On the Limitations of Representing Functions on Sets'}]","Yes, neural networks or Gaussian processes can be used to represent functions on sets.

Title: On the Limitations of Representing Functions on Sets

Abstract: Recent work on the representation of functions on sets has considered the use
of summation in a latent space to enforce permutation invariance. In
particular, it has been conjectured that the dimension of this latent space may
remain fixed as the cardinality of the sets under consideration increases.
However, we demonstrate that the analysis leading to this conjecture requires
mappings which are highly discontinuous and argue that this is only of limited
practical use. Motivated by this observation, we prove that an implementation
of this model via continuous mappings (as provided by e.g. neural networks or
Gaussian processes) actually imposes a constraint on the dimensionality of the
latent space. Practical universal function representation for set inputs can
only be achieved with a latent dimension at least the size of the maximum
number of input elements."
Are there any known ascending HNN extensions of finitely presented groups that do not have a semistable fundamental group at infinity?,ac7ec08d,"[{'abstract': ""  A 1-ended finitely presented group has semistable fundamental group at\n$\\infty$ if it acts geometrically on some (equivalently any) simply connected\nand locally finite complex $X$ with the property that any two proper rays in\n$X$ are properly homotopic. If $G$ has semistable fundamental group at $\\infty$\nthen one can unambiguously define the fundamental group at $\\infty$ for $G$.\nThe problem, asking if all finitely presented groups have semistable\nfundamental group at $\\infty$ has been studied for over 40 years. If $G$ is an\nascending HNN extension of a finitely presented group then indeed, $G$ has\nsemistable fundamental group at $\\infty$, but since the early 1980's it has\nbeen suggested that the finitely presented groups that are ascending HNN\nextensions of {\\it finitely generated} groups may include a group with\nnon-semistable fundamental group at $\\infty$. Ascending HNN extensions\nnaturally break into two classes, those with bounded depth and those with\nunbounded depth. Our main theorem shows that bounded depth finitely presented\nascending HNN extensions of finitely generated groups have semistable\nfundamental group at $\\infty$. Semistability is equivalent to two weaker\nasymptotic conditions on the group holding simultaneously. We show one of these\nconditions holds for all ascending HNN extensions, regardless of depth. We give\na technique for constructing ascending HNN extensions with unbounded depth.\nThis work focuses attention on a class of groups that may contain a group with\nnon-semistable fundamental group at $\\infty$.\n"", 'title': 'Bounded Depth Ascending HNN Extensions and $π_1$-Semistability at $\\infty$'}]","Based on the provided context, I found a relevant paper that addresses your question.

Title: Bounded Depth Ascending HNN Extensions and $π_1$-Semistability at $\infty$

Abstract:   A 1-ended finitely presented group has semistable fundamental group at
$\infty$ if it acts geometrically on some (equivalently any) simply connected
and locally finite complex $X$ with the property that any two proper rays in
$X$ are properly homotopic. If $G$ has semistable fundamental group at $\infty$
then one can unambiguously define the fundamental group at $\infty$ for $G$.
The problem, asking if all finitely presented groups have semistable
fundamental group at $\infty$ has been studied for over 40 years. If $G$ is an
ascending HNN extension of a finitely presented group then indeed, $G$ has
semistable fundamental group at $\infty$, but since the early 1980's it has
been suggested that the finitely presented groups that are ascending HNN
extensions of {\it finitely generated} groups may include a group with
non-semistable fundamental group at $\infty$. Ascending HNN extensions
naturally break into two classes, those with bounded depth and those with
unbounded depth. Our main theorem shows that bounded depth finitely presented
ascending HNN extensions of finitely generated groups have semistable
fundamental group at $\infty$. Semistability is equivalent to two weaker
asymptotic conditions on the group holding simultaneously. We show one of these
conditions holds for all ascending HNN extensions, regardless of depth. We give
a technique for constructing ascending HNN extensions with unbounded depth.
This work focuses attention on a class of groups that may contain a group with
non-semistable fundamental group at $\infty$.

Unfortunately, this paper does not provide a direct answer to your question, as it focuses on bounded depth ascending HNN extensions of finitely presented groups, which are shown to have semistable fundamental groups at infinity. However, it does suggest that there might be ascending HNN extensions with unbounded depth that do not have semistable fundamental groups at infinity, but it does not provide a specific example."
Describe an algorithm for sequential hypothesis testing that considers the partial order of hypotheses defined by a directed acyclic graph.,6cb89e78,"[{'abstract': ""  We propose a linear-time, single-pass, top-down algorithm for multiple\ntesting on directed acyclic graphs (DAGs), where nodes represent hypotheses and\nedges specify a partial ordering in which hypotheses must be tested. The\nprocedure is guaranteed to reject a sub-DAG with bounded false discovery rate\n(FDR) while satisfying the logical constraint that a rejected node's parents\nmust also be rejected. It is designed for sequential testing settings, when the\nDAG structure is known a priori, but the $p$-values are obtained selectively\n(such as in a sequence of experiments), but the algorithm is also applicable in\nnon-sequential settings when all $p$-values can be calculated in advance (such\nas variable/model selection). Our DAGGER algorithm, shorthand for Greedily\nEvolving Rejections on DAGs, provably controls the false discovery rate under\nindependence, positive dependence or arbitrary dependence of the $p$-values.\nThe DAGGER procedure specializes to known algorithms in the special cases of\ntrees and line graphs, and simplifies to the classical Benjamini-Hochberg\nprocedure when the DAG has no edges. We explore the empirical performance of\nDAGGER using simulations, as well as a real dataset corresponding to a gene\nontology, showing favorable performance in terms of time and power.\n"", 'title': 'DAGGER: A sequential algorithm for FDR control on DAGs'}]","I think I have found a relevant paper that describes an algorithm for sequential hypothesis testing that considers the partial order of hypotheses defined by a directed acyclic graph.

Title: DAGGER: A sequential algorithm for FDR control on DAGs

Abstract: We propose a linear-time, single-pass, top-down algorithm for multiple testing on directed acyclic graphs (DAGs), where nodes represent hypotheses and edges specify a partial ordering in which hypotheses must be tested. The procedure is guaranteed to reject a sub-DAG with bounded false discovery rate (FDR) while satisfying the logical constraint that a rejected node's parents must also be rejected. It is designed for sequential testing settings, when the DAG structure is known a priori, but the $p$-values are obtained selectively (such as in a sequence of experiments), but the algorithm is also applicable in non-sequential settings when all $p$-values can be calculated in advance (such as variable/model selection). Our DAGGER algorithm, shorthand for Greedily Evolving Rejections on DAGs, provably controls the false discovery rate under independence, positive dependence or arbitrary dependence of the $p$-values. The DAGGER procedure specializes to known algorithms in the special cases of trees and line graphs, and simplifies to the classical Benjamini-Hochberg procedure when the DAG has no edges. We explore the empirical performance of DAGGER using simulations, as well as a real dataset corresponding to a gene ontology, showing favorable performance in terms of time and power."
What physical factor besides viscosity influences the process of a ball becoming coated with fluid upon exiting a fluid reservoir?,2eb8a03e,"[{'abstract': ""  We consider the withdrawal of a ball from a fluid reservoir to understand the\nlongevity of the connection between that ball and the fluid it breaks away\nfrom, at intermediate Reynolds numbers. Scaling arguments based on the\nprocesses observed as the ball interacts with the fluid surface were applied to\nthe `pinch-off time', when the ball breaks its connection with the fluid from\nwhich it has been withdrawn, measured experimentally. At the lowest Reynolds\nnumbers tested, pinch-off occurs in a `surface seal' close to the reservoir\nsurface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal'\nclose to the ball. Our scaling analysis shows that the connection between ball\nand fluid is controlled by the fluid film draining from the ball as it\ncontinues to be winched away from the fluid reservoir. The draining flow itself\ndepends on the amount of fluid coating the ball on exit from the reservoir. We\nconsider the possibilities that this coating was created through: a surface\ntension driven Landau Levitch Derjaguin wetting of the surface; a\nvisco-inertial quick coating; or alternatively through the inertia of the fluid\nmoving with the ball through the reservoir. We show that although the pinch-off\nmechanism is controlled by viscosity, the coating mechanism is governed by a\ndifferent length and timescale, dictated by the inertial added mass of the ball\nwhen submersed.\n"", 'title': 'A Ball Breaking Away from a Fluid'}]","Based on the provided context, I found an answer to your question.

Title: A Ball Breaking Away from a Fluid

Abstract: We consider the withdrawal of a ball from a fluid reservoir to understand the longevity of the connection between that ball and the fluid it breaks away from, at intermediate Reynolds numbers. Scaling arguments based on the processes observed as the ball interacts with the fluid surface were applied to the `pinch-off time', when the ball breaks its connection with the fluid from which it has been withdrawn, measured experimentally. At the lowest Reynolds numbers tested, pinch-off occurs in a `surface seal' close to the reservoir surface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal' close to the ball. Our scaling analysis shows that the connection between ball and fluid is controlled by the fluid film draining from the ball as it continues to be winched away from the fluid reservoir. The draining flow itself depends on the amount of fluid coating the ball on exit from the reservoir. We consider the possibilities that this coating was created through: a surface tension driven Landau Levitch Derjaguin wetting of the surface; a visco-inertial quick coating; or alternatively through the inertia of the fluid moving with the ball through the reservoir. We show that although the pinch-off mechanism is controlled by viscosity, the coating mechanism is governed by a different length and timescale, dictated by the inertial added mass of the ball when submersed.

The physical factor besides viscosity that influences the process of a ball becoming coated with fluid upon exiting a fluid reservoir is the inertial added mass of the ball when submerged."
Can you describe a method for constructing inner functions based on Shapiro and Shields' work?,08d5eafa,"[{'abstract': '  We discuss the concept of inner function in reproducing kernel Hilbert spaces\nwith an orthogonal basis of monomials and examine connections between inner\nfunctions and optimal polynomial approximants to $1/f$, where $f$ is a function\nin the space. We revisit some classical examples from this perspective, and\nshow how a construction of Shapiro and Shields can be modified to produce inner\nfunctions.\n', 'title': 'Remarks on Inner Functions and Optimal Approximants'}]","Based on the context, I think I have found the relevant paper:

Title: Remarks on Inner Functions and Optimal Approximants

Abstract: We discuss the concept of inner function in reproducing kernel Hilbert spaces with an orthogonal basis of monomials and examine connections between inner functions and optimal polynomial approximants to $1/f$, where $f$ is a function in the space. We revisit some classical examples from this perspective, and show how a construction of Shapiro and Shields can be modified to produce inner functions."
What is the primary factor responsible for the observed features in the transient absorption spectrum of MoS2?,1500de3f,"[{'abstract': '  Transition metal dichalcogenides (TMDs) are emerging as promising\ntwo-dimensional (2d) semiconductors for optoelectronic and flexible devices.\nHowever, a microscopic explanation of their photophysics -- of pivotal\nimportance for the understanding and optimization of device operation -- is\nstill lacking. Here we use femtosecond transient absorption spectroscopy, with\npump pulse tunability and broadband probing, to monitor the relaxation dynamics\nof single-layer MoS2 over the entire visible range, upon photoexcitation of\ndifferent excitonic transitions. We find that, irrespective of excitation\nphoton energy, the transient absorption spectrum shows the simultaneous\nbleaching of all excitonic transitions and corresponding red-shifted\nphotoinduced absorption bands. First-principle modeling of the ultrafast\noptical response reveals that a transient bandgap renormalization, caused by\nthe presence of photo-excited carriers, is primarily responsible for the\nobserved features. Our results demonstrate the strong impact of many-body\neffects in the transient optical response of TMDs even in the\nlow-excitation-density regime.\n', 'title': 'Photo-Induced Bandgap Renormalization Governs the Ultrafast Response of Single-Layer MoS2'}]","Title: Photo-Induced Bandgap Renormalization Governs the Ultrafast Response of Single-Layer MoS2

Abstract:   Transition metal dichalcogenides (TMDs) are emerging as promising
two-dimensional (2d) semiconductors for optoelectronic and flexible devices.
However, a microscopic explanation of their photophysics -- of pivotal
importance for the understanding and optimization of device operation -- is
still lacking. Here we use femtosecond transient absorption spectroscopy, with
pump pulse tunability and broadband probing, to monitor the relaxation dynamics
of single-layer MoS2 over the entire visible range, upon photoexcitation of
different excitonic transitions. We find that, irrespective of excitation
photon energy, the transient absorption spectrum shows the simultaneous
bleaching of all excitonic transitions and corresponding red-shifted
photoinduced absorption bands. First-principle modeling of the ultrafast
optical response reveals that a transient bandgap renormalization, caused by
the presence of photo-excited carriers, is primarily responsible for the
observed features. Our results demonstrate the strong impact of many-body
effects in the transient optical response of TMDs even in the
low-excitation-density regime."
What is the integral of a generalized Ricci tensor with respect to the Boltzmann weight?,fa985aa6,"[{'abstract': '  Riemannian geometry is a particular case of Hamiltonian mechanics: the orbits\nof the hamiltonian $H=\\frac{1}{2}g^{ij}p_{i}p_{j}$ are the geodesics. Given a\nsymplectic manifold (\\Gamma,\\omega), a hamiltonian $H:\\Gamma\\to\\mathbb{R}$ and\na Lagrangian sub-manifold $M\\subset\\Gamma$ we find a generalization of the\nnotion of curvature. The particular case\n$H=\\frac{1}{2}g^{ij}\\left[p_{i}-A_{i}\\right]\\left[p_{j}-A_{j}\\right]+\\phi $ of\na particle moving in a gravitational, electromagnetic and scalar fields is\nstudied in more detail. The integral of the generalized Ricci tensor w.r.t. the\nBoltzmann weight reduces to the action principle\n$\\int\\left[R+\\frac{1}{4}F_{ik}F_{jl}g^{kl}g^{ij}-g^{ij}\\partial_{i}\\phi\\partial_{j}\\phi\\right]e^{-\\phi}\\sqrt{g}d^{n}q$\nfor the scalar, vector and tensor fields.\n', 'title': 'Curvature in Hamiltonian Mechanics And The Einstein-Maxwell-Dilaton Action'}]","Title: Curvature in Hamiltonian Mechanics And The Einstein-Maxwell-Dilaton Action

Abstract: Riemannian geometry is a particular case of Hamiltonian mechanics: the orbits of the hamiltonian $H=\frac{1}{2}g^{ij}p_{i}p_{j}$ are the geodesics. Given a symplectic manifold (\Gamma,\omega), a hamiltonian $H:\Gamma\to\mathbb{R}$ and a Lagrangian sub-manifold $M\subset\Gamma$ we find a generalization of the notion of curvature. The particular case $H=\frac{1}{2}g^{ij}\left[p_{i}-A_{i}\right]\left[p_{j}-A_{j}\right]+\phi $ of a particle moving in a gravitational, electromagnetic and scalar fields is studied in more detail. The integral of the generalized Ricci tensor w.r.t. the Boltzmann weight reduces to the action principle $\int\left[R+\frac{1}{4}F_{ik}F_{jl}g^{kl}g^{ij}-g^{ij}\partial_{i}\phi\partial_{j}\phi\right]e^{-\phi}\sqrt{g}d^{n}q$ for the scalar, vector and tensor fields."
How does the Memory Aware Synapses (MAS) method enable adaptation of parameter importance based on unlabeled data?,5daa856c,"[{'abstract': ""  Humans can learn in a continuous manner. Old rarely utilized knowledge can be\noverwritten by new incoming information while important, frequently used\nknowledge is prevented from being erased. In artificial learning systems,\nlifelong learning so far has focused mainly on accumulating knowledge over\ntasks and overcoming catastrophic forgetting. In this paper, we argue that,\ngiven the limited model capacity and the unlimited new information to be\nlearned, knowledge has to be preserved or erased selectively. Inspired by\nneuroplasticity, we propose a novel approach for lifelong learning, coined\nMemory Aware Synapses (MAS). It computes the importance of the parameters of a\nneural network in an unsupervised and online manner. Given a new sample which\nis fed to the network, MAS accumulates an importance measure for each parameter\nof the network, based on how sensitive the predicted output function is to a\nchange in this parameter. When learning a new task, changes to important\nparameters can then be penalized, effectively preventing important knowledge\nrelated to previous tasks from being overwritten. Further, we show an\ninteresting connection between a local version of our method and Hebb's\nrule,which is a model for the learning process in the brain. We test our method\non a sequence of object recognition tasks and on the challenging problem of\nlearning an embedding for predicting $<$subject, predicate, object$>$ triplets.\nWe show state-of-the-art performance and, for the first time, the ability to\nadapt the importance of the parameters based on unlabeled data towards what the\nnetwork needs (not) to forget, which may vary depending on test conditions.\n"", 'title': 'Memory Aware Synapses: Learning what (not) to forget'}]","Title: Memory Aware Synapses: Learning what (not) to forget

Abstract:   Humans can learn in a continuous manner. Old rarely utilized knowledge can be
overwritten by new incoming information while important, frequently used
knowledge is prevented from being erased. In artificial learning systems,
lifelong learning so far has focused mainly on accumulating knowledge over
tasks and overcoming catastrophic forgetting. In this paper, we argue that,
given the limited model capacity and the unlimited new information to be
learned, knowledge has to be preserved or erased selectively. Inspired by
neuroplasticity, we propose a novel approach for lifelong learning, coined
Memory Aware Synapses (MAS). It computes the importance of the parameters of a
neural network in an unsupervised and online manner. Given a new sample which
is fed to the network, MAS accumulates an importance measure for each parameter
of the network, based on how sensitive the predicted output function is to a
change in this parameter. When learning a new task, changes to important
parameters can then be penalized, effectively preventing important knowledge
related to previous tasks from being overwritten. Further, we show an
interesting connection between a local version of our method and Hebb's
rule,which is a model for the learning process in the brain. We test our method
on a sequence of object recognition tasks and on the challenging problem of
learning an embedding for predicting $<$subject, predicate, object$>$ triplets.
We show state-of-the-art performance and, for the first time, the ability to
adapt the importance of the parameters based on unlabeled data towards what the
network needs (not) to forget, which may vary depending on test conditions.

The answer can be found in the abstract, specifically the sentence: ""We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions."""
In what context does the author discuss black hole event horizons?,2ad242b7,"[{'abstract': '  Deconstruction of the theme of the 2017 FQXi essay contest is already an\ninteresting exercise in its own right: Teleology is rarely useful in physics\n--- the only known mainstream physics example (black hole event horizons) has a\nvery mixed score-card --- so the ""goals"" and ""aims and intentions"" alluded to\nin the theme of the 2017 FQXi essay contest are already somewhat pushing the\nlimits. Furthermore, ""aims and intentions"" certainly carries the implication of\nconsciousness, and opens up a whole can of worms related to the mind-body\nproblem. As for ""mindless mathematical laws"", that allusion is certainly in\ntension with at least some versions of the ""mathematical universe hypothesis"".\nFinally ""wandering towards a goal"" again carries the implication of\nconsciousness, with all its attendant problems.\nIn this essay I will argue, simply because we do not yet have any really good\nmathematical or physical theory of consciousness, that the theme of this essay\ncontest is premature, and unlikely to lead to any resolution that would be\nwidely accepted in the mathematics or physics communities.\n', 'title': 'From mindless mathematics to thinking meat?'}]","None of the abstracts mention black hole event horizons. The closest related topic is the discussion of supermassive black holes in the abstract of ""The X-ray reflection spectrum of the radio-loud quasar 4C 74.26"", but it does not mention event horizons."
Are there any modular curves with an infinite supply of cubic points?,d958f6ce,"[{'abstract': '  In this study, we determine all modular curves $X_0(N)$ that admit infinitely\nmany cubic points.\n', 'title': 'Modular curves with infinitely many cubic points'}]","You're looking for research papers related to modular curves with infinitely many cubic points. 

Title: Modular curves with infinitely many cubic points

Abstract: In this study, we determine all modular curves $X_0(N)$ that admit infinitely many cubic points.

I think I've found exactly what you're looking for!"
What are efficient methods for computing graph edit distance (GED)?,6d4e833e,"[{'abstract': '  Graph edit distance (GED) is an important similarity measure adopted in a\nsimilarity-based analysis between two graphs, and computing GED is a primitive\noperator in graph database analysis. Partially due to the NP-hardness, the\nexisting techniques for computing GED are only able to process very small\ngraphs with less than 30 vertices. Motivated by this, in this paper we\nsystematically study the problems of both GED computation, and GED verification\n(i.e., verify whether the GED between two graphs is no larger than a user-given\nthreshold). Firstly, we develop a unified framework that can be instantiated\ninto either a best-first search approach AStar+ or a depth-first search\napproach DFS+. Secondly, we design anchor-aware lower bound estimation\ntechniques to compute tighter lower bounds for intermediate search states,\nwhich significantly reduce the search spaces of both AStar+ and DFS+. We also\npropose efficient techniques to compute the lower bounds. Thirdly, based on our\nunified framework, we contrast AStar+ with DFS+ regarding their time and space\ncomplexities, and recommend that AStar+ is better than DFS+ by having a much\nsmaller search space. Extensive empirical studies validate that AStar+ performs\nbetter than DFS+, and show that our AStar+-BMa approach outperforms the\nstate-of-the-art technique by more than four orders of magnitude.\n', 'title': 'Efficient Graph Edit Distance Computation and Verification via Anchor-aware Lower Bound Estimation'}]","Title: Efficient Graph Edit Distance Computation and Verification via Anchor-aware Lower Bound Estimation

Abstract:   Graph edit distance (GED) is an important similarity measure adopted in a
similarity-based analysis between two graphs, and computing GED is a primitive
operator in graph database analysis. Partially due to the NP-hardness, the
existing techniques for computing GED are only able to process very small
graphs with less than 30 vertices. Motivated by this, in this paper we
systematically study the problems of both GED computation, and GED verification
(i.e., verify whether the GED between two graphs is no larger than a user-given
threshold). Firstly, we develop a unified framework that can be instantiated
into either a best-first search approach AStar+ or a depth-first search
approach DFS+. Secondly, we design anchor-aware lower bound estimation
techniques to compute tighter lower bounds for intermediate search states,
which significantly reduce the search spaces of both AStar+ and DFS+. We also
propose efficient techniques to compute the lower bounds. Thirdly, based on our
unified framework, we contrast AStar+ with DFS+ regarding their time and space
complexities, and recommend that AStar+ is better than DFS+ by having a much
smaller search space. Extensive empirical studies validate that AStar+ performs
better than DFS+, and show that our AStar+-BMa approach outperforms the
state-of-the-art technique by more than four orders of magnitude."
How does the probability distribution of the solution change when the conductivity and diffusivity matrices are not proportional?,93f15441,"[{'abstract': '  We investigate the density large deviation function for a multidimensional\nconservation law in the vanishing viscosity limit, when the probability\nconcentrates on weak solutions of a hyperbolic conservation law conservation\nlaw. When the conductivity and dif-fusivity matrices are proportional, i.e. an\nEinstein-like relation is satisfied, the problem has been solved in [4]. When\nthis proportionality does not hold, we compute explicitly the large deviation\nfunction for a step-like density profile, and we show that the associated\noptimal current has a non trivial structure. We also derive a lower bound for\nthe large deviation function, valid for a general weak solution, and leave the\ngeneral large deviation function upper bound as a conjecture.\n', 'title': 'Density large deviations for multidimensional stochastic hyperbolic conservation laws'}]","Title: Density large deviations for multidimensional stochastic hyperbolic conservation laws

Abstract: We investigate the density large deviation function for a multidimensional conservation law in the vanishing viscosity limit, when the probability concentrates on weak solutions of a hyperbolic conservation law conservation law. When the conductivity and dif-fusivity matrices are proportional, i.e. an Einstein-like relation is satisfied, the problem has been solved in [4]. When this proportionality does not hold, we compute explicitly the large deviation function for a step-like density profile, and we show that the associated optimal current has a non trivial structure. We also derive a lower bound for the large deviation function, valid for a general weak solution, and leave the general large deviation function upper bound as a conjecture."
What is a more efficient mass-preconditioning technique for the exact one-flavor action in lattice QCD with domain-wall fermions?,6f8cb6a5,"[{'abstract': '  The mass-preconditioning (MP) technique has become a standard tool to enhance\nthe efficiency of the hybrid Monte-Carlo simulation (HMC) of lattice QCD with\ndynamical quarks, for 2-flavors QCD with degenerate quark masses, as well as\nits extension to the case of one-flavor by taking the square-root of the\nfermion determinant of 2-flavors with degenerate masses. However, for lattice\nQCD with domain-wall fermion, the fermion determinant of any single fermion\nflavor can be expressed as a functional integral with an exact pseudofermion\naction $ \\phi^\\dagger H^{-1} \\phi $, where $ H^{-1} $ is a positive-definite\nHermitian operator without taking square-root, and with the chiral structure\n\\cite{Chen:2014hyy}. Consequently, the mass-preconditioning for the exact\none-flavor action (EOFA) does not necessarily follow the conventional (old) MP\npattern. In this paper, we present a new mass-preconditioning for the EOFA,\nwhich is more efficient than the old MP which we have used in Refs.\n\\cite{Chen:2014hyy,Chen:2014bbc}. We perform numerical tests in lattice QCD\nwith $ N_f = 1 $ and $ N_f = 1+1+1+1 $ optimal domain-wall quarks, with one\nmass-preconditioner applied to one of the exact one-flavor actions, and we find\nthat the efficiency of the new MP is more than 20\\% higher than that of the old\nMP.\n', 'title': 'Mass Preconditioning for the Exact One-Flavor Action in Lattice QCD with Domain-Wall Fermion'}]","Title: Mass Preconditioning for the Exact One-Flavor Action in Lattice QCD with Domain-Wall Fermion

Abstract: The mass-preconditioning (MP) technique has become a standard tool to enhance the efficiency of the hybrid Monte-Carlo simulation (HMC) of lattice QCD with dynamical quarks, for 2-flavors QCD with degenerate quark masses, as well as its extension to the case of one-flavor by taking the square-root of the fermion determinant of 2-flavors with degenerate masses. However, for lattice QCD with domain-wall fermion, the fermion determinant of any single fermion flavor can be expressed as a functional integral with an exact pseudofermion action $ \phi^\dagger H^{-1} \phi $, where $ H^{-1} $ is a positive-definite Hermitian operator without taking square-root, and with the chiral structure \cite{Chen:2014hyy}. Consequently, the mass-preconditioning for the exact one-flavor action (EOFA) does not necessarily follow the conventional (old) MP pattern. In this paper, we present a new mass-preconditioning for the EOFA, which is more efficient than the old MP which we have used in Refs. \cite{Chen:2014hyy,Chen:2014bbc}. We perform numerical tests in lattice QCD with $ N_f = 1 $ and $ N_f = 1+1+1+1 $ optimal domain-wall quarks, with one mass-preconditioner applied to one of the exact one-flavor actions, and we find that the efficiency of the new MP is more than 20\% higher than that of the old MP."
How does the Dijkstra algorithm contribute to calculating semantic distance between concepts?,808a67c9,"[{'abstract': '  With the use of ontologies in several domains such as semantic web,\ninformation retrieval, artificial intelligence, the concept of similarity\nmeasuring has become a very important domain of research. Therefore, in the\ncurrent paper, we propose our method of similarity measuring which uses the\nDijkstra algorithm to define and compute the shortest path. Then, we use this\none to compute the semantic distance between two concepts defined in the same\nhierarchy of ontology. Afterward, we base on this result to compute the\nsemantic similarity. Finally, we present an experimental comparison between our\nmethod and other methods of similarity measuring.\n', 'title': 'An enhanced method to compute the similarity between concepts of ontology'}]","Title: An enhanced method to compute the similarity between concepts of ontology

Abstract: With the use of ontologies in several domains such as semantic web, information retrieval, artificial intelligence, the concept of similarity measuring has become a very important domain of research. Therefore, in the current paper, we propose our method of similarity measuring which uses the Dijkstra algorithm to define and compute the shortest path. Then, we use this one to compute the semantic distance between two concepts defined in the same hierarchy of ontology. Afterward, we base on this result to compute the semantic similarity. Finally, we present an experimental comparison between our method and other methods of similarity measuring.

The Dijkstra algorithm contributes to calculating semantic distance between concepts by computing the shortest path between two concepts in the same hierarchy of ontology, which is then used to compute the semantic distance."
How does this new consensus algorithm achieve fault tolerance?,30bcb009,"[{'abstract': '  Recently a new fault tolerant and simple mechanism was designed for solving\ncommit consensus problem. It is based on replicated validation of messages sent\nbetween transaction participants and a special dispatcher validator manager\nnode. This paper presents a correctness, safety proofs and performance analysis\nof this algorithm.\n', 'title': 'Fault Tolerant Consensus Agreement Algorithm'}]","Title: Fault Tolerant Consensus Agreement Algorithm

Abstract: Recently a new fault tolerant and simple mechanism was designed for solving commit consensus problem. It is based on replicated validation of messages sent between transaction participants and a special dispatcher validator manager node. This paper presents a correctness, safety proofs and performance analysis of this algorithm."
How does the symmetric difference transformation change a set function?,01628af0,"[{'abstract': '  A set function $f$ on a finite set $V$ is submodular if $f(X) + f(Y) \\geq f(X\n\\cup Y) + f(X \\cap Y)$ for any pair $X, Y \\subseteq V$. The symmetric\ndifference transformation (SD-transformation) of $f$ by a canonical set $S\n\\subseteq V$ is a set function $g$ given by $g(X) = f(X \\vartriangle S)$ for $X\n\\subseteq V$,where $X \\vartriangle S = (X \\setminus S) \\cup (S \\setminus X)$\ndenotes the symmetric difference between $X$ and $S$. Submodularity and\nSD-transformations are regarded as the counterparts of convexity and affine\ntransformations in a discrete space, respectively. However, submodularity is\nnot preserved under SD-transformations, in contrast to the fact that convexity\nis invariant under affine transformations. This paper presents a\ncharacterization of SD-stransformations preserving submodularity. Then, we are\nconcerned with the problem of discovering a canonical set $S$, given the\nSD-transformation $g$ of a submodular function $f$ by $S$, provided that $g(X)$\nis given by a function value oracle. A submodular function $f$ on $V$ is said\nto be strict if $f(X) + f(Y) > f(X \\cup Y) + f(X \\cap Y)$ holds whenever both\n$X \\setminus Y$ and $Y \\setminus X$ are nonempty. We show that the problem is\nsolved by using ${\\rm O}(|V|)$ oracle calls when $f$ is strictly submodular,\nalthough it requires exponentially many oracle calls in general.\n', 'title': 'Finding Submodularity Hidden in Symmetric Difference'}]","The answer to your question can be found in the first abstract.

Title: Finding Submodularity Hidden in Symmetric Difference

Abstract: A set function $f$ on a finite set $V$ is submodular if $f(X) + f(Y) \geq f(X \cup Y) + f(X \cap Y)$ for any pair $X, Y \subseteq V$. The symmetric difference transformation (SD-transformation) of $f$ by a canonical set $S \subseteq V$ is a set function $g$ given by $g(X) = f(X \vartriangle S)$ for $X \subseteq V$, where $X \vartriangle S = (X \setminus S) \cup (S \setminus X)$ denotes the symmetric difference between $X$ and $S$."
Describe a reformulation of the static equilibrium constraint for tensegrity robots.,c5aec67d,"[{'abstract': ""  Tension-network (`tensegrity') robots encounter many control challenges as\narticulated soft robots, due to the structures' high-dimensional nonlinear\ndynamics. Control approaches have been developed which use the inverse\nkinematics of tensegrity structures, either for open-loop control or as\nequilibrium inputs for closed-loop controllers. However, current formulations\nof the tensegrity inverse kinematics problem are limited in robotics\napplications: first, they can lead to higher than needed cable tensions, and\nsecond, may lack solutions when applied to robots with high node-to-cable\nratios. This work provides progress in both directions. To address the first\nlimitation, the objective function for the inverse kinematics optimization\nproblem is modified to produce cable tensions as low or lower than before, thus\nreducing the load on the robots' motors. For the second, a reformulation of the\nstatic equilibrium constraint is proposed, which produces solutions independent\nof the number of nodes within each rigid body. Simulation results using the\nsecond reformulation on a specific tensegrity spine robot show reasonable\nopen-loop control results, whereas the previous formulation could not produce\nany solution.\n"", 'title': 'Inverse Kinematics for Control of Tensegrity Soft Robots: Existence and Optimality of Solutions'}]","Title: Inverse Kinematics for Control of Tensegrity Soft Robots: Existence and Optimality of Solutions

Abstract: Tension-network (`tensegrity') robots encounter many control challenges as articulated soft robots, due to the structures' high-dimensional nonlinear dynamics. Control approaches have been developed which use the inverse kinematics of tensegrity structures, either for open-loop control or as equilibrium inputs for closed-loop controllers. However, current formulations of the tensegrity inverse kinematics problem are limited in robotics applications: first, they can lead to higher than needed cable tensions, and second, may lack solutions when applied to robots with high node-to-cable ratios. This work provides progress in both directions. To address the first limitation, the objective function for the inverse kinematics optimization problem is modified to produce cable tensions as low or lower than before, thus reducing the load on the robots' motors. For the second, a reformulation of the static equilibrium constraint is proposed, which produces solutions independent of the number of nodes within each rigid body. Simulation results using the second reformulation on a specific tensegrity spine robot show reasonable open-loop control results, whereas the previous formulation could not produce any solution."
Is it possible to experimentally observe the phenomena described in a paper about fluid flow through actively changing porous media?,0c852316,"[{'abstract': '  Stimuli-responsive materials that modify their shape in response to changes\nin environmental conditions -- such as solute concentration, temperature, pH,\nand stress -- are widespread in nature and technology. Applications include\nmicro- and nanoporous materials used in filtration and flow control. The\nphysiochemical mechanisms that induce internal volume modifications have been\nwidely studies. The coupling between induced volume changes and solute\ntransport through porous materials, however, is not well understood. Here, we\nconsider advective and diffusive transport through a small channel linking two\nlarge reservoirs. A section of stimulus-responsive material regulates the\nchannel permeability, which is a function of the local solute concentration. We\nderive an exact solution to the coupled transport problem and demonstrate the\nexistence of a flow regime in which the steady state is reached via a damped\noscillation around the equilibrium concentration value. Finally, the\nfeasibility of an experimental observation of the phenomena is discussed.\nPlease note that this version of the paper has not been formally peer reviewed,\nrevised or accepted by a journal.\n', 'title': 'Transient flows in active porous media'}]","Title: Transient flows in active porous media

Abstract:   Stimuli-responsive materials that modify their shape in response to changes
in environmental conditions -- such as solute concentration, temperature, pH,
and stress -- are widespread in nature and technology. Applications include
micro- and nanoporous materials used in filtration and flow control. The
physiochemical mechanisms that induce internal volume modifications have been
widely studies. The coupling between induced volume changes and solute
transport through porous materials, however, is not well understood. Here, we
consider advective and diffusive transport through a small channel linking two
large reservoirs. A section of stimulus-responsive material regulates the
channel permeability, which is a function of the local solute concentration. We
derive an exact solution to the coupled transport problem and demonstrate the
existence of a flow regime in which the steady state is reached via a damped
oscillation around the equilibrium concentration value. Finally, the
feasibility of an experimental observation of the phenomena is discussed.

     Yes, according to the abstract, the authors discuss the feasibility of experimental observation of the phenomena."
How does the double exponential transformation compare to the single exponential transformation for approximating integrals?,ed0b6672,"[{'abstract': '  The double exponential formula was introduced for calculating definite\nintegrals with singular point oscillation functions and Fourier-integrals. The\ndouble exponential transformation is not only useful for numerical computations\nbut it is also used in different methods of Sinc theory. In this paper we use\ndouble exponential transformation for calculating particular improper\nintegrals. By improving integral estimates having singular final points. By\ncomparison between double exponential transformations and single exponential\ntransformations it is proved that the error margin of double exponential\ntransformations is smaller. Finally Fourier-integral and double exponential\ntransformations are discussed.\n', 'title': 'Criteria for the Application of Double Exponential Transformation'}]","I think I can help you with that!

Title: Criteria for the Application of Double Exponential Transformation

Abstract: The double exponential formula was introduced for calculating definite integrals with singular point oscillation functions and Fourier-integrals. The double exponential transformation is not only useful for numerical computations but it is also used in different methods of Sinc theory. In this paper we use double exponential transformation for calculating particular improper integrals. By improving integral estimates having singular final points. By comparison between double exponential transformations and single exponential transformations it is proved that the error margin of double exponential transformations is smaller. Finally Fourier-integral and double exponential transformations are discussed.

According to this abstract, the double exponential transformation has a smaller error margin compared to the single exponential transformation when approximating integrals."
In what scenarios might measuring the phase of a signal be impractical?,6f4a97ca,"[{'abstract': '  Phaseless super-resolution is the problem of recovering an unknown signal\nfrom measurements of the magnitudes of the low frequency Fourier transform of\nthe signal. This problem arises in applications where measuring the phase, and\nmaking high-frequency measurements, are either too costly or altogether\ninfeasible. The problem is especially challenging because it combines the\ndifficult problems of phase retrieval and classical super-resolution\n', 'title': 'Multiple Illumination Phaseless Super-Resolution (MIPS) with Applications To Phaseless DOA Estimation and Diffraction Imaging'}]","Based on the provided abstracts, I found one relevant paper that discusses scenarios where measuring the phase of a signal might be impractical:

Title: Multiple Illumination Phaseless Super-Resolution (MIPS) with Applications To Phaseless DOA Estimation and Diffraction Imaging

Abstract: Phaseless super-resolution is the problem of recovering an unknown signal from measurements of the magnitudes of the low frequency Fourier transform of the signal. This problem arises in applications where measuring the phase, and making high-frequency measurements, are either too costly or altogether infeasible.

This abstract suggests that measuring the phase of a signal might be impractical in scenarios where it is either too costly or technically challenging."
How did observations from the Hubble Space Telescope help clarify the nature of the X-ray source in Alpha Persei?,cbb39b5c,"[{'abstract': '  A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest\nstar, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness\nwere similar to coronally active late-type dwarf members. Later, in 2010, a\nHubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found\nfar-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious\noffset of the ROSAT source, suggested that a late-type companion might be\nresponsible for the X-rays. Recently, a multi-faceted program tested that\npremise. Groundbased optical coronography, and near-UV imaging with HST Wide\nField Camera 3, searched for any close-in faint candidate coronal objects, but\nwithout success. Then, a Chandra pointing found the X-ray source single and\ncoincident with the bright star. Significantly, the SiIV emissions of Alpha\nPersei, in a deeper FUV spectrum collected by HST COS as part of the joint\nprogram, aligned well with chromospheric atomic oxygen (which must be intrinsic\nto the luminous star), within the context of cooler late-F and early-G\nsupergiants, including Cepheid variables. This pointed to the X-rays as the\nfundamental anomaly. The over-luminous X-rays still support the case for a\nhyperactive dwarf secondary, albeit now spatially unresolved. However, an\nalternative is that Alpha Persei represents a novel class of coronal source.\nResolving the first possibility now has become more difficult, because the easy\nsolution -- a well separated companion -- has been eliminated. Testing the\nother possibility will require a broader high-energy census of the early-F\nsupergiants.\n', 'title': 'A Closer Look at the Alpha Persei Coronal Conundrum'}]","Title: A Closer Look at the Alpha Persei Coronal Conundrum

Abstract: A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest star, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness were similar to coronally active late-type dwarf members. Later, in 2010, a Hubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found far-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious offset of the ROSAT source, suggested that a late-type companion might be responsible for the X-rays. Recently, a multi-faceted program tested that premise. Groundbased optical coronography, and near-UV imaging with HST Wide Field Camera 3, searched for any close-in faint candidate coronal objects, but without success. Then, a Chandra pointing found the X-ray source single and coincident with the bright star. Significantly, the SiIV emissions of Alpha Persei, in a deeper FUV spectrum collected by HST COS as part of the joint program, aligned well with chromospheric atomic oxygen (which must be intrinsic to the luminous star), within the context of cooler late-F and early-G supergiants, including Cepheid variables. This pointed to the X-rays as the fundamental anomaly. The over-luminous X-rays still support the case for a hyperactive dwarf secondary, albeit now spatially unresolved. However, an alternative is that Alpha Persei represents a novel class of coronal source. 

The Hubble Space Telescope helped clarify the nature of the X-ray source in Alpha Persei by providing near-UV imaging with the Wide Field Camera 3, which searched for faint candidate coronal objects, and obtaining a deeper FUV spectrum with the Cosmic Origins Spectrograph (COS), which revealed that the SiIV emissions of Alpha Persei aligned well with chromospheric atomic oxygen, indicating that the X-rays are the fundamental anomaly."
What type of neural network architecture is used in a proposed system for detecting pathological voice?,9902e1f7,"[{'abstract': '  Many people are suffering from voice disorders, which can adversely affect\nthe quality of their lives. In response, some researchers have proposed\nalgorithms for automatic assessment of these disorders, based on voice signals.\nHowever, these signals can be sensitive to the recording devices. Indeed, the\nchannel effect is a pervasive problem in machine learning for healthcare. In\nthis study, we propose a detection system for pathological voice, which is\nrobust against the channel effect. This system is based on a bidirectional LSTM\nnetwork. To increase the performance robustness against channel mismatch, we\nintegrate domain adversarial training (DAT) to eliminate the differences\nbetween the devices. When we train on data recorded on a high-quality\nmicrophone and evaluate on smartphone data without labels, our robust detection\nsystem increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target\nsample labels). To the best of our knowledge, this is the first study applying\nunsupervised domain adaptation to pathological voice detection. Notably, our\nsystem does not need target device sample labels, which allows for\ngeneralization to many new devices.\n', 'title': 'Robustness against the channel effect in pathological voice detection'}]","Title: Robustness against the channel effect in pathological voice detection

Abstract:   Many people are suffering from voice disorders, which can adversely affect
the quality of their lives. In response, some researchers have proposed
algorithms for automatic assessment of these disorders, based on voice signals.
However, these signals can be sensitive to the recording devices. Indeed, the
channel effect is a pervasive problem in machine learning for healthcare. In
this study, we propose a detection system for pathological voice, which is
robust against the channel effect. This system is based on a bidirectional LSTM
network. To increase the performance robustness against channel mismatch, we
integrate domain adversarial training (DAT) to eliminate the differences
between the devices. When we train on data recorded on a high-quality
microphone and evaluate on smartphone data without labels, our robust detection
system increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target
sample labels). To the best of our knowledge, this is the first study applying
unsupervised domain adaptation to pathological voice detection. Notably, our
system does not need target device sample labels, which allows for
generalization to many new devices."
What strategies are explored in the paper to improve the training efficiency of RNN-T?,46b6be56,"[{'abstract': '  End-to-end approaches have drawn much attention recently for significantly\nsimplifying the construction of an automatic speech recognition (ASR) system.\nRNN transducer (RNN-T) is one of the popular end-to-end methods. Previous\nstudies have shown that RNN-T is difficult to train and a very complex training\nprocess is needed for a reasonable performance. In this paper, we explore RNN-T\nfor a Chinese large vocabulary continuous speech recognition (LVCSR) task and\naim to simplify the training process while maintaining performance. First, a\nnew strategy of learning rate decay is proposed to accelerate the model\nconvergence. Second, we find that adding convolutional layers at the beginning\nof the network and using ordered data can discard the pre-training process of\nthe encoder without loss of performance. Besides, we design experiments to find\na balance among the usage of GPU memory, training circle and model performance.\nFinally, we achieve 16.9% character error rate (CER) on our test set which is\n2% absolute improvement from a strong BLSTM CE system with language model\ntrained on the same text corpus.\n', 'title': 'Exploring RNN-Transducer for Chinese Speech Recognition'}]","Title: Exploring RNN-Transducer for Chinese Speech Recognition

Abstract: End-to-end approaches have drawn much attention recently for significantly simplifying the construction of an automatic speech recognition (ASR) system. RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous studies have shown that RNN-T is difficult to train and a very complex training process is needed for a reasonable performance. In this paper, we explore RNN-T for a Chinese large vocabulary continuous speech recognition (LVCSR) task and aim to simplify the training process while maintaining performance. First, a new strategy of learning rate decay is proposed to accelerate the model convergence. Second, we find that adding convolutional layers at the beginning of the network and using ordered data can discard the pre-training process of the encoder without loss of performance. Besides, we design experiments to find a balance among the usage of GPU memory, training circle and model performance. Finally, we achieve 16.9% character error rate (CER) on our test set which is 2% absolute improvement from a strong BLSTM CE system with language model trained on the same text corpus."
What are efficient numerical methods for calculating integrals used in electronic structure calculations?,4ed70552,"[{'abstract': '  Efficient methods are proposed, for computing integrals appeaing in\nelectronic structure calculations. The methods consist of two parts: the first\npart is to represent the integrals as contour integrals and the second one is\nto evaluate the contour integrals by the Clenshaw-Curtis quadrature. The\nefficiency of the proposed methods is demonstrated through numerical\nexperiments.\n', 'title': 'Efficient methods for computing integrals in electronic structure calculations'}]","Title: Efficient methods for computing integrals in electronic structure calculations

Abstract: Efficient methods are proposed, for computing integrals appeaing in electronic structure calculations. The methods consist of two parts: the first part is to represent the integrals as contour integrals and the second one is to evaluate the contour integrals by the Clenshaw-Curtis quadrature. The efficiency of the proposed methods is demonstrated through numerical experiments."
How were mean magnitudes derived for miras in the Large Magellanic Cloud?,b228f454,"[{'abstract': '  We study the near-infrared properties of 690 Mira candidates in the central\nregion of the Large Magellanic Cloud, based on time-series observations at\nJHKs. We use densely-sampled I-band observations from the OGLE project to\ngenerate template light curves in the near infrared and derive robust mean\nmagnitudes at those wavelengths. We obtain near-infrared Period-Luminosity\nrelations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We\nstudy the Period-Luminosity-Color relations and the color excesses of\nCarbon-rich Miras, which show evidence for a substantially different reddening\nlaw.\n', 'title': 'Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras'}]","I think I can help you with that!

Title: Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras

Abstract:   We study the near-infrared properties of 690 Mira candidates in the central
region of the Large Magellanic Cloud, based on time-series observations at
JHKs. We use densely-sampled I-band observations from the OGLE project to
generate template light curves in the near infrared and derive robust mean
magnitudes at those wavelengths. We obtain near-infrared Period-Luminosity
relations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We
study the Period-Luminosity-Color relations and the color excesses of
Carbon-rich Miras, which show evidence for a substantially different reddening
law.

According to the abstract, the mean magnitudes for Miras in the Large Magellanic Cloud were derived by using densely-sampled I-band observations from the OGLE project to generate template light curves in the near infrared."
How can a human interact with a robot housekeeper without needing to understand its internal workings?,a1246d77,"[{'abstract': '  This paper presents the design and implementation of a Human Interface for a\nhousekeeper robot. It bases on the idea of making the robot understand the\nhuman needs without making the human go through the details of robots work, for\nexample, the way that the robot implements the work or the method that the\nrobot uses to plan the path in order to reach the work area. The interface\ncommands based on idioms of the natural human language and designed in a manner\nthat the user gives the robot several commands with their execution date/time.\n', 'title': 'Robot human interface for housekepeer with wireless capabilities'}]","I think I can help you with that!

Title: Robot human interface for housekepeer with wireless capabilities

Abstract: This paper presents the design and implementation of a Human Interface for a housekeeper robot. It bases on the idea of making the robot understand the human needs without making the human go through the details of robots work, for example, the way that the robot implements the work or the method that the robot uses to plan the path in order to reach the work area. The interface commands based on idioms of the natural human language and designed in a manner that the user gives the robot several commands with their execution date/time.

This paper seems to suggest that humans can interact with the robot housekeeper using a user-friendly interface that accepts natural language commands, without needing to understand the robot's internal workings."
Is there a homomorphism connecting the quantum toroidal gl(n) algebra to another quantum algebra?,e35bd685,"[{'abstract': '  We present an affine analog of the evaluation map for quantum groups. Namely\nwe introduce a surjective homomorphism from the quantum toroidal gl(n) algebra\nto the quantum affine gl(n) algebra completed with respect to the homogeneous\ngrading. We give a brief discussion of evaluation modules.\n', 'title': 'An evaluation homomorphism for quantum toroidal gl(n) algebras'}]","Title: An evaluation homomorphism for quantum toroidal gl(n) algebras

Abstract: We present an affine analog of the evaluation map for quantum groups. Namely
we introduce a surjective homomorphism from the quantum toroidal gl(n) algebra
to the quantum affine gl(n) algebra completed with respect to the homogeneous
grading. We give a brief discussion of evaluation modules."
How is the impermeable boundary represented in the vortex method for exterior domains?,737f0bb5,"[{'abstract': '  The vortex method is a common numerical and theoretical approach used to\nimplement the motion of an ideal flow, in which the vorticity is approximated\nby a sum of point vortices, so that the Euler equations read as a system of\nordinary differential equations. Such a method is well justified in the full\nplane, thanks to the explicit representation formulas of Biot and Savart. In an\nexterior domain, we also replace the impermeable boundary by a collection of\npoint vortices generating the circulation around the obstacle. The density of\nthese point vortices is chosen in order that the flow remains tangent at\nmidpoints between adjacent vortices. In this work, we provide a rigorous\njustification for this method in exterior domains. One of the main mathematical\ndifficulties being that the Biot-Savart kernel defines a singular integral\noperator when restricted to a curve. For simplicity and clarity, we only treat\nthe case of the unit disk in the plane approximated by a uniformly distributed\nmesh of point vortices. The complete and general version of our work is\navailable in [arXiv:1707.01458].\n', 'title': 'The vortex method for 2D ideal flows in the exterior of a disk'}]","Title: The vortex method for 2D ideal flows in the exterior of a disk

Abstract:   The vortex method is a common numerical and theoretical approach used to
implement the motion of an ideal flow, in which the vorticity is approximated
by a sum of point vortices, so that the Euler equations read as a system of
ordinary differential equations. Such a method is well justified in the full
plane, thanks to the explicit representation formulas of Biot and Savart. In an
exterior domain, we also replace the impermeable boundary by a collection of
point vortices generating the circulation around the obstacle. The density of
these point vortices is chosen in order that the flow remains tangent at
midpoints between adjacent vortices."
Can delayed information about product popularity alter the outcome of adoption trends?,ecf1c3c3,"[{'abstract': '  The spread of opinions, memes, diseases, and ""alternative facts"" in a\npopulation depends both on the details of the spreading process and on the\nstructure of the social and communication networks on which they spread. In\nthis paper, we explore how \\textit{anti-establishment} nodes (e.g.,\n\\textit{hipsters}) influence the spreading dynamics of two competing products.\nWe consider a model in which spreading follows a deterministic rule for\nupdating node states (which describe which product has been adopted) in which\nan adjustable fraction $p_{\\rm Hip}$ of the nodes in a network are hipsters,\nwho choose to adopt the product that they believe is the less popular of the\ntwo. The remaining nodes are conformists, who choose which product to adopt by\nconsidering which products their immediate neighbors have adopted. We simulate\nour model on both synthetic and real networks, and we show that the hipsters\nhave a major effect on the final fraction of people who adopt each product:\neven when only one of the two products exists at the beginning of the\nsimulations, a very small fraction of hipsters in a network can still cause the\nother product to eventually become the more popular one. To account for this\nbehavior, we construct an approximation for the steady-state adoption fraction\non $k$-regular trees in the limit of few hipsters. Additionally, our\nsimulations demonstrate that a time delay $\\tau$ in the knowledge of the\nproduct distribution in a population, as compared to immediate knowledge of\nproduct adoption among nearest neighbors, can have a large effect on the final\ndistribution of product adoptions. Our simple model and analysis may help shed\nlight on the road to success for anti-establishment choices in elections, as\nsuch success can arise rather generically in our model from a small number of\nanti-establishment individuals and ordinary processes of social influence on\nnormal individuals.\n', 'title': 'Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority'}]","Title: Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority

Abstract: The spread of opinions, memes, diseases, and ""alternative facts"" in a population depends both on the details of the spreading process and on the structure of the social and communication networks on which they spread. In this paper, we explore how anti-establishment nodes (e.g., hipsters) influence the spreading dynamics of two competing products. We consider a model in which spreading follows a deterministic rule for updating node states (which describe which product has been adopted) in which an adjustable fraction $p_{\rm Hip}$ of the nodes in a network are hipsters, who choose to adopt the product that they believe is the less popular of the two. The remaining nodes are conformists, who choose which product to adopt by considering which products their immediate neighbors have adopted. We simulate our model on both synthetic and real networks, and we show that the hipsters have a major effect on the final fraction of people who adopt each product: even when only one of the two products exists at the beginning of the simulations, a very small fraction of hipsters in a network can still cause the other product to eventually become the more popular one. To account for this behavior, we construct an approximation for the steady-state adoption fraction on $k$-regular trees in the limit of few hipsters. Additionally, our simulations demonstrate that a time delay $\tau$ in the knowledge of the product distribution in a population, as compared to immediate knowledge of product adoption among nearest neighbors, can have a large effect on the final distribution of product adoptions. Our simple model and analysis may help shed light on the road to success for anti-establishment choices in elections, as such success can arise rather generically in our model from a small number of anti-establishment individuals and ordinary processes of social influence on normal individuals."
What is a key advantage of this new decentralized optimization algorithm compared to PG-EXTRA?,71d7fbec,"[{'abstract': '  This paper considers the problem of decentralized optimization with a\ncomposite objective containing smooth and non-smooth terms. To solve the\nproblem, a proximal-gradient scheme is studied. Specifically, the smooth and\nnonsmooth terms are dealt with by gradient update and proximal update,\nrespectively. The studied algorithm is closely related to a previous\ndecentralized optimization algorithm, PG-EXTRA [37], but has a few advantages.\nFirst of all, in our new scheme, agents use uncoordinated step-sizes and the\nstable upper bounds on step-sizes are independent from network topology. The\nstep-sizes depend on local objective functions, and they can be as large as\nthat of the gradient descent. Secondly, for the special case without non-smooth\nterms, linear convergence can be achieved under the strong convexity\nassumption. The dependence of the convergence rate on the objective functions\nand the network are separated, and the convergence rate of our new scheme is as\ngood as one of the two convergence rates that match the typical rates for the\ngeneral gradient descent and the consensus averaging. We also provide some\nnumerical experiments to demonstrate the efficacy of the introduced algorithms\nand validate our theoretical discoveries.\n', 'title': 'A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates'}]","Title: A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates

Abstract: This paper considers the problem of decentralized optimization with a composite objective containing smooth and non-smooth terms. To solve the problem, a proximal-gradient scheme is studied. Specifically, the smooth and nonsmooth terms are dealt with by gradient update and proximal update, respectively. The studied algorithm is closely related to a previous decentralized optimization algorithm, PG-EXTRA [37], but has a few advantages. First of all, in our new scheme, agents use uncoordinated step-sizes and the stable upper bounds on step-sizes are independent from network topology. The step-sizes depend on local objective functions, and they can be as large as that of the gradient descent. Secondly, for the special case without non-smooth terms, linear convergence can be achieved under the strong convexity assumption. The dependence of the convergence rate on the objective functions and the network are separated, and the convergence rate of our new scheme is as good as one of the two convergence rates that match the typical rates for the general gradient descent and the consensus averaging. We also provide some numerical experiments to demonstrate the efficacy of the introduced algorithms and validate our theoretical discoveries.

A key advantage of this new decentralized optimization algorithm compared to PG-EXTRA is that agents use uncoordinated step-sizes, and the stable upper bounds on step-sizes are independent from network topology."
What role do dorsal processing pathways play in multisensory integration?,d3a2d47b,"[{'abstract': '  When the brain receives input from multiple sensory systems, it is faced with\nthe question of whether it is appropriate to process the inputs in combination,\nas if they originated from the same event, or separately, as if they originated\nfrom distinct events. Furthermore, it must also have a mechanism through which\nit can keep sensory inputs calibrated to maintain the accuracy of its internal\nrepresentations. We have developed a neural network architecture capable of i)\napproximating optimal multisensory spatial integration, based on Bayesian\ncausal inference, and ii) recalibrating the spatial encoding of sensory\nsystems. The architecture is based on features of the dorsal processing\nhierarchy, including the spatial tuning properties of unisensory neurons and\nthe convergence of different sensory inputs onto multisensory neurons.\nFurthermore, we propose that these unisensory and multisensory neurons play\ndual roles in i) encoding spatial location as separate or integrated estimates\nand ii) accumulating evidence for the independence or relatedness of\nmultisensory stimuli. We further propose that top-down feedback connections\nspanning the dorsal pathway play key a role in recalibrating spatial encoding\nat the level of early unisensory cortices. Our proposed architecture provides\npossible explanations for a number of human electrophysiological and\nneuroimaging results and generates testable predictions linking neurophysiology\nwith behaviour.\n', 'title': 'Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration'}]","Title: Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration

Abstract: When the brain receives input from multiple sensory systems, it is faced with the question of whether it is appropriate to process the inputs in combination, as if they originated from the same event, or separately, as if they originated from distinct events. Furthermore, it must also have a mechanism through which it can keep sensory inputs calibrated to maintain the accuracy of its internal representations. We have developed a neural network architecture capable of i) approximating optimal multisensory spatial integration, based on Bayesian causal inference, and ii) recalibrating the spatial encoding of sensory systems. The architecture is based on features of the dorsal processing hierarchy, including the spatial tuning properties of unisensory neurons and the convergence of different sensory inputs onto multisensory neurons. Furthermore, we propose that these unisensory and multisensory neurons play dual roles in i) encoding spatial location as separate or integrated estimates and ii) accumulating evidence for the independence or relatedness of multisensory stimuli. We further propose that top-down feedback connections spanning the dorsal pathway play key a role in recalibrating spatial encoding at the level of early unisensory cortices. Our proposed architecture provides possible explanations for a number of human electrophysiological and neuroimaging results and generates testable predictions linking neurophysiology with behaviour.

The answer lies in the abstract, specifically: ""The architecture is based on features of the dorsal processing hierarchy, including the spatial tuning properties of unisensory neurons and the convergence of different sensory inputs onto multisensory neurons."" This suggests that the dorsal processing pathways play a key role in multisensory integration by providing a neural network architecture that enables the brain to process multiple sensory inputs in combination."
Are there universal aspects to numerical computations?,2150de2e,"[{'abstract': '  We discuss various universality aspects of numerical computations using\nstandard algorithms. These aspects include empirical observations and rigorous\nresults. We also make various speculations about computation in a broader\nsense.\n', 'title': 'Universality in numerical computation with random data. Case studies, analytic results and some speculations'}]","Yes, there are universal aspects to numerical computations.

Title: Universality in numerical computation with random data. Case studies, analytic results and some speculations

Abstract: We discuss various universality aspects of numerical computations using standard algorithms. These aspects include empirical observations and rigorous results. We also make various speculations about computation in a broader sense."
What is the effect of Glory rescattering on the transverse momentum distribution of photoelectrons during strong-field atomic ionization?,8b573c4b,"[{'abstract': '  During the ionization of atoms irradiated by linearly polarized intense laser\nfields, we find for the first time that the transverse momentum distribution of\nphotoelectrons can be well fitted by a squared zeroth-order Bessel function\nbecause of the quantum interference effect of Glory rescattering. The\ncharacteristic of the Bessel function is determined by the common angular\nmomentum of a bunch of semiclassical paths termed as Glory trajectories, which\nare launched with different nonzero initial transverse momenta distributed on a\nspecific circle in the momentum plane and finally deflected to the same\nasymptotic momentum, which is along the polarization direction, through\npost-tunneling rescattering. Glory rescattering theory (GRT) based on the\nsemiclassical path-integral formalism is developed to address this effect\nquantitatively. Our theory can resolve the long-standing discrepancies between\nexisting theories and experiments on the fringe location, predict the sudden\ntransition of the fringe structure in holographic patterns, and shed light on\nthe quantum interference aspects of low-energy structures in strong-field\natomic ionization.\n', 'title': 'Quantum Interference of Glory Rescattering in Strong-Field Atomic Ionization'}]","Title: Quantum Interference of Glory Rescattering in Strong-Field Atomic Ionization

Abstract: During the ionization of atoms irradiated by linearly polarized intense laser fields, we find for the first time that the transverse momentum distribution of photoelectrons can be well fitted by a squared zeroth-order Bessel function because of the quantum interference effect of Glory rescattering. The characteristic of the Bessel function is determined by the common angular momentum of a bunch of semiclassical paths termed as Glory trajectories, which are launched with different nonzero initial transverse momenta distributed on a specific circle in the momentum plane and finally deflected to the same asymptotic momentum, which is along the polarization direction, through post-tunneling rescattering. Glory rescattering theory (GRT) based on the semiclassical path-integral formalism is developed to address this effect quantitatively. Our theory can resolve the long-standing discrepancies between existing theories and experiments on the fringe location, predict the sudden transition of the fringe structure in holographic patterns, and shed light on the quantum interference aspects of low-energy structures in strong-field atomic ionization."
What are the canonical connections used in Heisenberg modules over quantum two tori?,a351cef7,"[{'abstract': '  The modular Gromov-Hausdorff propinquity is a distance on classes of modules\nendowed with quantum metric information, in the form of a metric form of a\nconnection and a left Hilbert module structure. This paper proves that the\nfamily of Heisenberg modules over quantum two tori, when endowed with their\ncanonical connections, form a family of metrized quantum vector bundles, as a\nfirst step in proving that Heisenberg modules form a continuous family for the\nmodular Gromov-Hausdorff propinquity.\n', 'title': 'Heisenberg Modules over Quantum 2-tori are metrized quantum vector bundles'}]","Here is the answer to your question:

Title: Heisenberg Modules over Quantum 2-tori are metrized quantum vector bundles

Abstract: The modular Gromov-Hausdorff propinquity is a distance on classes of modules endowed with quantum metric information, in the form of a metric form of a connection and a left Hilbert module structure. This paper proves that the family of Heisenberg modules over quantum two tori, when endowed with their canonical connections, form a family of metrized quantum vector bundles, as a first step in proving that Heisenberg modules form a continuous family for the modular Gromov-Hausdorff propinquity."
What are the main impacts of quantum fluctuations on the magnetic properties of single adatoms?,b464b44f,"[{'abstract': '  Stabilizing the magnetic signal of single adatoms is a crucial step towards\ntheir successful usage in widespread technological applications such as\nhigh-density magnetic data storage devices. The quantum mechanical nature of\nthese tiny objects, however, introduces intrinsic zero-point spin-fluctuations\nthat tend to destabilize the local magnetic moment of interest by dwindling the\nmagnetic anisotropy potential barrier even at absolute zero temperature. Here,\nwe elucidate the origins and quantify the effect of the fundamental ingredients\ndetermining the magnitude of the fluctuations, namely the ($i$) local magnetic\nmoment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner\nexcitations. Based on a systematic first-principles study of 3d and 4d adatoms,\nwe demonstrate that the transverse contribution of the fluctuations is\ncomparable in size to the magnetic moment itself, leading to a remarkable\n$\\gtrsim$50$\\%$ reduction of the magnetic anisotropy energy. Our analysis gives\nrise to a comprehensible diagram relating the fluctuation magnitude to\ncharacteristic features of adatoms, providing practical guidelines for\ndesigning magnetically stable nanomagnets with minimal quantum fluctuations.\n', 'title': 'Zero-point spin-fluctuations of single adatoms'}]","Here's the answer:

Title: Zero-point spin-fluctuations of single adatoms

Abstract: Stabilizing the magnetic signal of single adatoms is a crucial step towards their successful usage in widespread technological applications such as high-density magnetic data storage devices. The quantum mechanical nature of these tiny objects, however, introduces intrinsic zero-point spin-fluctuations that tend to destabilize the local magnetic moment of interest by dwindling the magnetic anisotropy potential barrier even at absolute zero temperature. Here, we elucidate the origins and quantify the effect of the fundamental ingredients determining the magnitude of the fluctuations, namely the ($i$) local magnetic moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner excitations. Based on a systematic first-principles study of 3d and 4d adatoms, we demonstrate that the transverse contribution of the fluctuations is comparable in size to the magnetic moment itself, leading to a remarkable $\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives rise to a comprehensible diagram relating the fluctuation magnitude to characteristic features of adatoms, providing practical guidelines for designing magnetically stable nanomagnets with minimal quantum fluctuations."
How does the short spin-spin correlation differ within and between chains in this model?,5987d4a6,"[{'abstract': '  We investigate the effect of dimensional crossover in the ground state of the\nantiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular\nlattice that interpolates between the regime of weakly coupled Haldane chains\n($J^{\\prime}\\! \\!\\ll\\!\\! J$) and the isotropic triangular lattice\n($J^{\\prime}\\!\\!=\\!\\!J$). We use the density-matrix renormalization group\n(DMRG) and Schwinger boson theory performed at the Gaussian correction level\nabove the saddle-point solution. Our DMRG results show an abrupt transition\nbetween decoupled spin chains and the spirally ordered regime at\n$(J^{\\prime}/J)_c\\sim 0.42$, signaled by the sudden closing of the spin gap.\nComing from the magnetically ordered side, the computation of the spin\nstiffness within Schwinger boson theory predicts the instability of the spiral\nmagnetic order toward a magnetically disordered phase with one-dimensional\nfeatures at $(J^{\\prime}/J)_c \\sim 0.43$. The agreement of these complementary\nmethods, along with the strong difference found between the intra- and the\ninterchain DMRG short spin-spin correlations; for sufficiently large values of\nthe interchain coupling, suggests that the interplay between the quantum\nfluctuations and the dimensional crossover effects gives rise to the\none-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.\n', 'title': 'One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice'}]","Title: One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice

Abstract:   We investigate the effect of dimensional crossover in the ground state of the
antiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular
lattice that interpolates between the regime of weakly coupled Haldane chains
($J^{\prime}\! \!\ll\!\! J$) and the isotropic triangular lattice
($J^{\prime}\!\!=\!\!J$). We use the density-matrix renormalization group
(DMRG) and Schwinger boson theory performed at the Gaussian correction level
above the saddle-point solution. Our DMRG results show an abrupt transition
between decoupled spin chains and the spirally ordered regime at
$(J^{\prime}/J)_c\sim 0.42$, signaled by the sudden closing of the spin gap.
Coming from the magnetically ordered side, the computation of the spin
stiffness within Schwinger boson theory predicts the instability of the spiral
magnetic order toward a magnetically disordered phase with one-dimensional
features at $(J^{\prime}/J)_c \sim 0.43$. The agreement of these complementary
methods, along with the strong difference found between the intra- and the
interchain DMRG short spin-spin correlations; for sufficiently large values of
the interchain coupling, suggests that the interplay between the quantum
fluctuations and the dimensional crossover effects gives rise to the
one-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian."
What are the speed and compression advantages offered by this method compared to traditional fluid simulation techniques? ,7a72be56,"[{'abstract': '  This paper presents a novel generative model to synthesize fluid simulations\nfrom a set of reduced parameters. A convolutional neural network is trained on\na collection of discrete, parameterizable fluid simulation velocity fields. Due\nto the capability of deep learning architectures to learn representative\nfeatures of the data, our generative model is able to accurately approximate\nthe training data set, while providing plausible interpolated in-betweens. The\nproposed generative model is optimized for fluids by a novel loss function that\nguarantees divergence-free velocity fields at all times. In addition, we\ndemonstrate that we can handle complex parameterizations in reduced spaces, and\nadvance simulations in time by integrating in the latent space with a second\nnetwork. Our method models a wide variety of fluid behaviors, thus enabling\napplications such as fast construction of simulations, interpolation of fluids\nwith different parameters, time re-sampling, latent space simulations, and\ncompression of fluid simulation data. Reconstructed velocity fields are\ngenerated up to 700x faster than traditional CPU solvers, while achieving\ncompression rates of over 1300x.\n', 'title': 'Deep Fluids: A Generative Network for Parameterized Fluid Simulations'}]","Title: Deep Fluids: A Generative Network for Parameterized Fluid Simulations

Abstract:   This paper presents a novel generative model to synthesize fluid simulations
from a set of reduced parameters. A convolutional neural network is trained on
a collection of discrete, parameterizable fluid simulation velocity fields. Due
to the capability of deep learning architectures to learn representative
features of the data, our generative model is able to accurately approximate
the training data set, while providing plausible interpolated in-betweens. The
proposed generative model is optimized for fluids by a novel loss function that
guarantees divergence-free velocity fields at all times. In addition, we
demonstrate that we can handle complex parameterizations in reduced spaces, and
advance simulations in time by integrating in the latent space with a second
network. Our method models a wide variety of fluid behaviors, thus enabling
applications such as fast construction of simulations, interpolation of fluids
with different parameters, time re-sampling, latent space simulations, and
compression of fluid simulation data. Reconstructed velocity fields are
generated up to 700x faster than traditional CPU solvers, while achieving
compression rates of over 1300x."
What are the conditions under which Glauber Dynamics converges to a near uniform random proper coloring of a hypergraph in O(n*log n) time?,2ab593bd,"[{'abstract': '  We study the problem of constructing a (near) uniform random proper\n$q$-coloring of a simple $k$-uniform hypergraph with $n$ vertices and maximum\ndegree $\\Delta$. (Proper in that no edge is mono-colored and simple in that two\nedges have maximum intersection of size one). We show that if $q\\geq\n\\max\\{C_k\\log n,500k^3\\Delta^{1/(k-1)}\\}$ then the Glauber Dynamics will become\nclose to uniform in $O(n\\log n)$ time, given a random (improper) start. This\nimproves on the results in Frieze and Melsted [5].\n', 'title': 'Randomly coloring simple hypergraphs with fewer colors'}]","I think I have found the answer to your question!

Title: Randomly coloring simple hypergraphs with fewer colors

Abstract:  We study the problem of constructing a (near) uniform random proper 
$q$-coloring of a simple $k$-uniform hypergraph with $n$ vertices and maximum 
degree $\Delta$. (Proper in that no edge is mono-colored and simple in that two 
edges have maximum intersection of size one). We show that if $q\geq 
\max\{C_k\log n,500k^3\Delta^{1/(k-1)}\}$ then the Glauber Dynamics will become 
close to uniform in $O(n\log n)$ time, given a random (improper) start. This 
improves on the results in Frieze and Melsted [5]."
Can the dynamics of vortex rings in 3D harmonic traps be understood using a particle picture?,26f6aa6e,"[{'abstract': '  In the present work, we explore the existence, stability and dynamics of\nsingle and multiple vortex ring states that can arise in Bose-Einstein\ncondensates. Earlier works have illustrated the bifurcation of such states, in\nthe vicinity of the linear limit, for isotropic or anisotropic\nthree-dimensional harmonic traps. Here, we extend these states to the regime of\nlarge chemical potentials, the so-called Thomas-Fermi limit, and explore their\nproperties such as equilibrium radii and inter-ring distance, for multi-ring\nstates, as well as their vibrational spectra and possible instabilities. In\nthis limit, both the existence and stability characteristics can be partially\ntraced to a particle picture that considers the rings as individual particles\noscillating within the trap and interacting pairwise with one another. Finally,\nwe examine some representative instability scenarios of the multi-ring dynamics\nincluding breakup and reconnections, as well as the transient formation of\nvortex lines.\n', 'title': 'Single and Multiple Vortex Rings in Three-Dimensional Bose-Einstein Condensates: Existence, Stability and Dynamics'}]","Title: Single and Multiple Vortex Rings in Three-Dimensional Bose-Einstein Condensates: Existence, Stability and Dynamics

Abstract: In the present work, we explore the existence, stability and dynamics of single and multiple vortex ring states that can arise in Bose-Einstein condensates. Earlier works have illustrated the bifurcation of such states, in the vicinity of the linear limit, for isotropic or anisotropic three-dimensional harmonic traps. Here, we extend these states to the regime of large chemical potentials, the so-called Thomas-Fermi limit, and explore their properties such as equilibrium radii and inter-ring distance, for multi-ring states, as well as their vibrational spectra and possible instabilities. In this limit, both the existence and stability characteristics can be partially traced to a particle picture that considers the rings as individual particles oscillating within the trap and interacting pairwise with one another. Finally, we examine some representative instability scenarios of the multi-ring dynamics including breakup and reconnections, as well as the transient formation of vortex lines."
What methods can be used to construct confidence intervals for missing data problems when unverifiable assumptions might be violated?,f4beb3f7,"[{'abstract': '  To identify the estimand in missing data problems and observational studies,\nit is common to base the statistical estimation on the ""missing at random"" and\n""no unmeasured confounder"" assumptions. However, these assumptions are\nunverifiable using empirical data and pose serious threats to the validity of\nthe qualitative conclusions of the statistical inference. A sensitivity\nanalysis asks how the conclusions may change if the unverifiable assumptions\nare violated to a certain degree. In this paper we consider a marginal\nsensitivity model which is a natural extension of Rosenbaum\'s sensitivity model\nthat is widely used for matched observational studies. We aim to construct\nconfidence intervals based on inverse probability weighting estimators, such\nthat asymptotically the intervals have at least nominal coverage of the\nestimand whenever the data generating distribution is in the collection of\nmarginal sensitivity models. We use a percentile bootstrap and a generalized\nminimax/maximin inequality to transform this intractable problem to a linear\nfractional programming problem, which can be solved very efficiently. We\nillustrate our method using a real dataset to estimate the causal effect of\nfish consumption on blood mercury level.\n', 'title': 'Sensitivity analysis for inverse probability weighting estimators via the percentile bootstrap'}]","I think I can help you with that!

Title: Sensitivity analysis for inverse probability weighting estimators via the percentile bootstrap

Abstract: To identify the estimand in missing data problems and observational studies, it is common to base the statistical estimation on the ""missing at random"" and ""no unmeasured confounder"" assumptions. However, these assumptions are unverifiable using empirical data and pose serious threats to the validity of the qualitative conclusions of the statistical inference. A sensitivity analysis asks how the conclusions may change if the unverifiable assumptions are violated to a certain degree. In this paper we consider a marginal sensitivity model which is a natural extension of Rosenbaum's sensitivity model that is widely used for matched observational studies. We aim to construct confidence intervals based on inverse probability weighting estimators, such that asymptotically the intervals have at least nominal coverage of the estimand whenever the data generating distribution is in the collection of marginal sensitivity models. We use a percentile bootstrap and a generalized minimax/maximin inequality to transform this intractable problem to a linear fractional programming problem, which can be solved very efficiently. We illustrate our method using a real dataset to estimate the causal effect of fish consumption on blood mercury level."
"Compared to baseline methods, how much does the proposed method reduce the diarization error rate?",b72a56c8,"[{'abstract': '  In this paper we propose a new method of speaker diarization that employs a\ndeep learning architecture to learn speaker embeddings. In contrast to the\ntraditional approaches that build their speaker embeddings using manually\nhand-crafted spectral features, we propose to train for this purpose a\nrecurrent convolutional neural network applied directly on magnitude\nspectrograms. To compare our approach with the state of the art, we collect and\nrelease for the public an additional dataset of over 6 hours of fully annotated\nbroadcast material. The results of our evaluation on the new dataset and three\nother benchmark datasets show that our proposed method significantly\noutperforms the competitors and reduces diarization error rate by a large\nmargin of over 30% with respect to the baseline.\n', 'title': 'Speaker Diarization using Deep Recurrent Convolutional Neural Networks for Speaker Embeddings'}]","Title: Speaker Diarization using Deep Recurrent Convolutional Neural Networks for Speaker Embeddings

Abstract: In this paper we propose a new method of speaker diarization that employs a deep learning architecture to learn speaker embeddings. In contrast to the traditional approaches that build their speaker embeddings using manually hand-crafted spectral features, we propose to train for this purpose a recurrent convolutional neural network applied directly on magnitude spectrograms. To compare our approach with the state of the art, we collect and release for the public an additional dataset of over 6 hours of fully annotated broadcast material. The results of our evaluation on the new dataset and three other benchmark datasets show that our proposed method significantly outperforms the competitors and reduces diarization error rate by a large margin of over 30% with respect to the baseline.

Answer: The proposed method reduces the diarization error rate by over 30% compared to the baseline method."
Can changes in the gas phase composition influence the pattern of convection observed in a volatile liquid layer? ,0b9112c0,"[{'abstract': '  Buoyancy-thermocapillary convection in a layer of volatile liquid driven by a\nhorizontal temperature gradient arises in a variety of situations. Recent\nstudies have shown that the composition of the gas phase, which is typically a\nmixture of vapour and air, has a noticeable effect on the critical Marangoni\nnumber describing the onset of convection as well as on the observed convection\npattern. Specifically, as the total pressure or, equivalently, the average\nconcentration of air is decreased, the threshold of the instability leading to\nthe emergence of convective rolls is found to increase rather significantly. We\npresent a linear stability analysis of the problem which shows that this trend\ncan be readily understood by considering the transport of heat and vapour\nthrough the gas phase. In particular, we show that transport in the gas phase\nhas a noticeable effect even at atmospheric conditions, when phase change is\ngreatly suppressed.\n', 'title': 'The effect of phase change on stability of convective flow in a layer of volatile liquid driven by a horizontal temperature gradient'}]","Title: The effect of phase change on stability of convective flow in a layer of volatile liquid driven by a horizontal temperature gradient

Abstract: Buoyancy-thermocapillary convection in a layer of volatile liquid driven by a horizontal temperature gradient arises in a variety of situations. Recent studies have shown that the composition of the gas phase, which is typically a mixture of vapour and air, has a noticeable effect on the critical Marangoni number describing the onset of convection as well as on the observed convection pattern. Specifically, as the total pressure or, equivalently, the average concentration of air is decreased, the threshold of the instability leading to the emergence of convective rolls is found to increase rather significantly. We present a linear stability analysis of the problem which shows that this trend can be readily understood by considering the transport of heat and vapour through the gas phase. In particular, we show that transport in the gas phase has a noticeable effect even at atmospheric conditions, when phase change is greatly suppressed."
What type of large corpus was used to train the model and where were entity annotations sourced?,f304705a,"[{'abstract': '  We describe a neural network model that jointly learns distributed\nrepresentations of texts and knowledge base (KB) entities. Given a text in the\nKB, we train our proposed model to predict entities that are relevant to the\ntext. Our model is designed to be generic with the ability to address various\nNLP tasks with ease. We train the model using a large corpus of texts and their\nentity annotations extracted from Wikipedia. We evaluated the model on three\nimportant NLP tasks (i.e., sentence textual similarity, entity linking, and\nfactoid question answering) involving both unsupervised and supervised\nsettings. As a result, we achieved state-of-the-art results on all three of\nthese tasks. Our code and trained models are publicly available for further\nacademic research.\n', 'title': 'Learning Distributed Representations of Texts and Entities from Knowledge Base'}]","Title: Learning Distributed Representations of Texts and Entities from Knowledge Base

Abstract: We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research.

The answer to the question is: A large corpus of texts from Wikipedia was used to train the model, and entity annotations were sourced from Wikipedia."
What resource offers a user-friendly introduction to the Karmarkar's Algorithm for solving linear programming problems?,dcb42f5f,"[{'abstract': ""  An extremely simple, description of Karmarkar's algorithm with very few\ntechnical terms is given.\n"", 'title': ""A simple introduction to Karmarkar's Algorithm for Linear Programming""}]","I think I can help you with that!

The resource that offers a user-friendly introduction to Karmarkar's Algorithm for solving linear programming problems is:

Title: A simple introduction to Karmarkar's Algorithm for Linear Programming

Abstract: An extremely simple, description of Karmarkar's algorithm with very few technical terms is given."
What are some commonly used feature selection algorithms and how do they differ?,b14f69d3,"[{'abstract': '  This paper aims to explore models based on the extreme gradient boosting\n(XGBoost) approach for business risk classification. Feature selection (FS)\nalgorithms and hyper-parameter optimizations are simultaneously considered\nduring model training. The five most commonly used FS methods including weight\nby Gini, weight by Chi-square, hierarchical variable clustering, weight by\ncorrelation, and weight by information are applied to alleviate the effect of\nredundant features. Two hyper-parameter optimization approaches, random search\n(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in\nXGBoost. The effect of different FS and hyper-parameter optimization methods on\nthe model performance are investigated by the Wilcoxon Signed Rank Test. The\nperformance of XGBoost is compared to the traditionally utilized logistic\nregression (LR) model in terms of classification accuracy, area under the curve\n(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results\nshow that hierarchical clustering is the optimal FS method for LR while weight\nby Chi-square achieves the best performance in XG-Boost. Both TPE and RS\noptimization in XGBoost outperform LR significantly. TPE optimization shows a\nsuperiority over RS since it results in a significantly higher accuracy and a\nmarginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE\ntuning shows a lower variability than the RS method. Finally, the ranking of\nfeature importance based on XGBoost enhances the model interpretation.\nTherefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an\noperative while powerful approach for business risk modeling.\n', 'title': 'A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization'}]","Title: A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization

Abstract:   This paper aims to explore models based on the extreme gradient boosting
(XGBoost) approach for business risk classification. Feature selection (FS)
algorithms and hyper-parameter optimizations are simultaneously considered
during model training. The five most commonly used FS methods including weight
by Gini, weight by Chi-square, hierarchical variable clustering, weight by
correlation, and weight by information are applied to alleviate the effect of
redundant features. Two hyper-parameter optimization approaches, random search
(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
XGBoost. The effect of different FS and hyper-parameter optimization methods on
the model performance are investigated by the Wilcoxon Signed Rank Test. The
performance of XGBoost is compared to the traditionally utilized logistic
regression (LR) model in terms of classification accuracy, area under the curve
(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
show that hierarchical clustering is the optimal FS method for LR while weight
by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
optimization in XGBoost outperform LR significantly. TPE optimization shows a
superiority over RS since it results in a significantly higher accuracy and a
marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
tuning shows a lower variability than the RS method. Finally, the ranking of
feature importance based on XGBoost enhances the model interpretation.
Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
operative while powerful approach for business risk modeling.

In this paper, the five commonly used feature selection algorithms are:

1. Weight by Gini
2. Weight by Chi-square
3. Hierarchical variable clustering
4. Weight by correlation
5. Weight by information

These algorithms differ in their approach to feature selection. Weight by Gini and Weight by Chi-square are statistical methods that assign weights to features based on their importance. Hierarchical variable clustering groups features into clusters and selects the most important ones. Weight by correlation selects features based on their correlation with the target variable. Weight by information selects features based on their information gain."
What type of  solutions does the paper analyze for scattering?,a94c45ed,"[{'abstract': '  In this paper, we consider a Hamiltonian system combining a nonlinear Schr\\""\nodinger equation (NLS) and an ordinary differential equation (ODE). This system\nis a simplified model of the NLS around soliton solutions. Following Nakanishi\n\\cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.\nThe proof is based on Nakanishi\'s framework and Fermi Golden Rule estimates on\n$L^4$ in time norms.\n', 'title': 'On nonlinear profile decompositions and scattering for a NLS-ODE model'}]","Based on the context, I found a paper that analyzes scattering solutions.

Title: On nonlinear profile decompositions and scattering for a NLS-ODE model

Abstract:   In this paper, we consider a Hamiltonian system combining a nonlinear Schr\""odinger equation (NLS) and an ordinary differential equation (ODE). This system is a simplified model of the NLS around soliton solutions. Following Nakanishi \cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions. The proof is based on Nakanishi's framework and Fermi Golden Rule estimates on $L^4$ in time norms."
What are the challenges of using traditional mapping techniques for navigation in environments with cul-de-sacs?,93727c66,"[{'abstract': '  We investigate how a neural network can learn perception actions loops for\nnavigation in unknown environments. Specifically, we consider how to learn to\nnavigate in environments populated with cul-de-sacs that represent convex local\nminima that the robot could fall into instead of finding a set of feasible\nactions that take it to the goal. Traditional methods rely on maintaining a\nglobal map to solve the problem of over coming a long cul-de-sac. However, due\nto errors induced from local and global drift, it is highly challenging to\nmaintain such a map for long periods of time. One way to mitigate this problem\nis by using learning techniques that do not rely on hand engineered map\nrepresentations and instead output appropriate control policies directly from\ntheir sensory input. We first demonstrate that such a problem cannot be solved\ndirectly by deep reinforcement learning due to the sparse reward structure of\nthe environment. Further, we demonstrate that deep supervised learning also\ncannot be used directly to solve this problem. We then investigate network\nmodels that offer a combination of reinforcement learning and supervised\nlearning and highlight the significance of adding fully differentiable memory\nunits to such networks. We evaluate our networks on their ability to generalize\nto new environments and show that adding memory to such networks offers huge\njumps in performance\n', 'title': 'End-to-End Navigation in Unknown Environments using Neural Networks'}]","Title: End-to-End Navigation in Unknown Environments using Neural Networks

Abstract:   We investigate how a neural network can learn perception actions loops for
navigation in unknown environments. Specifically, we consider how to learn to
navigate in environments populated with cul-de-sacs that represent convex local
minima that the robot could fall into instead of finding a set of feasible
actions that take it to the goal. Traditional methods rely on maintaining a
global map to solve the problem of over coming a long cul-de-sac. However, due
to errors induced from local and global drift, it is highly challenging to
maintain such a map for long periods of time. One way to mitigate this problem
is by using learning techniques that do not rely on hand engineered map
representations and instead output appropriate control policies directly from
their sensory input."
What is the topic of the paper 'On some polynomials and series of Bloch-Polya Type'?,77539175,"[{'abstract': '  We will show that $(1-q)(1-q^2)\\dots (1-q^m)$ is a polynomial in $q$ with\ncoefficients from $\\{-1,0,1\\}$ iff $m=1,\\ 2,\\ 3,$ or $5$ and explore some\ninteresting consequences of this result. We find explicit formulas for the\n$q$-series coefficients of $(1-q^2)(1-q^3)(1-q^4)(1-q^5)\\dots$ and\n$(1-q^3)(1-q^4)(1-q^5)(1-q^6)\\dots$. In doing so, we extend certain\nobservations made by Sudler in 1964. We also discuss the classification of the\nproducts $(1-q)(1-q^2)\\dots (1-q^m)$ and some related series with respect to\ntheir absolute largest coefficients.\n', 'title': 'On some polynomials and series of Bloch-Polya Type'}]","Title: On some polynomials and series of Bloch-Polya Type

Abstract: We will show that $(1-q)(1-q^2)\dots (1-q^m)$ is a polynomial in $q$ with coefficients from $\{-1,0,1\}$ iff $m=1,\ 2,\ 3,$ or $5$ and explore some interesting consequences of this result. We find explicit formulas for the $q$-series coefficients of $(1-q^2)(1-q^3)(1-q^4)(1-q^5)\dots$ and $(1-q^3)(1-q^4)(1-q^5)(1-q^6)\dots$. In doing so, we extend certain observations made by Sudler in 1964. We also discuss the classification of the products $(1-q)(1-q^2)\dots (1-q^m)$ and some related series with respect to their absolute largest coefficients."
What type of mathematical formalism is used to analyze the impact of noise on chemical reaction networks?,dbcc51b3,"[{'abstract': '  The impact of random fluctuations on the dynamical behavior a complex\nbiological systems is a longstanding issue, whose understanding would shed\nlight on the evolutionary pressure that nature imposes on the intrinsic noise\nlevels and would allow rationally designing synthetic networks with controlled\nnoise. Using the Itō stochastic differential equation formalism, we performed\nboth analytic and numerical analyses of several model systems containing\ndifferent molecular species in contact with the environment and interacting\nwith each other through mass-action kinetics. These systems represent for\nexample biomolecular oligomerization processes, complex-breakage reactions,\nsignaling cascades or metabolic networks. For chemical reaction networks with\nzero deficiency values, which admit a detailed- or complex-balanced steady\nstate, all molecular species are uncorrelated. The number of molecules of each\nspecies follow a Poisson distribution and their Fano factors, which measure the\nintrinsic noise, are equal to one. Systems with deficiency one have an\nunbalanced non-equilibrium steady state and a non-zero S-flux, defined as the\nflux flowing between the complexes multiplied by an adequate stoichiometric\ncoefficient. In this case, the noise on each species is reduced if the flux\nflows from the species of lowest to highest complexity, and is amplified is the\nflux goes in the opposite direction. These results are generalized to systems\nof deficiency two, which possess two independent non-vanishing S-fluxes, and we\nconjecture that a similar relation holds for higher deficiency systems.\n', 'title': 'Deciphering noise amplification and reduction in open chemical reaction networks'}]","Title: Deciphering noise amplification and reduction in open chemical reaction networks

Abstract: The impact of random fluctuations on the dynamical behavior a complex
biological systems is a longstanding issue, whose understanding would shed
light on the evolutionary pressure that nature imposes on the intrinsic noise
levels and would allow rationally designing synthetic networks with controlled
noise. Using the Itō stochastic differential equation formalism, we performed
both analytic and numerical analyses of several model systems containing
different molecular species in contact with the environment and interacting
with each other through mass-action kinetics. These systems represent for
example biomolecular oligomerization processes, complex-breakage reactions,
signaling cascades or metabolic networks. For chemical reaction networks with
zero deficiency values, which admit a detailed- or complex-balanced steady
state, all molecular species are uncorrelated. The number of molecules of each
species follow a Poisson distribution and their Fano factors, which measure the
intrinsic noise, are equal to one. Systems with deficiency one have an
unbalanced non-equilibrium steady state and a non-zero S-flux, defined as the
flux flowing between the complexes multiplied by an adequate stoichiometric
coefficient. In this case, the noise on each species is reduced if the flux
flows from the species of lowest to highest complexity, and is amplified is the
flux goes in the opposite direction. These results are generalized to systems
of deficiency two, which possess two independent non-vanishing S-fluxes, and we
conjecture that a similar relation holds for higher deficiency systems.

The answer to the question is that the Itō stochastic differential equation formalism is used to analyze the impact of noise on chemical reaction networks."
How can distributed computing techniques be used to improve the efficiency of block-based matrix approximation?,1476108a,"[{'abstract': ""  A common problem in large-scale data analysis is to approximate a matrix\nusing a combination of specifically sampled rows and columns, known as CUR\ndecomposition. Unfortunately, in many real-world environments, the ability to\nsample specific individual rows or columns of the matrix is limited by either\nsystem constraints or cost. In this paper, we consider matrix approximation by\nsampling predefined \\emph{blocks} of columns (or rows) from the matrix. We\npresent an algorithm for sampling useful column blocks and provide novel\nguarantees for the quality of the approximation. This algorithm has application\nin problems as diverse as biometric data analysis to distributed computing. We\ndemonstrate the effectiveness of the proposed algorithms for computing the\nBlock CUR decomposition of large matrices in a distributed setting with\nmultiple nodes in a compute cluster, where such blocks correspond to columns\n(or rows) of the matrix stored on the same node, which can be retrieved with\nmuch less overhead than retrieving individual columns stored across different\nnodes. In the biometric setting, the rows correspond to different users and\ncolumns correspond to users' biometric reaction to external stimuli, {\\em\ne.g.,}~watching video content, at a particular time instant. There is\nsignificant cost in acquiring each user's reaction to lengthy content so we\nsample a few important scenes to approximate the biometric response. An\nindividual time sample in this use case cannot be queried in isolation due to\nthe lack of context that caused that biometric reaction. Instead, collections\nof time segments ({\\em i.e.,} blocks) must be presented to the user. The\npractical application of these algorithms is shown via experimental results\nusing real-world user biometric data from a content testing environment.\n"", 'title': 'Block CUR: Decomposing Matrices using Groups of Columns'}]","After reviewing the provided abstracts, I found a relevant paper that addresses the question of how distributed computing techniques can be used to improve the efficiency of block-based matrix approximation.

Title: Block CUR: Decomposing Matrices using Groups of Columns

Abstract:
A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. We present an algorithm for sampling useful column blocks and provide novel guarantees for the quality of the approximation. This algorithm has application in problems as diverse as biometric data analysis to distributed computing. We demonstrate the effectiveness of the proposed algorithms for computing the Block CUR decomposition of large matrices in a distributed setting with multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. In the biometric setting, the rows correspond to different users and columns correspond to users' biometric reaction to external stimuli, {\em e.g.,} watching video content, at a particular time instant. There is significant cost in acquiring each user's reaction to lengthy content so we sample a few important scenes to approximate the biometric response. An individual time sample in this use case cannot be queried in isolation due to the lack of context that caused that biometric reaction. Instead, collections of time segments ({\em i.e.,} blocks) must be presented to the user. The practical application of these algorithms is shown via experimental results using real-world user biometric data from a content testing environment.

This paper proposes a block-based matrix approximation algorithm that can be applied in a distributed computing setting, which can improve the efficiency of matrix approximation by reducing the overhead of retrieving individual columns stored across different nodes."
 Does adding slight positive changes to semi-algebraic constraints guarantee Mangasarian-Fromovitz constraint qualification at any feasible point?,4dd374bc,"[{'abstract': '  For an arbitrary finite family of semi-algebraic/definable functions, we\nconsider the corresponding inequality constraint set and we study qualification\nconditions for perturbations of this set. In particular we prove that all\npositive diagonal perturbations, save perhaps a finite number of them, ensure\nthat any point within the feasible set satisfies Mangasarian-Fromovitz\nconstraint qualification. Using the Milnor-Thom theorem, we provide a bound for\nthe number of singular perturbations when the constraints are polynomial\nfunctions. Examples show that the order of magnitude of our exponential bound\nis relevant. Our perturbation approach provides a simple protocol to build\nsequences of ""regular"" problems approximating an arbitrary\nsemi-algebraic/definable problem. Applications to sequential quadratic\nprogramming methods and sum of squares relaxation are provided.\n', 'title': 'Qualification Conditions in Semi-algebraic Programming'}]","Title: Qualification Conditions in Semi-algebraic Programming

Abstract: For an arbitrary finite family of semi-algebraic/definable functions, we consider the corresponding inequality constraint set and we study qualification conditions for perturbations of this set. In particular we prove that all positive diagonal perturbations, save perhaps a finite number of them, ensure that any point within the feasible set satisfies Mangasarian-Fromovitz constraint qualification. Using the Milnor-Thom theorem, we provide a bound for the number of singular perturbations when the constraints are polynomial functions. Examples show that the order of magnitude of our exponential bound is relevant. Our perturbation approach provides a simple protocol to build sequences of ""regular"" problems approximating an arbitrary semi-algebraic/definable problem. Applications to sequential quadratic programming methods and sum of squares relaxation are provided."
Does a finite Hausdorff spectrum guarantee a pro-$p$ group is $p$-adic analytic?,390890c0,"[{'abstract': '  Let $G$ be a finitely generated pro-$p$ group, equipped with the $p$-power\nseries. The associated metric and Hausdorff dimension function give rise to the\nHausdorff spectrum, which consists of the Hausdorff dimensions of closed\nsubgroups of $G$. In the case where $G$ is $p$-adic analytic, the Hausdorff\ndimension function is well understood; in particular, the Hausdorff spectrum\nconsists of finitely many rational numbers closely linked to the analytic\ndimensions of subgroups of $G$.\nConversely, it is a long-standing open question whether the finiteness of the\nHausdorff spectrum implies that $G$ is $p$-adic analytic. We prove that the\nanswer is yes, in a strong sense, under the extra condition that $G$ is\nsoluble.\nFurthermore, we explore the problem and related questions also for other\nfiltration series, such as the lower $p$-series, the Frattini series, the\nmodular dimension subgroup series and quite general filtration series. For\ninstance, we prove, for odd primes $p$, that every countably based pro-$p$\ngroup $G$ with an open subgroup mapping onto 2 copies of the $p$-adic integers\nadmits a filtration series such that the corresponding Hausdorff spectrum\ncontains an infinite real interval.\n', 'title': 'Hausdorff dimensions in $p$-adic analytic groups'}]","Title: Hausdorff dimensions in $p$-adic analytic groups

Abstract: Let $G$ be a finitely generated pro-$p$ group, equipped with the $p$-power
series. The associated metric and Hausdorff dimension function give rise to the
Hausdorff spectrum, which consists of the Hausdorff dimensions of closed
subgroups of $G$. In the case where $G$ is $p$-adic analytic, the Hausdorff
dimension function is well understood; in particular, the Hausdorff spectrum
consists of finitely many rational numbers closely linked to the analytic
dimensions of subgroups of $G$.
Conversely, it is a long-standing open question whether the finiteness of the
Hausdorff spectrum implies that $G$ is $p$-adic analytic. We prove that the
answer is yes, in a strong sense, under the extra condition that $G$ is
soluble."
What is the impact of even-frequency components on odd-frequency superconductivity?,c0f82224,"[{'abstract': '  Even- and odd-frequency superconductivity coexist due to broken time-reversal\nsymmetry under magnetic field. In order to describe this mixing, we extend the\nlinearized Eliashberg equation for the spin and charge fluctuation mechanism in\nstrongly correlated electron systems. We apply this extended Eliashberg\nequation to the odd-frequency superconductivity on a quasi-one-dimensional\nisosceles triangular lattice under in-plane magnetic field and examine the\neffect of the even-frequency component.\n', 'title': 'Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field'}]","Title: Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field

Abstract: Even- and odd-frequency superconductivity coexist due to broken time-reversal symmetry under magnetic field. In order to describe this mixing, we extend the linearized Eliashberg equation for the spin and charge fluctuation mechanism in strongly correlated electron systems. We apply this extended Eliashberg equation to the odd-frequency superconductivity on a quasi-one-dimensional isosceles triangular lattice under in-plane magnetic field and examine the effect of the even-frequency component."
Can a continuous-time Markov chain with multiple wells be represented as a combination of simpler states?,046844be,"[{'abstract': '  We consider continuous-time Markov chains which display a family of wells at\nthe same depth. We provide sufficient conditions which entail the convergence\nof the finite-dimensional distributions of the order parameter to the ones of a\nfinite state Markov chain. We also show that the state of the process can be\nrepresented as a time-dependent convex combination of metastable states, each\nof which is supported on one well.\n', 'title': 'Metastable Markov chains: from the convergence of the trace to the convergence of the finite-dimensional distributions'}]","I think I can help you with that!

Title: Metastable Markov chains: from the convergence of the trace to the convergence of the finite-dimensional distributions

Abstract: We consider continuous-time Markov chains which display a family of wells at the same depth. We provide sufficient conditions which entail the convergence of the finite-dimensional distributions of the order parameter to the ones of a finite state Markov chain. We also show that the state of the process can be represented as a time-dependent convex combination of metastable states, each of which is supported on one well."
What are the advantages of the DSBGK method for simulating gas mixtures compared to DSMC simulations?,07c3b9e3,"[{'abstract': '  Molecular reflections on usual wall surfaces can be statistically described\nby the Maxwell diffuse reflection model, which has been successfully applied in\nthe DSBGK simulations. We develop the DSBGK algorithm to implement the\nCercignani-Lampis-Lord (CLL) reflection model, which is widely applied to\npolished surfaces and used particularly in modeling space shuttles to predict\nthe heat and force loads exerted by the high-speed flows around the surfaces.\nWe also extend the DSBGK method to simulate gas mixtures and high contrast of\nnumber densities of different components can be handled at a cost of memory\nusage much lower than that needed by the DSMC simulations because the average\nnumbers of simulated molecules of different components per cell can be equal in\nthe DSBGK simulations.\n', 'title': 'DSBGK Method to Incorporate the CLL Reflection Model and to Simulate Gas Mixtures'}]","Title: DSBGK Method to Incorporate the CLL Reflection Model and to Simulate Gas Mixtures

Abstract: Molecular reflections on usual wall surfaces can be statistically described
by the Maxwell diffuse reflection model, which has been successfully applied in
the DSBGK simulations. We develop the DSBGK algorithm to implement the
Cercignani-Lampis-Lord (CLL) reflection model, which is widely applied to
polished surfaces and used particularly in modeling space shuttles to predict
the heat and force loads exerted by the high-speed flows around the surfaces.
We also extend the DSBGK method to simulate gas mixtures and high contrast of
number densities of different components can be handled at a cost of memory
usage much lower than that needed by the DSMC simulations because the average
numbers of simulated molecules of different components per cell can be equal in
the DSBGK simulations.

The advantage of the DSBGK method is that it can handle high contrast of number densities of different components at a lower cost of memory usage compared to DSMC simulations."
What are the observed phase noise statistics for ALMA datasets from the long-baseline science verification campaign with and without the application of the WVR scaling factor?,d63791e6,"[{'abstract': '  The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water\nvapour radiometers (WVR), which monitor the atmospheric water vapour line at\n183 GHz along the line of sight above each antenna to correct for phase delays\nintroduced by the wet component of the troposphere. The application of WVR\nderived phase corrections improve the image quality and facilitate successful\nobservations in weather conditions that were classically marginal or poor. We\npresent work to indicate that a scaling factor applied to the WVR solutions can\nact to further improve the phase stability and image quality of ALMA data. We\nfind reduced phase noise statistics for 62 out of 75 datasets from the\nlong-baseline science verification campaign after a WVR scaling factor is\napplied. The improvement of phase noise translates to an expected coherence\nimprovement in 39 datasets. When imaging the bandpass source, we find 33 of the\n39 datasets show an improvement in the signal-to-noise ratio (S/N) between a\nfew to ~30 percent. There are 23 datasets where the S/N of the science image is\nimproved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies\nstudied (band 6 and band 7) are those most improved, specifically datasets with\nlow precipitable water vapour (PWV), <1mm, where the dominance of the wet\ncomponent is reduced. Although these improvements are not profound, phase\nstability improvements via the WVR scaling factor come into play for the higher\nfrequency (>450 GHz) and long-baseline (>5km) observations. These inherently\nhave poorer phase stability and are taken in low PWV (<1mm) conditions for\nwhich we find the scaling to be most effective. A promising explanation for the\nscaling factor is the mixing of dry and wet air components, although other\norigins are discussed. We have produced a python code to allow ALMA users to\nundertake WVR scaling tests and make improvements to their data.\n', 'title': 'Phase correction for ALMA - Investigating water vapour radiometer scaling:The long-baseline science verification data case study'}]","Title: Phase correction for ALMA - Investigating water vapour radiometer scaling: The long-baseline science verification data case study

Abstract: The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water vapour radiometers (WVR), which monitor the atmospheric water vapour line at 183 GHz along the line of sight above each antenna to correct for phase delays introduced by the wet component of the troposphere. The application of WVR derived phase corrections improve the image quality and facilitate successful observations in weather conditions that were classically marginal or poor. We present work to indicate that a scaling factor applied to the WVR solutions can act to further improve the phase stability and image quality of ALMA data. We find reduced phase noise statistics for 62 out of 75 datasets from the long-baseline science verification campaign after a WVR scaling factor is applied."
  How does the proposed model simplify the study of the nonlinear Schrödinger equation?,a94c45ed,"[{'abstract': '  In this paper, we consider a Hamiltonian system combining a nonlinear Schr\\""\nodinger equation (NLS) and an ordinary differential equation (ODE). This system\nis a simplified model of the NLS around soliton solutions. Following Nakanishi\n\\cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.\nThe proof is based on Nakanishi\'s framework and Fermi Golden Rule estimates on\n$L^4$ in time norms.\n', 'title': 'On nonlinear profile decompositions and scattering for a NLS-ODE model'}]","Unfortunately, I couldn't find a direct answer to the question ""How does the proposed model simplify the study of the nonlinear Schrödinger equation?"" in the provided abstracts.

However, I found a research paper that might be related to the nonlinear Schrödinger equation:

Title: On nonlinear profile decompositions and scattering for a NLS-ODE model

Abstract: In this paper, we consider a Hamiltonian system combining a nonlinear Schrödinger equation (NLS) and an ordinary differential equation (ODE). This system is a simplified model of the NLS around soliton solutions. Following Nakanishi \cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions. The proof is based on Nakanishi's framework and Fermi Golden Rule estimates on $L^4$ in time norms.

This paper discusses a simplified model of the nonlinear Schrödinger equation (NLS) around soliton solutions, but it doesn't explicitly mention how the proposed model simplifies the study of the NLS."
Does calibrating this model to market smiles require solving an optimization problem?,84f29a2d,"[{'abstract': ""  The paper proposes an expanded version of the Local Variance Gamma model of\nCarr and Nadtochiy by adding drift to the governing underlying process. Still\nin this new model it is possible to derive an ordinary differential equation\nfor the option price which plays a role of Dupire's equation for the standard\nlocal volatility model. It is shown how calibration of multiple smiles (the\nwhole local volatility surface) can be done in such a case. Further, assuming\nthe local variance to be a piecewise linear function of strike and piecewise\nconstant function of time this ODE is solved in closed form in terms of\nConfluent hypergeometric functions. Calibration of the model to market smiles\ndoes not require solving any optimization problem and, in contrast, can be done\nterm-by-term by solving a system of non-linear algebraic equations for each\nmaturity, which is fast.\n"", 'title': 'An Expanded Local Variance Gamma model'}]","I think I can help you with that!

The answer to your question is: No, calibrating this model to market smiles does not require solving an optimization problem.

Here's the relevant information:

Title: An Expanded Local Variance Gamma model

Abstract: The paper proposes an expanded version of the Local Variance Gamma model of Carr and Nadtochiy by adding drift to the governing underlying process. Still in this new model it is possible to derive an ordinary differential equation for the option price which plays a role of Dupire's equation for the standard local volatility model. It is shown how calibration of multiple smiles (the whole local volatility surface) can be done in such a case. Further, assuming the local variance to be a piecewise linear function of strike and piecewise constant function of time this ODE is solved in closed form in terms of Confluent hypergeometric functions. Calibration of the model to market smiles does not require solving any optimization problem and, in contrast, can be done term-by-term by solving a system of non-linear algebraic equations for each maturity, which is fast."
What are the estimated rotation period values for 'Oumuamua?,3e10061f,"[{'abstract': ""  The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a\nplanetesimal born in another planetary system. This interloper exhibits a\nvariable colour within a range that is broadly consistent with local small\nbodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited\nKuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an\naxial ratio exceeding 5:1. Rotation period estimates are inconsistent and\nvaried, with reported values between 6.9 and 8.3 hours. Here we analyse all\navailable optical photometry reported to date. No single rotation period can\nexplain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be\nin an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or\ntumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135\nand 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the\navailable data are insufficient to uniquely constrain the true frequencies and\nshape. Assuming a body that responds to NPA rotation in a similar manner to\nSolar System asteroids and comets, the timescale to damp 1I/'Oumuamua's\ntumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling\nwithin its parent planetary system, and will remain tumbling well after it has\nleft ours.\n"", 'title': 'The tumbling rotational state of 1I/`Oumuamua'}]","I can help you with that!

Title: The tumbling rotational state of 1I/`Oumuamua

Abstract: The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a planetesimal born in another planetary system. This interloper exhibits a variable colour within a range that is broadly consistent with local small bodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited Kuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an axial ratio exceeding 5:1. Rotation period estimates are inconsistent and varied, with reported values between 6.9 and 8.3 hours. Here we analyse all available optical photometry reported to date. No single rotation period can explain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be in an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or tumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135 and 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the available data are insufficient to uniquely constrain the true frequencies and shape. Assuming a body that responds to NPA rotation in a similar manner to Solar System asteroids and comets, the timescale to damp 1I/'Oumuamua's tumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling within its parent planetary system, and will remain tumbling well after it has left ours.

Title: On the rotation period and shape of the hyperbolic asteroid 1I/`Oumuamua (2017) U1 from its lightcurve

Abstract: We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017 U1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel Telescope. From these observations, we derived a partial lightcurve with peak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out rotation periods less than 3 hr and suggests that the period is at least 5 hr. On the assumption that the variability is due to a changing cross section, the axial ratio is at least 3:1. We saw no evidence for a coma or tail in either individual images or in a stacked image having an equivalent exposure time of 9000 s.

The estimated rotation period values for 'Oumuamua are between 6.9 and 8.3 hours, and at least 5 hours."
What are the potential impacts of nearby supernovae on Earth's atmosphere and life?,e758332a,"[{'abstract': '  Recent 60Fe results have suggested that the estimated distances of supernovae\nin the last few million years should be reduced from 100 pc to 50 pc. Two\nevents or series of events are suggested, one about 2.7 million years to 1.7\nmillion years ago, and another may at 6.5 to 8.7 million years ago. We ask what\neffects such supernovae are expected to have on the terrestrial atmosphere and\nbiota. Assuming that the Local Bubble was formed before the event being\nconsidered, and that the supernova and the Earth were both inside a weak,\ndisordered magnetic field at that time, TeV-PeV cosmic rays at Earth will\nincrease by a factor of a few hundred. Tropospheric ionization will increase\nproportionately, and the overall muon radiation load on terrestrial organisms\nwill increase by a factor of 150. All return to pre-burst levels within 10kyr.\nIn the case of an ordered magnetic field, effects depend strongly on the field\norientation. The upper bound in this case is with a largely coherent field\naligned along the line of sight to the supernova, in which case TeV-PeV cosmic\nray flux increases are 10^4; in the case of a transverse field they are below\ncurrent levels. We suggest a substantial increase in the extended effects of\nsupernovae on Earth and in the lethal distance estimate; more work is\nneeded.This paper is an explicit followup to Thomas et al. (2016). We also here\nprovide more detail on the computational procedures used in both works.\n', 'title': ""A supernova at 50 pc: Effects on the Earth's atmosphere and biota""}]","Title: A supernova at 50 pc: Effects on the Earth's atmosphere and biota

Abstract: Recent 60Fe results have suggested that the estimated distances of supernovae in the last few million years should be reduced from 100 pc to 50 pc. Two events or series of events are suggested, one about 2.7 million years to 1.7 million years ago, and another may at 6.5 to 8.7 million years ago. We ask what effects such supernovae are expected to have on the terrestrial atmosphere and biota. Assuming that the Local Bubble was formed before the event being considered, and that the supernova and the Earth were both inside a weak, disordered magnetic field at that time, TeV-PeV cosmic rays at Earth will increase by a factor of a few hundred. Tropospheric ionization will increase proportionately, and the overall muon radiation load on terrestrial organisms will increase by a factor of 150. All return to pre-burst levels within 10kyr. In the case of an ordered magnetic field, effects depend strongly on the field orientation. The upper bound in this case is with a largely coherent field aligned along the line of sight to the supernova, in which case TeV-PeV cosmic ray flux increases are 10^4; in the case of a transverse field they are below current levels. We suggest a substantial increase in the extended effects of supernovae on Earth and in the lethal distance estimate; more work is needed. This paper is an explicit followup to Thomas et al. (2016). We also here provide more detail on the computational procedures used in both works."
How can high variance in CNN representations impact classification accuracy?,0bb8c1ec,"[{'abstract': ""  Convolutional Neural Networks (CNNs) can learn effective features, though\nhave been shown to suffer from a performance drop when the distribution of the\ndata changes from training to test data. In this paper we analyze the internal\nrepresentations of CNNs and observe that the representations of unseen data in\neach class, spread more (with higher variance) in the embedding space of the\nCNN compared to representations of the training data. More importantly, this\ndifference is more extreme if the unseen data comes from a shifted\ndistribution. Based on this observation, we objectively evaluate the degree of\nrepresentation's variance in each class via eigenvalue decomposition on the\nwithin-class covariance of the internal representations of CNNs and observe the\nsame behaviour. This can be problematic as larger variances might lead to\nmis-classification if the sample crosses the decision boundary of its class. We\napply nearest neighbor classification on the representations and empirically\nshow that the embeddings with the high variance actually have significantly\nworse KNN classification performances, although this could not be foreseen from\ntheir end-to-end classification results. To tackle this problem, we propose\nDeep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that\nsignificantly reduces the within-class covariance of a DNN's representation,\nimproving performance on unseen test data from a shifted distribution. We\nempirically evaluate DWCCA on two datasets for Acoustic Scene Classification\n(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA\nsignificantly improve the network's internal representation, it also increases\nthe end-to-end classification accuracy, especially when the test set exhibits a\ndistribution shift. By adding DWCCA to a VGG network, we achieve around 6\npercentage points improvement in the case of a distribution mismatch.\n"", 'title': 'Deep Within-Class Covariance Analysis for Robust Audio Representation Learning'}]","Title: Deep Within-Class Covariance Analysis for Robust Audio Representation Learning

Abstract: Convolutional Neural Networks (CNNs) can learn effective features, though have been shown to suffer from a performance drop when the distribution of the data changes from training to test data. In this paper we analyze the internal representations of CNNs and observe that the representations of unseen data in each class, spread more (with higher variance) in the embedding space of the CNN compared to representations of the training data. More importantly, this difference is more extreme if the unseen data comes from a shifted distribution. Based on this observation, we objectively evaluate the degree of representation's variance in each class via eigenvalue decomposition on the within-class covariance of the internal representations of CNNs and observe the same behaviour. This can be problematic as larger variances might lead to mis-classification if the sample crosses the decision boundary of its class."
What specific performance metrics indicate the state-of-the-art capabilities of this hybrid collaborative filtering model? ,36c0b16d,"[{'abstract': '  In this paper, we present a novel structure, Semi-AutoEncoder, based on\nAutoEncoder. We generalize it into a hybrid collaborative filtering model for\nrating prediction as well as personalized top-n recommendations. Experimental\nresults on two real-world datasets demonstrate its state-of-the-art\nperformances.\n', 'title': 'Hybrid Collaborative Recommendation via Semi-AutoEncoder'}]","The question is asking about the specific performance metrics that indicate the state-of-the-art capabilities of the hybrid collaborative filtering model.

Based on the provided context, the answer can be found in the first abstract:

Title: Hybrid Collaborative Recommendation via Semi-AutoEncoder

Abstract: In this paper, we present a novel structure, Semi-AutoEncoder, based on AutoEncoder. We generalize it into a hybrid collaborative filtering model for rating prediction as well as personalized top-n recommendations. Experimental results on two real-world datasets demonstrate its state-of-the-art performances.

In this case, the specific performance metrics are not explicitly mentioned. However, it is stated that the model demonstrates state-of-the-art performances on two real-world datasets, implying that the model's performance is superior to existing models in terms of rating prediction and personalized top-n recommendations."
What kind of experimental results demonstrate the effectiveness of deep learning for sparse ternary projections in compressed sensing?,a22b9d6e,"[{'abstract': '  Compressed sensing (CS) is a sampling theory that allows reconstruction of\nsparse (or compressible) signals from an incomplete number of measurements,\nusing of a sensing mechanism implemented by an appropriate projection matrix.\nThe CS theory is based on random Gaussian projection matrices, which satisfy\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\nprojections are more suitable for hardware implementation. In this paper, we\npresent a deep learning approach to obtain very sparse ternary projections for\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\nexperimental results on real images demonstrate the effectiveness of the\nproposed approach compared to state-of-the-art methods, with significant\nadvantage in terms of complexity.\n', 'title': 'Deep Learning Sparse Ternary Projections for Compressed Sensing of Images'}]","Title: Deep Learning Sparse Ternary Projections for Compressed Sensing of Images

Abstract:   Compressed sensing (CS) is a sampling theory that allows reconstruction of
sparse (or compressible) signals from an incomplete number of measurements,
using of a sensing mechanism implemented by an appropriate projection matrix.
The CS theory is based on random Gaussian projection matrices, which satisfy
recovery guarantees with high probability; however, sparse ternary {0, -1, +1}
projections are more suitable for hardware implementation. In this paper, we
present a deep learning approach to obtain very sparse ternary projections for
compressed sensing. Our deep learning architecture jointly learns a pair of a
projection matrix and a reconstruction operator in an end-to-end fashion. The
experimental results on real images demonstrate the effectiveness of the
proposed approach compared to state-of-the-art methods, with significant
advantage in terms of complexity."
How can graph theory be used to understand the interaction of vortices in fluid dynamics?,8a9d659e,"[{'abstract': '  We examine discrete vortex dynamics in two-dimensional flow through a\nnetwork-theoretic approach. The interaction of the vortices is represented with\na graph, which allows the use of network-theoretic approaches to identify key\nvortex-to-vortex interactions. We employ sparsification techniques on these\ngraph representations based on spectral theory for constructing sparsified\nmodels and evaluating the dynamics of vortices in the sparsified setup.\nIdentification of vortex structures based on graph sparsification and sparse\nvortex dynamics are illustrated through an example of point-vortex clusters\ninteracting amongst themselves. We also evaluate the performance of\nsparsification with increasing number of point vortices. The\nsparsified-dynamics model developed with spectral graph theory requires reduced\nnumber of vortex-to-vortex interactions but agrees well with the full nonlinear\ndynamics. Furthermore, the sparsified model derived from the sparse graphs\nconserves the invariants of discrete vortex dynamics. We highlight the\nsimilarities and differences between the present sparsified-dynamics model and\nthe reduced-order models.\n', 'title': 'Network-theoretic approach to sparsified discrete vortex dynamics'}]","Title: Network-theoretic approach to sparsified discrete vortex dynamics

Abstract: We examine discrete vortex dynamics in two-dimensional flow through a network-theoretic approach. The interaction of the vortices is represented with a graph, which allows the use of network-theoretic approaches to identify key vortex-to-vortex interactions. We employ sparsification techniques on these graph representations based on spectral theory for constructing sparsified models and evaluating the dynamics of vortices in the sparsified setup. Identification of vortex structures based on graph sparsification and sparse vortex dynamics are illustrated through an example of point-vortex clusters interacting amongst themselves. We also evaluate the performance of sparsification with increasing number of point vortices. The sparsified-dynamics model developed with spectral graph theory requires reduced number of vortex-to-vortex interactions but agrees well with the full nonlinear dynamics. Furthermore, the sparsified model derived from the sparse graphs conserves the invariants of discrete vortex dynamics. We highlight the similarities and differences between the present sparsified-dynamics model and the reduced-order models."
How can prior knowledge about a task be used to design communication protocols for robot swarms?,72a15f06,"[{'abstract': '  Swarm systems constitute a challenging problem for reinforcement learning\n(RL) as the algorithm needs to learn decentralized control policies that can\ncope with limited local sensing and communication abilities of the agents.\nWhile it is often difficult to directly define the behavior of the agents,\nsimple communication protocols can be defined more easily using prior knowledge\nabout the given task. In this paper, we propose a number of simple\ncommunication protocols that can be exploited by deep reinforcement learning to\nfind decentralized control policies in a multi-robot swarm environment. The\nprotocols are based on histograms that encode the local neighborhood relations\nof the agents and can also transmit task-specific information, such as the\nshortest distance and direction to a desired target. In our framework, we use\nan adaptation of Trust Region Policy Optimization to learn complex\ncollaborative tasks, such as formation building and building a communication\nlink. We evaluate our findings in a simulated 2D-physics environment, and\ncompare the implications of different communication protocols.\n', 'title': 'Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning'}]","I think I can help you with that!

Title: Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning

Abstract:  Swarm systems constitute a challenging problem for reinforcement learning (RL) as the algorithm needs to learn decentralized control policies that can cope with limited local sensing and communication abilities of the agents. While it is often difficult to directly define the behavior of the agents, simple communication protocols can be defined more easily using prior knowledge about the given task. In this paper, we propose a number of simple communication protocols that can be exploited by deep reinforcement learning to find decentralized control policies in a multi-robot swarm environment. The protocols are based on histograms that encode the local neighborhood relations of the agents and can also transmit task-specific information, such as the shortest distance and direction to a desired target. In our framework, we use an adaptation of Trust Region Policy Optimization to learn complex collaborative tasks, such as formation building and building a communication link. We evaluate our findings in a simulated 2D-physics environment, and compare the implications of different communication protocols."
What are the advantages of using a medium-based approach to understand diffraction-free refraction compared to a circuit-based approach?,329bf4d2,"[{'abstract': '  Refraction represents one of the most fundamental operations that may be\nperformed by a metasurface. However, simple phasegradient metasurface designs\nsuffer from restricted angular deflection due to spurious diffraction orders.\nIt has been recently shown, using a circuit-based approach, that refraction\nwithout spurious diffraction, or diffraction-free, can fortunately be achieved\nby a transverse metasurface exhibiting either loss-gain or bianisotropy. Here,\nwe rederive these conditions using a medium-based - and hence more insightfull\n- approach based on Generalized Sheet Transition Conditions (GSTCs) and surface\nsusceptibility tensors, and experimentally demonstrate two diffraction-free\nrefractive metasurfaces that are essentially lossless, passive, bianisotropic\nand reciprocal.\n', 'title': 'Refracting Metasurfaces without Spurious Diffraction'}]","Title: Refracting Metasurfaces without Spurious Diffraction

Abstract: Refraction represents one of the most fundamental operations that may be performed by a metasurface. However, simple phasegradient metasurface designs suffer from restricted angular deflection due to spurious diffraction orders. It has been recently shown, using a circuit-based approach, that refraction without spurious diffraction, or diffraction-free, can fortunately be achieved by a transverse metasurface exhibiting either loss-gain or bianisotropy. Here, we rederive these conditions using a medium-based - and hence more insightful - approach based on Generalized Sheet Transition Conditions (GSTCs) and surface susceptibility tensors, and experimentally demonstrate two diffraction-free refractive metasurfaces that are essentially lossless, passive, bianisotropic and reciprocal.

The advantage of using a medium-based approach is that it provides more insight into the phenomenon of diffraction-free refraction compared to a circuit-based approach."
Are there theoretical bounds on the total energy of a turbulent dynamo?,412f5254,"[{'abstract': '  Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both\nwith and without a significant mean field are obtained. Exact bounds are\nderived for the total energy which conform with the Kolmogorov phenomenology of\nturbulence. Best fits to numerical data show the same functional dependences as\nthe inequalities obtained from optimum theory.\n', 'title': 'Scaling laws and bounds for the turbulent G.O. Roberts dynamo'}]","Title: Scaling laws and bounds for the turbulent G.O. Roberts dynamo

Abstract: Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both with and without a significant mean field are obtained. Exact bounds are derived for the total energy which conform with the Kolmogorov phenomenology of turbulence. Best fits to numerical data show the same functional dependences as the inequalities obtained from optimum theory."
How does the proposed method compare to traditional video summarization techniques using ground truth data?,4d3e2431,"[{'abstract': '  The best summary of a long video differs among different people due to its\nhighly subjective nature. Even for the same person, the best summary may change\nwith time or mood. In this paper, we introduce the task of generating\ncustomized video summaries through simple text. First, we train a deep\narchitecture to effectively learn semantic embeddings of video frames by\nleveraging the abundance of image-caption data via a progressive and residual\nmanner. Given a user-specific text description, our algorithm is able to select\nsemantically relevant video segments and produce a temporally aligned video\nsummary. In order to evaluate our textually customized video summaries, we\nconduct experimental comparison with baseline methods that utilize ground-truth\ninformation. Despite the challenging baselines, our method still manages to\nshow comparable or even exceeding performance. We also show that our method is\nable to generate semantically diverse video summaries by only utilizing the\nlearned visual embeddings.\n', 'title': 'Contextually Customized Video Summaries via Natural Language'}]","Title: Contextually Customized Video Summaries via Natural Language

Abstract: The best summary of a long video differs among different people due to its highly subjective nature. Even for the same person, the best summary may change with time or mood. In this paper, we introduce the task of generating customized video summaries through simple text. First, we train a deep architecture to effectively learn semantic embeddings of video frames by leveraging the abundance of image-caption data via a progressive and residual manner. Given a user-specific text description, our algorithm is able to select semantically relevant video segments and produce a temporally aligned video summary. In order to evaluate our textually customized video summaries, we conduct experimental comparison with baseline methods that utilize ground-truth information. Despite the challenging baselines, our method still manages to show comparable or even exceeding performance. We also show that our method is able to generate semantically diverse video summaries by only utilizing the learned visual embeddings."
Lecture notes on elliptic curve cryptography for master's students,3027d5ca,"[{'abstract': ""  These lectures notes were written for a summer school on Mathematics for\npost-quantum cryptography in Thiès, Senegal. They try to provide a guide for\nMasters' students to get through the vast literature on elliptic curves,\nwithout getting lost on their way to learning isogeny based cryptography. They\nare by no means a reference text on the theory of elliptic curves, nor on\ncryptography; students are encouraged to complement these notes with some of\nthe books recommended in the bibliography.\nThe presentation is divided in three parts, roughly corresponding to the\nthree lectures given. In an effort to keep the reader interested, each part\nalternates between the fundamental theory of elliptic curves, and applications\nin cryptography. We often prefer to have the main ideas flow smoothly, rather\nthan having a rigorous presentation as one would have in a more classical book.\nThe reader will excuse us for the inaccuracies and the omissions.\n"", 'title': 'Mathematics of Isogeny Based Cryptography'}]","Title: Mathematics of Isogeny Based Cryptography

Abstract: These lectures notes were written for a summer school on Mathematics for post-quantum cryptography in Thiès, Senegal. They try to provide a guide for Masters' students to get through the vast literature on elliptic curves, without getting lost on their way to learning isogeny based cryptography. They are by no means a reference text on the theory of elliptic curves, nor on cryptography; students are encouraged to complement these notes with some of the books recommended in the bibliography.
The presentation is divided in three parts, roughly corresponding to the three lectures given. In an effort to keep the reader interested, each part alternates between the fundamental theory of elliptic curves, and applications in cryptography. We often prefer to have the main ideas flow smoothly, rather than having a rigorous presentation as one would have in a more classical book. The reader will excuse us for the inaccuracies and the omissions."
How can researchers utilize large datasets from clinical trials to discover biomarkers and personalize medicine?,632558b6,"[{'abstract': '  Detection of interactions between treatment effects and patient descriptors\nin clinical trials is critical for optimizing the drug development process. The\nincreasing volume of data accumulated in clinical trials provides a unique\nopportunity to discover new biomarkers and further the goal of personalized\nmedicine, but it also requires innovative robust biomarker detection methods\ncapable of detecting non-linear, and sometimes weak, signals. We propose a set\nof novel univariate statistical tests, based on the theory of random walks,\nwhich are able to capture non-linear and non-monotonic covariate-treatment\ninteractions. We also propose a novel combined test, which leverages the power\nof all of our proposed univariate tests into a single general-case tool. We\npresent results for both synthetic trials as well as real-world clinical\ntrials, where we compare our method with state-of-the-art techniques and\ndemonstrate the utility and robustness of our approach.\n', 'title': 'Robust Detection of Covariate-Treatment Interactions in Clinical Trials'}]","Title: Robust Detection of Covariate-Treatment Interactions in Clinical Trials

Abstract: Detection of interactions between treatment effects and patient descriptors in clinical trials is critical for optimizing the drug development process. The increasing volume of data accumulated in clinical trials provides a unique opportunity to discover new biomarkers and further the goal of personalized medicine, but it also requires innovative robust biomarker detection methods capable of detecting non-linear, and sometimes weak, signals. We propose a set of novel univariate statistical tests, based on the theory of random walks, which are able to capture non-linear and non-monotonic covariate-treatment interactions. We also propose a novel combined test, which leverages the power of all of our proposed univariate tests into a single general-case tool. We present results for both synthetic trials as well as real-world clinical trials, where we compare our method with state-of-the-art techniques and demonstrate the utility and robustness of our approach."
What techniques can be utilized to construct new endomorphisms of totally disconnected locally compact groups from existing ones?,fc941b5c,"[{'abstract': '  We study the scale and tidy subgroups of an endomorphism of a totally\ndisconnected locally compact group using a geometric framework. This leads to\nnew interpretations of tidy subgroups and the scale function. Foremost, we\nobtain a geometric tidying procedure which applies to endomorphisms as well as\na geometric proof of the fact that tidiness is equivalent to being minimizing\nfor a given endomorphism. Our framework also yields an endomorphism version of\nthe Baumgartner-Willis tree representation theorem. We conclude with a\nconstruction of new endomorphisms of totally disconnected locally compact\ngroups from old via HNN-extensions.\n', 'title': 'Willis Theory via Graphs'}]","I think I can help you with that!

Title: Willis Theory via Graphs

Abstract:   We study the scale and tidy subgroups of an endomorphism of a totally
disconnected locally compact group using a geometric framework. This leads to
new interpretations of tidy subgroups and the scale function. Foremost, we
obtain a geometric tidying procedure which applies to endomorphisms as well
as a geometric proof of the fact that tidiness is equivalent to being minimizing
for a given endomorphism. Our framework also yields an endomorphism version of
the Baumgartner-Willis tree representation theorem. We conclude with a
construction of new endomorphisms of totally disconnected locally compact
groups from old via HNN-extensions."
How does the concept of inner functions apply to problems involving rational functions and functions in a specific space?,08d5eafa,"[{'abstract': '  We discuss the concept of inner function in reproducing kernel Hilbert spaces\nwith an orthogonal basis of monomials and examine connections between inner\nfunctions and optimal polynomial approximants to $1/f$, where $f$ is a function\nin the space. We revisit some classical examples from this perspective, and\nshow how a construction of Shapiro and Shields can be modified to produce inner\nfunctions.\n', 'title': 'Remarks on Inner Functions and Optimal Approximants'}]","Based on the context, I found a research paper that might be relevant to your question.

Title: Remarks on Inner Functions and Optimal Approximants

Abstract: We discuss the concept of inner function in reproducing kernel Hilbert spaces with an orthogonal basis of monomials and examine connections between inner functions and optimal polynomial approximants to $1/f$, where $f$ is a function in the space. We revisit some classical examples from this perspective, and show how a construction of Shapiro and Shields can be modified to produce inner functions.

This paper seems to discuss the concept of inner functions in relation to reproducing kernel Hilbert spaces and their connections to optimal polynomial approximants, which might be relevant to problems involving rational functions and functions in a specific space."
What is the significance of the total integrated Huang-Rhys factor in determining exciton transfer efficiency within the FMO complex?,c4d955bc,"[{'abstract': '  The coupled exciton-vibrational dynamics of a three-site model of the FMO\ncomplex is investigated using the Multi-layer Multi-configuration\nTime-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of\nthe spectral density on the exciton state populations as well as on the\nvibrational and vibronic non-equilibrium excitations. Models which use either a\nsingle or site-specific spectral densities are contrasted to a spectral density\nadapted from experiment. For the transfer efficiency, the total integrated\nHuang-Rhys factor is found to be more important than details of the spectral\ndistributions. However, the latter are relevant for the obtained\nnon-equilibrium vibrational and vibronic distributions and thus influence the\nactual pattern of population relaxation.\n', 'title': 'The Effect of Site-Specific Spectral Densities on the High-Dimensional Exciton-Vibrational Dynamics in the FMO Complex'}]","Title: The Effect of Site-Specific Spectral Densities on the High-Dimensional Exciton-Vibrational Dynamics in the FMO Complex

Abstract: The coupled exciton-vibrational dynamics of a three-site model of the FMO complex is investigated using the Multi-layer Multi-configuration Time-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of the spectral density on the exciton state populations as well as on the vibrational and vibronic non-equilibrium excitations. Models which use either a single or site-specific spectral densities are contrasted to a spectral density adapted from experiment. For the transfer efficiency, the total integrated Huang-Rhys factor is found to be more important than details of the spectral distributions. However, the latter are relevant for the obtained non-equilibrium vibrational and vibronic distributions and thus influence the actual pattern of population relaxation.

According to the abstract, the total integrated Huang-Rhys factor is more important than the spectral distributions in determining the transfer efficiency within the FMO complex."
What is the connection between the algebraic structure of a singularity and the intermediate K-semistable cone of its metric tangent cone?,76a77f10,"[{'abstract': '  Given a klt singularity $x\\in (X, D)$, we show that a quasi-monomial\nvaluation $v$ with a finitely generated associated graded ring is the minimizer\nof the normalized volume function $\\widehat{\\rm vol}_{(X,D),x}$, if and only if\n$v$ induces a degeneration to a K-semistable log Fano cone singularity.\nMoreover, such a minimizer is unique among all quasi-monomial valuations up to\nrescaling. As a consequence, we prove that for a klt singularity $x\\in X$ on\nthe Gromov-Hausdorff limit of Kähler-Einstein Fano manifolds, the\nintermediate K-semistable cone associated to its metric tangent cone is\nuniquely determined by the algebraic structure of $x\\in X$, hence confirming a\nconjecture by Donaldson-Sun.\n', 'title': 'Stability of Valuations: Higher Rational Rank'}]","Title: Stability of Valuations: Higher Rational Rank

Abstract: Given a klt singularity $x\in (X, D)$, we show that a quasi-monomial valuation $v$ with a finitely generated associated graded ring is the minimizer of the normalized volume function $\widehat{\rm vol}_{(X,D),x}$, if and only if $v$ induces a degeneration to a K-semistable log Fano cone singularity. Moreover, such a minimizer is unique among all quasi-monomial valuations up to rescaling. As a consequence, we prove that for a klt singularity $x\in X$ on the Gromov-Hausdorff limit of Kähler-Einstein Fano manifolds, the intermediate K-semistable cone associated to its metric tangent cone is uniquely determined by the algebraic structure of $x\in X$, hence confirming a conjecture by Donaldson-Sun."
What types of recognition tasks can be addressed using the learned embedding from the proposed framework?,408f2f1c,"[{'abstract': '  The novel unseen classes can be formulated as the extreme values of known\nclasses. This inspired the recent works on open-set recognition\n\\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no\nway of naming the novel unseen classes. To solve this problem, we propose the\nExtreme Value Learning (EVL) formulation to learn the mapping from visual\nfeature to semantic space. To model the margin and coverage distributions of\neach class, the Vocabulary-informed Learning (ViL) is adopted by using vast\nopen vocabulary in the semantic space. Essentially, by incorporating the EVL\nand ViL, we for the first time propose a novel semantic embedding paradigm --\nVocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual\nfeatures into semantic space in a probabilistic way. The learned embedding can\nbe directly used to solve supervised learning, zero-shot and open set\nrecognition simultaneously. Experiments on two benchmark datasets demonstrate\nthe effectiveness of proposed frameworks.\n', 'title': 'Vocabulary-informed Extreme Value Learning'}]","I think I can help you with that!

The answer can be found in the first paper:

Title: Vocabulary-informed Extreme Value Learning

Abstract: The novel unseen classes can be formulated as the extreme values of known classes. This inspired the recent works on open-set recognition \cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no way of naming the novel unseen classes. To solve this problem, we propose the Extreme Value Learning (EVL) formulation to learn the mapping from visual feature to semantic space. To model the margin and coverage distributions of each class, the Vocabulary-informed Learning (ViL) is adopted by using vast open vocabulary in the semantic space. Essentially, by incorporating the EVL and ViL, we for the first time propose a novel semantic embedding paradigm -- Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual features into semantic space in a probabilistic way. The learned embedding can be directly used to solve supervised learning, zero-shot and open set recognition simultaneously. Experiments on two benchmark datasets demonstrate the effectiveness of proposed frameworks.

According to the abstract, the learned embedding from the proposed framework can be used to address supervised learning, zero-shot, and open-set recognition tasks."
What are the main contributions of a mixed Galerkin approach to discretizing port-Hamiltonian systems?,3cea44dd,"[{'abstract': '  We present the mixed Galerkin discretization of distributed parameter\nport-Hamiltonian systems. On the prototypical example of hyperbolic systems of\ntwo conservation laws in arbitrary spatial dimension, we derive the main\ncontributions: (i) A weak formulation of the underlying geometric\n(Stokes-Dirac) structure with a segmented boundary according to the causality\nof the boundary ports. (ii) The geometric approximation of the Stokes-Dirac\nstructure by a finite-dimensional Dirac structure is realized using a mixed\nGalerkin approach and power-preserving linear maps, which define minimal\ndiscrete power variables. (iii) With a consistent approximation of the\nHamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.\nBy the degrees of freedom in the power-preserving maps, the resulting family of\nstructure-preserving schemes allows for trade-offs between centered\napproximations and upwinding. We illustrate the method on the example of\nWhitney finite elements on a 2D simplicial triangulation and compare the\neigenvalue approximation in 1D with a related approach.\n', 'title': 'Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems'}]","Title: Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems

Abstract: We present the mixed Galerkin discretization of distributed parameter port-Hamiltonian systems. On the prototypical example of hyperbolic systems of two conservation laws in arbitrary spatial dimension, we derive the main contributions: (i) A weak formulation of the underlying geometric (Stokes-Dirac) structure with a segmented boundary according to the causality of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac structure by a finite-dimensional Dirac structure is realized using a mixed Galerkin approach and power-preserving linear maps, which define minimal discrete power variables. (iii) With a consistent approximation of the Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models. By the degrees of freedom in the power-preserving maps, the resulting family of structure-preserving schemes allows for trade-offs between centered approximations and upwinding. We illustrate the method on the example of Whitney finite elements on a 2D simplicial triangulation and compare the eigenvalue approximation in 1D with a related approach.

The main contributions of a mixed Galerkin approach to discretizing port-Hamiltonian systems are:

* A weak formulation of the underlying geometric (Stokes-Dirac) structure with a segmented boundary according to the causality of the boundary ports.
* The geometric approximation of the Stokes-Dirac structure by a finite-dimensional Dirac structure using a mixed Galerkin approach and power-preserving linear maps, which define minimal discrete power variables.
* A consistent approximation of the Hamiltonian, resulting in finite-dimensional port-Hamiltonian state space models that allow for trade-offs between centered approximations and upwinding."
What are the statistical implications of the difference between the classical CAPM beta and the weighted-Gini beta in the insurance industry?,81ff7fb0,"[{'abstract': ""  The `beta' is one of the key quantities in the capital asset pricing model\n(CAPM). In statistical language, the beta can be viewed as the slope of the\nregression line fitted to financial returns on the market against the returns\non the asset under consideration. The insurance counterpart of CAPM, called the\nweighted insurance pricing model (WIPM), gives rise to the so-called\nweighted-Gini beta. The aforementioned two betas may or may not coincide,\ndepending on the form of the underlying regression function, and this has\nprofound implications when designing portfolios and allocating risk capital. To\nfacilitate these tasks, in this paper we develop large-sample statistical\ninference results that, in a straightforward fashion, imply confidence\nintervals for, and hypothesis tests about, the equality of the two betas.\n"", 'title': 'Statistical foundations for assessing the difference between the classical and weighted-Gini betas'}]","After reviewing the provided research papers, I found a relevant paper that addresses the statistical implications of the difference between the classical CAPM beta and the weighted-Gini beta in the insurance industry.

Title: Statistical foundations for assessing the difference between the classical and weighted-Gini betas

Abstract: The `beta' is one of the key quantities in the capital asset pricing model (CAPM). In statistical language, the beta can be viewed as the slope of the regression line fitted to financial returns on the market against the returns on the asset under consideration. The insurance counterpart of CAPM, called the weighted insurance pricing model (WIPM), gives rise to the so-called weighted-Gini beta. The aforementioned two betas may or may not coincide, depending on the form of the underlying regression function, and this has profound implications when designing portfolios and allocating risk capital. To facilitate these tasks, in this paper we develop large-sample statistical inference results that, in a straightforward fashion, imply confidence intervals for, and hypothesis tests about, the equality of the two betas."
How do you compute the number of connected functional graphs?,69299b4d,"[{'abstract': '  We study functional graphs generated by quadratic polynomials over prime\nfields. We introduce efficient algorithms for methodical computations and\nprovide the values of various direct and cumulative statistical parameters of\ninterest. These include: the number of connected functional graphs, the number\nof graphs having a maximal cycle, the number of cycles of fixed size, the\nnumber of components of fixed size, as well as the shape of trees extracted\nfrom functional graphs. We particularly focus on connected functional graphs,\nthat is, the graphs which contain only one component (and thus only one cycle).\nBased on the results of our computations, we formulate several conjectures\nhighlighting the similarities and differences between these functional graphs\nand random mappings.\n', 'title': 'On Functional Graphs of Quadratic Polynomials'}]","I think I can help you with that!

Title: On Functional Graphs of Quadratic Polynomials

Abstract: We study functional graphs generated by quadratic polynomials over prime fields. We introduce efficient algorithms for methodical computations and provide the values of various direct and cumulative statistical parameters of interest. These include: the number of connected functional graphs, the number of graphs having a maximal cycle, the number of cycles of fixed size, the number of components of fixed size, as well as the shape of trees extracted from functional graphs. We particularly focus on connected functional graphs, that is, the graphs which contain only one component (and thus only one cycle). Based on the results of our computations, we formulate several conjectures highlighting the similarities and differences between these functional graphs and random mappings.

The research paper ""On Functional Graphs of Quadratic Polynomials"" provides efficient algorithms for computing the number of connected functional graphs, as well as other statistical parameters. However, the exact method for computing the number of connected functional graphs is not explicitly stated in the abstract. You may need to read the full paper to find the specific algorithm or method used."
How does this network architecture directly output a probabilistic clustering?,59688626,"[{'abstract': ""  We propose a novel end-to-end neural network architecture that, once trained,\ndirectly outputs a probabilistic clustering of a batch of input examples in one\npass. It estimates a distribution over the number of clusters $k$, and for each\n$1 \\leq k \\leq k_\\mathrm{max}$, a distribution over the individual cluster\nassignment for each data point. The network is trained in advance in a\nsupervised fashion on separate data to learn grouping by any perceptual\nsimilarity criterion based on pairwise labels (same/different group). It can\nthen be applied to different data containing different groups. We demonstrate\npromising performance on high-dimensional data like images (COIL-100) and\nspeech (TIMIT). We call this ``learning to cluster'' and show its conceptual\ndifference to deep metric learning, semi-supervise clustering and other related\napproaches while having the advantage of performing learnable clustering fully\nend-to-end.\n"", 'title': 'Learning Neural Models for End-to-End Clustering'}]","I think I can help you with that!

The answer to your question can be found in the first paper:

Title: Learning Neural Models for End-to-End Clustering

Abstract: We propose a novel end-to-end neural network architecture that, once trained, directly outputs a probabilistic clustering of a batch of input examples in one pass. It estimates a distribution over the number of clusters $k$, and for each $1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster assignment for each data point. The network is trained in advance in a supervised fashion on separate data to learn grouping by any perceptual similarity criterion based on pairwise labels (same/different group). It can then be applied to different data containing different groups. We demonstrate promising performance on high-dimensional data like images (COIL-100) and speech (TIMIT). We call this ``learning to cluster'' and show its conceptual difference to deep metric learning, semi-supervise clustering and other related approaches while having the advantage of performing learnable clustering fully end-to-end."
How does the structure of the Barabasi-Albert scale-free network influence the spread of information?,f574294f,"[{'abstract': '  This paper mainly discusses the diffusion on complex networks with\ntime-varying couplings. We propose a model to describe the adaptive diffusion\nprocess of local topological and dynamical information, and find that the\nBarabasi-Albert scale-free network (BA network) is beneficial to the diffusion\nand leads nodes to arrive at a larger state value than other networks do. The\nability of diffusion for a node is related to its own degree. Specifically,\nnodes with smaller degrees are more likely to change their states and reach\nlarger values, while those with larger degrees tend to stick to their original\nstates. We introduce state entropy to analyze the thermodynamic mechanism of\nthe diffusion process, and interestingly find that this kind of diffusion\nprocess is a minimization process of state entropy. We use the inequality\nconstrained optimization method to reveal the restriction function of the\nminimization and find that it has the same form as the Gibbs free energy. The\nthermodynamical concept allows us to understand dynamical processes on complex\nnetworks from a brand-new perspective. The result provides a convenient means\nof optimizing relevant dynamical processes on practical circuits as well as\nrelated complex systems.\n', 'title': 'Adaptive Diffusion Processes of Time-Varying Local Information on Networks'}]","Title: Adaptive Diffusion Processes of Time-Varying Local Information on Networks

Abstract: This paper mainly discusses the diffusion on complex networks with time-varying couplings. We propose a model to describe the adaptive diffusion process of local topological and dynamical information, and find that the Barabasi-Albert scale-free network (BA network) is beneficial to the diffusion and leads nodes to arrive at a larger state value than other networks do. The ability of diffusion for a node is related to its own degree. Specifically, nodes with smaller degrees are more likely to change their states and reach larger values, while those with larger degrees tend to stick to their original states."
For what values of n does the paper explicitly work out the singularities and representations of stabilizer groups on the tangent spaces?,eb7cc396,"[{'abstract': '  For $n\\ge5$, it is well known that the moduli space $\\mathfrak{M_{0,\\:n}}$ of\nunordered $n$ points on the Riemann sphere is a quotient space of the Zariski\nopen set $K_n$ of $\\mathbb C^{n-3}$ by an $S_n$ action. The stabilizers of this\n$S_n$ action at certain points of this Zariski open set $K_n$ correspond to the\ngroups fixing the sets of $n$ points on the Riemann sphere. Let $\\alpha$ be a\nsubset of $n$ distinct points on the Riemann sphere. We call the group of all\nlinear fractional transformations leaving $\\alpha$ invariant the stabilizer of\n$\\alpha$, which is finite by observation. For each non-trivial finite subgroup\n$G$ of the group ${\\rm PSL}(2,{\\Bbb C})$ of linear fractional transformations,\nwe give the necessary and sufficient condition for finite subsets of the\nRiemann sphere under which the stabilizers of them are conjugate to $G$. We\nalso prove that there does exist some finite subset of the Riemann sphere whose\nstabilizer coincides with $G$. Next we obtain the irreducible decompositions of\nthe representations of the stabilizers on the tangent spaces at the\nsingularities of $\\mathfrak{M_{0,\\:n}}$. At last, on $\\mathfrak{M_{0,\\:5}}$ and\n$\\mathfrak{M_{0,\\:6}}$, we work out explicitly the singularities and the\nrepresentations of their stabilizers on the tangent spaces at them.\n', 'title': 'Moduli Spaces of Unordered $n\\ge5$ Points on the Riemann Sphere and Their Singularities'}]","I think I can help you with that!

The answer can be found in the first paper:

Title: Moduli Spaces of Unordered $n\ge5$ Points on the Riemann Sphere and Their Singularities

Abstract:  For $n\ge5$, it is well known that the moduli space $\mathfrak{M_{0,\:n}}$ of
unordered $n$ points on the Riemann sphere is a quotient space of the Zariski
open set $K_n$ of $\mathbb C^{n-3}$ by an $S_n$ action. The stabilizers of this
$S_n$ action at certain points of this Zariski open set $K_n$ correspond to the
groups fixing the sets of $n$ points on the Riemann sphere. Let $\alpha$ be a
subset of $n$ distinct points on the Riemann sphere. We call the group of all
linear fractional transformations leaving $\alpha$ invariant the stabilizer of
$\alpha$, which is finite by observation. For each non-trivial finite subgroup
$G$ of the group ${\rm PSL}(2,{\Bbb C})$ of linear fractional transformations,
we give the necessary and sufficient condition for finite subsets of the
Riemann sphere under which the stabilizers of them are conjugate to $G$. We
also prove that there does exist some finite subset of the Riemann sphere whose
stabilizer coincides with $G$. Next we obtain the irreducible decompositions of
the representations of the stabilizers on the tangent spaces at the
singularities of $\mathfrak{M_{0,\:n}}$. At last, on $\mathfrak{M_{0,\:5}}$ and
$\mathfrak{M_{0,\:6}}$, we work out explicitly the singularities and the
representations of their stabilizers on the tangent spaces at them.

The answer is: $n=5$ and $n=6$."
How have recent techniques categorized the methods for compressing and accelerating CNN models?,db40eb59,"[{'abstract': '  Deep convolutional neural networks (CNNs) have recently achieved great\nsuccess in many visual recognition tasks. However, existing deep neural network\nmodels are computationally expensive and memory intensive, hindering their\ndeployment in devices with low memory resources or in applications with strict\nlatency requirements. Therefore, a natural thought is to perform model\ncompression and acceleration in deep networks without significantly decreasing\nthe model performance. During the past few years, tremendous progress has been\nmade in this area. In this paper, we survey the recent advanced techniques for\ncompacting and accelerating CNNs model developed. These techniques are roughly\ncategorized into four schemes: parameter pruning and sharing, low-rank\nfactorization, transferred/compact convolutional filters, and knowledge\ndistillation. Methods of parameter pruning and sharing will be described at the\nbeginning, after that the other techniques will be introduced. For each scheme,\nwe provide insightful analysis regarding the performance, related applications,\nadvantages, and drawbacks etc. Then we will go through a few very recent\nadditional successful methods, for example, dynamic capacity networks and\nstochastic depths networks. After that, we survey the evaluation matrix, the\nmain datasets used for evaluating the model performance and recent benchmarking\nefforts. Finally, we conclude this paper, discuss remaining challenges and\npossible directions on this topic.\n', 'title': 'A Survey of Model Compression and Acceleration for Deep Neural Networks'}]","Title: A Survey of Model Compression and Acceleration for Deep Neural Networks

Abstract: Deep convolutional neural networks (CNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past few years, tremendous progress has been made in this area. In this paper, we survey the recent advanced techniques for compacting and accelerating CNNs model developed. These techniques are roughly categorized into four schemes: parameter pruning and sharing, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and sharing will be described at the beginning, after that the other techniques will be introduced. For each scheme, we provide insightful analysis regarding the performance, related applications, advantages, and drawbacks etc. Then we will go through a few very recent additional successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrix, the main datasets used for evaluating the model performance and recent benchmarking efforts. Finally, we conclude this paper, discuss remaining challenges and possible directions on this topic."
What is a new method for quantifying connectivity in transportation networks?,aadfb609,"[{'abstract': '  This work presents a new method to quantify connectivity in transportation\nnetworks. Inspired by the field of topological data analysis, we propose a\nnovel approach to explore the robustness of road network connectivity in the\npresence of congestion on the roadway. The robustness of the pattern is\nsummarized in a congestion barcode, which can be constructed directly from\ntraffic datasets commonly used for navigation. As an initial demonstration, we\nillustrate the main technique on a publicly available traffic dataset in a\nneighborhood in New York City.\n', 'title': 'Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology'}]","Title: Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology

Abstract: This work presents a new method to quantify connectivity in transportation networks. Inspired by the field of topological data analysis, we propose a novel approach to explore the robustness of road network connectivity in the presence of congestion on the roadway. The robustness of the pattern is summarized in a congestion barcode, which can be constructed directly from traffic datasets commonly used for navigation. As an initial demonstration, we illustrate the main technique on a publicly available traffic dataset in a neighborhood in New York City."
"Using Floer cohomology and Schubert cycles, what is proven about Lagrangian tori in full flag manifolds?",9b931a93,"[{'abstract': '  Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of\nGelfand-Cetlin systems over complex partial flag manifolds, we provide a\ncomplete description of the topology of Gelfand-Cetlin fibers. We prove that\nall fibers are \\emph{smooth} isotropic submanifolds and give a complete\ndescription of the fiber to be Lagrangian in terms of combinatorics of\nGelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian\nfibers. After a few combinatorial and numercal tests for the displaceability,\nusing the bulk-deformation of Floer cohomology by Schubert cycles, we prove\nthat every full flag manifold $\\mathcal{F}(n)$ ($n \\geq 3$) with a monotone\nKirillov-Kostant-Souriau symplectic form carries a continuum of\nnon-displaceable Lagrangian tori which degenerates to a non-torus fiber in the\nHausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\\mathcal{F}(3)$\nis non-displaceable the question of which was raised by Nohara-Ueda who\ncomputed its Floer cohomology to be vanishing.\n', 'title': 'Lagrangian fibers of Gelfand-Cetlin systems'}]","Title: Lagrangian fibers of Gelfand-Cetlin systems

Abstract: Motivated by the study of Nishinou-Nohara-Ueda on the Floer theory of Gelfand-Cetlin systems over complex partial flag manifolds, we provide a complete description of the topology of Gelfand-Cetlin fibers. We prove that all fibers are smooth isotropic submanifolds and give a complete description of the fiber to be Lagrangian in terms of combinatorics of Gelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian fibers. After a few combinatorial and numerical tests for the displaceability, using the bulk-deformation of Floer cohomology by Schubert cycles, we prove that every full flag manifold $\mathcal{F}(n)$ ($n \geq 3$) with a monotone Kirillov-Kostant-Souriau symplectic form carries a continuum of non-displaceable Lagrangian tori which degenerates to a non-torus fiber in the Hausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\mathcal{F}(3)$ is non-displaceable the question of which was raised by Nohara-Ueda who computed its Floer cohomology to be vanishing."
What specific observations of the Sagittarius dwarf galaxy and its tidal stream were used to constrain the model?,eafdfaf6,"[{'abstract': '  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy\noffer a unique way to constrain the shape of galactic gravitational potentials.\nSuch streams can be used as leaning tower gravitational experiments on galactic\nscales. The most well motivated modification of gravity proposed as an\nalternative to dark matter on galactic scales is Milgromian dynamics (MOND),\nand we present here the first ever N-body simulations of the dynamical\nevolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a\nrealistic baryonic mass model for the Milky Way, we attempt to reproduce the\npresent-day spatial and kinematic structure of the Sagittarius dwarf and its\nimmense tidal stream that wraps around the Milky Way. With very little freedom\non the original structure of the progenitor, constrained by the total\nluminosity of the Sagittarius structure and by the observed stellar mass-size\nrelation for isolated dwarf galaxies, we find reasonable agreement between our\nsimulations and observations of this system. The observed stellar velocities in\nthe leading arm can be reproduced if we include a massive hot gas corona around\nthe Milky Way that is flattened in the direction of the principal plane of its\nsatellites. This is the first time that tidal dissolution in MOND has been\ntested rigorously at these mass and acceleration scales.\n', 'title': 'Stellar streams as gravitational experiments I. The case of Sagittarius'}]","Title: Stellar streams as gravitational experiments I. The case of Sagittarius

Abstract:   Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy
offer a unique way to constrain the shape of galactic gravitational potentials.
Such streams can be used as leaning tower gravitational experiments on galactic
scales. The most well motivated modification of gravity proposed as an
alternative to dark matter on galactic scales is Milgromian dynamics (MOND),
and we present here the first ever N-body simulations of the dynamical
evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a
realistic baryonic mass model for the Milky Way, we attempt to reproduce the
present-day spatial and kinematic structure of the Sagittarius dwarf and its
immense tidal stream that wraps around the Milky Way. With very little freedom
on the original structure of the progenitor, constrained by the total
luminosity of the Sagittarius structure and by the observed stellar mass-size
relation for isolated dwarf galaxies, we find reasonable agreement between our
simulations and observations of this system. The observed stellar velocities in
the leading arm can be reproduced if we include a massive hot gas corona around
the Milky Way that is flattened in the direction of the principal plane of its
satellites. This is the first time that tidal dissolution in MOND has been
tested rigorously at these mass and acceleration scales.

The specific observations of the Sagittarius dwarf galaxy and its tidal stream used to constrain the model are:

* Total luminosity of the Sagittarius structure
* Observed stellar mass-size relation for isolated dwarf galaxies
* Present-day spatial and kinematic structure of the Sagittarius dwarf and its tidal stream
* Observed stellar velocities in the leading arm of the tidal stream."
What are the necessary conditions for a normal lambda-distributive lambda^+ aronszajn tree to exist?,69b3dd6c,"[{'abstract': '  Ben-David and Shelah proved that if $\\lambda$ is a singular strong-limit\ncardinal and $2^\\lambda=\\lambda^+$, then $\\square^*_\\lambda$ entails the\nexistence of a normal $\\lambda$-distributive $\\lambda^+$-Aronszajn tree. Here,\nit is proved that the same conclusion remains valid after replacing the\nhypothesis $\\square^*_\\lambda$ by $\\square(\\lambda^+,{<}\\lambda)$.\nAs $\\square(\\lambda^+,{<}\\lambda)$ does not impose a bound on the order-type\nof the witnessing clubs, our construction is necessarily different from that of\nBen-David and Shelah, and instead uses walks on ordinals augmented with club\nguessing.\nA major component of this work is the study of postprocessing functions and\ntheir effect on square sequences. A byproduct of this study is the finding that\nfor $\\kappa$ regular uncountable, $\\square(\\kappa)$ entails the existence of a\npartition of $\\kappa$ into $\\kappa$ many fat sets. When contrasted with a\nclassic model of Magidor, this shows that it is equiconsistent with the\nexistence of a weakly compact cardinal that $\\omega_2$ cannot be split into two\nfat sets.\n', 'title': 'Distributive Aronszajn trees'}]","Title: Distributive Aronszajn trees

Abstract: Ben-David and Shelah proved that if $\lambda$ is a singular strong-limit
cardinal and $2^\lambda=\lambda^+$, then $\square^*_\lambda$ entails the
existence of a normal $\lambda$-distributive $\lambda^+$-Aronszajn tree. Here,
it is proved that the same conclusion remains valid after replacing the
hypothesis $\square^*_\lambda$ by $\square(\lambda^+,{<}\lambda)$."
In what type of graph is the Mahler measure of its Laplacian determinant polynomial the growth rate of the complexity of finite quotients?,ed3926dc,"[{'abstract': ""  The (torsion) complexity of a finite edge-weighted graph is defined to be the\norder of the torsion subgroup of the abelian group presented by its Laplacian\nmatrix. When G is d-periodic (i.e., G has a free action of the rank-d free\nabelian group by graph automorphisms, with finite quotient) the Mahler measure\nof its Laplacian determinant polynomial is the growth rate of the complexity of\nfinite quotients of G. Lehmer's question, an open question about the roots of\nmonic integral polynomials, is equivalent to a question about the complexity\ngrowth of edge-weighted 1-periodic graphs.\n"", 'title': 'Graph complexity and Mahler measure'}]","The answer can be found in the first abstract:

Title: Graph complexity and Mahler measure

Abstract:   The (torsion) complexity of a finite edge-weighted graph is defined to be the
order of the torsion subgroup of the abelian group presented by its Laplacian
matrix. When G is d-periodic (i.e., G has a free action of the rank-d free
abelian group by graph automorphisms, with finite quotient) the Mahler measure
of its Laplacian determinant polynomial is the growth rate of the complexity of
finite quotients of G. Lehmer's question, an open question about the roots of
monic integral polynomials, is equivalent to a question about the complexity
growth of edge-weighted 1-periodic graphs."
What are techniques for simplifying the analysis of high-order linear time-periodic dynamical systems?,b993ab04,"[{'abstract': '  Linear time-periodic (LTP) dynamical systems frequently appear in the\nmodeling of phenomena related to fluid dynamics, electronic circuits, and\nstructural mechanics via linearization centered around known periodic orbits of\nnonlinear models. Such LTP systems can reach orders that make repeated\nsimulation or other necessary analysis prohibitive, motivating the need for\nmodel reduction.\nWe develop here an algorithmic framework for constructing reduced models that\nretains the linear time-periodic structure of the original LTP system. Our\napproach generalizes optimal approaches that have been established previously\nfor linear time-invariant (LTI) model reduction problems. We employ an\nextension of the usual H2 Hardy space defined for the LTI setting to\ntime-periodic systems and within this broader framework develop an a posteriori\nerror bound expressible in terms of related LTI systems. Optimization of this\nbound motivates our algorithm. We illustrate the success of our method on two\nnumerical examples.\n', 'title': 'Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework'}]","It seems like you're looking for techniques to simplify the analysis of high-order linear time-periodic dynamical systems.

Based on the context, I think I can help you with that.󠁧󠁢󠁥󠁣󠁴󠁿

Title: Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework

Abstract: Linear time-periodic (LTP) dynamical systems frequently appear in the modeling of phenomena related to fluid dynamics, electronic circuits, and structural mechanics via linearization centered around known periodic orbits of nonlinear models. Such LTP systems can reach orders that make repeated simulation or other necessary analysis prohibitive, motivating the need for model reduction. We develop here an algorithmic framework for constructing reduced models that retains the linear time-periodic structure of the original LTP system. Our approach generalizes optimal approaches that have been established previously for linear time-invariant (LTI) model reduction problems. We employ an extension of the usual H2 Hardy space defined for the LTI setting to time-periodic systems and within this broader framework develop an a posteriori error bound expressible in terms of related LTI systems. Optimization of this bound motivates our algorithm. We illustrate the success of our method on two numerical examples.

This research paper proposes an algorithmic framework for model reduction, which can help simplify the analysis of high-order linear time-periodic dynamical systems."
How does the GMU model determine the influence of different data modalities?,a91bce10,"[{'abstract': '  This paper presents a novel model for multimodal learning based on gated\nneural networks. The Gated Multimodal Unit (GMU) model is intended to be used\nas an internal unit in a neural network architecture whose purpose is to find\nan intermediate representation based on a combination of data from different\nmodalities. The GMU learns to decide how modalities influence the activation of\nthe unit using multiplicative gates. It was evaluated on a multilabel scenario\nfor genre classification of movies using the plot and the poster. The GMU\nimproved the macro f-score performance of single-modality approaches and\noutperformed other fusion strategies, including mixture of experts models.\nAlong with this work, the MM-IMDb dataset is released which, to the best of our\nknowledge, is the largest publicly available multimodal dataset for genre\nprediction on movies.\n', 'title': 'Gated Multimodal Units for Information Fusion'}]","Title: Gated Multimodal Units for Information Fusion

Abstract: This paper presents a novel model for multimodal learning based on gated
neural networks. The Gated Multimodal Unit (GMU) model is intended to be used
as an internal unit in a neural network architecture whose purpose is to find
an intermediate representation based on a combination of data from different
modalities. The GMU learns to decide how modalities influence the activation of
the unit using multiplicative gates. It was evaluated on a multilabel scenario
for genre classification of movies using the plot and the poster. The GMU
improved the macro f-score performance of single-modality approaches and
outperformed other fusion strategies, including mixture of experts models.
Along with this work, the MM-IMDb dataset is released which, to the best of our
knowledge, is the largest publicly available multimodal dataset for genre
prediction on movies.

The GMU model determines the influence of different data modalities by learning to decide how modalities influence the activation of the unit using multiplicative gates."
How was the effectiveness of the sparse learning approach validated?,331b62b5,"[{'abstract': '  In this paper, we propose an optimization-based sparse learning approach to\nidentify the set of most influential reactions in a chemical reaction network.\nThis reduced set of reactions is then employed to construct a reduced chemical\nreaction mechanism, which is relevant to chemical interaction network modeling.\nThe problem of identifying influential reactions is first formulated as a\nmixed-integer quadratic program, and then a relaxation method is leveraged to\nreduce the computational complexity of our approach. Qualitative and\nquantitative validation of the sparse encoding approach demonstrates that the\nmodel captures important network structural properties with moderate\ncomputational load.\n', 'title': 'A Data-Driven Sparse-Learning Approach to Model Reduction in Chemical Reaction Networks'}]","Title: A Data-Driven Sparse-Learning Approach to Model Reduction in Chemical Reaction Networks

Abstract: In this paper, we propose an optimization-based sparse learning approach to identify the set of most influential reactions in a chemical reaction network. This reduced set of reactions is then employed to construct a reduced chemical reaction mechanism, which is relevant to chemical interaction network modeling. The problem of identifying influential reactions is first formulated as a mixed-integer quadratic program, and then a relaxation method is leveraged to reduce the computational complexity of our approach. Qualitative and quantitative validation of the sparse encoding approach demonstrates that the model captures important network structural properties with moderate computational load."
Why is a systematic approach to quantum theory important for physics studies?,0797fc48,"[{'abstract': '  The ongoing progress in quantum theory emphasizes the crucial role of the\nvery basic principles of quantum theory. However, this is not properly followed\nin teaching quantum mechanics on the graduate and undergraduate levels of\nphysics studies. The existing textbooks typically avoid the axiomatic\npresentation of the theory. We emphasize usefulness of the systematic,\naxiomatic approach to the basics of quantum theory as well as its importance in\nthe light of the modern scientific-research context.\n', 'title': 'Axiomatic quantum mechanics: Necessity and benefits for the physics studies'}]","Title: Axiomatic quantum mechanics: Necessity and benefits for the physics studies

Abstract: The ongoing progress in quantum theory emphasizes the crucial role of the very basic principles of quantum theory. However, this is not properly followed in teaching quantum mechanics on the graduate and undergraduate levels of physics studies. The existing textbooks typically avoid the axiomatic presentation of the theory. We emphasize usefulness of the systematic, axiomatic approach to the basics of quantum theory as well as its importance in the light of the modern scientific-research context."
What are the advantages of reimplementing the subtract-with-borrow random number generator as a linear congruential generator?,b11ac4a6,"[{'abstract': '  The most popular and widely used subtract-with-borrow generator, also known\nas RANLUX, is reimplemented as a linear congruential generator using large\ninteger arithmetic with the modulus size of 576 bits. Modern computers, as well\nas the specific structure of the modulus inferred from RANLUX, allow for the\ndevelopment of a fast modular multiplication -- the core of the procedure. This\nwas previously believed to be slow and have too high cost in terms of computing\nresources. Our tests show a significant gain in generation speed which is\ncomparable with other fast, high quality random number generators. An\nadditional feature is the fast skipping of generator states leading to a\nseeding scheme which guarantees the uniqueness of random number sequences.\n', 'title': 'A revision of the subtract-with-borrow random number generators'}]","Title: A revision of the subtract-with-borrow random number generators

Abstract: The most popular and widely used subtract-with-borrow generator, also known as RANLUX, is reimplemented as a linear congruential generator using large integer arithmetic with the modulus size of 576 bits. Modern computers, as well as the specific structure of the modulus inferred from RANLUX, allow for the development of a fast modular multiplication -- the core of the procedure. This was previously believed to be slow and have too high cost in terms of computing resources. Our tests show a significant gain in generation speed which is comparable with other fast, high quality random number generators. An additional feature is the fast skipping of generator states leading to a seeding scheme which guarantees the uniqueness of random number sequences."
"What type of number must n be for the line graph of Qn(1,2) to be a Cayley graph?",94b11944,"[{'abstract': '  Let $n >3$ and $ 0< k < \\frac{n}{2} $ be integers. In this paper, we\ninvestigate some algebraic properties of the line graph of the graph $\n{Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$\nwhich is induced by the set of vertices of weights $k$ and $k+1$. In the first\nstep, we determine the automorphism groups of these graphs for all values of\n$k$. In the second step, we study Cayley properties of the line graph of these\ngraphs. In particular, we show that for $ k>2, $ if $ 2k+1 \\neq n$, then the\nline graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley\ngraph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a\nCayley graph if and only if $ n$ is a power of a prime $p$.\n', 'title': 'Cayley properties of the line graphs induced by of consecutive layers of the hypercube'}]","Title: Cayley properties of the line graphs induced by of consecutive layers of the hypercube

Abstract: Let $n >3$ and $ 0< k < \frac{n}{2} $ be integers. In this paper, we investigate some algebraic properties of the line graph of the graph $ {Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$ which is induced by the set of vertices of weights $k$ and $k+1$. In the first step, we determine the automorphism groups of these graphs for all values of $k$. In the second step, we study Cayley properties of the line graph of these graphs. In particular, we show that for $ k>2, $ if $ 2k+1 \neq n$, then the line graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley graph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a Cayley graph if and only if $ n$ is a power of a prime $p$.

According to the abstract, the line graph of Qn(1,2) is a Cayley graph if and only if n is a power of a prime p."
What is a quick and easy method for boosting classification accuracy in deep learning models?,ebe4826c,"[{'abstract': '  We found an easy and quick post-learning method named ""Icing on the Cake"" to\nenhance a classification performance in deep learning. The method is that we\ntrain only the final classifier again after an ordinary training is done.\n', 'title': 'Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning'}]","I think I can help you with that!

Title: Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning

Abstract: We found an easy and quick post-learning method named ""Icing on the Cake"" to enhance a classification performance in deep learning. The method is that we train only the final classifier again after an ordinary training is done."
What are the challenges of using voice signals for automatic assessment of voice disorders?,9902e1f7,"[{'abstract': '  Many people are suffering from voice disorders, which can adversely affect\nthe quality of their lives. In response, some researchers have proposed\nalgorithms for automatic assessment of these disorders, based on voice signals.\nHowever, these signals can be sensitive to the recording devices. Indeed, the\nchannel effect is a pervasive problem in machine learning for healthcare. In\nthis study, we propose a detection system for pathological voice, which is\nrobust against the channel effect. This system is based on a bidirectional LSTM\nnetwork. To increase the performance robustness against channel mismatch, we\nintegrate domain adversarial training (DAT) to eliminate the differences\nbetween the devices. When we train on data recorded on a high-quality\nmicrophone and evaluate on smartphone data without labels, our robust detection\nsystem increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target\nsample labels). To the best of our knowledge, this is the first study applying\nunsupervised domain adaptation to pathological voice detection. Notably, our\nsystem does not need target device sample labels, which allows for\ngeneralization to many new devices.\n', 'title': 'Robustness against the channel effect in pathological voice detection'}]","Based on the context, I found an answer to your question.

Title: Robustness against the channel effect in pathological voice detection

Abstract: Many people are suffering from voice disorders, which can adversely affect the quality of their lives. In response, some researchers have proposed algorithms for automatic assessment of these disorders, based on voice signals. However, these signals can be sensitive to the recording devices. Indeed, the channel effect is a pervasive problem in machine learning for healthcare.

The challenge mentioned is that voice signals can be sensitive to the recording devices, which is referred to as the ""channel effect"". This implies that the accuracy of automatic assessment of voice disorders may be affected by the device used to record the voice signals."
How does a state sum over triangulations contribute to calculating quantum invariants of three manifolds?,c7bef549,"[{'abstract': '  We define a family of quantum invariants of closed oriented $3$-manifolds\nusing spherical multi-fusion categories. The state sum nature of this invariant\nleads directly to $(2+1)$-dimensional topological quantum field theories\n($\\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury\n($\\text{TVBW}$) $\\text{TQFT}$s from spherical fusion categories. The invariant\nis given as a state sum over labeled triangulations, which is mostly parallel\nto, but richer than the $\\text{TVBW}$ approach in that here the labels live not\nonly on $1$-simplices but also on $0$-simplices. It is shown that a\nmulti-fusion category in general cannot be a spherical fusion category in the\nusual sense. Thus we introduce the concept of a spherical multi-fusion category\nby imposing a weakened version of sphericity. Besides containing the\n$\\text{TVBW}$ theory, our construction also includes the recent higher gauge\ntheory $(2+1)$-$\\text{TQFT}$s given by Kapustin and Thorngren, which was not\nknown to have a categorical origin before.\n', 'title': 'State Sum Invariants of Three Manifolds from Spherical Multi-fusion Categories'}]","Title: State Sum Invariants of Three Manifolds from Spherical Multi-fusion Categories

Abstract:   We define a family of quantum invariants of closed oriented $3$-manifolds
using spherical multi-fusion categories. The state sum nature of this invariant
leads directly to $(2+1)$-dimensional topological quantum field theories
($\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury
($\text{TVBW}$) $\text{TQFT}$s from spherical fusion categories. The invariant
is given as a state sum over labeled triangulations, which is mostly parallel
to, but richer than the $\text{TVBW}$ approach in that here the labels live not
only on $1$-simplices but also on $0$-simplices. It is shown that a
multi-fusion category in general cannot be a spherical fusion category in the
usual sense. Thus we introduce the concept of a spherical multi-fusion category
by imposing a weakened version of sphericity. Besides containing the
$\text{TVBW}$ theory, our construction also includes the recent higher gauge
theory $(2+1)$-$\text{TQFT}$s given by Kapustin and Thorngren, which was not
known to have a categorical origin before.

The state sum over triangulations contributes to calculating quantum invariants of three manifolds by providing a way to define a family of quantum invariants using spherical multi-fusion categories."
What is the relationship between the size of blocking objects and the reliability of a cellular network?,bda0c341,"[{'abstract': ""  Blocking objects (blockages) between a transmitter and receiver cause\nwireless communication links to transition from line-of-sight (LOS) to\nnon-line-of-sight (NLOS) propagation, which can greatly reduce the received\npower, particularly at higher frequencies such as millimeter wave (mmWave). We\nconsider a cellular network in which a mobile user attempts to connect to two\nor more base stations (BSs) simultaneously, to increase the probability of at\nleast one LOS link, which is a form of macrodiversity. We develop a framework\nfor determining the LOS probability as a function of the number of BSs, when\ntaking into account the correlation between blockages: for example, a single\nblockage close to the device -- including the user's own body -- could block\nmultiple BSs. We consider the impact of the size of blocking objects on the\nsystem reliability probability and show that macrodiversity gains are higher\nwhen the blocking objects are small. We also show that the BS density must\nscale as the square of the blockage density to maintain a given level of\nreliability.\n"", 'title': 'Macro diversity in Cellular Networks with Random Blockages'}]","Based on the provided context, I found the relevant information in the following paper:

Title: Macro diversity in Cellular Networks with Random Blockages

Abstract: Blocking objects (blockages) between a transmitter and receiver cause wireless communication links to transition from line-of-sight (LOS) to non-line-of-sight (NLOS) propagation, which can greatly reduce the received power, particularly at higher frequencies such as millimeter wave (mmWave). We consider a cellular network in which a mobile user attempts to connect to two or more base stations (BSs) simultaneously, to increase the probability of at least one LOS link, which is a form of macrodiversity. We develop a framework for determining the LOS probability as a function of the number of BSs, when taking into account the correlation between blockages: for example, a single blockage close to the device -- including the user's own body -- could block multiple BSs. We consider the impact of the size of blocking objects on the system reliability probability and show that macrodiversity gains are higher when the blocking objects are small. We also show that the BS density must scale as the square of the blockage density to maintain a given level of reliability.

According to this paper, the relationship between the size of blocking objects and the reliability of a cellular network is that macrodiversity gains are higher when the blocking objects are small."
What are the computational challenges associated with finding the primitive element of an extended field in the context of exponent lattice basis computation?,5f1f4d73,"[{'abstract': '  Computing a basis for the exponent lattice of algebraic numbers is a basic\nproblem in the field of computational number theory with applications to many\nother areas. The main cost of a well-known algorithm\n\\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on\ncomputing the primitive element of the extended field generated by the given\nalgebraic numbers. When the extended field is of large degree, the problem\nseems intractable by the tool implementing the algorithm. In this paper, a\nspecial kind of exponent lattice basis is introduced. An important feature of\nthe basis is that it can be inductively constructed, which allows us to deal\nwith the given algebraic numbers one by one when computing the basis. Based on\nthis, an effective framework for constructing exponent lattice basis is\nproposed. Through computing a so-called pre-basis first and then solving some\nlinear Diophantine equations, the basis can be efficiently constructed. A new\ncertificate for multiplicative independence and some techniques for decreasing\ndegrees of algebraic numbers are provided to speed up the computation. The new\nalgorithm has been implemented with Mathematica and its effectiveness is\nverified by testing various examples. Moreover, the algorithm is applied to\nprogram verification for finding invariants of linear loops.\n', 'title': 'An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers'}]","Title: An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers

Abstract: Computing a basis for the exponent lattice of algebraic numbers is a basic problem in the field of computational number theory with applications to many other areas. The main cost of a well-known algorithm solving the problem is on computing the primitive element of the extended field generated by the given algebraic numbers. When the extended field is of large degree, the problem seems intractable by the tool implementing the algorithm.

The computational challenge mentioned in this paper is that the problem of computing the primitive element of the extended field becomes intractable when the extended field is of large degree. This suggests that the difficulty lies in handling high-degree extended fields, which can be computationally expensive."
Are there papers that extend the study of balanced graphs in random distance graphs to general graphs?,c548a393,"[{'abstract': '  In previous papers, threshold probabilities for the properties of a random\ndistance graph to contain strictly balanced graphs were found. We extend this\nresult to arbitrary graphs and prove that the number of copies of a strictly\nbalanced graph has asymptotically Poisson distribution at the threshold.\n', 'title': 'Small subgraphs and their extensions in a random distance graph'}]","I think I have found the paper you are looking for!

Title: Small subgraphs and their extensions in a random distance graph

Abstract: In previous papers, threshold probabilities for the properties of a random distance graph to contain strictly balanced graphs were found. We extend this result to arbitrary graphs and prove that the number of copies of a strictly balanced graph has asymptotically Poisson distribution at the threshold."
What type of transition is responsible for the terahertz optical diode effect in FeZnMo$_3$O$_8$?,1c559d3b,"[{'abstract': '  We present a terahertz spectroscopic study of polar ferrimagnet\nFeZnMo$_3$O$_8$. Our main finding is a giant high-temperature optical diode\neffect, or nonreciprocal directional dichroism, where the transmitted light\nintensity in one direction is over 100 times lower than intensity transmitted\nin the opposite direction. The effect takes place in the paramagnetic phase\nwith no long-range magnetic order in the crystal, which contrasts sharply with\nall existing reports of the terahertz optical diode effect in other\nmagnetoelectric materials, where the long-range magnetic ordering is a\nnecessary prerequisite. In \\fzmo, the effect occurs resonantly with a strong\nmagnetic dipole active transition centered at 1.27 THz and assigned as electron\nspin resonance between the eigenstates of the single-ion anisotropy\nHamiltonian. We propose that the optical diode effect in paramagnetic\nFeZnMo$_3$O$_8$ is driven by signle-ion terms in magnetoelectric free energy.\n', 'title': 'High-temperature terahertz optical diode effect without magnetic order in polar FeZnMo$_3$O$_8$'}]","Title: High-temperature terahertz optical diode effect without magnetic order in polar FeZnMo$_3$O$_8$

Abstract: We present a terahertz spectroscopic study of polar ferrimagnet 
FeZnMo$_3$O$_8$. Our main finding is a giant high-temperature optical diode 
effect, or nonreciprocal directional dichroism, where the transmitted light 
intensity in one direction is over 100 times lower than intensity transmitted 
in the opposite direction. The effect takes place in the paramagnetic phase 
with no long-range magnetic order in the crystal, which contrasts sharply with 
all existing reports of the terahertz optical diode effect in other 
magnetoelectric materials, where the long-range magnetic ordering is a 
necessary prerequisite. In FeZnMo$_3$O$_8$, the effect occurs resonantly with a 
strong magnetic dipole active transition centered at 1.27 THz and assigned as 
electron spin resonance between the eigenstates of the single-ion anisotropy 
Hamiltonian. We propose that the optical diode effect in paramagnetic 
FeZnMo$_3$O$_8$ is driven by single-ion terms in magnetoelectric free energy.

The transition responsible for the terahertz optical diode effect in FeZnMo$_3$O$_8$ is an electron spin resonance between the eigenstates of the single-ion anisotropy Hamiltonian, which occurs at 1.27 THz."
What two observational techniques were used in the study to verify redMaPPer's centring algorithm?,c00f55db,"[{'abstract': '  Galaxy cluster centring is a key issue for precision cosmology studies using\ngalaxy surveys. Mis-identification of central galaxies causes systematics in\nvarious studies such as cluster lensing, satellite kinematics, and galaxy\nclustering. The red-sequence Matched-filter Probabilistic Percolation\n(redMaPPer) estimates the probability that each member galaxy is central from\nphotometric information rather than specifying one central galaxy. The\nredMaPPer estimates can be used for calibrating the off-centring effect,\nhowever, the centring algorithm has not previously been well-tested. We test\nthe centring probabilities of redMaPPer cluster catalog using the projected\ncross correlation between redMaPPer clusters with photometric red galaxies and\ngalaxy-galaxy lensing. We focus on the subsample of redMaPPer clusters in which\nthe redMaPPer central galaxies (RMCGs) are not the brightest member galaxies\n(BMEM) and both of them have spectroscopic redshift. This subsample represents\nnearly 10% of the whole cluster sample. We find a clear difference in the\ncross-correlation measurements between RMCGs and BMEMs, and the estimated\ncentring probability is 74$\\pm$10% for RMCGs and 13$\\pm$4% for BMEMs in the\nGaussian offset model and 78$\\pm$9% for RMCGs and 5$\\pm$5% for BMEMs in the NFW\noffset model. These values are in agreement with the centring probability\nvalues reported by redMaPPer (75% for RMCG and 10% for BMEMs) within 1$\\sigma$.\nOur analysis provides a strong consistency test of the redMaPPer centring\nprobabilities. Our results suggest that redMaPPer centring probabilities are\nreliably estimated. We confirm that the brightest galaxy in the cluster is not\nalways the central galaxy as has been shown in previous works.\n', 'title': 'Testing redMaPPer centring probabilities using galaxy clustering and galaxy-galaxy lensing'}]","Title: Testing redMaPPer centring probabilities using galaxy clustering and galaxy-galaxy lensing

Abstract: Galaxy cluster centring is a key issue for precision cosmology studies using galaxy surveys. Mis-identification of central galaxies causes systematics in various studies such as cluster lensing, satellite kinematics, and galaxy clustering. The red-sequence Matched-filter Probabilistic Percolation (redMaPPer) estimates the probability that each member galaxy is central from photometric information rather than specifying one central galaxy. The redMaPPer estimates can be used for calibrating the off-centring effect, however, the centring algorithm has not previously been well-tested. We test the centring probabilities of redMaPPer cluster catalog using the projected cross correlation between redMaPPer clusters with photometric red galaxies and galaxy-galaxy lensing. We focus on the subsample of redMaPPer clusters in which the redMaPPer central galaxies (RMCGs) are not the brightest member galaxies (BMEM) and both of them have spectroscopic redshift. This subsample represents nearly 10% of the whole cluster sample. We find a clear difference in the cross-correlation measurements between RMCGs and BMEMs, and the estimated centring probability is 74$\pm$10% for RMCGs and 13$\pm$4% for BMEMs in the Gaussian offset model and 78$\pm$9% for RMCGs and 5$\pm$5% for BMEMs in the NFW offset model. These values are in agreement with the centring probability values reported by redMaPPer (75% for RMCG and 10% for BMEMs) within 1$\sigma$. Our analysis provides a strong consistency test of the redMaPPer centring probabilities. Our results suggest that redMaPPer centring probabilities are reliably estimated. We confirm that the brightest galaxy in the cluster is not always the central galaxy as has been shown in previous works.

The two observational techniques used to verify redMaPPer's centring algorithm are galaxy clustering and galaxy-galaxy lensing."
Which countries were studied in this research on YouTube for kids channels?,5e8be66e,"[{'abstract': ""  Online video services, messaging systems, games and social media services are\ntremendously popular among young people and children in many countries. Most of\nthe digital services offered on the internet are advertising funded, which\nmakes advertising ubiquitous in children's everyday life. To understand the\nimpact of advertising-based digital services on children, we study the\ncollective behavior of users of YouTube for kids channels and present the\ndemographics of a large number of users. We collected data from 12,848 videos\nfrom 17 channels in US and UK and 24 channels in Brazil. The channels in\nEnglish have been viewed more than 37 billion times. We also collected more\nthan 14 million comments made by users. Based on a combination of text-analysis\nand face recognition tools, we show the presence of racial and gender biases in\nour large sample of users. We also identify children actively using YouTube,\nalthough the minimum age for using the service is 13 years in most countries.\nWe provide comparisons of user behavior among the three countries, which\nrepresent large user populations in the global North and the global South.\n"", 'title': 'Characterizing videos, audience and advertising in Youtube channels for kids'}]","Title: Characterizing videos, audience and advertising in Youtube channels for kids

Abstract: Online video services, messaging systems, games and social media services are tremendously popular among young people and children in many countries. Most of the digital services offered on the internet are advertising funded, which makes advertising ubiquitous in children's everyday life. To understand the impact of advertising-based digital services on children, we study the collective behavior of users of YouTube for kids channels and present the demographics of a large number of users. We collected data from 12,848 videos from 17 channels in US and UK and 24 channels in Brazil. The channels in English have been viewed more than 37 billion times. We also collected more than 14 million comments made by users. Based on a combination of text-analysis and face recognition tools, we show the presence of racial and gender biases in our large sample of users. We also identify children actively using YouTube, although the minimum age for using the service is 13 years in most countries. We provide comparisons of user behavior among the three countries, which represent large user populations in the global North and the global South.

The countries studied in this research are the United States (US), the United Kingdom (UK), and Brazil."
Which type of star is found to excite the Ap2-1 site within the molecular cloud G35.20-0.74?,0788e34c,"[{'abstract': '  To probe the star-formation (SF) processes, we present results of an analysis\nof the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency\nobservations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.\nAn almost horseshoe-like structure embedded within the MCG35.2 is evident in\nthe infrared and millimeter images and harbors the previously known sites,\nultra-compact/hyper-compact G35.20$-$0.74N H\\,{\\sc ii} region, Ap2-1, and\nMercer 14 at its base. The site, Ap2-1 is found to be excited by a radio\nspectral type of B0.5V star where the distribution of 20 cm and H$\\alpha$\nemission is surrounded by the extended molecular hydrogen emission. Using the\n{\\it Herschel} 160-500 $\\mu$m and photometric 1-24 $\\mu$m data analysis,\nseveral embedded clumps and clusters of young stellar objects (YSOs) are\ninvestigated within the MCG35.2, revealing the SF activities. Majority of the\nYSOs clusters and massive clumps (500-4250 M$_{\\odot}$) are seen toward the\nhorseshoe-like structure. The position-velocity analysis of $^{13}$CO emission\nshows a blue-shifted peak (at 33 km s$^{-1}$) and a red-shifted peak (at 37 km\ns$^{-1}$) interconnected by lower intensity intermediated velocity emission,\ntracing a broad bridge feature. The presence of such broad bridge feature\nsuggests the onset of a collision between molecular components in the MCG35.2.\nA noticeable change in the H-band starlight mean polarization angles has also\nbeen observed in the MCG35.2, probably tracing the interaction between\nmolecular components. Taken together, it seems that the cloud-cloud collision\nprocess has influenced the birth of massive stars and YSOs clusters in the\nMCG35.2.\n', 'title': 'Star Formation Activity in the molecular cloud G35.20$-$0.74: onset of cloud-cloud collision'}]","Title: Star Formation Activity in the molecular cloud G35.20$-$0.74: onset of cloud-cloud collision

Abstract: 
To probe the star-formation (SF) processes, we present results of an analysis
of the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency
observations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.
An almost horseshoe-like structure embedded within the MCG35.2 is evident in
the infrared and millimeter images and harbors the previously known sites,
ultra-compact/hyper-compact G35.20$-$0.74N H\,{\sc ii} region, Ap2-1, and
Mercer 14 at its base. The site, Ap2-1 is found to be excited by a radio
spectral type of B0.5V star where the distribution of 20 cm and H$\alpha$
emission is surrounded by the extended molecular hydrogen emission."
"What are Morse geodesics, and in what type of groups have they previously been found?",10c473bb,"[{'abstract': '  In this paper we exhibit Morse geodesics, often called ""hyperbolic\ndirections"", in infinite unbounded torsion groups. The groups studied are\nlacunary hyperbolic groups and constructed using graded small cancellation\nconditions. In all previously known examples, Morse geodesics were found in\ngroups which also contained Morse elements, infinite order elements whose\ncyclic subgroup gives a Morse quasi-geodesic. Our result presents the first\nexample of a group which contains Morse geodesics but no Morse elements. In\nfact, we show that there is an isometrically embedded $7$-regular tree inside\nsuch groups where every infinite, simple path is a Morse geodesic.\n', 'title': 'Morse geodesics in torsion groups'}]","Title: Morse geodesics in torsion groups

Abstract: In this paper we exhibit Morse geodesics, often called ""hyperbolic directions"", in infinite unbounded torsion groups. The groups studied are lacunary hyperbolic groups and constructed using graded small cancellation conditions. In all previously known examples, Morse geodesics were found in groups which also contained Morse elements, infinite order elements whose cyclic subgroup gives a Morse quasi-geodesic. Our result presents the first example of a group which contains Morse geodesics but no Morse elements. In fact, we show that there is an isometrically embedded $7$-regular tree inside such groups where every infinite, simple path is a Morse geodesic.

According to this abstract, Morse geodesics have previously been found in groups that also contained Morse elements, which are infinite order elements whose cyclic subgroup gives a Morse quasi-geodesic."
What is the relationship between a node's degree and its ability to change state during a diffusion process?,f574294f,"[{'abstract': '  This paper mainly discusses the diffusion on complex networks with\ntime-varying couplings. We propose a model to describe the adaptive diffusion\nprocess of local topological and dynamical information, and find that the\nBarabasi-Albert scale-free network (BA network) is beneficial to the diffusion\nand leads nodes to arrive at a larger state value than other networks do. The\nability of diffusion for a node is related to its own degree. Specifically,\nnodes with smaller degrees are more likely to change their states and reach\nlarger values, while those with larger degrees tend to stick to their original\nstates. We introduce state entropy to analyze the thermodynamic mechanism of\nthe diffusion process, and interestingly find that this kind of diffusion\nprocess is a minimization process of state entropy. We use the inequality\nconstrained optimization method to reveal the restriction function of the\nminimization and find that it has the same form as the Gibbs free energy. The\nthermodynamical concept allows us to understand dynamical processes on complex\nnetworks from a brand-new perspective. The result provides a convenient means\nof optimizing relevant dynamical processes on practical circuits as well as\nrelated complex systems.\n', 'title': 'Adaptive Diffusion Processes of Time-Varying Local Information on Networks'}]","Title: Adaptive Diffusion Processes of Time-Varying Local Information on Networks

Abstract: This paper mainly discusses the diffusion on complex networks with time-varying couplings. We propose a model to describe the adaptive diffusion process of local topological and dynamical information, and find that the Barabasi-Albert scale-free network (BA network) is beneficial to the diffusion and leads nodes to arrive at a larger state value than other networks do. The ability of diffusion for a node is related to its own degree. Specifically, nodes with smaller degrees are more likely to change their states and reach larger values, while those with larger degrees tend to stick to their original states. We introduce state entropy to analyze the thermodynamic mechanism of the diffusion process, and interestingly find that this kind of diffusion process is a minimization process of state entropy. We use the inequality constrained optimization method to reveal the restriction function of the minimization and find that it has the same form as the Gibbs free energy. The thermodynamical concept allows us to understand dynamical processes on complex networks from a brand-new perspective. The result provides a convenient means of optimizing relevant dynamical processes on practical circuits as well as related complex systems."
Which gamma-ray burst catalogue was used in this study?,ae871b07,"[{'abstract': '  We consider the problem related to clustering of gamma-ray bursts (from\n""BATSE"" catalogue) through kernel principal component analysis in which our\nproposed kernel outperforms results of other competent kernels in terms of\nclustering accuracy and we obtain three physically interpretable groups of\ngamma-ray bursts. The effectivity of the suggested kernel in combination with\nkernel principal component analysis in revealing natural clusters in noisy and\nnonlinear data while reducing the dimension of the data is also explored in two\nsimulated data sets.\n', 'title': 'Clustering of Gamma-Ray bursts through kernel principal component analysis'}]","I think I can help you with that!

The answer to your question is:

Title: Clustering of Gamma-Ray bursts through kernel principal component analysis

Abstract: We consider the problem related to clustering of gamma-ray bursts (from ""BATSE"" catalogue) through kernel principal component analysis in which our proposed kernel outperforms results of other competent kernels in terms of clustering accuracy and we obtain three physically interpretable groups of gamma-ray bursts. The effectivity of the suggested kernel in combination with kernel principal component analysis in revealing natural clusters in noisy and nonlinear data while reducing the dimension of the data is also explored in two simulated data sets.

The catalogue used in this study is the ""BATSE"" catalogue."
Automata group with 4 states and 8 letter alphabet,d9ef3320,"[{'abstract': '  We give a new example of an automata group of intermediate growth. It is\ngenerated by an automaton with 4 states on an alphabet with 8 letters. This\nautomata group has exponential activity and its limit space is not simply\nconnected.\n', 'title': 'An automata group of intermediate growth and exponential activity'}]","I think I can help you with that!

Title: An automata group of intermediate growth and exponential activity

Abstract: We give a new example of an automata group of intermediate growth. It is generated by an automaton with 4 states on an alphabet with 8 letters. This automata group has exponential activity and its limit space is not simply connected."
What are the challenges associated with utilizing  electronic health records for patient clustering?,ea693a0b,"[{'abstract': '  Electronic health records (EHR) contain a large variety of information on the\nclinical history of patients such as vital signs, demographics, diagnostic\ncodes and imaging data. The enormous potential for discovery in this rich\ndataset is hampered by its complexity and heterogeneity.\nWe present the first study to assess unsupervised homogenization pipelines\ndesigned for EHR clustering. To identify the optimal pipeline, we tested\naccuracy on simulated data with varying amounts of redundancy, heterogeneity,\nand missingness. We identified two optimal pipelines: 1) Multiple Imputation by\nChained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,\nZ-scoring, and Deep Autoencoders.\n', 'title': 'An Unsupervised Homogenization Pipeline for Clustering Similar Patients using Electronic Health Record Data'}]","Title: An Unsupervised Homogenization Pipeline for Clustering Similar Patients using Electronic Health Record Data

Abstract: Electronic health records (EHR) contain a large variety of information on the clinical history of patients such as vital signs, demographics, diagnostic codes and imaging data. The enormous potential for discovery in this rich dataset is hampered by its complexity and heterogeneity."
In what type of scenarios could a system that learns to succeed on a positive task while failing on a negative task be beneficial?,7693e6f7,"[{'abstract': '  Security, privacy, and fairness have become critical in the era of data\nscience and machine learning. More and more we see that achieving universally\nsecure, private, and fair systems is practically impossible. We have seen for\nexample how generative adversarial networks can be used to learn about the\nexpected private training data; how the exploitation of additional data can\nreveal private information in the original one; and how what looks like\nunrelated features can teach us about each other. Confronted with this\nchallenge, in this paper we open a new line of research, where the security,\nprivacy, and fairness is learned and used in a closed environment. The goal is\nto ensure that a given entity (e.g., the company or the government), trusted to\ninfer certain information with our data, is blocked from inferring protected\ninformation from it. For example, a hospital might be allowed to produce\ndiagnosis on the patient (the positive task), without being able to infer the\ngender of the subject (negative task). Similarly, a company can guarantee that\ninternally it is not using the provided data for any undesired task, an\nimportant goal that is not contradicting the virtually impossible challenge of\nblocking everybody from the undesired task. We design a system that learns to\nsucceed on the positive task while simultaneously fail at the negative one, and\nillustrate this with challenging cases where the positive task is actually\nharder than the negative one being blocked. Fairness, to the information in the\nnegative task, is often automatically obtained as a result of this proposed\napproach. The particular framework and examples open the door to security,\nprivacy, and fairness in very important closed scenarios, ranging from private\ndata accumulation companies like social networks to law-enforcement and\nhospitals.\n', 'title': 'Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems'}]","Title: Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems

Abstract: Security, privacy, and fairness have become critical in the era of data science and machine learning. More and more we see that achieving universally secure, private, and fair systems is practically impossible. We have seen for example how generative adversarial networks can be used to learn about the expected private training data; how the exploitation of additional data can reveal private information in the original one; and how what looks like unrelated features can teach us about each other. Confronted with this challenge, in this paper we open a new line of research, where the security, privacy, and fairness is learned and used in a closed environment. The goal is to ensure that a given entity (e.g., the company or the government), trusted to infer certain information with our data, is blocked from inferring protected information from it. For example, a hospital might be allowed to produce diagnosis on the patient (the positive task), without being able to infer the gender of the subject (negative task). Similarly, a company can guarantee that internally it is not using the provided data for any undesired task, an important goal that is not contradicting the virtually impossible challenge of blocking everybody from the undesired task. We design a system that learns to succeed on the positive task while simultaneously fail at the negative one, and illustrate this with challenging cases where the positive task is actually harder than the negative one being blocked. Fairness, to the information in the negative task, is often automatically obtained as a result of this proposed approach. The particular framework and examples open the door to security, privacy, and fairness in very important closed scenarios, ranging from private data accumulation companies like social networks to law-enforcement and hospitals.

In this scenario, a system that learns to succeed on a positive task while failing on a negative task could be beneficial in scenarios where an entity needs to access certain information but not other sensitive information, such as a hospital making diagnoses without accessing patient gender, or a company using data for a specific task without using it for an undesired task."
How does the forward-backward algorithm perform on infinite dimensional convex minimization problems?,e8aebeac,"[{'abstract': '  We provide a comprehensive study of the convergence of forward-backward\nalgorithm under suitable geometric conditions leading to fast rates. We present\nseveral new results and collect in a unified view a variety of results\nscattered in the literature, often providing simplified proofs. Novel\ncontributions include the analysis of infinite dimensional convex minimization\nproblems, allowing the case where minimizers might not exist. Further, we\nanalyze the relation between different geometric conditions, and discuss novel\nconnections with a priori conditions in linear inverse problems, including\nsource conditions, restricted isometry properties and partial smoothness.\n', 'title': 'Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry'}]","Title: Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry

Abstract: We provide a comprehensive study of the convergence of forward-backward algorithm under suitable geometric conditions leading to fast rates. We present several new results and collect in a unified view a variety of results scattered in the literature, often providing simplified proofs. Novel contributions include the analysis of infinite dimensional convex minimization problems, allowing the case where minimizers might not exist. Further, we analyze the relation between different geometric conditions, and discuss novel connections with a priori conditions in linear inverse problems, including source conditions, restricted isometry properties and partial smoothness.

According to the abstract, the forward-backward algorithm can handle infinite dimensional convex minimization problems, even when minimizers might not exist."
What are the challenges of regression analysis when dealing with spatially dependent outcomes?,c1c78bcb,"[{'abstract': ""  Regression for spatially dependent outcomes poses many challenges, for\ninference and for computation. Non-spatial models and traditional spatial\nmixed-effects models each have their advantages and disadvantages, making it\ndifficult for practitioners to determine how to carry out a spatial regression\nanalysis. We discuss the data-generating mechanisms implicitly assumed by\nvarious popular spatial regression models, and discuss the implications of\nthese assumptions. We propose Bayesian spatial filtering as an approximate\nmiddle way between non-spatial models and traditional spatial mixed models. We\nshow by simulation that our Bayesian spatial filtering model has several\ndesirable properties and hence may be a useful addition to a spatial\nstatistician's toolkit.\n"", 'title': 'Spatial Regression and the Bayesian Filter'}]","Title: Spatial Regression and the Bayesian Filter

Abstract:   Regression for spatially dependent outcomes poses many challenges, for
inference and for computation. Non-spatial models and traditional spatial
mixed-effects models each have their advantages and disadvantages, making it
difficult for practitioners to determine how to carry out a spatial regression
analysis. We discuss the data-generating mechanisms implicitly assumed by
various popular spatial regression models, and discuss the implications of
these assumptions. We propose Bayesian spatial filtering as an approximate
middle way between non-spatial models and traditional spatial mixed models. We
show by simulation that our Bayesian spatial filtering model has several
desirable properties and hence may be a useful addition to a spatial
statistician's toolkit.

In this paper, the authors mention that the challenges of regression analysis when dealing with spatially dependent outcomes are difficulties in inference and computation. Non-spatial models and traditional spatial mixed-effects models have their own advantages and disadvantages, making it hard for practitioners to choose the right approach."
What are the  motivations for developing quantum computers based on topological phases of matter?,151bd875,"[{'abstract': '  In topological quantum computing, information is encoded in ""knotted"" quantum\nstates of topological phases of matter, thus being locked into topology to\nprevent decay. Topological precision has been confirmed in quantum Hall liquids\nby experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum\nmemory. In this survey, we discuss the conceptual development of this\ninterdisciplinary field at the juncture of mathematics, physics and computer\nscience. Our focus is on computing and physical motivations, basic mathematical\nnotions and results, open problems and future directions related to and/or\ninspired by topological quantum computing.\n', 'title': 'Mathematics of Topological Quantum Computing'}]","Title: Mathematics of Topological Quantum Computing

Abstract:   In topological quantum computing, information is encoded in ""knotted"" quantum
states of topological phases of matter, thus being locked into topology to
prevent decay. Topological precision has been confirmed in quantum Hall liquids
by experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum
memory. In this survey, we discuss the conceptual development of this
interdisciplinary field at the juncture of mathematics, physics and computer
science. Our focus is on computing and physical motivations, basic mathematical
notions and results, open problems and future directions related to and/or
inspired by topological quantum computing.

The motivation for developing quantum computers based on topological phases of matter is to harness the topological precision to stabilize quantum memory, as mentioned in the abstract."
What insights does the paper offer about dengue importation routes?,705ca500,"[{'abstract': ""  With approximately half of the world's population at risk of contracting\ndengue, this mosquito-borne disease is of global concern. International\ntravellers significantly contribute to dengue's rapid and large-scale spread by\nimporting the disease from endemic into non-endemic countries. To prevent\nfuture outbreaks and dengue from establishing in non-endemic countries,\nknowledge about the arrival time and location of infected travellers is\ncrucial. We propose a network model that predicts the monthly number of dengue\ninfected air passengers arriving at any given airport. We consider\ninternational air travel volumes, monthly dengue incidence rates and temporal\ninfection dynamics. Our findings shed light onto dengue importation routes and\nreveal country-specific reporting rates that have been until now largely\nunknown.\n"", 'title': 'A global model for predicting the arrival of imported dengue infections'}]","Title: A global model for predicting the arrival of imported dengue infections

Abstract: With approximately half of the world's population at risk of contracting dengue, this mosquito-borne disease is of global concern. International travellers significantly contribute to dengue's rapid and large-scale spread by importing the disease from endemic into non-endemic countries. To prevent future outbreaks and dengue from establishing in non-endemic countries, knowledge about the arrival time and location of infected travellers is crucial. We propose a network model that predicts the monthly number of dengue infected air passengers arriving at any given airport. We consider international air travel volumes, monthly dengue incidence rates and temporal infection dynamics. Our findings shed light onto dengue importation routes and reveal country-specific reporting rates that have been until now largely unknown.

The insight offered by this paper is that it sheds light on dengue importation routes."
What is the relationship between action languages and answer set programming modulo theories?,4f738703,"[{'abstract': '  Both hybrid automata and action languages are formalisms for describing the\nevolution of dynamic systems. This paper establishes a formal relationship\nbetween them. We show how to succinctly represent hybrid automata in an action\nlanguage which in turn is defined as a high-level notation for answer set\nprogramming modulo theories (ASPMT) --- an extension of answer set programs to\nthe first-order level similar to the way satisfiability modulo theories (SMT)\nextends propositional satisfiability (SAT). We first show how to represent\nlinear hybrid automata with convex invariants by an action language modulo\ntheories. A further translation into SMT allows for computing them using SMT\nsolvers that support arithmetic over reals. Next, we extend the representation\nto the general class of non-linear hybrid automata allowing even non-convex\ninvariants. We represent them by an action language modulo ODE (Ordinary\nDifferential Equations), which can be compiled into satisfiability modulo ODE.\nWe developed a prototype system cplus2aspmt based on these translations, which\nallows for a succinct representation of hybrid transition systems that can be\ncomputed effectively by the state-of-the-art SMT solver dReal.\n', 'title': 'Representing Hybrid Automata by Action Language Modulo Theories'}]","I think I can help you with that!

Title: Representing Hybrid Automata by Action Language Modulo Theories

Abstract: Both hybrid automata and action languages are formalisms for describing the evolution of dynamic systems. This paper establishes a formal relationship between them. We show how to succinctly represent hybrid automata in an action language which in turn is defined as a high-level notation for answer set programming modulo theories (ASPMT) --- an extension of answer set programs to the first-order level similar to the way satisfiability modulo theories (SMT) extends propositional satisfiability (SAT). We first show how to represent linear hybrid automata with convex invariants by an action language modulo theories. A further translation into SMT allows for computing them using SMT solvers that support arithmetic over reals. Next, we extend the representation to the general class of non-linear hybrid automata allowing even non-convex invariants. We represent them by an action language modulo ODE (Ordinary Differential Equations), which can be compiled into satisfiability modulo ODE. We developed a prototype system cplus2aspmt based on these translations, which allows for a succinct representation of hybrid transition systems that can be computed effectively by the state-of-the-art SMT solver dReal."
how do affine structures in financial models lead to specific transformation formulas,816dd9b7,"[{'abstract': '  The goal of this survey article is to explain and elucidate the affine\nstructure of recent models appearing in the rough volatility literature, and\nshow how it leads to exponential-affine transform formulas.\n', 'title': 'Affine Rough Models'}]","I think I can help you with that!

Based on the context, I found a match that might interest you:

Title: Affine Rough Models

Abstract: The goal of this survey article is to explain and elucidate the affine structure of recent models appearing in the rough volatility literature, and show how it leads to exponential-affine transform formulas.

This paper seems to be directly addressing your question, exploring the connection between affine structures in financial models and their transformation formulas."
What is a technique to reduce computation in back propagation of traditional neural networks?,ed0d5921,"[{'abstract': '  As traditional neural network consumes a significant amount of computing\nresources during back propagation, \\citet{Sun2017mePropSB} propose a simple yet\neffective technique to alleviate this problem. In this technique, only a small\nsubset of the full gradients are computed to update the model parameters. In\nthis paper we extend this technique into the Convolutional Neural Network(CNN)\nto reduce calculation in back propagation, and the surprising results verify\nits validity in CNN: only 5\\% of the gradients are passed back but the model\nstill achieves the same effect as the traditional CNN, or even better. We also\nshow that the top-$k$ selection of gradients leads to a sparse calculation in\nback propagation, which may bring significant computational benefits for high\ncomputational complexity of convolution operation in CNN.\n', 'title': 'Minimal Effort Back Propagation for Convolutional Neural Networks'}]","I think I can help you with that!

Title: Minimal Effort Back Propagation for Convolutional Neural Networks

Abstract: As traditional neural network consumes a significant amount of computing resources during back propagation, \citet{Sun2017mePropSB} propose a simple yet effective technique to alleviate this problem. In this technique, only a small subset of the full gradients are computed to update the model parameters. In this paper we extend this technique into the Convolutional Neural Network(CNN) to reduce calculation in back propagation, and the surprising results verify its validity in CNN: only 5% of the gradients are passed back but the model still achieves the same effect as the traditional CNN, or even better. We also show that the top-$k$ selection of gradients leads to a sparse calculation in back propagation, which may bring significant computational benefits for high computational complexity of convolution operation in CNN."
What is a formula for calculating the genus of a subextension in a  Z_p-tower of a function field?,7d92050f,"[{'abstract': '  Let $K$ be a function field over a finite field $k$ of characteristic $p$ and\nlet $K_{\\infty}/K$ be a geometric extension with Galois group $\\mathbb{Z}_p$.\nLet $K_n$ be the corresponding subextension with Galois group\n$\\mathbb{Z}/p^n\\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple\nexplicit formula $g_n$ in terms of an explicit Witt vector construction of the\n$\\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which\nis quadratic in $p^n$. Furthermore, we determine all $\\mathbb{Z}_p$-towers for\nwhich the genus sequence is stable, in the sense that there are $a,b,c \\in\n\\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus\nstable towers are expected to have strong stable arithmetic properties for\ntheir zeta functions. A key technical contribution of this work is a new\nsimplified formula for the Schmid-Witt symbol coming from local class field\ntheory.\n', 'title': 'Genus growth in $\\mathbb{Z}_p$-towers of function fields'}]","I think I can help you with that!

Title: Genus growth in $\mathbb{Z}_p$-towers of function fields

Abstract:   Let $K$ be a function field over a finite field $k$ of characteristic $p$ and
let $K_{\infty}/K$ be a geometric extension with Galois group $\mathbb{Z}_p$.
Let $K_n$ be the corresponding subextension with Galois group
$\mathbb{Z}/p^n\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple
explicit formula $g_n$ in terms of an explicit Witt vector construction of the
$\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which
is quadratic in $p^n$. Furthermore, we determine all $\mathbb{Z}_p$-towers for
which the genus sequence is stable, in the sense that there are $a,b,c \in
\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus
stable towers are expected to have strong stable arithmetic properties for
their zeta functions. A key technical contribution of this work is a new
simplified formula for the Schmid-Witt symbol coming from local class field
theory."
In what type of applications is the proposed handwritten string recognition method easy to apply?,8ac6b16b,"[{'abstract': ""  The handwritten string recognition is still a challengeable task, though the\npowerful deep learning tools were introduced. In this paper, based on TAO-FCN,\nwe proposed an end-to-end system for handwritten string recognition. Compared\nwith the conventional methods, there is no preprocess nor manually designed\nrules employed. With enough labelled data, it is easy to apply the proposed\nmethod to different applications. Although the performance of the proposed\nmethod may not be comparable with the state-of-the-art approaches, it's\nusability and robustness are more meaningful for practical applications.\n"", 'title': 'On Study of the Reliable Fully Convolutional Networks with Tree Arranged Outputs (TAO-FCN) for Handwritten String Recognition'}]","Title: On Study of the Reliable Fully Convolutional Networks with Tree Arranged Outputs (TAO-FCN) for Handwritten String Recognition

Abstract:  The handwritten string recognition is still a challengeable task, though the powerful deep learning tools were introduced. In this paper, based on TAO-FCN, we proposed an end-to-end system for handwritten string recognition. Compared with the conventional methods, there is no preprocess nor manually designed rules employed. With enough labelled data, it is easy to apply the proposed method to different applications. Although the performance of the proposed method may not be comparable with the state-of-the-art approaches, it's usability and robustness are more meaningful for practical applications.

According to the abstract, the proposed handwritten string recognition method is easy to apply to ""different applications""."
What was the primary experimental setup used to measure photoelectron yields from extruded scintillation counters?,79e86979,"[{'abstract': '  Photoelectron yields of extruded scintillation counters with titanium dioxide\ncoating and embedded wavelength shifting fibers read out by silicon\nphotomultipliers have been measured at the Fermilab Test Beam Facility using\n120\\,GeV protons. The yields were measured as a function of transverse,\nlongitudinal, and angular positions for a variety of scintillator compositions\nand reflective coating mixtures, fiber diameters, and photosensor sizes. Timing\nperformance was also studied. These studies were carried out by the Cosmic Ray\nVeto Group of the Mu2e collaboration as part of their R\\&D program.\n', 'title': 'Photoelectron Yields of Scintillation Counters with Embedded Wavelength-Shifting Fibers Read Out With Silicon Photomultipliers'}]","Title: Photoelectron Yields of Scintillation Counters with Embedded Wavelength-Shifting Fibers Read Out With Silicon Photomultipliers

Abstract: Photoelectron yields of extruded scintillation counters with titanium dioxide coating and embedded wavelength shifting fibers read out by silicon photomultipliers have been measured at the Fermilab Test Beam Facility using 120\,GeV protons."
What type of variational posteriors can be trained using noisy versions of different optimization algorithms?,d481f458,"[{'abstract': ""  Variational Bayesian neural nets combine the flexibility of deep learning\nwith Bayesian uncertainty estimation. Unfortunately, there is a tradeoff\nbetween cheap but simple variational families (e.g.~fully factorized) or\nexpensive and complicated inference procedures. We show that natural gradient\nascent with adaptive weight noise implicitly fits a variational posterior to\nmaximize the evidence lower bound (ELBO). This insight allows us to train\nfull-covariance, fully factorized, or matrix-variate Gaussian variational\nposteriors using noisy versions of natural gradient, Adam, and K-FAC,\nrespectively, making it possible to scale up to modern-size ConvNets. On\nstandard regression benchmarks, our noisy K-FAC algorithm makes better\npredictions and matches Hamiltonian Monte Carlo's predictive variances better\nthan existing methods. Its improved uncertainty estimates lead to more\nefficient exploration in active learning, and intrinsic motivation for\nreinforcement learning.\n"", 'title': 'Noisy Natural Gradient as Variational Inference'}]","Based on the context, I found the answer to your question.

Title: Noisy Natural Gradient as Variational Inference

Abstract: Variational Bayesian neural nets combine the flexibility of deep learning
with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff
between cheap but simple variational families (e.g.~fully factorized) or
expensive and complicated inference procedures. We show that natural gradient
ascent with adaptive weight noise implicitly fits a variational posterior to
maximize the evidence lower bound (ELBO). This insight allows us to train
full-covariance, fully factorized, or matrix-variate Gaussian variational
posteriors using noisy versions of natural gradient, Adam, and K-FAC,
respectively, making it possible to scale up to modern-size ConvNets. On
standard regression benchmarks, our noisy K-FAC algorithm makes better
predictions and matches Hamiltonian Monte Carlo's predictive variances better
than existing methods. Its improved uncertainty estimates lead to more
efficient exploration in active learning, and intrinsic motivation for
reinforcement learning.

According to this paper, the types of variational posteriors that can be trained using noisy versions of different optimization algorithms are:

* Full-covariance
* Fully factorized
* Matrix-variate Gaussian"
Can a multi-class classifier be used to aid in the calibration of a regression model?,42331c6d,"[{'abstract': '  The task of calibration is to retrospectively adjust the outputs from a\nmachine learning model to provide better probability estimates on the target\nvariable. While calibration has been investigated thoroughly in classification,\nit has not yet been well-established for regression tasks. This paper considers\nthe problem of calibrating a probabilistic regression model to improve the\nestimated probability densities over the real-valued targets. We propose to\ncalibrate a regression model through the cumulative probability density, which\ncan be derived from calibrating a multi-class classifier. We provide three\nnon-parametric approaches to solve the problem, two of which provide empirical\nestimates and the third providing smooth density estimates. The proposed\napproaches are experimentally evaluated to show their ability to improve the\nperformance of regression models on the predictive likelihood.\n', 'title': 'Non-Parametric Calibration of Probabilistic Regression'}]","Title: Non-Parametric Calibration of Probabilistic Regression

Abstract: The task of calibration is to retrospectively adjust the outputs from a machine learning model to provide better probability estimates on the target variable. While calibration has been investigated thoroughly in classification, it has not yet been well-established for regression tasks. This paper considers the problem of calibrating a probabilistic regression model to improve the estimated probability densities over the real-valued targets. We propose to calibrate a regression model through the cumulative probability density, which can be derived from calibrating a multi-class classifier. We provide three non-parametric approaches to solve the problem, two of which provide empirical estimates and the third providing smooth density estimates. The proposed approaches are experimentally evaluated to show their ability to improve the performance of regression models on the predictive likelihood."
Does the abstract find a causal link between moral preferences and behavior in the Ultimatum Game?,ae574768,"[{'abstract': '  In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to\nallocate a certain amount of money between herself and a ""responder"". If the\noffer is greater than or equal to the responder\'s minimum acceptable offer\n(MAO), then the money is split as proposed, otherwise, neither the proposer nor\nthe responder get anything. The UG has intrigued generations of behavioral\nscientists because people in experiments blatantly violate the equilibrium\npredictions that self-interested proposers offer the minimum available non-zero\namount, and self-interested responders accept. Why are these predictions\nviolated? Previous research has mainly focused on the role of social\npreferences. Little is known about the role of general moral preferences for\ndoing the right thing, preferences that have been shown to play a major role in\nother social interactions (e.g., Dictator Game and Prisoner\'s Dilemma). Here I\ndevelop a theoretical model and an experiment designed to pit social\npreferences against moral preferences. I find that, although people recognize\nthat offering half and rejecting low offers are the morally right things to do,\nmoral preferences have no causal impact on UG behavior. The experimental data\nare indeed well fit by a model according to which: (i) high UG offers are\nmotivated by inequity aversion and, to a lesser extent, self-interest; (ii)\nhigh MAOs are motivated by inequity aversion.\n', 'title': 'Social versus Moral preferences in the Ultimatum Game: A theoretical model and an experiment'}]","Title: Social versus Moral preferences in the Ultimatum Game: A theoretical model and an experiment

Abstract: In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to allocate a certain amount of money between herself and a ""responder"". If the offer is greater than or equal to the responder's minimum acceptable offer (MAO), then the money is split as proposed, otherwise, neither the proposer nor the responder get anything. The UG has intrigued generations of behavioral scientists because people in experiments blatantly violate the equilibrium predictions that self-interested proposers offer the minimum available non-zero amount, and self-interested responders accept. Why are these predictions violated? Previous research has mainly focused on the role of social preferences. Little is known about the role of general moral preferences for doing the right thing, preferences that have been shown to play a major role in other social interactions (e.g., Dictator Game and Prisoner's Dilemma). Here I develop a theoretical model and an experiment designed to pit social preferences against moral preferences. I find that, although people recognize that offering half and rejecting low offers are the morally right things to do, moral preferences have no causal impact on UG behavior. The experimental data are indeed well fit by a model according to which: (i) high UG offers are motivated by inequity aversion and, to a lesser extent, self-interest; (ii) high MAOs are motivated by inequity aversion.

The answer is: No, the abstract does not find a causal link between moral preferences and behavior in the Ultimatum Game."
"Researchers propose a new class of algorithms, what is the proposed name for these algorithms?",23c0f86a,"[{'abstract': '  Examining games from a fresh perspective we present the idea of game-inspired\nand game-based algorithms, dubbed ""gamorithms"".\n', 'title': 'Gamorithm'}]","I think I can help you with that!

The proposed name for the new class of algorithms is ""gamorithms"". This is mentioned in the abstract of the paper titled ""Gamorithm"".

Here is the relevant information:

Title: Gamorithm

Abstract: Examining games from a fresh perspective we present the idea of game-inspired and game-based algorithms, dubbed ""gamorithms""."
Why is understanding the geometry of a generative model important for analyzing its output density?,e8ab2bbb,"[{'abstract': '  It can be difficult to tell whether a trained generative model has learned to\ngenerate novel examples or has simply memorized a specific set of outputs. In\npublished work, it is common to attempt to address this visually, for example\nby displaying a generated example and its nearest neighbor(s) in the training\nset (in, for example, the L2 metric). As any generative model induces a\nprobability density on its output domain, we propose studying this density\ndirectly. We first study the geometry of the latent representation and\ngenerator, relate this to the output density, and then develop techniques to\ncompute and inspect the output density. As an application, we demonstrate that\n""memorization"" tends to a density made of delta functions concentrated on the\nmemorized examples. We note that without first understanding the geometry, the\nmeasurement would be essentially impossible to make.\n', 'title': 'Latent Geometry and Memorization in Generative Models'}]","Title: Latent Geometry and Memorization in Generative Models

Abstract: It can be difficult to tell whether a trained generative model has learned to generate novel examples or has simply memorized a specific set of outputs. In published work, it is common to attempt to address this visually, for example by displaying a generated example and its nearest neighbor(s) in the training set (in, for example, the L2 metric). As any generative model induces a probability density on its output domain, we propose studying this density directly. We first study the geometry of the latent representation and generator, relate this to the output density, and then develop techniques to compute and inspect the output density. As an application, we demonstrate that ""memorization"" tends to a density made of delta functions concentrated on the memorized examples. We note that without first understanding the geometry, the measurement would be essentially impossible to make."
What type of nonlinearity is the proposed receiver designed to handle and what is the associated performance improvement?,53c2de90,"[{'abstract': '  In this paper, we propose a practical receiver for multicarrier signals\nsubjected to a strong memoryless nonlinearity. The receiver design is based on\na generalized approximate message passing (GAMP) framework, and this allows\nreal-time algorithm implementation in software or hardware with moderate\ncomplexity. We demonstrate that the proposed receiver can provide more than a\n2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range\n$10^{-4}\\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to\nclipping nonlinearity and the crest-factor of the clipped waveform is only\n1.9dB. Simulation results also demonstrate that the proposed receiver provides\nsignificant performance gain in frequency-selective multipath channels\n', 'title': 'Detection of Nonlinearly Distorted OFDM Signals via Generalized Approximate Message Passing'}]","Based on the context, I can help you find the answer to your question.

Title: Detection of Nonlinearly Distorted OFDM Signals via Generalized Approximate Message Passing

Abstract: In this paper, we propose a practical receiver for multicarrier signals subject to a strong memoryless nonlinearity. The receiver design is based on a generalized approximate message passing (GAMP) framework, and this allows real-time algorithm implementation in software or hardware with moderate complexity. We demonstrate that the proposed receiver can provide more than a 2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range $10^{-4}\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to clipping nonlinearity and the crest-factor of the clipped waveform is only 1.9dB. Simulation results also demonstrate that the proposed receiver provides significant performance gain in frequency-selective multipath channels

The proposed receiver is designed to handle clipping nonlinearity, and the associated performance improvement is more than a 2dB gain compared to an ideal uncoded linear OFDM transmission."
Under what condition can this algorithm achieve linear convergence?,71d7fbec,"[{'abstract': '  This paper considers the problem of decentralized optimization with a\ncomposite objective containing smooth and non-smooth terms. To solve the\nproblem, a proximal-gradient scheme is studied. Specifically, the smooth and\nnonsmooth terms are dealt with by gradient update and proximal update,\nrespectively. The studied algorithm is closely related to a previous\ndecentralized optimization algorithm, PG-EXTRA [37], but has a few advantages.\nFirst of all, in our new scheme, agents use uncoordinated step-sizes and the\nstable upper bounds on step-sizes are independent from network topology. The\nstep-sizes depend on local objective functions, and they can be as large as\nthat of the gradient descent. Secondly, for the special case without non-smooth\nterms, linear convergence can be achieved under the strong convexity\nassumption. The dependence of the convergence rate on the objective functions\nand the network are separated, and the convergence rate of our new scheme is as\ngood as one of the two convergence rates that match the typical rates for the\ngeneral gradient descent and the consensus averaging. We also provide some\nnumerical experiments to demonstrate the efficacy of the introduced algorithms\nand validate our theoretical discoveries.\n', 'title': 'A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates'}]","After reviewing the abstracts, I found a relevant answer.

Title: A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates

Abstract: This paper considers the problem of decentralized optimization with a composite objective containing smooth and non-smooth terms. To solve the problem, a proximal-gradient scheme is studied. Specifically, the smooth and nonsmooth terms are dealt with by gradient update and proximal update, respectively. The studied algorithm is closely related to a previous decentralized optimization algorithm, PG-EXTRA [37], but has a few advantages. First of all, in our new scheme, agents use uncoordinated step-sizes and the stable upper bounds on step-sizes are independent from network topology. The step-sizes depend on local objective functions, and they can be as large as that of the gradient descent. Secondly, for the special case without non-smooth terms, linear convergence can be achieved under the strong convexity assumption. The dependence of the convergence rate on the objective functions and the network are separated, and the convergence rate of our new scheme is as good as one of the two convergence rates that match the typical rates for the general gradient descent and the consensus averaging. We also provide some numerical experiments to demonstrate the efficacy of the introduced algorithms and validate our theoretical discoveries.

According to this paper, linear convergence can be achieved under the strong convexity assumption, specifically when there are no non-smooth terms in the objective function."
"Can you provide examples of groups with infinite, unbounded torsion that contain Morse geodesics? ",10c473bb,"[{'abstract': '  In this paper we exhibit Morse geodesics, often called ""hyperbolic\ndirections"", in infinite unbounded torsion groups. The groups studied are\nlacunary hyperbolic groups and constructed using graded small cancellation\nconditions. In all previously known examples, Morse geodesics were found in\ngroups which also contained Morse elements, infinite order elements whose\ncyclic subgroup gives a Morse quasi-geodesic. Our result presents the first\nexample of a group which contains Morse geodesics but no Morse elements. In\nfact, we show that there is an isometrically embedded $7$-regular tree inside\nsuch groups where every infinite, simple path is a Morse geodesic.\n', 'title': 'Morse geodesics in torsion groups'}]","I think I can help you with that!

Title: Morse geodesics in torsion groups

Abstract: In this paper we exhibit Morse geodesics, often called ""hyperbolic directions"", in infinite unbounded torsion groups. The groups studied are lacunary hyperbolic groups and constructed using graded small cancellation conditions. In all previously known examples, Morse geodesics were found in groups which also contained Morse elements, infinite order elements whose cyclic subgroup gives a Morse quasi-geodesic. Our result presents the first example of a group which contains Morse geodesics but no Morse elements. In fact, we show that there is an isometrically embedded 7-regular tree inside such groups where every infinite, simple path is a Morse geodesic."
"What computational approaches are described for calculating this network distance, including any proposed lower bounds?",a7ed7722,"[{'abstract': '  We develop the theoretical foundations of a network distance that has\nrecently been applied to various subfields of topological data analysis, namely\npersistent homology and hierarchical clustering. While this network distance\nhas previously appeared in the context of finite networks, we extend the\nsetting to that of compact networks. The main challenge in this new setting is\nthe lack of an easy notion of sampling from compact networks; we solve this\nproblem in the process of obtaining our results. The generality of our setting\nmeans that we automatically establish results for exotic objects such as\ndirected metric spaces and Finsler manifolds. We identify readily computable\nnetwork invariants and establish their quantitative stability under this\nnetwork distance. We also discuss the computational complexity involved in\nprecisely computing this distance, and develop easily-computable lower bounds\nby using the identified invariants. By constructing a wide range of explicit\nexamples, we show that these lower bounds are effective in distinguishing\nbetween networks. Finally, we provide a simple algorithm that computes a lower\nbound on the distance between two networks in polynomial time and illustrate\nour metric and invariant constructions on a database of random networks and a\ndatabase of simulated hippocampal networks.\n', 'title': 'Distances and Isomorphism between Networks and the Stability of Network Invariants'}]","After reviewing the abstracts, I found a relevant paper that describes computational approaches for calculating a network distance, including proposed lower bounds.

Title: Distances and Isomorphism between Networks and the Stability of Network Invariants

Abstract: We develop the theoretical foundations of a network distance that has recently been applied to various subfields of topological data analysis, namely persistent homology and hierarchical clustering. While this network distance has previously appeared in the context of finite networks, we extend the setting to that of compact networks. The main challenge in this new setting is the lack of an easy notion of sampling from compact networks; we solve this problem in the process of obtaining our results. The generality of our setting means that we automatically establish results for exotic objects such as directed metric spaces and Finsler manifolds. We identify readily computable network invariants and establish their quantitative stability under this network distance. We also discuss the computational complexity involved in precisely computing this distance, and develop easily-computable lower bounds by using the identified invariants. By constructing a wide range of explicit examples, we show that these lower bounds are effective in distinguishing between networks. Finally, we provide a simple algorithm that computes a lower bound on the distance between two networks in polynomial time and illustrate our metric and invariant constructions on a database of random networks and a database of simulated hippocampal networks."
"Besides cosmic rays, what other types of particles does SimProp v2r4 simulate?",87449d6e,"[{'abstract': '  We introduce the new version of SimProp, a Monte Carlo code for simulating\nthe propagation of ultra-high energy cosmic rays in intergalactic space. This\nversion, SimProp v2r4, together with an overall improvement of the code\ncapabilities with a substantial reduction in the computation time, also\ncomputes secondary cosmogenic particles such as electron-positron pairs and\ngamma rays produced during the propagation of ultra-high energy cosmic rays. As\nrecently pointed out by several authors, the flux of this secondary radiation\nand its products, within reach of the current observatories, provides useful\ninformation about models of ultra-high energy cosmic ray sources which would be\nhard to discriminate otherwise.\n', 'title': 'SimProp v2r4: Monte Carlo simulation code for UHECR propagation'}]","I think I have found the answer!

Title: SimProp v2r4: Monte Carlo simulation code for UHECR propagation

Abstract: We introduce the new version of SimProp, a Monte Carlo code for simulating the propagation of ultra-high energy cosmic rays in intergalactic space. This version, SimProp v2r4, together with an overall improvement of the code capabilities with a substantial reduction in the computation time, also computes secondary cosmogenic particles such as electron-positron pairs and gamma rays produced during the propagation of ultra-high energy cosmic rays. 

According to the abstract, besides cosmic rays, SimProp v2r4 also simulates electron-positron pairs and gamma rays."
"How do velocity, volume, lensing, and gravitational potential contribute to corrections in gravitational wave luminosity distance?",f52e95ca,"[{'abstract': '  Waveforms of gravitational waves provide information about a variety of\nparameters for the binary system merging. However, standard calculations have\nbeen performed assuming a FLRW universe with no perturbations. In reality this\nassumption should be dropped: we show that the inclusion of cosmological\nperturbations translates into corrections to the estimate of astrophysical\nparameters derived for the merging binary systems. We compute corrections to\nthe estimate of the luminosity distance due to velocity, volume, lensing and\ngravitational potential effects. Our results show that the amplitude of the\ncorrections will be negligible for current instruments, mildly important for\nexperiments like the planned DECIGO, and very important for future ones such as\nthe Big Bang Observer.\n', 'title': 'Cosmological perturbation effects on gravitational-wave luminosity distance estimates'}]","Title: Cosmological perturbation effects on gravitational-wave luminosity distance estimates

Abstract: Waveforms of gravitational waves provide information about a variety of parameters for the binary system merging. However, standard calculations have been performed assuming a FLRW universe with no perturbations. In reality this assumption should be dropped: we show that the inclusion of cosmological perturbations translates into corrections to the estimate of astrophysical parameters derived for the merging binary systems. We compute corrections to the estimate of the luminosity distance due to velocity, volume, lensing and gravitational potential effects. Our results show that the amplitude of the corrections will be negligible for current instruments, mildly important for experiments like the planned DECIGO, and very important for future ones such as the Big Bang Observer."
Who are the authors of the original integer programming model for cost-benefit analysis of flood protection through dike building in the Netherlands?,376d67df,"[{'abstract': '  We consider a fundamental integer programming (IP) model for cost-benefit\nanalysis flood protection through dike building in the Netherlands, due to\nVerweij and Zwaneveld.\nExperimental analysis with data for the Ijsselmeer lead to integral optimal\nsolution of the linear programming relaxation of the IP model.\nThis naturally led to the question of integrality of the polytope associated\nwith the IP model.\nIn this paper we first give a negative answer to this question by\nestablishing non-integrality of the polytope.\nSecond, we establish natural conditions that guarantee the linear programming\nrelaxation of the IP model to be integral.\nWe then test the most recent data on flood probabilities, damage and\ninvestment costs of the IJsselmeer for these conditions.\nThird, we show that the IP model can be solved in polynomial time when the\nnumber of dike segments, or the number of feasible barrier heights, are\nconstant.\n', 'title': 'On the complexity of solving a decision problem with flow-depending costs: the case of the IJsselmeer dikes'}]","Title: On the complexity of solving a decision problem with flow-depending costs: the case of the IJsselmeer dikes

Abstract: We consider a fundamental integer programming (IP) model for cost-benefit analysis flood protection through dike building in the Netherlands, due to Verweij and Zwaneveld. Experimental analysis with data for the Ijsselmeer lead to integral optimal solution of the linear programming relaxation of the IP model. This naturally led to the question of integrality of the polytope associated with the IP model. In this paper we first give a negative answer to this question by establishing non-integrality of the polytope. Second, we establish natural conditions that guarantee the linear programming relaxation of the IP model to be integral. We then test the most recent data on flood probabilities, damage and investment costs of the IJsselmeer for these conditions. Third, we show that the IP model can be solved in polynomial time when the number of dike segments, or the number of feasible barrier heights, are constant.

The authors of the original integer programming model are Verweij and Zwaneveld."
What does this functional expand the energy in?,770970c4,"[{'abstract': '  We further progress along the line of Ref. [Phys. Rev. {\\bf A 94}, 043614\n(2016)] where a functional for Fermi systems with anomalously large $s$-wave\nscattering length $a_s$ was proposed that has no free parameters. The\nfunctional is designed to correctly reproduce the unitary limit in Fermi gases\ntogether with the leading-order contributions in the s- and p-wave channels at\nlow density. The functional is shown to be predictive up to densities\n$\\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang\nfunctional, valid for $\\rho < 10^{-6}$ fm$^{-3}$. The form of the functional\nretained in this work is further motivated. It is shown that the new functional\ncorresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all\norders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One\nconclusion from the present work is that, except in the extremely low--density\nregime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with\nrespect to the unitary limit. Starting from the functional, we introduce\ndensity--dependent scales and show that scales associated to the bare\ninteraction are strongly renormalized by medium effects. As a consequence, some\nof the scales at play around saturation are dominated by the unitary gas\nproperties and not directly to low-energy constants. For instance, we show that\nthe scale in the s-wave channel around saturation is proportional to the\nso-called Bertsch parameter $\\xi_0$ and becomes independent of $a_s$. We also\npoint out that these scales are of the same order of magnitude than those\nempirically obtained in the Skyrme energy density functional. We finally\npropose a slight modification of the functional such that it becomes accurate\nup to the saturation density $\\rho\\simeq 0.16$ fm$^{-3}$.\n', 'title': 'From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter'}]","Title: From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter

Abstract:   We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum."
How did improvements to water quality in early 20th century Japan impact mortality rates from diseases not related to water?,45fad64f,"[{'abstract': '  This study explores the validity of chain effects of clean water, which are\nknown as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan.\nRecent studies have reported that water purifications systems are responsible\nfor huge contributions to human capital. Although a few studies have\ninvestigated the short-term effects of water-supply systems in pre-war Japan,\nlittle is known about the benefits associated with these systems. By analyzing\ncity-level cause-specific mortality data from the years 1922-1940, we found\nthat eliminating typhoid fever infections decreased the risk of deaths due to\nnon-waterborne diseases. Our estimates show that for one additional typhoid\ndeath, there were approximately one to three deaths due to other causes, such\nas tuberculosis and pneumonia. This suggests that the observed Mills-Reincke\nphenomenon could have resulted from the prevention typhoid fever in a\npreviously-developing Asian country.\n', 'title': 'Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan'}]","Title: Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan

Abstract: This study explores the validity of chain effects of clean water, which are 
known as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan. 
Recent studies have reported that water purifications systems are responsible 
for huge contributions to human capital. Although a few studies have 
investigated the short-term effects of water-supply systems in pre-war Japan, 
little is known about the benefits associated with these systems. By analyzing 
city-level cause-specific mortality data from the years 1922-1940, we found 
that eliminating typhoid fever infections decreased the risk of deaths due to 
non-waterborne diseases. Our estimates show that for one additional typhoid 
death, there were approximately one to three deaths due to other causes, such 
as tuberculosis and pneumonia. This suggests that the observed Mills-Reincke 
phenomenon could have resulted from the prevention typhoid fever in a 
previously-developing Asian country."
What types of experiments were conducted to evaluate the performance of batch normalization for uncertainty estimation?,3c437b92,"[{'abstract': '  We show that training a deep network using batch normalization is equivalent\nto approximate inference in Bayesian models. We further demonstrate that this\nfinding allows us to make meaningful estimates of the model uncertainty using\nconventional architectures, without modifications to the network or the\ntraining procedure. Our approach is thoroughly validated by measuring the\nquality of uncertainty in a series of empirical experiments on different tasks.\nIt outperforms baselines with strong statistical significance, and displays\ncompetitive performance with recent Bayesian approaches.\n', 'title': 'Bayesian Uncertainty Estimation for Batch Normalized Deep Networks'}]","Title: Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Abstract: We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches."
What does the tangle-tree duality theorem assert about the relationship between local cohesiveness and global tree structure?,77d735a2,"[{'abstract': '  We prove a general width duality theorem for combinatorial structures with\nwell-defined notions of cohesion and separation. These might be graphs and\nmatroids, but can be much more general or quite different. The theorem asserts\na duality between the existence of high cohesiveness somewhere local and a\nglobal overall tree structure.\nWe describe cohesive substructures in a unified way in the format of tangles:\nas orientations of low-order separations satisfying certain consistency axioms.\nThese axioms can be expressed without reference to the underlying structure,\nsuch as a graph or matroid, but just in terms of the poset of the separations\nthemselves. This makes it possible to identify tangles, and apply our\ntangle-tree duality theorem, in very diverse settings.\nOur result implies all the classical duality theorems for width parameters in\ngraph minor theory, such as path-width, tree-width, branch-width or rank-width.\nIt yields new, tangle-type, duality theorems for tree-width and path-width. It\nimplies the existence of width parameters dual to cohesive substructures such\nas $k$-blocks, edge-tangles, or given subsets of tangles, for which no width\nduality theorems were previously known.\nAbstract separation systems can be found also in structures quite unlike\ngraphs and matroids. For example, our theorem can be applied to image analysis\nby capturing the regions of an image as tangles of separations defined as\nnatural partitions of its set of pixels. It can be applied in big data contexts\nby capturing clusters as tangles. It can be applied in the social sciences,\ne.g. by capturing as tangles the few typical mindsets of individuals found by a\nsurvey. It could also be applied in pure mathematics, e.g. to separations of\ncompact manifolds.\n', 'title': 'Tangle-tree duality in abstract separation systems'}]","Title: Tangle-tree duality in abstract separation systems

Abstract: We prove a general width duality theorem for combinatorial structures with well-defined notions of cohesion and separation. These might be graphs and matroids, but can be much more general or quite different. The theorem asserts a duality between the existence of high cohesiveness somewhere local and a global overall tree structure."
How does a Smoothed Particle Hydrodynamics model simulate impacts on a planet of a size similar to Mars?,a9361912,"[{'abstract': '  We model large-scale ($\\approx$2000km) impacts on a Mars-like planet using a\nSmoothed Particle Hydrodynamics code. The effects of material strength and of\nusing different Equations of State on the post-impact material and temperature\ndistributions are investigated. The properties of the ejected material in terms\nof escaping and disc mass are analysed as well. We also study potential\nnumerical effects in the context of density discontinuities and rigid body\nrotation. We find that in the large-scale collision regime considered here\n(with impact velocities of 4km/s), the effect of material strength is\nsubstantial for the post-impact distribution of the temperature and the\nimpactor material, while the influence of the Equation of State is more subtle\nand present only at very high temperatures.\n', 'title': 'SPH calculations of Mars-scale collisions: the role of the Equation of State, material rheologies, and numerical effects'}]","Title: SPH calculations of Mars-scale collisions: the role of the Equation of State, material rheologies, and numerical effects

Abstract:  We model large-scale ($\approx$2000km) impacts on a Mars-like planet using a Smoothed Particle Hydrodynamics code. The effects of material strength and of using different Equations of State on the post-impact material and temperature distributions are investigated. The properties of the ejected material in terms of escaping and disc mass are analysed as well. We also study potential numerical effects in the context of density discontinuities and rigid body rotation. We find that in the large-scale collision regime considered here (with impact velocities of 4km/s), the effect of material strength is substantial for the post-impact distribution of the temperature and the impactor material, while the influence of the Equation of State is more subtle and present only at very high temperatures."
What challenge arises when directly optimizing the 'direct loss' objective for beam search decoding?,66ccf294,"[{'abstract': '  Beam search is a desirable choice of test-time decoding algorithm for neural\nsequence models because it potentially avoids search errors made by simpler\ngreedy methods. However, typical cross entropy training procedures for these\nmodels do not directly consider the behaviour of the final decoding method. As\na result, for cross-entropy trained models, beam decoding can sometimes yield\nreduced test performance when compared with greedy decoding. In order to train\nmodels that can more effectively make use of beam search, we propose a new\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\nevaluated on the output of beam search. While well-defined, this ""direct loss""\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\napproach, we form a sub-differentiable surrogate objective by introducing a\nnovel continuous approximation of the beam search decoding procedure. In\nexperiments, we show that optimizing this new training objective yields\nsubstantially better results on two sequence tasks (Named Entity Recognition\nand CCG Supertagging) when compared with both cross entropy trained greedy\ndecoding and cross entropy trained beam decoding baselines.\n', 'title': 'A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models'}]","The challenge that arises when directly optimizing the 'direct loss' objective for beam search decoding is that this objective is itself discontinuous and thus difficult to optimize.

Title: A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models

Abstract: Beam search is a desirable choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when compared with greedy decoding. In order to train models that can more effectively make use of beam search, we propose a new training procedure that focuses on the final loss metric (e.g. Hamming loss) evaluated on the output of beam search. While well-defined, this ""direct loss"" objective is itself discontinuous and thus difficult to optimize. Hence, in our approach, we form a sub-differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure. In experiments, we show that optimizing this new training objective yields substantially better results on two sequence tasks (Named Entity Recognition and CCG Supertagging) when compared with both cross entropy trained greedy decoding and cross entropy trained beam decoding baselines."
How do randomized Frank-Wolfe algorithms compare to deterministic versions in terms of convergence rate?,37d3e845,"[{'abstract': '  We analyze two novel randomized variants of the Frank-Wolfe (FW) or\nconditional gradient algorithm. While classical FW algorithms require solving a\nlinear minimization problem over the domain at each iteration, the proposed\nmethod only requires to solve a linear minimization problem over a small\n\\emph{subset} of the original domain. The first algorithm that we propose is a\nrandomized variant of the original FW algorithm and achieves a\n$\\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic\ncounterpart. The second algorithm is a randomized variant of the Away-step FW\nalgorithm, and again as its deterministic counterpart, reaches linear (i.e.,\nexponential) convergence rate making it the first provably convergent\nrandomized variant of Away-step FW. In both cases, while subsampling reduces\nthe convergence rate by a constant factor, the linear minimization step can be\na fraction of the cost of that of the deterministic versions, especially when\nthe data is streamed. We illustrate computational gains of the algorithms on\nregression problems, involving both $\\ell_1$ and latent group lasso penalties.\n', 'title': 'Frank-Wolfe with Subsampling Oracle'}]","Title: Frank-Wolfe with Subsampling Oracle

Abstract: We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algorithm. While classical FW algorithms require solving a linear minimization problem over the domain at each iteration, the proposed method only requires to solve a linear minimization problem over a small \emph{subset} of the original domain. The first algorithm that we propose is a randomized variant of the original FW algorithm and achieves a $\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic counterpart. The second algorithm is a randomized variant of the Away-step FW algorithm, and again as its deterministic counterpart, reaches linear (i.e., exponential) convergence rate making it the first provably convergent randomized variant of Away-step FW. In both cases, while subsampling reduces the convergence rate by a constant factor, the linear minimization step can be a fraction of the cost of that of the deterministic versions, especially when the data is streamed. We illustrate computational gains of the algorithms on regression problems, involving both $\ell_1$ and latent group lasso penalties."
What type of graph structure is the focus of the paper 'The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes'?,eef2e7b6,"[{'abstract': '  In the work of Peng et al. in 2012, a new measure was proposed for fault\ndiagnosis of systems: namely, g-good-neighbor conditional diagnosability, which\nrequires that any fault-free vertex has at least g fault-free neighbors in the\nsystem. In this paper, we establish the g-good-neighbor conditional\ndiagnosability of locally twisted cubes under the PMC model and the MM^* model.\n', 'title': 'The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes'}]","Title: The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes

Abstract: In the work of Peng et al. in 2012, a new measure was proposed for fault diagnosis of systems: namely, g-good-neighbor conditional diagnosability, which requires that any fault-free vertex has at least g fault-free neighbors in the system. In this paper, we establish the g-good-neighbor conditional diagnosability of locally twisted cubes under the PMC model and the MM^* model.

The focus of this paper is on the graph structure of locally twisted cubes."
How does the accuracy of Gaussian process regression change when a subset of training data is used?,2f5b270f,"[{'abstract': '  In this paper, we study random subsampling of Gaussian process regression,\none of the simplest approximation baselines, from a theoretical perspective.\nAlthough subsampling discards a large part of training data, we show provable\nguarantees on the accuracy of the predictive mean/variance and its\ngeneralization ability. For analysis, we consider embedding kernel matrices\ninto graphons, which encapsulate the difference of the sample size and enables\nus to evaluate the approximation and generalization errors in a unified manner.\nThe experimental results show that the subsampling approximation achieves a\nbetter trade-off regarding accuracy and runtime than the Nyström and random\nFourier expansion methods.\n', 'title': 'On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis'}]","I think I can help you with that!

Title: On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis

Abstract: In this paper, we study random subsampling of Gaussian process regression, one of the simplest approximation baselines, from a theoretical perspective. Although subsampling discards a large part of training data, we show provable guarantees on the accuracy of the predictive mean/variance and its generalization ability. For analysis, we consider embedding kernel matrices into graphons, which encapsulate the difference of the sample size and enables us to evaluate the approximation and generalization errors in a unified manner. The experimental results show that the subsampling approximation achieves a better trade-off regarding accuracy and runtime than the Nyström and random Fourier expansion methods.

This paper discusses the accuracy of Gaussian process regression when a subset of training data is used, specifically looking at the predictive mean and variance, as well as the generalization ability."
What types of research methods are commonly used in Smart Grid data analysis studies?,ef678b92,"[{'abstract': '  Data analytics and data science play a significant role in nowadays society.\nIn the context of Smart Grids (SG), the collection of vast amounts of data has\nseen the emergence of a plethora of data analysis approaches. In this paper, we\nconduct a Systematic Mapping Study (SMS) aimed at getting insights about\ndifferent facets of SG data analysis: application sub-domains (e.g., power load\ncontrol), aspects covered (e.g., forecasting), used techniques (e.g.,\nclustering), tool-support, research methods (e.g., experiments/simulations),\nreplicability/reproducibility of research. The final goal is to provide a view\nof the current status of research. Overall, we found that each sub-domain has\nits peculiarities in terms of techniques, approaches and research methodologies\napplied. Simulations and experiments play a crucial role in many areas. The\nreplicability of studies is limited concerning the provided implemented\nalgorithms, and to a lower extent due to the usage of private datasets.\n', 'title': 'Smart Grids Data Analysis: A Systematic Mapping Study'}]","Based on the provided abstracts, I found that the research methods used in Smart Grid data analysis studies are mentioned in the first abstract.

Title: Smart Grids Data Analysis: A Systematic Mapping Study

Abstract: Data analytics and data science play a significant role in nowadays society.
In the context of Smart Grids (SG), the collection of vast amounts of data has
seen the emergence of a plethora of data analysis approaches. In this paper, we
conduct a Systematic Mapping Study (SMS) aimed at getting insights about
different facets of SG data analysis: application sub-domains (e.g., power load
control), aspects covered (e.g., forecasting), used techniques (e.g.,
clustering), tool-support, research methods (e.g., experiments/simulations),
replicability/reproducibility of research. The final goal is to provide a view
of the current status of research. Overall, we found that each sub-domain has
its peculiarities in terms of techniques, approaches and research methodologies
applied. Simulations and experiments play a crucial role in many areas. The
replicability of studies is limited concerning the provided implemented
algorithms, and to a lower extent due to the usage of private datasets.

The research methods mentioned are:

* Experiments
* Simulations

These methods are specifically mentioned as being commonly used in Smart Grid data analysis studies."
Can you find a paper that uses a graph model to analyze message passing?,eca312a3,"[{'abstract': '  In the paper we consider a graph model of message passing processes and\npresent a method verification of message passing processes. The method is\nillustrated by an example of a verification of sliding window protocol.\n', 'title': 'A graph model of message passing processes'}]","I think I can help you with that!

Title: A graph model of message passing processes

Abstract: In the paper we consider a graph model of message passing processes and present a method verification of message passing processes. The method is illustrated by an example of a verification of sliding window protocol."
What is the key concept explored in the paper 'Improper posteriors are not improper'?,2b9ac1f7,"[{'abstract': ""  In 1933 Kolmogorov constructed a general theory that defines the modern\nconcept of conditional expectation. In 1955 Renyi fomulated a new axiomatic\ntheory for probability motivated by the need to include unbounded measures. We\nintroduce a general concept of conditional expectation in Renyi spaces. In this\ntheory improper priors are allowed, and the resulting posterior can also be\nimproper.\nIn 1965 Lindley published his classic text on Bayesian statistics using the\ntheory of Renyi, but retracted this idea in 1973 due to the appearance of\nmarginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes\nare investigated, and the seemingly conflicting results are explained. The\ntheory of Renyi can hence be used as an axiomatic basis for statistics that\nallows use of unbounded priors.\nKeywords: Haldane's prior; Poisson intensity; Marginalization paradox;\nMeasure theory; conditional probability space; axioms for statistics;\nconditioning on a sigma field; improper prior\n"", 'title': 'Improper posteriors are not improper'}]","Title: Improper posteriors are not improper

Abstract: In 1933 Kolmogorov constructed a general theory that defines the modern concept of conditional expectation. In 1955 Renyi formulated a new axiomatic theory for probability motivated by the need to include unbounded measures. We introduce a general concept of conditional expectation in Renyi spaces. In this theory improper priors are allowed, and the resulting posterior can also be improper. In 1965 Lindley published his classic text on Bayesian statistics using the theory of Renyi, but retracted this idea in 1973 due to the appearance of marginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes are investigated, and the seemingly conflicting results are explained. The theory of Renyi can hence be used as an axiomatic basis for statistics that allows use of unbounded priors.

The key concept explored in this paper is the concept of conditional expectation in Renyi spaces, which allows for improper priors and improper posteriors."
What is the effect of a bottleneck on the flow of two species of particles moving in the same direction?,d5454239,"[{'abstract': '  We develope a two-species exclusion process with a distinct pair of entry and\nexit sites for each species of rigid rods. The relatively slower forward\nstepping of the rods in an extended bottleneck region, located in between the\ntwo entry sites, controls the extent of interference of the co-directional flow\nof the two species of rods. The relative positions of the sites of entry of the\ntwo species of rods with respect to the location of the bottleneck are\nmotivated by a biological phenomenon. However, the primary focus of the study\nhere is to explore the effects of the interference of the flow of the two\nspecies of rods on their spatio-temporal organization and the regulations of\nthis interference by the extended bottleneck. By a combination of mean-field\ntheory and computer simulation we calculate the flux of both species of rods\nand their density profiles as well as the composite phase diagrams of the\nsystem. If the bottleneck is sufficiently stringent some of the phases become\npractically unrealizable although not ruled out on the basis of any fundamental\nphysical principle. Moreover the extent of suppression of flow of the\ndownstream entrants by the flow of the upstream entrants can also be regulated\nby the strength of the bottleneck. We speculate on the possible implications of\nthe results in the context of the biological phenomenon that motivated the\nformulation of the theoretical model.\n', 'title': 'Interference of two co-directional exclusion processes in the presence of a static bottleneck: a biologically motivated model'}]","The answer to your question can be found in the research paper titled:

Title: Interference of two co-directional exclusion processes in the presence of a static bottleneck: a biologically motivated model

Abstract: We develope a two-species exclusion process with a distinct pair of entry and exit sites for each species of rigid rods. The relatively slower forward stepping of the rods in an extended bottleneck region, located in between the two entry sites, controls the extent of interference of the co-directional flow of the two species of rods. The relative positions of the sites of entry of the two species of rods with respect to the location of the bottleneck are motivated by a biological phenomenon. However, the primary focus of the study here is to explore the effects of the interference of the flow of the two species of rods on their spatio-temporal organization and the regulations of this interference by the extended bottleneck. By a combination of mean-field theory and computer simulation we calculate the flux of both species of rods and their density profiles as well as the composite phase diagrams of the system. If the bottleneck is sufficiently stringent some of the phases become practically unrealizable although not ruled out on the basis of any fundamental physical principle. Moreover the extent of suppression of flow of the downstream entrants by the flow of the upstream entrants can also be regulated by the strength of the bottleneck. We speculate on the possible implications of the results in the context of the biological phenomenon that motivated the formulation of the theoretical model."
How is the quality of the Default Mode Network (DMN) derived from rs-fASL  related to the acquisition duration?,de31f884,"[{'abstract': '  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily\npractice and academic research stay discreet compared to resting-state BOLD.\nHowever, by giving direct access to cerebral blood flow maps, rs-fASL leads to\nsignificant clinical subject scaled application as CBF can be considered as a\nbiomarker in common neuropathology. Our work here focuses on the link between\noverall quality of rs-fASL and duration of acquisition. To this end, we\nconsider subject self-Default Mode Network (DMN), and assess DMN quality\ndepletion compared to a gold standard DMN depending on the duration of\nacquisition.\n', 'title': 'Resting-state ASL : Toward an optimal sequence duration'}]","Title: Resting-state ASL : Toward an optimal sequence duration

Abstract: Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily practice and academic research stay discreet compared to resting-state BOLD. However, by giving direct access to cerebral blood flow maps, rs-fASL leads to significant clinical subject scaled application as CBF can be considered as a biomarker in common neuropathology. Our work here focuses on the link between overall quality of rs-fASL and duration of acquisition. To this end, we consider subject self-Default Mode Network (DMN), and assess DMN quality depletion compared to a gold standard DMN depending on the duration of acquisition."
How do the equivalent kernels of MLPs with ReLU or Leaky ReLU activations change depending on the weight distribution?,b22aab69,"[{'abstract': '  An interesting approach to analyzing neural networks that has received\nrenewed attention is to examine the equivalent kernel of the neural network.\nThis is based on the fact that a fully connected feedforward network with one\nhidden layer, a certain weight distribution, an activation function, and an\ninfinite number of neurons can be viewed as a mapping into a Hilbert space. We\nderive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for\nall rotationally-invariant weight distributions, generalizing a previous result\nthat required Gaussian weight distributions. Additionally, the Central Limit\nTheorem is used to show that for certain activation functions, kernels\ncorresponding to layers with weight distributions having $0$ mean and finite\nabsolute third moment are asymptotically universal, and are well approximated\nby the kernel corresponding to layers with spherical Gaussian weights. In deep\nnetworks, as depth increases the equivalent kernel approaches a pathological\nfixed point, which can be used to argue why training randomly initialized\nnetworks can be difficult. Our results also have implications for weight\ninitialization.\n', 'title': 'Invariance of Weight Distributions in Rectified MLPs'}]","Title: Invariance of Weight Distributions in Rectified MLPs

Abstract: An interesting approach to analyzing neural networks that has received renewed attention is to examine the equivalent kernel of the neural network. This is based on the fact that a fully connected feedforward network with one hidden layer, a certain weight distribution, an activation function, and an infinite number of neurons can be viewed as a mapping into a Hilbert space. We derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for all rotationally-invariant weight distributions, generalizing a previous result that required Gaussian weight distributions. Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresponding to layers with weight distributions having $0$ mean and finite absolute third moment are asymptotically universal, and are well approximated by the kernel corresponding to layers with spherical Gaussian weights. In deep networks, as depth increases the equivalent kernel approaches a pathological fixed point, which can be used to argue why training randomly initialized networks can be difficult. Our results also have implications for weight initialization."
What is the impact of environmental factors like wind and ground vibration on beam alignment?,400eec78,"[{'abstract': '  Wireless backhaul communication has been recently realized with large\nantennas operating in the millimeter wave (mmWave) frequency band and\nimplementing highly directional beamforming. In this paper, we focus on the\nalignment problem of narrow beams between fixed position network nodes in\nmmWave backhaul systems that are subject to small displacements due to wind\nflow or ground vibration. We consider nodes equipped with antenna arrays that\nare capable of performing only analog processing and communicate through\nwireless channels including a line-of-sight component. Aiming at minimizing the\ntime needed to achieve beam alignment, we present an efficient method that\ncapitalizes on the exchange of position information between the nodes to design\ntheir beamforming and combining vectors. Some numerical results on the outage\nprobability with the proposed beam alignment method offer useful preliminary\ninsights on the impact of some system and operation parameters.\n', 'title': 'Position Aided Beam Alignment for Millimeter Wave Backhaul Systems with Large Phased Arrays'}]","Title: Position Aided Beam Alignment for Millimeter Wave Backhaul Systems with Large Phased Arrays

Abstract: Wireless backhaul communication has been recently realized with large
antennas operating in the millimeter wave (mmWave) frequency band and
implementing highly directional beamforming. In this paper, we focus on the
alignment problem of narrow beams between fixed position network nodes in
mmWave backhaul systems that are subject to small displacements due to wind
flow or ground vibration. We consider nodes equipped with antenna arrays that
are capable of performing only analog processing and communicate through
wireless channels including a line-of-sight component. Aiming at minimizing the
time needed to achieve beam alignment, we present an efficient method that
capitalizes on the exchange of position information between the nodes to design
their beamforming and combining vectors. Some numerical results on the outage
probability with the proposed beam alignment method offer useful preliminary
insights on the impact of some system and operation parameters."
How does the presence of a greenhouse effect influence the potential for liquid water on the surfaces of the TRAPPIST-1 planets?,d9d2efd9,"[{'abstract': '  The recent discovery of the planetary system hosted by the ultracool dwarf\nstar TRAPPIST-1 could open new perspectives into the investigation of planetary\nclimates of Earth-sized exoplanets, their atmospheres and their possible\nhabitability. In this paper, we use a simple climate-vegetation energy-balance\nmodel to study the climate of the seven TRAPPIST-1 planets and the climate\ndependence on the global albedo, on the fraction of vegetation that could cover\ntheir surfaces and on the different greenhouse conditions. The model allows us\nto investigate whether liquid water could be maintained on the planetary\nsurfaces (i.e., by defining a ""surface water zone"") in different planetary\nconditions, with or without the presence of greenhouse effect.\nIt is shown that planet TRAPPIST-1d seems to be the most stable from an\nEarth-like perspective, since it resides in the surface water zone for a wide\nrange of reasonable values of the model parameters. Moreover, according to the\nmodel outer planets (f, g and h) cannot host liquid water on their surfaces,\neven for Earth-like conditions, entering a snowball state. Although very\nsimple, the model allows to extract the main features of the TRAPPIST-1\nplanetary climates.\n', 'title': 'Comparative Climates of TRAPPIST-1 planetary system: results from a simple climate-vegetation model'}]","I think I can help you with that!

Title: Comparative Climates of TRAPPIST-1 planetary system: results from a simple climate-vegetation model

Abstract:   The recent discovery of the planetary system hosted by the ultracool dwarf
star TRAPPIST-1 could open new perspectives into the investigation of planetary
climates of Earth-sized exoplanets, their atmospheres and their possible
habitability. In this paper, we use a simple climate-vegetation energy-balance
model to study the climate of the seven TRAPPIST-1 planets and the climate
dependence on the global albedo, on the fraction of vegetation that could cover
their surfaces and on the different greenhouse conditions. The model allows us
to investigate whether liquid water could be maintained on the planetary
surfaces (i.e., by defining a ""surface water zone"") in different planetary
conditions, with or without the presence of greenhouse effect.
It is shown that planet TRAPPIST-1d seems to be the most stable from an
Earth-like perspective, since it resides in the surface water zone for a wide
range of reasonable values of the model parameters. Moreover, according to the
model outer planets (f, g and h) cannot host liquid water on their surfaces,
even for Earth-like conditions, entering a snowball state. Although very
simple, the model allows to extract the main features of the TRAPPIST-1
planetary climates."
How can multi-color Magnetic Particle Imaging be used to differentiate between nanoparticles?,ec36d39f,"[{'abstract': '  Magnetic Particle Imaging (MPI) is a novel imaging modality with important\napplications such as angiography, stem cell tracking, and cancer imaging.\nRecently, there have been efforts to increase the functionality of MPI via\nmulti-color imaging methods that can distinguish the responses of different\nnanoparticles, or nanoparticles in different environmental conditions. The\nproposed techniques typically rely on extensive calibrations that capture the\ndifferences in the harmonic responses of the nanoparticles. In this work, we\npropose a method to directly estimate the relaxation time constant of the\nnanoparticles from the MPI signal, which is then used to generate a multi-color\nrelaxation map. The technique is based on the underlying mirror symmetry of the\nadiabatic MPI signal when the same region is scanned back and forth. We\nvalidate the proposed method via extensive simulations, and via experiments on\nour in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our\nin-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be\nsuccessfully distinguished with the proposed technique, without any calibration\nor prior knowledge about the nanoparticles.\n', 'title': 'Calibration-Free Relaxation-Based Multi-Color Magnetic Particle Imaging'}]","Title: Calibration-Free Relaxation-Based Multi-Color Magnetic Particle Imaging

Abstract: Magnetic Particle Imaging (MPI) is a novel imaging modality with important applications such as angiography, stem cell tracking, and cancer imaging. Recently, there have been efforts to increase the functionality of MPI via multi-color imaging methods that can distinguish the responses of different nanoparticles, or nanoparticles in different environmental conditions. The proposed techniques typically rely on extensive calibrations that capture the differences in the harmonic responses of the nanoparticles. In this work, we propose a method to directly estimate the relaxation time constant of the nanoparticles from the MPI signal, which is then used to generate a multi-color relaxation map. The technique is based on the underlying mirror symmetry of the adiabatic MPI signal when the same region is scanned back and forth. We validate the proposed method via extensive simulations, and via experiments on our in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our in-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be successfully distinguished with the proposed technique, without any calibration or prior knowledge about the nanoparticles.

Multi-color Magnetic Particle Imaging can be used to differentiate between nanoparticles by estimating the relaxation time constant of the nanoparticles from the MPI signal, which is then used to generate a multi-color relaxation map."
How can the sparsity level of the data be used to determine the optimal value of p for solving PCS via its nonconvex relaxation?,bbca4326,"[{'abstract': '  As a natural extension of compressive sensing and the requirement of some\npractical problems, Phaseless Compressed Sensing (PCS) has been introduced and\nstudied recently. Many theoretical results have been obtained for PCS with the\naid of its convex relaxation. Motivated by successful applications of nonconvex\nrelaxed methods for solving compressive sensing, in this paper, we try to\ninvestigate PCS via its nonconvex relaxation. Specifically, we relax PCS in the\nreal context by the corresponding $\\ell_p$-minimization with $p\\in (0,1)$. We\nshow that there exists a constant $p^\\ast\\in (0,1]$ such that for any fixed\n$p\\in(0, p^\\ast)$, every optimal solution to the $\\ell_p$-minimization also\nsolves the concerned problem; and derive an expression of such a constant\n$p^\\ast$ by making use of the known data and the sparsity level of the\nconcerned problem. These provide a theoretical basis for solving this class of\nproblems via the corresponding $\\ell_p$-minimization.\n', 'title': 'A Theoretical Perspective of Solving Phaseless Compressed Sensing via Its Nonconvex Relaxation'}]","Title: A Theoretical Perspective of Solving Phaseless Compressed Sensing via Its Nonconvex Relaxation

Abstract: As a natural extension of compressive sensing and the requirement of some practical problems, Phaseless Compressed Sensing (PCS) has been introduced and studied recently. Many theoretical results have been obtained for PCS with the aid of its convex relaxation. Motivated by successful applications of nonconvex relaxed methods for solving compressive sensing, in this paper, we try to investigate PCS via its nonconvex relaxation. Specifically, we relax PCS in the real context by the corresponding $\ell_p$-minimization with $p\in (0,1)$. We show that there exists a constant $p^\ast\in (0,1]$ such that for any fixed $p\in(0, p^\ast)$, every optimal solution to the $\ell_p$-minimization also solves the concerned problem; and derive an expression of such a constant $p^\ast$ by making use of the known data and the sparsity level of the concerned problem. These provide a theoretical basis for solving this class of problems via the corresponding $\ell_p$-minimization.

The answer to the question can be found in the abstract above. According to the text, the sparsity level of the data can be used to derive an expression of the optimal value of $p$, denoted as $p^\ast$, which is a constant that depends on the known data and the sparsity level of the concerned problem."
How was the reliability of individual Twitter accounts' predictions evaluated?,ec8b782e,"[{'abstract': '  Social media users often make explicit predictions about upcoming events.\nSuch statements vary in the degree of certainty the author expresses toward the\noutcome:""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo DiCaprio may win""\nor ""No way Leonardo wins!"". Can popular beliefs on social media predict who\nwill win? To answer this question, we build a corpus of tweets annotated for\nveridicality on which we train a log-linear classifier that detects positive\nveridicality with high precision. We then forecast uncertain outcomes using the\nwisdom of crowds, by aggregating users\' explicit predictions. Our method for\nforecasting winners is fully automated, relying only on a set of contenders as\ninput. It requires no training data of past outcomes and outperforms sentiment\nand tweet volume baselines on a broad range of contest prediction tasks. We\nfurther demonstrate how our approach can be used to measure the reliability of\nindividual accounts\' predictions and retrospectively identify surprise\noutcomes.\n', 'title': '""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter'}]","I think I can help you with that!

Title: ""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter

Abstract: Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome:""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo DiCaprio may win"" or ""No way Leonardo wins!"". Can popular beliefs on social media predict who will win? To answer this question, we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision. We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users' explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts' predictions and retrospectively identify surprise outcomes.

You were asking about how the reliability of individual Twitter accounts' predictions was evaluated, and the answer is in this paper!"
How do spin-orbit coupling and electron-hole Stoner excitations contribute to zero-point spin fluctuations in adatoms?,b464b44f,"[{'abstract': '  Stabilizing the magnetic signal of single adatoms is a crucial step towards\ntheir successful usage in widespread technological applications such as\nhigh-density magnetic data storage devices. The quantum mechanical nature of\nthese tiny objects, however, introduces intrinsic zero-point spin-fluctuations\nthat tend to destabilize the local magnetic moment of interest by dwindling the\nmagnetic anisotropy potential barrier even at absolute zero temperature. Here,\nwe elucidate the origins and quantify the effect of the fundamental ingredients\ndetermining the magnitude of the fluctuations, namely the ($i$) local magnetic\nmoment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner\nexcitations. Based on a systematic first-principles study of 3d and 4d adatoms,\nwe demonstrate that the transverse contribution of the fluctuations is\ncomparable in size to the magnetic moment itself, leading to a remarkable\n$\\gtrsim$50$\\%$ reduction of the magnetic anisotropy energy. Our analysis gives\nrise to a comprehensible diagram relating the fluctuation magnitude to\ncharacteristic features of adatoms, providing practical guidelines for\ndesigning magnetically stable nanomagnets with minimal quantum fluctuations.\n', 'title': 'Zero-point spin-fluctuations of single adatoms'}]","Title: Zero-point spin-fluctuations of single adatoms

Abstract: Stabilizing the magnetic signal of single adatoms is a crucial step towards their successful usage in widespread technological applications such as high-density magnetic data storage devices. The quantum mechanical nature of these tiny objects, however, introduces intrinsic zero-point spin-fluctuations that tend to destabilize the local magnetic moment of interest by dwindling the magnetic anisotropy potential barrier even at absolute zero temperature. Here, we elucidate the origins and quantify the effect of the fundamental ingredients determining the magnitude of the fluctuations, namely the ($i$) local magnetic moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner excitations. Based on a systematic first-principles study of 3d and 4d adatoms, we demonstrate that the transverse contribution of the fluctuations is comparable in size to the magnetic moment itself, leading to a remarkable $\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives rise to a comprehensible diagram relating the fluctuation magnitude to characteristic features of adatoms, providing practical guidelines for designing magnetically stable nanomagnets with minimal quantum fluctuations."
What are the implications of this research for housing policy in Australia?,1ce801da,"[{'abstract': '  As affordability pressures and tight rental markets in global cities mount,\nonline shared accommodation sites proliferate. Home sharing arrangements\npresent dilemmas for planning that aims to improve health and safety standards,\nwhile supporting positives such as the usage of dormant stock and the relieving\nof rental pressures on middle/lower income earners. Currently, no formal data\nexists on this internationally growing trend. Here, we present a first\nquantitative glance on shared accommodation practices across all major urban\ncenters of Australia enabled via collection and analysis of thousands of online\nlistings. We examine, countrywide, the spatial and short time scale temporal\ncharacteristics of this market, along with preliminary analysis on rents,\ndwelling types and other characteristics. Findings have implications for\nhousing policy makers and planning practitioners seeking to monitor and respond\nto housing policy and affordability pressures in formal and informal housing\nmarkets.\n', 'title': 'Shared urbanism: Big data on accommodation sharing in urban Australia'}]","Title: Shared urbanism: Big data on accommodation sharing in urban Australia

Abstract: As affordability pressures and tight rental markets in global cities mount, online shared accommodation sites proliferate. Home sharing arrangements present dilemmas for planning that aims to improve health and safety standards, while supporting positives such as the usage of dormant stock and the relieving of rental pressures on middle/lower income earners. Currently, no formal data exists on this internationally growing trend. Here, we present a first quantitative glance on shared accommodation practices across all major urban centers of Australia enabled via collection and analysis of thousands of online listings. We examine, countrywide, the spatial and short time scale temporal characteristics of this market, along with preliminary analysis on rents, dwelling types and other characteristics. Findings have implications for housing policy makers and planning practitioners seeking to monitor and respond to housing policy and affordability pressures in formal and informal housing markets.

The implication of this research for housing policy in Australia is that it provides a first quantitative look at shared accommodation practices across major urban centers in Australia, which can inform housing policy makers and planning practitioners in monitoring and responding to housing policy and affordability pressures in formal and informal housing markets."
"In the paper, what specific mathematical tool is used to characterize the approximation error?",73790098,"[{'abstract': '  Let $E_n(f)_{\\alpha,\\beta,\\gamma}$ denote the error of best approximation by\npolynomials of degree at most $n$ in the space\n$L^2(\\varpi_{\\alpha,\\beta,\\gamma})$ on the triangle $\\{(x,y): x, y \\ge 0, x+y\n\\le 1\\}$, where $\\varpi_{\\alpha,\\beta,\\gamma}(x,y) := x^\\alpha y ^\\beta\n(1-x-y)^\\gamma$ for $\\alpha,\\beta,\\gamma > -1$. Our main result gives a sharp\nestimate of $E_n(f)_{\\alpha,\\beta,\\gamma}$ in terms of the error of best\napproximation for higher order derivatives of $f$ in appropriate Sobolev\nspaces. The result also leads to a characterization of\n$E_n(f)_{\\alpha,\\beta,\\gamma}$ by a weighted $K$-functional.\n', 'title': 'Best polynomial approximation on the triangle'}]","After carefully reviewing the abstracts, I think I have found the answer:

Title: Best polynomial approximation on the triangle

Abstract: Let $E_n(f)_{\alpha,\beta,\gamma}$ denote the error of best approximation by polynomials of degree at most $n$ in the space $L^2(\varpi_{\alpha,\beta,\gamma})$ on the triangle $\{(x,y): x, y \ge 0, x+y \le 1\}$, where $\varpi_{\alpha,\beta,\gamma}(x,y) := x^\alpha y ^\beta (1-x-y)^\gamma$ for $\alpha,\beta,\gamma > -1$. Our main result gives a sharp estimate of $E_n(f)_{\alpha,\beta,\gamma}$ in terms of the error of best approximation for higher order derivatives of $f$ in appropriate Sobolev spaces. The result also leads to a characterization of $E_n(f)_{\alpha,\beta,\gamma}$ by a weighted $K$-functional.

In this paper, the researcher uses a weighted $K$-functional to characterize the approximation error."
What is the 'thermoelectric cooperative effect' and how does it improve device performance?,de94eb12,"[{'abstract': '  The energy efficiency and power of a three-terminal thermoelectric nanodevice\nare studied by considering elastic tunneling through a single quantum dot.\nFacilitated by the three-terminal geometry, the nanodevice is able to generate\nsimultaneously two electrical powers by utilizing only one temperature bias.\nThese two electrical powers can add up constructively or destructively,\ndepending on their signs. It is demonstrated that the constructive addition\nleads to the enhancement of both energy efficiency and output power for various\nsystem parameters. In fact, such enhancement, dubbed as thermoelectric\ncooperative effect, can lead to maximum efficiency and power no less than when\nonly one of the electrical power is harvested.\n', 'title': 'Thermoelectric Cooperative Effect in Three-Terminal Elastic Transport through a Quantum Dot'}]","Title: Thermoelectric Cooperative Effect in Three-Terminal Elastic Transport through a Quantum Dot

Abstract: The energy efficiency and power of a three-terminal thermoelectric nanodevice are studied by considering elastic tunneling through a single quantum dot. Facilitated by the three-terminal geometry, the nanodevice is able to generate simultaneously two electrical powers by utilizing only one temperature bias. These two electrical powers can add up constructively or destructively, depending on their signs. It is demonstrated that the constructive addition leads to the enhancement of both energy efficiency and output power for various system parameters. In fact, such enhancement, dubbed as thermoelectric cooperative effect, can lead to maximum efficiency and power no less than when only one of the electrical power is harvested.

The thermoelectric cooperative effect is a phenomenon where the constructive addition of two electrical powers, generated by a three-terminal thermoelectric nanodevice, enhances both energy efficiency and output power. This effect improves device performance by allowing the device to generate maximum efficiency and power, similar to when only one electrical power is harvested, while utilizing only one temperature bias."
"How do character networks differ across biographical, legendary, and fictional texts?",c68723c8,"[{'abstract': ""  We compare the social character networks of biographical, legendary and\nfictional texts, in search for marks of genre differentiation. We examine the\ndegree distribution of character appearance and find a power law that does not\ndepend on the literary genre or historical content. We also analyze local and\nglobal complex networks measures, in particular, correlation plots between the\nrecently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and\nCloseness centralities. Assortativity plots, which previous literature claims\nto separate fictional from real social networks, were also studied. We've found\nno relevant differences in the books for these network measures and we give a\nplausible explanation why the previous assortativity result is not correct.\n"", 'title': 'Character Networks and Book Genre Classification'}]","Title: Character Networks and Book Genre Classification

Abstract: We compare the social character networks of biographical, legendary and fictional texts, in search for marks of genre differentiation. We examine the degree distribution of character appearance and find a power law that does not depend on the literary genre or historical content. We also analyze local and global complex networks measures, in particular, correlation plots between the recently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and Closeness centralities. Assortativity plots, which previous literature claims to separate fictional from real social networks, were also studied. We've found no relevant differences in the books for these network measures and we give a plausible explanation why the previous assortativity result is not correct."
What is the role of the H2 Hardy space in the context of linear time-periodic model reduction?,b993ab04,"[{'abstract': '  Linear time-periodic (LTP) dynamical systems frequently appear in the\nmodeling of phenomena related to fluid dynamics, electronic circuits, and\nstructural mechanics via linearization centered around known periodic orbits of\nnonlinear models. Such LTP systems can reach orders that make repeated\nsimulation or other necessary analysis prohibitive, motivating the need for\nmodel reduction.\nWe develop here an algorithmic framework for constructing reduced models that\nretains the linear time-periodic structure of the original LTP system. Our\napproach generalizes optimal approaches that have been established previously\nfor linear time-invariant (LTI) model reduction problems. We employ an\nextension of the usual H2 Hardy space defined for the LTI setting to\ntime-periodic systems and within this broader framework develop an a posteriori\nerror bound expressible in terms of related LTI systems. Optimization of this\nbound motivates our algorithm. We illustrate the success of our method on two\nnumerical examples.\n', 'title': 'Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework'}]","Title: Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework

Abstract: Linear time-periodic (LTP) dynamical systems frequently appear in the
modeling of phenomena related to fluid dynamics, electronic circuits, and
structural mechanics via linearization centered around known periodic orbits of
nonlinear models. Such LTP systems can reach orders that make repeated
simulation or other necessary analysis prohibitive, motivating the need for
model reduction.
We develop here an algorithmic framework for constructing reduced models that
retains the linear time-periodic structure of the original LTP system. Our
approach generalizes optimal approaches that have been established previously
for linear time-invariant (LTI) model reduction problems. We employ an
extension of the usual H2 Hardy space defined for the LTI setting to
time-periodic systems and within this broader framework develop an a posteriori
error bound expressible in terms of related LTI systems. Optimization of this
bound motivates our algorithm. We illustrate the success of our method on two
numerical examples.

The H2 Hardy space is used in this context to develop an a posteriori error bound for the model reduction framework, which is then optimized to motivate the algorithm."
What types of recommender systems are susceptible to poisoning attacks?,77d27988,"[{'abstract': '  Recommender system is an important component of many web services to help\nusers locate items that match their interests. Several studies showed that\nrecommender systems are vulnerable to poisoning attacks, in which an attacker\ninjects fake data to a given system such that the system makes recommendations\nas the attacker desires. However, these poisoning attacks are either agnostic\nto recommendation algorithms or optimized to recommender systems that are not\ngraph-based. Like association-rule-based and matrix-factorization-based\nrecommender systems, graph-based recommender system is also deployed in\npractice, e.g., eBay, Huawei App Store. However, how to design optimized\npoisoning attacks for graph-based recommender systems is still an open problem.\nIn this work, we perform a systematic study on poisoning attacks to graph-based\nrecommender systems. Due to limited resources and to avoid detection, we assume\nthe number of fake users that can be injected into the system is bounded. The\nkey challenge is how to assign rating scores to the fake users such that the\ntarget item is recommended to as many normal users as possible. To address the\nchallenge, we formulate the poisoning attacks as an optimization problem,\nsolving which determines the rating scores for the fake users. We also propose\ntechniques to solve the optimization problem. We evaluate our attacks and\ncompare them with existing attacks under white-box (recommendation algorithm\nand its parameters are known), gray-box (recommendation algorithm is known but\nits parameters are unknown), and black-box (recommendation algorithm is\nunknown) settings using two real-world datasets. Our results show that our\nattack is effective and outperforms existing attacks for graph-based\nrecommender systems. For instance, when 1% fake users are injected, our attack\ncan make a target item recommended to 580 times more normal users in certain\nscenarios.\n', 'title': 'Poisoning Attacks to Graph-Based Recommender Systems'}]","Title: Poisoning Attacks to Graph-Based Recommender Systems

Abstract: Recommender system is an important component of many web services to help users locate items that match their interests. Several studies showed that recommender systems are vulnerable to poisoning attacks, in which an attacker injects fake data to a given system such that the system makes recommendations as the attacker desires. However, these poisoning attacks are either agnostic to recommendation algorithms or optimized to recommender systems that are not graph-based. Like association-rule-based and matrix-factorization-based recommender systems, graph-based recommender system is also deployed in practice, e.g., eBay, Huawei App Store. However, how to design optimized poisoning attacks for graph-based recommender systems is still an open problem.

According to this abstract, the types of recommender systems that are susceptible to poisoning attacks are:

1. Association-rule-based recommender systems
2. Matrix-factorization-based recommender systems
3. Graph-based recommender systems"
How does the density of inter-links between layers impact global synchronization in duplex networks? ,47a6008e,"[{'abstract': ""  Synchronization on multiplex networks have attracted increasing attention in\nthe past few years. We investigate collective behaviors of Kuramoto oscillators\non single layer and duplex spacial networks with total cost restriction, which\nwas introduced by Li et. al [Li G., Reis S. D., Moreira A. A., Havlin S.,\nStanley H. E. and Jr A. J., {\\it Phys. Rev. Lett.} 104, 018701 (2010)] and\ntermed as the Li network afterwards. In the Li network model, with the increase\nof its spacial exponent, the network's structure will vary from the random type\nto the small-world one, and finally to the regular lattice.We first explore how\nthe spacial exponent influences the synchronizability of Kuramoto oscillators\non single layer Li networks and find that the closer the Li network is to a\nregular lattice, the more difficult for it to evolve into synchronization. Then\nwe investigate synchronizability of duplex Li networks and find that the\nexistence of inter-layer interaction can greatly enhance inter-layer and global\nsynchronizability. When the inter-layer coupling strength is larger than a\ncertain critical value, whatever the intra-layer coupling strength is, the\ninter-layer synchronization will always occur. Furthermore, on single layer Li\nnetworks, nodes with larger degrees more easily reach global synchronization,\nwhile on duplex Li networks, this phenomenon becomes much less obvious.\nFinally, we study the impact of inter-link density on global synchronization\nand obtain that sparse inter-links can lead to the emergence of global\nsynchronization for duplex Li networks just as dense inter-links do. In a word,\ninter-layer interaction plays a vital role in determining synchronizability for\nduplex spacial networks with total cost constraint.\n"", 'title': 'Phase Synchronization on Spacially Embeded Duplex Networks with Total Cost Constraint'}]","I think I have found the answer you're looking for!

Title: Phase Synchronization on Spacially Embeded Duplex Networks with Total Cost Constraint

Abstract: Synchronization on multiplex networks have attracted increasing attention in the past few years. We investigate collective behaviors of Kuramoto oscillators on single layer and duplex spacial networks with total cost restriction, which was introduced by Li et. al [Li G., Reis S. D., Moreira A. A., Havlin S., Stanley H. E. and Jr A. J., {\it Phys. Rev. Lett.} 104, 018701 (2010)] and termed as the Li network afterwards. In the Li network model, with the increase of its spacial exponent, the network's structure will vary from the random type to the small-world one, and finally to the regular lattice.We first explore how the spacial exponent influences the synchronizability of Kuramoto oscillators on single layer Li networks and find that the closer the Li network is to a regular lattice, the more difficult for it to evolve into synchronization. Then we investigate synchronizability of duplex Li networks and find that the existence of inter-layer interaction can greatly enhance inter-layer and global synchronizability. When the inter-layer coupling strength is larger than a certain critical value, whatever the intra-layer coupling strength is, the inter-layer synchronization will always occur. Furthermore, on single layer Li networks, nodes with larger degrees more easily reach global synchronization, while on duplex Li networks, this phenomenon becomes much less obvious. Finally, we study the impact of inter-link density on global synchronization and obtain that sparse inter-links can lead to the emergence of global synchronization for duplex Li networks just as dense inter-links do. In a word, inter-layer interaction plays a vital role in determining synchronizability for duplex spacial networks with total cost constraint.

According to the abstract, the density of inter-links between layers does not have a negative impact on global synchronization in duplex networks. In fact, both sparse and dense inter-links can lead to the emergence of global synchronization."
Where can I find a paper that offers a straightforward description of Karmarkar's Algorithm? ,dcb42f5f,"[{'abstract': ""  An extremely simple, description of Karmarkar's algorithm with very few\ntechnical terms is given.\n"", 'title': ""A simple introduction to Karmarkar's Algorithm for Linear Programming""}]","I think I can help you with that!

Title: A simple introduction to Karmarkar's Algorithm for Linear Programming

Abstract: An extremely simple, description of Karmarkar's algorithm with very few technical terms is given."
In what type of network setting does a paper examine the use of Coded Caching with multiple-antenna base stations?,ed653419,"[{'abstract': '  In this paper we consider a single-cell downlink scenario where a\nmultiple-antenna base station delivers contents to multiple cache-enabled user\nterminals. Based on the multicasting opportunities provided by the so-called\nCoded Caching technique, we investigate three delivery approaches. Our baseline\nscheme employs the coded caching technique on top of max-min fair multicasting.\nThe second one consists of a joint design of Zero-Forcing (ZF) and coded\ncaching, where the coded chunks are formed in the signal domain (complex\nfield). The third scheme is similar to the second one with the difference that\nthe coded chunks are formed in the data domain (finite field). We derive\nclosed-form rate expressions where our results suggest that the latter two\nschemes surpass the first one in terms of Degrees of Freedom (DoF). However, at\nthe intermediate SNR regime forming coded chunks in the signal domain results\nin power loss, and will deteriorate throughput of the second scheme. The main\nmessage of our paper is that the schemes performing well in terms of DoF may\nnot be directly appropriate for intermediate SNR regimes, and modified schemes\nshould be employed.\n', 'title': 'Multi-Antenna Coded Caching'}]","Title: Multi-Antenna Coded Caching

Abstract: In this paper we consider a single-cell downlink scenario where a multiple-antenna base station delivers contents to multiple cache-enabled user terminals. Based on the multicasting opportunities provided by the so-called Coded Caching technique, we investigate three delivery approaches. Our baseline scheme employs the coded caching technique on top of max-min fair multicasting. The second one consists of a joint design of Zero-Forcing (ZF) and coded caching, where the coded chunks are formed in the signal domain (complex field). The third scheme is similar to the second one with the difference that the coded chunks are formed in the data domain (finite field). We derive closed-form rate expressions where our results suggest that the latter two schemes surpass the first one in terms of Degrees of Freedom (DoF). However, at the intermediate SNR regime forming coded chunks in the signal domain results in power loss, and will deteriorate throughput of the second scheme. The main message of our paper is that the schemes performing well in terms of DoF may not be directly appropriate for intermediate SNR regimes, and modified schemes should be employed."
What are the advantages of using a sparse representation for modeling vortex dynamics?,8a9d659e,"[{'abstract': '  We examine discrete vortex dynamics in two-dimensional flow through a\nnetwork-theoretic approach. The interaction of the vortices is represented with\na graph, which allows the use of network-theoretic approaches to identify key\nvortex-to-vortex interactions. We employ sparsification techniques on these\ngraph representations based on spectral theory for constructing sparsified\nmodels and evaluating the dynamics of vortices in the sparsified setup.\nIdentification of vortex structures based on graph sparsification and sparse\nvortex dynamics are illustrated through an example of point-vortex clusters\ninteracting amongst themselves. We also evaluate the performance of\nsparsification with increasing number of point vortices. The\nsparsified-dynamics model developed with spectral graph theory requires reduced\nnumber of vortex-to-vortex interactions but agrees well with the full nonlinear\ndynamics. Furthermore, the sparsified model derived from the sparse graphs\nconserves the invariants of discrete vortex dynamics. We highlight the\nsimilarities and differences between the present sparsified-dynamics model and\nthe reduced-order models.\n', 'title': 'Network-theoretic approach to sparsified discrete vortex dynamics'}]","The researcher is looking for the advantages of using a sparse representation for modeling vortex dynamics. 

Title: Network-theoretic approach to sparsified discrete vortex dynamics

Abstract: We examine discrete vortex dynamics in two-dimensional flow through a network-theoretic approach. The interaction of the vortices is represented with a graph, which allows the use of network-theoretic approaches to identify key vortex-to-vortex interactions. We employ sparsification techniques on these graph representations based on spectral theory for constructing sparsified models and evaluating the dynamics of vortices in the sparsified setup. Identification of vortex structures based on graph sparsification and sparse vortex dynamics are illustrated through an example of point-vortex clusters interacting amongst themselves. We also evaluate the performance of sparsification with increasing number of point vortices. The sparsified-dynamics model developed with spectral graph theory requires reduced number of vortex-to-vortex interactions but agrees well with the full nonlinear dynamics. Furthermore, the sparsified model derived from the sparse graphs conserves the invariants of discrete vortex dynamics. We highlight the similarities and differences between the present sparsified-dynamics model and the reduced-order models.

The advantages of using a sparse representation for modeling vortex dynamics are that it requires a reduced number of vortex-to-vortex interactions and agrees well with the full nonlinear dynamics, while also conserving the invariants of discrete vortex dynamics."
How does an oracle-based computational model help establish minimax bounds for these data analysis problems?,02e195cb,"[{'abstract': '  We study the fundamental tradeoffs between statistical accuracy and\ncomputational tractability in the analysis of high dimensional heterogeneous\ndata. As examples, we study sparse Gaussian mixture model, mixture of sparse\nlinear regressions, and sparse phase retrieval model. For these models, we\nexploit an oracle-based computational model to establish conjecture-free\ncomputationally feasible minimax lower bounds, which quantify the minimum\nsignal strength required for the existence of any algorithm that is both\ncomputationally tractable and statistically accurate. Our analysis shows that\nthere exist significant gaps between computationally feasible minimax risks and\nclassical ones. These gaps quantify the statistical price we must pay to\nachieve computational tractability in the presence of data heterogeneity. Our\nresults cover the problems of detection, estimation, support recovery, and\nclustering, and moreover, resolve several conjectures of Azizyan et al. (2013,\n2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our\nresults reveal a new but counter-intuitive phenomenon in heterogeneous data\nanalysis that more data might lead to less computation complexity.\n', 'title': 'Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models and Phase Retrieval'}]","Here is the answer to the question:

Title: Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models and Phase Retrieval

Abstract: We study the fundamental tradeoffs between statistical accuracy and computational tractability in the analysis of high dimensional heterogeneous data. As examples, we study sparse Gaussian mixture model, mixture of sparse linear regressions, and sparse phase retrieval model. For these models, we exploit an oracle-based computational model to establish conjecture-free computationally feasible minimax lower bounds, which quantify the minimum signal strength required for the existence of any algorithm that is both computationally tractable and statistically accurate. Our analysis shows that there exist significant gaps between computationally feasible minimax risks and classical ones. These gaps quantify the statistical price we must pay to achieve computational tractability in the presence of data heterogeneity. Our results cover the problems of detection, estimation, support recovery, and clustering, and moreover, resolve several conjectures of Azizyan et al. (2013, 2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our results reveal a new but counter-intuitive phenomenon in heterogeneous data analysis that more data might lead to less computation complexity."
How was the accuracy of identifying pregnant women on social media evaluated?,a5429ad3,"[{'abstract': '  Widespread use of social media has led to the generation of substantial\namounts of information about individuals, including health-related information.\nSocial media provides the opportunity to study health-related information about\nselected population groups who may be of interest for a particular study. In\nthis paper, we explore the possibility of utilizing social media to perform\ntargeted data collection and analysis from a particular population group --\npregnant women. We hypothesize that we can use social media to identify cohorts\nof pregnant women and follow them over time to analyze crucial health-related\ninformation. To identify potentially pregnant women, we employ simple\nrule-based searches that attempt to detect pregnancy announcements with\nmoderate precision. To further filter out false positives and noise, we employ\na supervised classifier using a small number of hand-annotated data. We then\ncollect their posts over time to create longitudinal health timelines and\nattempt to divide the timelines into different pregnancy trimesters. Finally,\nwe assess the usefulness of the timelines by performing a preliminary analysis\nto estimate drug intake patterns of our cohort at different trimesters. Our\nrule-based cohort identification technique collected 53,820 users over thirty\nmonths from Twitter. Our pregnancy announcement classification technique\nachieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user\ntimelines. Analysis of the timelines revealed that pertinent health-related\ninformation, such as drug-intake and adverse reactions can be mined from the\ndata. Our approach to using user timelines in this fashion has produced very\nencouraging results and can be employed for other important tasks where\ncohorts, for which health-related information may not be available from other\nsources, are required to be followed over time to derive population-based\nestimates.\n', 'title': 'Social media mining for identification and exploration of health-related information from pregnant women'}]","Title: Social media mining for identification and exploration of health-related information from pregnant women

Abstract: Widespread use of social media has led to the generation of substantial
amounts of information about individuals, including health-related information.
Social media provides the opportunity to study health-related information about
selected population groups who may be of interest for a particular study. In
this paper, we explore the possibility of utilizing social media to perform
targeted data collection and analysis from a particular population group --
pregnant women. We hypothesize that we can use social media to identify cohorts
of pregnant women and follow them over time to analyze crucial health-related
information. To identify potentially pregnant women, we employ simple
rule-based searches that attempt to detect pregnancy announcements with
moderate precision. To further filter out false positives and noise, we employ
a supervised classifier using a small number of hand-annotated data. We then
collect their posts over time to create longitudinal health timelines and
attempt to divide the timelines into different pregnancy trimesters. Finally,
we assess the usefulness of the timelines by performing a preliminary analysis
to estimate drug intake patterns of our cohort at different trimesters. Our
rule-based cohort identification technique collected 53,820 users over thirty
months from Twitter. Our pregnancy announcement classification technique
achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user
timelines. Analysis of the timelines revealed that pertinent health-related
information, such as drug-intake and adverse reactions can be mined from the
data. Our approach to using user timelines in this fashion has produced very
encouraging results and can be employed for other important tasks where
cohorts, for which health-related information may not be available from other
sources, are required to be followed over time to derive population-based
estimates.

The accuracy of identifying pregnant women on social media was evaluated by using a supervised classifier with a small number of hand-annotated data, which achieved an F-measure of 0.81 for the pregnancy class."
How is a finite-dimensional Dirac structure approximated from a Stokes-Dirac structure in the context of port-Hamiltonian systems?,3cea44dd,"[{'abstract': '  We present the mixed Galerkin discretization of distributed parameter\nport-Hamiltonian systems. On the prototypical example of hyperbolic systems of\ntwo conservation laws in arbitrary spatial dimension, we derive the main\ncontributions: (i) A weak formulation of the underlying geometric\n(Stokes-Dirac) structure with a segmented boundary according to the causality\nof the boundary ports. (ii) The geometric approximation of the Stokes-Dirac\nstructure by a finite-dimensional Dirac structure is realized using a mixed\nGalerkin approach and power-preserving linear maps, which define minimal\ndiscrete power variables. (iii) With a consistent approximation of the\nHamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.\nBy the degrees of freedom in the power-preserving maps, the resulting family of\nstructure-preserving schemes allows for trade-offs between centered\napproximations and upwinding. We illustrate the method on the example of\nWhitney finite elements on a 2D simplicial triangulation and compare the\neigenvalue approximation in 1D with a related approach.\n', 'title': 'Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems'}]","The answer to your question can be found in the paper:

Title: Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems

Abstract: We present the mixed Galerkin discretization of distributed parameter port-Hamiltonian systems. On the prototypical example of hyperbolic systems of two conservation laws in arbitrary spatial dimension, we derive the main contributions: (i) A weak formulation of the underlying geometric (Stokes-Dirac) structure with a segmented boundary according to the causality of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac structure by a finite-dimensional Dirac structure is realized using a mixed Galerkin approach and power-preserving linear maps, which define minimal discrete power variables. (iii) With a consistent approximation of the Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models. By the degrees of freedom in the power-preserving maps, the resulting family of structure-preserving schemes allows for trade-offs between centered approximations and upwinding. We illustrate the method on the example of Whitney finite elements on a 2D simplicial triangulation and compare the eigenvalue approximation in 1D with a related approach."
How do sparse neural network architectures differ from traditional fully connected neural networks?,dd4ea735,"[{'abstract': '  We propose Sparse Neural Network architectures that are based on random or\nstructured bipartite graph topologies. Sparse architectures provide compression\nof the models learned and speed-ups of computations, they can also surpass\ntheir unstructured or fully connected counterparts. As we show, even more\ncompact topologies of the so-called SNN (Sparse Neural Network) can be achieved\nwith the use of structured graphs of connections between consecutive layers of\nneurons. In this paper, we investigate how the accuracy and training speed of\nthe models depend on the topology and sparsity of the neural network. Previous\napproaches using sparcity are all based on fully connected neural network\nmodels and create sparcity during training phase, instead we explicitly define\na sparse architectures of connections before the training. Building compact\nneural network models is coherent with empirical observations showing that\nthere is much redundancy in learned neural network models. We show\nexperimentally that the accuracy of the models learned with neural networks\ndepends on expander-like properties of the underlying topologies such as the\nspectral gap and algebraic connectivity rather than the density of the graphs\nof connections.\n', 'title': 'Sparse Neural Networks Topologies'}]","Title: Sparse Neural Networks Topologies

Abstract: We propose Sparse Neural Network architectures that are based on random or structured bipartite graph topologies. Sparse architectures provide compression of the models learned and speed-ups of computations, they can also surpass their unstructured or fully connected counterparts. As we show, even more compact topologies of the so-called SNN (Sparse Neural Network) can be achieved with the use of structured graphs of connections between consecutive layers of neurons. In this paper, we investigate how the accuracy and training speed of the models depend on the topology and sparsity of the neural network. Previous approaches using sparcity are all based on fully connected neural network models and create sparcity during training phase, instead we explicitly define a sparse architectures of connections before the training. Building compact neural network models is coherent with empirical observations showing that there is much redundancy in learned neural network models. We show experimentally that the accuracy of the models learned with neural networks depends on expander-like properties of the underlying topologies such as the spectral gap and algebraic connectivity rather than the density of the graphs of connections.

The key difference mentioned in this abstract is that sparse neural networks are explicitly defined with a sparse architecture of connections before training, whereas traditional fully connected neural networks create sparsity during the training phase. This difference leads to more compact models, compression of learned models, and speed-ups of computations."
"What happens to the four-point structure factor S4(q,t) as the packing fraction of a bidisperse granular fluid approaches its critical value?",b5cc7e62,"[{'abstract': '  A two-dimensional bidisperse granular fluid is shown to exhibit pronounced\nlong-ranged dynamical heterogeneities as dynamical arrest is approached. Here\nwe focus on the most direct approach to study these heterogeneities: we\nidentify clusters of slow particles and determine their size, $N_c$, and their\nradius of gyration, $R_G$. We show that $N_c\\propto R_G^{d_f}$, providing\ndirect evidence that the most immobile particles arrange in fractal objects\nwith a fractal dimension, $d_f$, that is observed to increase with packing\nfraction $\\phi$. The cluster size distribution obeys scaling, approaching an\nalgebraic decay in the limit of structural arrest, i.e., $\\phi\\to\\phi_c$.\nAlternatively, dynamical heterogeneities are analyzed via the four-point\nstructure factor $S_4(q,t)$ and the dynamical susceptibility $\\chi_4(t)$.\n$S_4(q,t)$ is shown to obey scaling in the full range of packing fractions,\n$0.6\\leq\\phi\\leq 0.805$, and to become increasingly long-ranged as\n$\\phi\\to\\phi_c$. Finite size scaling of $\\chi_4(t)$ provides a consistency\ncheck for the previously analyzed divergences of $\\chi_4(t)\\propto\n(\\phi-\\phi_c)^{-\\gamma_{\\chi}}$ and the correlation length $\\xi\\propto\n(\\phi-\\phi_c)^{-\\gamma_{\\xi}}$. We check the robustness of our results with\nrespect to our definition of mobility. The divergences and the scaling for\n$\\phi\\to\\phi_c$ suggest a non-equilibrium glass transition which seems\nqualitatively independent of the coefficient of restitution.\n', 'title': 'Slow and Long-ranged Dynamical Heterogeneities in Dissipative Fluids'}]","Here is the answer:

Title: Slow and Long-ranged Dynamical Heterogeneities in Dissipative Fluids

Abstract:  

A two-dimensional bidisperse granular fluid is shown to exhibit pronounced
long-ranged dynamical heterogeneities as dynamical arrest is approached. Here
we focus on the most direct approach to study these heterogeneities: we
identify clusters of slow particles and determine their size, $N_c$, and their
radius of gyration, $R_G$. We show that $N_c\propto R_G^{d_f}$, providing
direct evidence that the most immobile particles arrange in fractal objects
with a fractal dimension, $d_f$, that is observed to increase with packing
fraction $\phi$. The cluster size distribution obeys scaling, approaching an
algebraic decay in the limit of structural arrest, i.e., $\phi\to\phi_c$.
Alternatively, dynamical heterogeneities are analyzed via the four-point
structure factor $S_4(q,t)$ and the dynamical susceptibility $\chi_4(t)$.
$S_4(q,t)$ is shown to obey scaling in the full range of packing fractions,
$0.6\leq\phi\leq 0.805$, and to become increasingly long-ranged as
$\phi\to\phi_c$.

According to this abstract, as the packing fraction of a bidisperse granular fluid approaches its critical value ($\phi\to\phi_c$), the four-point structure factor $S_4(q,t)$ is shown to obey scaling and becomes increasingly long-ranged."
Does a log-linear classifier accurately detect positive veridicality in tweets about upcoming events?,ec8b782e,"[{'abstract': '  Social media users often make explicit predictions about upcoming events.\nSuch statements vary in the degree of certainty the author expresses toward the\noutcome:""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo DiCaprio may win""\nor ""No way Leonardo wins!"". Can popular beliefs on social media predict who\nwill win? To answer this question, we build a corpus of tweets annotated for\nveridicality on which we train a log-linear classifier that detects positive\nveridicality with high precision. We then forecast uncertain outcomes using the\nwisdom of crowds, by aggregating users\' explicit predictions. Our method for\nforecasting winners is fully automated, relying only on a set of contenders as\ninput. It requires no training data of past outcomes and outperforms sentiment\nand tweet volume baselines on a broad range of contest prediction tasks. We\nfurther demonstrate how our approach can be used to measure the reliability of\nindividual accounts\' predictions and retrospectively identify surprise\noutcomes.\n', 'title': '""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter'}]","Title: ""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter

Abstract: Social media users often make explicit predictions about upcoming events.
Such statements vary in the degree of certainty the author expresses toward the
outcome:""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo DiCaprio may win""
or ""No way Leonardo wins!"". Can popular beliefs on social media predict who
will win? To answer this question, we build a corpus of tweets annotated for
veridicality on which we train a log-linear classifier that detects positive
veridicality with high precision. We then forecast uncertain outcomes using the
wisdom of crowds, by aggregating users' explicit predictions. Our method for
forecasting winners is fully automated, relying only on a set of contenders as
input. It requires no training data of past outcomes and outperforms sentiment
and tweet volume baselines on a broad range of contest prediction tasks. We
further demonstrate how our approach can be used to measure the reliability of
individual accounts' predictions and retrospectively identify surprise
outcomes."
What is the Ultimatum Game and how does payout depend on the players' decisions?,ae574768,"[{'abstract': '  In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to\nallocate a certain amount of money between herself and a ""responder"". If the\noffer is greater than or equal to the responder\'s minimum acceptable offer\n(MAO), then the money is split as proposed, otherwise, neither the proposer nor\nthe responder get anything. The UG has intrigued generations of behavioral\nscientists because people in experiments blatantly violate the equilibrium\npredictions that self-interested proposers offer the minimum available non-zero\namount, and self-interested responders accept. Why are these predictions\nviolated? Previous research has mainly focused on the role of social\npreferences. Little is known about the role of general moral preferences for\ndoing the right thing, preferences that have been shown to play a major role in\nother social interactions (e.g., Dictator Game and Prisoner\'s Dilemma). Here I\ndevelop a theoretical model and an experiment designed to pit social\npreferences against moral preferences. I find that, although people recognize\nthat offering half and rejecting low offers are the morally right things to do,\nmoral preferences have no causal impact on UG behavior. The experimental data\nare indeed well fit by a model according to which: (i) high UG offers are\nmotivated by inequity aversion and, to a lesser extent, self-interest; (ii)\nhigh MAOs are motivated by inequity aversion.\n', 'title': 'Social versus Moral preferences in the Ultimatum Game: A theoretical model and an experiment'}]","Title: Social versus Moral preferences in the Ultimatum Game: A theoretical model and an experiment

Abstract: In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to allocate a certain amount of money between herself and a ""responder"". If the offer is greater than or equal to the responder's minimum acceptable offer (MAO), then the money is split as proposed, otherwise, neither the proposer nor the responder get anything."
Can a neural network improve the efficiency of importance sampling?,d1a7a20a,"[{'abstract': '  We consider the problem of inference in a causal generative model where the\nset of available observations differs between data instances. We show how\ncombining samples drawn from the graphical model with an appropriate masking\nfunction makes it possible to train a single neural network to approximate all\nthe corresponding conditional marginal distributions and thus amortize the cost\nof inference. We further demonstrate that the efficiency of importance sampling\nmay be improved by basing proposals on the output of the neural network. We\nalso outline how the same network can be used to generate samples from an\napproximate joint posterior via a chain decomposition of the graph.\n', 'title': 'A Universal Marginalizer for Amortized Inference in Generative Models'}]","Title: A Universal Marginalizer for Amortized Inference in Generative Models

Abstract: We consider the problem of inference in a causal generative model where the set of available observations differs between data instances. We show how combining samples drawn from the graphical model with an appropriate masking function makes it possible to train a single neural network to approximate all the corresponding conditional marginal distributions and thus amortize the cost of inference. We further demonstrate that the efficiency of importance sampling may be improved by basing proposals on the output of the neural network. We also outline how the same network can be used to generate samples from an approximate joint posterior via a chain decomposition of the graph."
What type of finite elements are typically used in conjunction with LR NURBS for discretizing contact surfaces in solids?,a11c6f18,"[{'abstract': '  A novel adaptive local surface refinement technique based on Locally Refined\nNon-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model\ncomplex geometries exactly and are the rational extension of LR B-splines. The\nlocal representation of the parameter space overcomes the drawback of\nnon-existent local refinement in standard NURBS-based isogeometric analysis.\nFor a convenient embedding into general finite element code, the Bézier\nextraction operator for LR NURBS is formulated. An automatic remeshing\ntechnique is presented that allows adaptive local refinement and coarsening of\nLR NURBS. In this work, LR NURBS are applied to contact computations of 3D\nsolids and membranes. For solids, LR NURBS-enriched finite elements are used to\ndiscretize the contact surfaces with LR NURBS finite elements, while the rest\nof the body is discretized by linear Lagrange finite elements. For membranes,\nthe entire surface is discretized by LR NURBS. Various numerical examples are\nshown, and they demonstrate the benefit of using LR NURBS: Compared to uniform\nrefinement, LR NURBS can achieve high accuracy at lower computational cost.\n', 'title': 'Adaptive local surface refinement based on LR NURBS and its application to contact'}]","Title: Adaptive local surface refinement based on LR NURBS and its application to contact

Abstract:   A novel adaptive local surface refinement technique based on Locally Refined
Non-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model
complex geometries exactly and are the rational extension of LR B-splines. The
local representation of the parameter space overcomes the drawback of
non-existent local refinement in standard NURBS-based isogeometric analysis.
For a convenient embedding into general finite element code, the Bézier
extraction operator for LR NURBS is formulated. An automatic remeshing
technique is presented that allows adaptive local refinement and coarsening of
LR NURBS. In this work, LR NURBS are applied to contact computations of 3D
solids and membranes. For solids, LR NURBS-enriched finite elements are used to
discretize the contact surfaces with LR NURBS finite elements, while the rest
of the body is discretized by linear Lagrange finite elements. For membranes,
the entire surface is discretized by LR NURBS. Various numerical examples are
shown, and they demonstrate the benefit of using LR NURBS: Compared to uniform
refinement, LR NURBS can achieve high accuracy at lower computational cost.

Answer: LR NURBS-enriched finite elements are typically used in conjunction with LR NURBS for discretizing contact surfaces in solids."
What type of scheduling policy is presented in the paper for Networked Control Systems?,42e17545,"[{'abstract': '  Networked control systems (NCS) have attracted considerable attention in\nrecent years. While the stabilizability and optimal control of NCS for a given\ncommunication system has already been studied extensively, the design of the\ncommunication system for NCS has recently seen an increase in more thorough\ninvestigation. In this paper, we address an optimal scheduling problem for a\nset of NCS sharing a dedicated communication channel, providing performance\nbounds and asymptotic stability. We derive a suboptimal scheduling policy with\ndynamic state-based priorities calculated at the sensors, which are then used\nfor stateless priority queuing in the network, making it both scalable and\nefficient to implement on routers or multi-layer switches. These properties are\nbeneficial towards leveraging existing IP networks for control, which will be a\ncrucial factor for the proliferation of wide-area NCS applications. By allowing\nfor an arbitrary number of concurrent transmissions, we are able to investigate\nthe relationship between available bandwidth, transmission rate, and delay. To\ndemonstrate the feasibility of our approach, we provide a proof-of-concept\nimplementation of the priority scheduler using real networking hardware.\n', 'title': 'State-dependent Priority Scheduling for Networked Control Systems'}]","Title: State-dependent Priority Scheduling for Networked Control Systems

Abstract: Networked control systems (NCS) have attracted considerable attention in recent years. While the stabilizability and optimal control of NCS for a given communication system has already been studied extensively, the design of the communication system for NCS has recently seen an increase in more thorough investigation. In this paper, we address an optimal scheduling problem for a set of NCS sharing a dedicated communication channel, providing performance bounds and asymptotic stability. We derive a suboptimal scheduling policy with dynamic state-based priorities calculated at the sensors, which are then used for stateless priority queuing in the network, making it both scalable and efficient to implement on routers or multi-layer switches. These properties are beneficial towards leveraging existing IP networks for control, which will be a crucial factor for the proliferation of wide-area NCS applications. By allowing for an arbitrary number of concurrent transmissions, we are able to investigate the relationship between available bandwidth, transmission rate, and delay. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of the priority scheduler using real networking hardware.

The scheduling policy presented in the paper is a suboptimal scheduling policy with dynamic state-based priorities calculated at the sensors."
What are the potential applications of a controllable and ultra-high strain in silicon carbide?,63e8cec4,"[{'abstract': '  Strain engineering has attracted great attention, particularly for epitaxial\nfilms grown on a different substrate. Residual strains of SiC have been widely\nemployed to form ultra-high frequency and high Q factor resonators. However, to\ndate the highest residual strain of SiC was reported to be limited to\napproximately 0.6%. Large strains induced into SiC could lead to several\ninteresting physical phenomena, as well as significant improvement of resonant\nfrequencies. We report an unprecedented nano strain-amplifier structure with an\nultra-high residual strain up to 8% utilizing the natural residual stress\nbetween epitaxial 3C SiC and Si. In addition, the applied strain can be tuned\nby changing the dimensions of the amplifier structure. The possibility of\nintroducing such a controllable and ultra-high strain will open the door to\ninvestigating the physics of SiC in large strain regimes, and the development\nof ultra sensitive mechanical sensors.\n', 'title': 'Ultra-high strain in epitaxial silicon carbide nanostructures utilizing residual stress amplification'}]","Title: Ultra-high strain in epitaxial silicon carbide nanostructures utilizing residual stress amplification

Abstract: Strain engineering has attracted great attention, particularly for epitaxial films grown on a different substrate. Residual strains of SiC have been widely employed to form ultra-high frequency and high Q factor resonators. However, to date the highest residual strain of SiC was reported to be limited to approximately 0.6%. Large strains induced into SiC could lead to several interesting physical phenomena, as well as significant improvement of resonant frequencies. We report an unprecedented nano strain-amplifier structure with an ultra-high residual strain up to 8% utilizing the natural residual stress between epitaxial 3C SiC and Si. In addition, the applied strain can be tuned by changing the dimensions of the amplifier structure. The possibility of introducing such a controllable and ultra-high strain will open the door to investigating the physics of SiC in large strain regimes, and the development of ultra sensitive mechanical sensors.

The potential application mentioned in the abstract is the development of ultra-sensitive mechanical sensors."
What convergence analyses are performed on the proposed Riemannian stochastic variance reduced gradient algorithm?,b06c0299,"[{'abstract': '  Stochastic variance reduction algorithms have recently become popular for\nminimizing the average of a large but finite number of loss functions. In this\npaper, we propose a novel Riemannian extension of the Euclidean stochastic\nvariance reduced gradient algorithm (R-SVRG) to a manifold search space. The\nkey challenges of averaging, adding, and subtracting multiple gradients are\naddressed with retraction and vector transport. We present a global convergence\nanalysis of the proposed algorithm with a decay step size and a local\nconvergence rate analysis under a fixed step size under some natural\nassumptions. The proposed algorithm is applied to problems on the Grassmann\nmanifold, such as principal component analysis, low-rank matrix completion, and\ncomputation of the Karcher mean of subspaces, and outperforms the standard\nRiemannian stochastic gradient descent algorithm in each case.\n', 'title': 'Riemannian stochastic variance reduced gradient'}]","Title: Riemannian stochastic variance reduced gradient

Abstract: Stochastic variance reduction algorithms have recently become popular for 
minimizing the average of a large but finite number of loss functions. In this 
paper, we propose a novel Riemannian extension of the Euclidean stochastic 
variance reduced gradient algorithm (R-SVRG) to a manifold search space. The 
key challenges of averaging, adding, and subtracting multiple gradients are 
addressed with retraction and vector transport. We present a global convergence 
analysis of the proposed algorithm with a decay step size and a local 
convergence rate analysis under a fixed step size under some natural 
assumptions. The proposed algorithm is applied to problems on the Grassmann 
manifold, such as principal component analysis, low-rank matrix completion, and 
computation of the Karcher mean of subspaces, and outperforms the standard 
Riemannian stochastic gradient descent algorithm in each case."
How can resource adaptation decisions balance economic factors and performance in cloud environments?,bacc490d,"[{'abstract': '  Elasticity is a cloud property that enables applications and its execution\nsystems to dynamically acquire and release shared computational resources on\ndemand. Moreover, it unfolds the advantage of economies of scale in the cloud\nthrough a drop in the average costs of these shared resources. However, it is\nstill an open challenge to achieve a perfect match between resource demand and\nprovision in autonomous elasticity management. Resource adaptation decisions\nessentially involve a trade-off between economics and performance, which\nproduces a gap between the ideal and actual resource provisioning. This gap, if\nnot properly managed, can negatively impact the aggregate utility of a cloud\ncustomer in the long run. To address this limitation, we propose a technical\ndebt-aware learning approach for autonomous elasticity management based on a\nreinforcement learning of elasticity debts in resource provisioning; the\nadaptation pursues strategic decisions that trades off economics against\nperformance. We extend CloudSim and Burlap to evaluate our approach. The\nevaluation shows that a reinforcement learning of technical debts in elasticity\nobtains a higher utility for a cloud customer, while conforming expected levels\nof performance.\n', 'title': 'A Debt-Aware Learning Approach for Resource Adaptations in Cloud Elasticity Management'}]","Title: A Debt-Aware Learning Approach for Resource Adaptations in Cloud Elasticity Management

Abstract: Elasticity is a cloud property that enables applications and its execution systems to dynamically acquire and release shared computational resources on demand. Moreover, it unfolds the advantage of economies of scale in the cloud through a drop in the average costs of these shared resources. However, it is still an open challenge to achieve a perfect match between resource demand and provision in autonomous elasticity management. Resource adaptation decisions essentially involve a trade-off between economics and performance, which produces a gap between the ideal and actual resource provisioning. This gap, if not properly managed, can negatively impact the aggregate utility of a cloud customer in the long run. To address this limitation, we propose a technical debt-aware learning approach for autonomous elasticity management based on a reinforcement learning of elasticity debts in resource provisioning; the adaptation pursues strategic decisions that trades off economics against performance. We extend CloudSim and Burlap to evaluate our approach. The evaluation shows that a reinforcement learning of technical debts in elasticity obtains a higher utility for a cloud customer, while conforming expected levels of performance."
What are the challenges in achieving second-order convergence for stochastic non-convex optimization?,ecbae517,"[{'abstract': ""  In this paper, we study stochastic non-convex optimization with non-convex\nrandom functions. Recent studies on non-convex optimization revolve around\nestablishing second-order convergence, i.e., converging to a nearly\nsecond-order optimal stationary points. However, existing results on stochastic\nnon-convex optimization are limited, especially with a high probability\nsecond-order convergence. We propose a novel updating step (named NCG-S) by\nleveraging a stochastic gradient and a noisy negative curvature of a stochastic\nHessian, where the stochastic gradient and Hessian are based on a proper\nmini-batch of random functions. Building on this step, we develop two\nalgorithms and establish their high probability second-order convergence. To\nthe best of our knowledge, the proposed stochastic algorithms are the first\nwith a second-order convergence in {\\it high probability} and a time complexity\nthat is {\\it almost linear} in the problem's dimensionality.\n"", 'title': 'Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence'}]","Title: Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence

Abstract: In this paper, we study stochastic non-convex optimization with non-convex random functions. Recent studies on non-convex optimization revolve around establishing second-order convergence, i.e., converging to a nearly second-order optimal stationary points. However, existing results on stochastic non-convex optimization are limited, especially with a high probability second-order convergence."
How was the photometric stability of white dwarf stars tested in a recent study?,21193d2b,"[{'abstract': '  White dwarf stars have been used as flux standards for decades, thanks to\ntheir staid simplicity. We have empirically tested their photometric stability\nby analyzing the light curves of 398 high-probability candidates and\nspectroscopically confirmed white dwarfs observed during the original Kepler\nmission and later with K2 Campaigns 0-8. We find that the vast majority (>97\nper cent) of non-pulsating and apparently isolated white dwarfs are stable to\nbetter than 1 per cent in the Kepler bandpass on 1-hr to 10-d timescales,\nconfirming that these stellar remnants are useful flux standards. From the\ncases that do exhibit significant variability, we caution that binarity,\nmagnetism, and pulsations are three important attributes to rule out when\nestablishing white dwarfs as flux standards, especially those hotter than\n30,000 K.\n', 'title': 'When flux standards go wild: white dwarfs in the age of Kepler'}]","Title: When flux standards go wild: white dwarfs in the age of Kepler

Abstract: White dwarf stars have been used as flux standards for decades, thanks to
their staid simplicity. We have empirically tested their photometric stability
by analyzing the light curves of 398 high-probability candidates and
spectroscopically confirmed white dwarfs observed during the original Kepler
mission and later with K2 Campaigns 0-8. We find that the vast majority (>97
per cent) of non-pulsating and apparently isolated white dwarfs are stable to
better than 1 per cent in the Kepler bandpass on 1-hr to 10-d timescales,
confirming that these stellar remnants are useful flux standards. From the
cases that do exhibit significant variability, we caution that binarity,
magnetism, and pulsations are three important attributes to rule out when
establishing white dwarfs as flux standards, especially those hotter than
30,000 K."
How can we regulate the complexity of the inference model in a variational autoencoder?,db99fa73,"[{'abstract': '  The variational autoencoder (VAE) is a popular model for density estimation\nand representation learning. Canonically, the variational principle suggests to\nprefer an expressive inference model so that the variational approximation is\naccurate. However, it is often overlooked that an overly-expressive inference\nmodel can be detrimental to the test set performance of both the amortized\nposterior approximator and, more importantly, the generative density estimator.\nIn this paper, we leverage the fact that VAEs rely on amortized inference and\npropose techniques for amortized inference regularization (AIR) that control\nthe smoothness of the inference model. We demonstrate that, by applying AIR, it\nis possible to improve VAE generalization on both inference and generative\nperformance. Our paper challenges the belief that amortized inference is simply\na mechanism for approximating maximum likelihood training and illustrates that\nregularization of the amortization family provides a new direction for\nunderstanding and improving generalization in VAEs.\n', 'title': 'Amortized Inference Regularization'}]","Title: Amortized Inference Regularization

Abstract: The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs."
What algorithms exist for calculating the matrices representing the action of Frobenius on cohomology groups of algebraic varieties?,275af301,"[{'abstract': '  This paper is concerned with the computation of representation matrices for\nthe action of Frobenius to the cohomology groups of algebraic varieties.\nSpecifically we shall give an algorithm to compute the matrices for arbitrary\nalgebraic varieties with defining equations over perfect fields of positive\ncharacteristic, and estimate its complexity. Moreover, we propose a specific\nefficient method, which works for complete intersections.\n', 'title': 'Computing representation matrices for the action of Frobenius to cohomology groups'}]","I think I can help you with that!

Title: Computing representation matrices for the action of Frobenius to cohomology groups

Abstract: This paper is concerned with the computation of representation matrices for the action of Frobenius to the cohomology groups of algebraic varieties. Specifically we shall give an algorithm to compute the matrices for arbitrary algebraic varieties with defining equations over perfect fields of positive characteristic, and estimate its complexity. Moreover, we propose a specific efficient method, which works for complete intersections.

This paper seems to provide an algorithm for calculating the matrices representing the action of Frobenius on cohomology groups of algebraic varieties."
What is a textbook chapter on neural machine translation that covers neural networks and computation graphs?,7ac18b32,"[{'abstract': '  Draft of textbook chapter on neural machine translation. a comprehensive\ntreatment of the topic, ranging from introduction to neural networks,\ncomputation graphs, description of the currently dominant attentional\nsequence-to-sequence model, recent refinements, alternative architectures and\nchallenges. Written as chapter for the textbook Statistical Machine\nTranslation. Used in the JHU Fall 2017 class on machine translation.\n', 'title': 'Neural Machine Translation'}]","I think I have the answer you're looking for!

Title: Neural Machine Translation

Abstract: Draft of textbook chapter on neural machine translation. a comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges. Written as chapter for the textbook Statistical Machine Translation. Used in the JHU Fall 2017 class on machine translation."
How does the distance between impurities affect the frequency of localized nonlinear high-amplitude waves?,6dbbdbc7,"[{'abstract': '  The particular type of four-kink multi-solitons (or quadrons) adiabatic\ndynamics of the sine-Gordon equation in a model with two identical point\nattracting impurities has been studied. This model can be used for describing\nmagnetization localized waves in multilayer ferromagnet. The quadrons structure\nand properties has been numerically investigated. The cases of both large and\nsmall distances between impurities has been viewed. The dependence of the\nlocalized in impurity region nonlinear high-amplitude waves frequencies on the\ndistance between the impurities has been found. For an analytical description\nof two bound localized on impurities nonlinear waves dynamics, using\nperturbation theory, the system of differential equations for harmonic\noscillators with elastic link has been found. The analytical model\nqualitatively describes the results of the sine-Gordon equation numerical\nsimulation.\n', 'title': 'Dynamics of the multi-soliton waves in the sine-Gordon model with two identical point impurities'}]","Title: Dynamics of the multi-soliton waves in the sine-Gordon model with two identical point impurities

Abstract: The particular type of four-kink multi-solitons (or quadrons) adiabatic dynamics of the sine-Gordon equation in a model with two identical point attracting impurities has been studied. This model can be used for describing magnetization localized waves in multilayer ferromagnet. The quadrons structure and properties has been numerically investigated. The cases of both large and small distances between impurities has been viewed. The dependence of the localized in impurity region nonlinear high-amplitude waves frequencies on the distance between the impurities has been found. For an analytical description of two bound localized on impurities nonlinear waves dynamics, using perturbation theory, the system of differential equations for harmonic oscillators with elastic link has been found. The analytical model qualitatively describes the results of the sine-Gordon equation numerical simulation."
What term is used to describe the measure of noise in representations where states with the same information cost are considered degenerate?,c2deb041,"[{'abstract': ""  Deep learning has been successfully applied to various tasks, but its\nunderlying mechanism remains unclear. Neural networks associate similar inputs\nin the visible layer to the same state of hidden variables in deep layers. The\nfraction of inputs that are associated to the same state is a natural measure\nof similarity and is simply related to the cost in bits required to represent\nthese inputs. The degeneracy of states with the same information cost provides\ninstead a natural measure of noise and is simply related the entropy of the\nfrequency of states, that we call relevance. Representations with minimal\nnoise, at a given level of similarity (resolution), are those that maximise the\nrelevance. A signature of such efficient representations is that frequency\ndistributions follow power laws. We show, in extensive numerical experiments,\nthat deep neural networks extract a hierarchy of efficient representations from\ndata, because they i) achieve low levels of noise (i.e. high relevance) and ii)\nexhibit power law distributions. We also find that the layer that is most\nefficient to reliably generate patterns of training data is the one for which\nrelevance and resolution are traded at the same price, which implies that\nfrequency distribution follows Zipf's law.\n"", 'title': 'Resolution and Relevance Trade-offs in Deep Learning'}]","The answer to the question is:

Title: Resolution and Relevance Trade-offs in Deep Learning

Abstract:  Deep learning has been successfully applied to various tasks, but its
underlying mechanism remains unclear. Neural networks associate similar inputs
in the visible layer to the same state of hidden variables in deep layers. The
fraction of inputs that are associated to the same state is a natural measure
of similarity and is simply related to the cost in bits required to represent
these inputs. The degeneracy of states with the same information cost provides
instead a natural measure of noise and is simply related the entropy of the
frequency of states, that we call relevance. Representations with minimal
noise, at a given level of similarity (resolution), are those that maximise the
relevance. A signature of such efficient representations is that frequency
distributions follow power laws. We show, in extensive numerical experiments,
that deep neural networks extract a hierarchy of efficient representations from
data, because they i) achieve low levels of noise (i.e. high relevance) and ii)
exhibit power law distributions. We also find that the layer that is most
efficient to reliably generate patterns of training data is the one for which
relevance and resolution are traded at the same price, which implies that
frequency distribution follows Zipf's law.

The term used to describe the measure of noise in representations is ""relevance""."
What is a neural network approach to recognizing emotions in music?,f976f7f6,"[{'abstract': ""  This paper studies the emotion recognition from musical tracks in the\n2-dimensional valence-arousal (V-A) emotional space. We propose a method based\non convolutional (CNN) and recurrent neural networks (RNN), having\nsignificantly fewer parameters compared with the state-of-the-art method for\nthe same task. We utilize one CNN layer followed by two branches of RNNs\ntrained separately for arousal and valence. The method was evaluated using the\n'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for\narousal and 0.268 for valence, which is the best result reported on this\ndataset.\n"", 'title': 'Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition'}]","Title: Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition

Abstract: This paper studies the emotion recognition from musical tracks in the 2-dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with the state-of-the-art method for the same task. We utilize one CNN layer followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the 'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset."
What are the advantages of integrating a Uniform Theory of Diffraction (UTD) model into a sound propagation algorithm?,2ad12bfa,"[{'abstract': '  We present a novel sound localization algorithm for a non-line-of-sight\n(NLOS) sound source in indoor environments. Our approach exploits the\ndiffraction properties of sound waves as they bend around a barrier or an\nobstacle in the scene. We combine a ray tracing based sound propagation\nalgorithm with a Uniform Theory of Diffraction (UTD) model, which simulate\nbending effects by placing a virtual sound source on a wedge in the\nenvironment. We precompute the wedges of a reconstructed mesh of an indoor\nscene and use them to generate diffraction acoustic rays to localize the 3D\nposition of the source. Our method identifies the convergence region of those\ngenerated acoustic rays as the estimated source position based on a particle\nfilter. We have evaluated our algorithm in multiple scenarios consisting of a\nstatic and dynamic NLOS sound source. In our tested cases, our approach can\nlocalize a source position with an average accuracy error, 0.7m, measured by\nthe L2 distance between estimated and actual source locations in a 7m*7m*3m\nroom. Furthermore, we observe 37% to 130% improvement in accuracy over a\nstate-of-the-art localization method that does not model diffraction effects,\nespecially when a sound source is not visible to the robot.\n', 'title': 'Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source'}]","Title: Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source

Abstract: We present a novel sound localization algorithm for a non-line-of-sight (NLOS) sound source in indoor environments. Our approach exploits the diffraction properties of sound waves as they bend around a barrier or an obstacle in the scene. We combine a ray tracing based sound propagation algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate bending effects by placing a virtual sound source on a wedge in the environment. We precompute the wedges of a reconstructed mesh of an indoor scene and use them to generate diffraction acoustic rays to localize the 3D position of the source. Our method identifies the convergence region of those generated acoustic rays as the estimated source position based on a particle filter. We have evaluated our algorithm in multiple scenarios consisting of a static and dynamic NLOS sound source. In our tested cases, our approach can localize a source position with an average accuracy error, 0.7m, measured by the L2 distance between estimated and actual source locations in a 7m*7m*3m room. Furthermore, we observe 37% to 130% improvement in accuracy over a state-of-the-art localization method that does not model diffraction effects, especially when a sound source is not visible to the robot.

The advantages of integrating a Uniform Theory of Diffraction (UTD) model into a sound propagation algorithm are:

* Improved accuracy in localizing sound sources, with an average accuracy error of 0.7m.
* Ability to localize sound sources even when they are not visible to the robot.
* 37% to 130% improvement in accuracy over state-of-the-art localization methods that do not model diffraction effects."
What methods can be used to perform inference in generative models where observed data varies between instances?,d1a7a20a,"[{'abstract': '  We consider the problem of inference in a causal generative model where the\nset of available observations differs between data instances. We show how\ncombining samples drawn from the graphical model with an appropriate masking\nfunction makes it possible to train a single neural network to approximate all\nthe corresponding conditional marginal distributions and thus amortize the cost\nof inference. We further demonstrate that the efficiency of importance sampling\nmay be improved by basing proposals on the output of the neural network. We\nalso outline how the same network can be used to generate samples from an\napproximate joint posterior via a chain decomposition of the graph.\n', 'title': 'A Universal Marginalizer for Amortized Inference in Generative Models'}]","Title: A Universal Marginalizer for Amortized Inference in Generative Models

Abstract:   We consider the problem of inference in a causal generative model where the
set of available observations differs between data instances. We show how
combining samples drawn from the graphical model with an appropriate masking
function makes it possible to train a single neural network to approximate all
the corresponding conditional marginal distributions and thus amortize the cost
of inference. We further demonstrate that the efficiency of importance sampling
may be improved by basing proposals on the output of the neural network. We
also outline how the same network can be used to generate samples from an
approximate joint posterior via a chain decomposition of the graph."
Are there specific types of compact manifolds that can be fibered on the circle S^1?,d3181628,"[{'abstract': '  In this paper, we show that any compact manifold that carries a\nSL(n;R)-foliation is fibered on the circle S^1.\n', 'title': 'On fibering compact manifold over the circle'}]","I think I can help you with that!

Title: On fibering compact manifold over the circle

Abstract:  In this paper, we show that any compact manifold that carries a SL(n;R)-foliation is fibered on the circle S^1.

So, yes, it appears that compact manifolds that carry a SL(n;R)-foliation can be fibered on the circle S^1."
Researchers compared the exam scores of students who participated in labs to those who did not. What was the result of this comparison?,de76883b,"[{'abstract': '  Instructional labs are widely seen as a unique, albeit expensive, way to\nteach scientific content. We measured the effectiveness of introductory lab\ncourses at achieving this educational goal across nine different lab courses at\nthree very different institutions. These institutions and courses encompassed a\nbroad range of student populations and instructional styles. The nine courses\nstudied had two key things in common: the labs aimed to reinforce the content\npresented in lectures, and the labs were optional. By comparing the performance\nof students who did and did not take the labs (with careful normalization for\nselection effects), we found universally and precisely no added value to\nlearning from taking the labs as measured by course exam performance. This work\nshould motivate institutions and departments to reexamine the goals and conduct\nof their lab courses, given their resource-intensive nature. We show why these\nresults make sense when looking at the comparative mental processes of students\ninvolved in research and instructional labs, and offer alternative goals and\ninstructional approaches that would make lab courses more educationally\nvaluable.\n', 'title': 'Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content'}]","Title: Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content

Abstract:   Instructional labs are widely seen as a unique, albeit expensive, way to
teach scientific content. We measured the effectiveness of introductory lab
courses at achieving this educational goal across nine different lab courses at
three very different institutions. These institutions and courses encompassed a
broad range of student populations and instructional styles. The nine courses
studied had two key things in common: the labs aimed to reinforce the content
presented in lectures, and the labs were optional. By comparing the performance
of students who did and did not take the labs (with careful normalization for
selection effects), we found universally and precisely no added value to
learning from taking the labs as measured by course exam performance. This work
should motivate institutions and departments to reexamine the goals and conduct
of their lab courses, given their resource-intensive nature. We show why these
results make sense when looking at the comparative mental processes of students
involved in research and instructional labs, and offer alternative goals and
instructional approaches that would make lab courses more educationally
valuable."
Are there connections between shifted quantum affine algebras and relativistic quantum Toda lattices?,55af4c01,"[{'abstract': '  We introduce the shifted quantum affine algebras. They map homomorphically\ninto the quantized $K$-theoretic Coulomb branches of $3d\\ {\\mathcal N}=4$ SUSY\nquiver gauge theories. In type $A$, they are endowed with a coproduct, and they\nact on the equivariant $K$-theory of parabolic Laumon spaces. In type $A_1$,\nthey are closely related to the open relativistic quantum Toda lattice of type\n$A$.\n', 'title': 'Multiplicative slices, relativistic Toda and shifted quantum affine algebras'}]","The connection between shifted quantum affine algebras and relativistic quantum Toda lattices can be found in:

Title: Multiplicative slices, relativistic Toda and shifted quantum affine algebras

Abstract:   We introduce the shifted quantum affine algebras. They map homomorphically
into the quantized $K$-theoretic Coulomb branches of $3d\ {\mathcal N}=4$ SUSY
quiver gauge theories. In type $A$, they are endowed with a coproduct, and they
act on the equivariant $K$-theory of parabolic Laumon spaces. In type $A_1$,
they are closely related to the open relativistic quantum Toda lattice of type
$A$."
What are some limitations of current transformation optics techniques?,5905cb28,"[{'abstract': ""  The advances in geometric approaches to optical devices due to transformation\noptics has led to the development of cloaks, concentrators, and other devices.\nIt has also been shown that transformation optics can be used to gravitational\nfields from general relativity. However, the technique is currently constrained\nto linear devices, as a consistent approach to nonlinearity (including both the\ncase of a nonlinear background medium and a nonlinear transformation) remains\nan open question. Here we show that nonlinearity can be incorporated into\ntransformation optics in a consistent way. We use this to illustrate a number\nof novel effects, including cloaking an optical soliton, modeling nonlinear\nsolutions to Einstein's field equations, controlling transport in a Debye\nsolid, and developing a set of constitutive to relations for relativistic\ncloaks in arbitrary nonlinear backgrounds.\n"", 'title': 'A Unified Approach to Nonlinear Transformation Materials'}]","Title: A Unified Approach to Nonlinear Transformation Materials

Abstract: The advances in geometric approaches to optical devices due to transformation optics has led to the development of cloaks, concentrators, and other devices. It has also been shown that transformation optics can be used to gravitational fields from general relativity. However, the technique is currently constrained to linear devices, as a consistent approach to nonlinearity (including both the case of a nonlinear background medium and a nonlinear transformation) remains an open question."
How have researchers established the correctness of parallel CHR semantics?,31e01f2d,"[{'abstract': '  Constraint Handling Rules is an effective concurrent declarative programming\nlanguage and a versatile computational logic formalism. CHR programs consist of\nguarded reactive rules that transform multisets of constraints. One of the main\nfeatures of CHR is its inherent concurrency. Intuitively, rules can be applied\nto parts of a multiset in parallel. In this comprehensive survey, we give an\noverview of concurrent and parallel as well as distributed CHR semantics,\nstandard and more exotic, that have been proposed over the years at various\nlevels of refinement. These semantics range from the abstract to the concrete.\nThey are related by formal soundness results. Their correctness is established\nas correspondence between parallel and sequential computations. We present\ncommon concise sample CHR programs that have been widely used in experiments\nand benchmarks. We review parallel CHR implementations in software and\nhardware. The experimental results obtained show a consistent parallel speedup.\nMost implementations are available online. The CHR formalism can also be used\nto implement and reason with models for concurrency. To this end, the Software\nTransaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus\nhave been faithfully encoded in CHR. Under consideration in Theory and Practice\nof Logic Programming (TPLP).\n', 'title': 'Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey'}]","Title: Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey

Abstract:   Constraint Handling Rules is an effective concurrent declarative programming
language and a versatile computational logic formalism. CHR programs consist of
guarded reactive rules that transform multisets of constraints. One of the main
features of CHR is its inherent concurrency. Intuitively, rules can be applied
to parts of a multiset in parallel. In this comprehensive survey, we give an
overview of concurrent and parallel as well as distributed CHR semantics,
standard and more exotic, that have been proposed over the years at various
levels of refinement. These semantics range from the abstract to the concrete.
They are related by formal soundness results. Their correctness is established
as correspondence between parallel and sequential computations. We present
common concise sample CHR programs that have been widely used in experiments
and benchmarks. We review parallel CHR implementations in software and
hardware. The experimental results obtained show a consistent parallel speedup.
Most implementations are available online. The CHR formalism can also be used
to implement and reason with models for concurrency. To this end, the Software
Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus
have been faithfully encoded in CHR. Under consideration in Theory and Practice
of Logic Programming (TPLP)."
What is the name of the technique used to find optimal architectures for meta-learners in the paper?,e5acd0c3,"[{'abstract': '  Fully automating machine learning pipelines is one of the key challenges of\ncurrent artificial intelligence research, since practical machine learning\noften requires costly and time-consuming human-powered processes such as model\ndesign, algorithm development, and hyperparameter tuning. In this paper, we\nverify that automated architecture search synergizes with the effect of\ngradient-based meta learning. We adopt the progressive neural architecture\nsearch \\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal\narchitectures for meta-learners. The gradient based meta-learner whose\narchitecture was automatically found achieved state-of-the-art results on the\n5-shot 5-way Mini-ImageNet classification problem with $74.65\\%$ accuracy,\nwhich is $11.54\\%$ improvement over the result obtained by the first\ngradient-based meta-learner called MAML\n\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is\nthe first successful neural architecture search implementation in the context\nof meta learning.\n', 'title': 'Auto-Meta: Automated Gradient Based Meta Learner Search'}]","Title: Auto-Meta: Automated Gradient Based Meta Learner Search

Abstract:   Fully automating machine learning pipelines is one of the key challenges of
current artificial intelligence research, since practical machine learning
often requires costly and time-consuming human-powered processes such as model
design, algorithm development, and hyperparameter tuning. In this paper, we
verify that automated architecture search synergizes with the effect of
gradient-based meta learning. We adopt the progressive neural architecture
search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal
architectures for meta-learners. The gradient based meta-learner whose
architecture was automatically found achieved state-of-the-art results on the
5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy,
which is $11.54\%$ improvement over the result obtained by the first
gradient-based meta-learner called MAML
\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is
the first successful neural architecture search implementation in the context
of meta learning.

The technique used to find optimal architectures for meta-learners in the paper is called ""progressive neural architecture search""."
What techniques can be used to identify high-value targets for phishing attacks?,54e1e293,"[{'abstract': ""  Historically, machine learning in computer security has prioritized defense:\nthink intrusion detection systems, malware classification, and botnet traffic\nidentification. Offense can benefit from data just as well. Social networks,\nwith their access to extensive personal data, bot-friendly APIs, colloquial\nsyntax, and prevalence of shortened links, are the perfect venues for spreading\nmachine-generated malicious content. We aim to discover what capabilities an\nadversary might utilize in such a domain. We present a long short-term memory\n(LSTM) neural network that learns to socially engineer specific users into\nclicking on deceptive URLs. The model is trained with word vector\nrepresentations of social media posts, and in order to make a click-through\nmore likely, it is dynamically seeded with topics extracted from the target's\ntimeline. We augment the model with clustering to triage high value targets\nbased on their level of social engagement, and measure success of the LSTM's\nphishing expedition using click-rates of IP-tracked links. We achieve state of\nthe art success rates, tripling those of historic email attack campaigns, and\noutperform humans manually performing the same task.\n"", 'title': 'Generative Models for Spear Phishing Posts on Social Media'}]","I think I can help you with that!

Title: Generative Models for Spear Phishing Posts on Social Media

Abstract:   Historically, machine learning in computer security has prioritized defense:
think intrusion detection systems, malware classification, and botnet traffic
identification. Offense can benefit from data just as well. Social networks,
with their access to extensive personal data, bot-friendly APIs, colloquial
syntax, and prevalence of shortened links, are the perfect venues for spreading
machine-generated malicious content. We aim to discover what capabilities an
adversary might utilize in such a domain. We present a long short-term memory
(LSTM) neural network that learns to socially engineer specific users into
clicking on deceptive URLs. The model is trained with word vector
representations of social media posts, and in order to make a click-through
more likely, it is dynamically seeded with topics extracted from the target's
timeline. We augment the model with clustering to triage high value targets
based on their level of social engagement, and measure success of the LSTM's
phishing expedition using click-rates of IP-tracked links. We achieve state of
the art success rates, tripling those of historic email attack campaigns, and
outperform humans manually performing the same task.

It seems that the technique used to identify high-value targets for phishing attacks is clustering based on the level of social engagement."
What finite element method is used to illustrate the method of geometric discretization of port-Hamiltonian systems?,3cea44dd,"[{'abstract': '  We present the mixed Galerkin discretization of distributed parameter\nport-Hamiltonian systems. On the prototypical example of hyperbolic systems of\ntwo conservation laws in arbitrary spatial dimension, we derive the main\ncontributions: (i) A weak formulation of the underlying geometric\n(Stokes-Dirac) structure with a segmented boundary according to the causality\nof the boundary ports. (ii) The geometric approximation of the Stokes-Dirac\nstructure by a finite-dimensional Dirac structure is realized using a mixed\nGalerkin approach and power-preserving linear maps, which define minimal\ndiscrete power variables. (iii) With a consistent approximation of the\nHamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.\nBy the degrees of freedom in the power-preserving maps, the resulting family of\nstructure-preserving schemes allows for trade-offs between centered\napproximations and upwinding. We illustrate the method on the example of\nWhitney finite elements on a 2D simplicial triangulation and compare the\neigenvalue approximation in 1D with a related approach.\n', 'title': 'Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems'}]","Title: Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems

Abstract:   We present the mixed Galerkin discretization of distributed parameter
port-Hamiltonian systems. On the prototypical example of hyperbolic systems of
two conservation laws in arbitrary spatial dimension, we derive the main
contributions: (i) A weak formulation of the underlying geometric
(Stokes-Dirac) structure with a segmented boundary according to the causality
of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac
structure by a finite-dimensional Dirac structure is realized using a mixed
Galerkin approach and power-preserving linear maps, which define minimal
discrete power variables. (iii) With a consistent approximation of the
Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.
By the degrees of freedom in the power-preserving maps, the resulting family of
structure-preserving schemes allows for trade-offs between centered
approximations and upwinding. We illustrate the method on the example of
Whitney finite elements on a 2D simplicial triangulation and compare the
eigenvalue approximation in 1D with a related approach.

The answer is Whitney finite elements."
Can you find an open-source processing core family designed for IoT end-nodes?,8cb826d0,"[{'abstract': '  Internet-of-Things end-nodes demand low power processing platforms\ncharacterized by heterogeneous dedicated units, controlled by a processor core\nrunning concurrent control threads. Such architecture scheme fits one of the\nmain target application domain of the RISC-V instruction set. We present an\nopen-source processing core compliant with RISC-V on the software side and with\nthe popular Pulpino processor platform on the hardware side, while supporting\ninterleaved multi-threading for IoT applications. The latter feature is a novel\ncontribution in this application domain. We report details about the\nmicroarchitecture design along with performance data.\n', 'title': 'The microarchitecture of a multi-threaded RISC-V compliant processing core family for IoT end-nodes'}]","Yes, I think I can help you with that!

Title: The microarchitecture of a multi-threaded RISC-V compliant processing core family for IoT end-nodes

Abstract: Internet-of-Things end-nodes demand low power processing platforms characterized by heterogeneous dedicated units, controlled by a processor core running concurrent control threads. Such architecture scheme fits one of the main target application domain of the RISC-V instruction set. We present an open-source processing core compliant with RISC-V on the software side and with the popular Pulpino processor platform on the hardware side, while supporting interleaved multi-threading for IoT applications. The latter feature is a novel contribution in this application domain. We report details about the microarchitecture design along with performance data."
What type of lattice structure and magnetic field configuration was used to study the mixing of even and odd frequency pairings?,c0f82224,"[{'abstract': '  Even- and odd-frequency superconductivity coexist due to broken time-reversal\nsymmetry under magnetic field. In order to describe this mixing, we extend the\nlinearized Eliashberg equation for the spin and charge fluctuation mechanism in\nstrongly correlated electron systems. We apply this extended Eliashberg\nequation to the odd-frequency superconductivity on a quasi-one-dimensional\nisosceles triangular lattice under in-plane magnetic field and examine the\neffect of the even-frequency component.\n', 'title': 'Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field'}]","Title: Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field

Abstract: Even- and odd-frequency superconductivity coexist due to broken time-reversal symmetry under magnetic field. In order to describe this mixing, we extend the linearized Eliashberg equation for the spin and charge fluctuation mechanism in strongly correlated electron systems. We apply this extended Eliashberg equation to the odd-frequency superconductivity on a quasi-one-dimensional isosceles triangular lattice under in-plane magnetic field and examine the effect of the even-frequency component."
Are there tight bounds for computing envy-free cake allocations among three players?,a898bbfc,"[{'abstract': '  We study the query complexity of cake cutting and give lower and upper bounds\nfor computing approximately envy-free, perfect, and equitable allocations with\nthe minimum number of cuts. The lower bounds are tight for computing connected\nenvy-free allocations among n=3 players and for computing perfect and equitable\nallocations with minimum number of cuts between n=2 players.\nWe also formalize moving knife procedures and show that a large subclass of\nthis family, which captures all the known moving knife procedures, can be\nsimulated efficiently with arbitrarily small error in the Robertson-Webb query\nmodel.\n', 'title': 'The Query Complexity of Cake Cutting'}]","Title: The Query Complexity of Cake Cutting

Abstract: We study the query complexity of cake cutting and give lower and upper bounds for computing approximately envy-free, perfect, and equitable allocations with the minimum number of cuts. The lower bounds are tight for computing connected envy-free allocations among n=3 players and for computing perfect and equitable allocations with minimum number of cuts between n=2 players."
Did users who retweeted tweets from different political parties share similar characteristics?,68a28b02,"[{'abstract': ""  In modern election campaigns, political parties utilize social media to\nadvertise their policies and candidates and to communicate to the electorate.\nIn Japan's latest general election in 2017, the 48th general election for the\nLower House, social media, especially Twitter, was actively used. In this\npaper, we analyze the users who retweeted tweets of political parties on\nTwitter during the election. Our aim is to clarify what kinds of users are\ndiffusing (retweeting) tweets of political parties. The results indicate that\nthe characteristics of retweeters of the largest ruling party (Liberal\nDemocratic Party of Japan) and the largest opposition party (The Constitutional\nDemocratic Party of Japan) were similar, even though the retweeters did not\noverlap each other. We also found that a particular opposition party (Japanese\nCommunist Party) had quite different characteristics from other political\nparties.\n"", 'title': ""Analysis of Political Party Twitter Accounts' Retweeters During Japan's 2017 Election""}]","Title: Analysis of Political Party Twitter Accounts' Retweeters During Japan's 2017 Election

Abstract:  In modern election campaigns, political parties utilize social media to advertise their policies and candidates and to communicate to the electorate. In Japan's latest general election in 2017, the 48th general election for the Lower House, social media, especially Twitter, was actively used. In this paper, we analyze the users who retweeted tweets of political parties on Twitter during the election. Our aim is to clarify what kinds of users are diffusing (retweeting) tweets of political parties. The results indicate that the characteristics of retweeters of the largest ruling party (Liberal Democratic Party of Japan) and the largest opposition party (The Constitutional Democratic Party of Japan) were similar, even though the retweeters did not overlap each other. We also found that a particular opposition party (Japanese Communist Party) had quite different characteristics from other political parties."
What are the limitations of the conventional graph Fourier transform (GFT) when applied to signals on Cartesian product graphs? ,bec9f6fe,"[{'abstract': '  Many signals on Cartesian product graphs appear in the real world, such as\ndigital images, sensor observation time series, and movie ratings on Netflix.\nThese signals are ""multi-dimensional"" and have directional characteristics\nalong each factor graph. However, the existing graph Fourier transform does not\ndistinguish these directions, and assigns 1-D spectra to signals on product\ngraphs. Further, these spectra are often multi-valued at some frequencies. Our\nmain result is a multi-dimensional graph Fourier transform that solves such\nproblems associated with the conventional GFT. Using algebraic properties of\nCartesian products, the proposed transform rearranges 1-D spectra obtained by\nthe conventional GFT into the multi-dimensional frequency domain, of which each\ndimension represents a directional frequency along each factor graph. Thus, the\nmulti-dimensional graph Fourier transform enables directional frequency\nanalysis, in addition to frequency analysis with the conventional GFT.\nMoreover, this rearrangement resolves the multi-valuedness of spectra in some\ncases. The multi-dimensional graph Fourier transform is a foundation of novel\nfilterings and stationarities that utilize dimensional information of graph\nsignals, which are also discussed in this study. The proposed methods are\napplicable to a wide variety of data that can be regarded as signals on\nCartesian product graphs. This study also notes that multivariate graph signals\ncan be regarded as 2-D univariate graph signals. This correspondence provides\nnatural definitions of the multivariate graph Fourier transform and the\nmultivariate stationarity based on their 2-D univariate versions.\n', 'title': 'Multi-dimensional Graph Fourier Transform'}]","Title: Multi-dimensional Graph Fourier Transform

Abstract:   Many signals on Cartesian product graphs appear in the real world, such as
digital images, sensor observation time series, and movie ratings on Netflix.
These signals are ""multi-dimensional"" and have directional characteristics
along each factor graph. However, the existing graph Fourier transform does not
distinguish these directions, and assigns 1-D spectra to signals on product
graphs. Further, these spectra are often multi-valued at some frequencies. Our
main result is a multi-dimensional graph Fourier transform that solves such
problems associated with the conventional GFT. Using algebraic properties of
Cartesian products, the proposed transform rearranges 1-D spectra obtained by
the conventional GFT into the multi-dimensional frequency domain, of which each
dimension represents a directional frequency along each factor graph. Thus, the
multi-dimensional graph Fourier transform enables directional frequency
analysis, in addition to frequency analysis with the conventional GFT.
Moreover, this rearrangement resolves the multi-valuedness of spectra in some
cases. The multi-dimensional graph Fourier transform is a foundation of novel
filterings and stationarities that utilize dimensional information of graph
signals, which are also discussed in this study. The proposed methods are
applicable to a wide variety of data that can be regarded as signals on
Cartesian product graphs. This study also notes that multivariate graph signals
can be regarded as 2-D univariate graph signals. This correspondence provides
natural definitions of the multivariate graph Fourier transform and the
multivariate stationarity based on their 2-D univariate versions.

The limitations of the conventional Graph Fourier Transform (GFT) when applied to signals on Cartesian product graphs are:

1. Failing to distinguish directional characteristics along each factor graph.
2. Assigning 1-D spectra to signals on product graphs.
3. Producing multi-valued spectra at some frequencies."
How does the isoelectric point of casein micelles change after genipin cross-linking?,d6117c69,"[{'abstract': '  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their\nstability against dissociating agents. In this paper, a comparative study of\nstability between native CMs and CMs cross-linked with genipin (CMs-GP) as a\nfunction of pH is described. Stability to temperature and ethanol were\ninvestigated in the pH range 2.0-7.0. The size and the charge\n($\\zeta$-potential) of the particles were determined by dynamic light\nscattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH\n3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.\nThe isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability\nagainst heat and ethanol was observed for CMs-GP at pH 2, where visible\ncoagulation was determined only after 800 s at 140 $^\\circ$C or 87.5% (v/v) of\nethanol. These results confirmed the hypothesis that cross-linking by GP\nincreased the stability of CMs.\n', 'title': 'Stability of casein micelles cross-linked with genipin: a physicochemical study as a function of pH'}]","Title: Stability of casein micelles cross-linked with genipin: a physicochemical study as a function of pH

Abstract: Chemical or enzymatic cross-linking of casein micelles (CMs) increases their stability against dissociating agents. In this paper, a comparative study of stability between native CMs and CMs cross-linked with genipin (CMs-GP) as a function of pH is described. Stability to temperature and ethanol were investigated in the pH range 2.0-7.0. The size and the charge ($\zeta$-potential) of the particles were determined by dynamic light scattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH 3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0. The isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability against heat and ethanol was observed for CMs-GP at pH 2, where visible coagulation was determined only after 800 s at 140 $^\circ$C or 87.5% (v/v) of ethanol. These results confirmed the hypothesis that cross-linking by GP increased the stability of CMs.

Answer: The isoelectric point of casein micelles changes to pH 3.7 after genipin cross-linking."
What is a key challenge in diagnosing glaucoma that this research addresses?,ae25676e,"[{'abstract': '  Glaucoma is the second leading cause of blindness all over the world, with\napproximately 60 million cases reported worldwide in 2010. If undiagnosed in\ntime, glaucoma causes irreversible damage to the optic nerve leading to\nblindness. The optic nerve head examination, which involves measurement of\ncup-to-disc ratio, is considered one of the most valuable methods of structural\ndiagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation\nof optic disc and optic cup on eye fundus images and can be performed by modern\ncomputer vision algorithms. This work presents universal approach for automatic\noptic disc and cup segmentation, which is based on deep learning, namely,\nmodification of U-Net convolutional neural network. Our experiments include\ncomparison with the best known methods on publicly available databases\nDRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,\nour method achieves quality comparable to current state-of-the-art methods,\noutperforming them in terms of the prediction time.\n', 'title': 'Optic Disc and Cup Segmentation Methods for Glaucoma Detection with Modification of U-Net Convolutional Neural Network'}]","Title: Optic Disc and Cup Segmentation Methods for Glaucoma Detection with Modification of U-Net Convolutional Neural Network

Abstract: Glaucoma is the second leading cause of blindness all over the world, with approximately 60 million cases reported worldwide in 2010. If undiagnosed in time, glaucoma causes irreversible damage to the optic nerve leading to blindness. The optic nerve head examination, which involves measurement of cup-to-disc ratio, is considered one of the most valuable methods of structural diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation of optic disc and optic cup on eye fundus images and can be performed by modern computer vision algorithms. This work presents universal approach for automatic optic disc and cup segmentation, which is based on deep learning, namely, modification of U-Net convolutional neural network. Our experiments include comparison with the best known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation, our method achieves quality comparable to current state-of-the-art methods, outperforming them in terms of the prediction time.

The key challenge in diagnosing glaucoma that this research addresses is the accurate estimation of cup-to-disc ratio, which requires segmentation of optic disc and optic cup on eye fundus images."
Can you summarize the relationship between different geometric conditions for the forward-backward algorithm?,e8aebeac,"[{'abstract': '  We provide a comprehensive study of the convergence of forward-backward\nalgorithm under suitable geometric conditions leading to fast rates. We present\nseveral new results and collect in a unified view a variety of results\nscattered in the literature, often providing simplified proofs. Novel\ncontributions include the analysis of infinite dimensional convex minimization\nproblems, allowing the case where minimizers might not exist. Further, we\nanalyze the relation between different geometric conditions, and discuss novel\nconnections with a priori conditions in linear inverse problems, including\nsource conditions, restricted isometry properties and partial smoothness.\n', 'title': 'Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry'}]","Based on the context, I can summarize the relationship between different geometric conditions for the forward-backward algorithm.

Title: Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry

Abstract: We provide a comprehensive study of the convergence of forward-backward algorithm under suitable geometric conditions leading to fast rates. We present several new results and collect in a unified view a variety of results scattered in the literature, often providing simplified proofs. Novel contributions include the analysis of infinite dimensional convex minimization problems, allowing the case where minimizers might not exist. Further, we analyze the relation between different geometric conditions, and discuss novel connections with a priori conditions in linear inverse problems, including source conditions, restricted isometry properties and partial smoothness."
What were the results of evaluating the model on the three NLP tasks?,f304705a,"[{'abstract': '  We describe a neural network model that jointly learns distributed\nrepresentations of texts and knowledge base (KB) entities. Given a text in the\nKB, we train our proposed model to predict entities that are relevant to the\ntext. Our model is designed to be generic with the ability to address various\nNLP tasks with ease. We train the model using a large corpus of texts and their\nentity annotations extracted from Wikipedia. We evaluated the model on three\nimportant NLP tasks (i.e., sentence textual similarity, entity linking, and\nfactoid question answering) involving both unsupervised and supervised\nsettings. As a result, we achieved state-of-the-art results on all three of\nthese tasks. Our code and trained models are publicly available for further\nacademic research.\n', 'title': 'Learning Distributed Representations of Texts and Entities from Knowledge Base'}]","Title: Learning Distributed Representations of Texts and Entities from Knowledge Base

Abstract: We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research.

The results of evaluating the model on the three NLP tasks were that we achieved state-of-the-art results on all three tasks."
Which signal processing methods resulted in the most significant improvement in clustering quality and logistic regression accuracy?,b6b8595c,"[{'abstract': '  We describe a variant construction of the unstable Adams spectral the\nsequence for a space $Y$, associated to any free simplicial resolution of\n$H^*(Y;R)$ for $R=\\mathbb{F}_p$ or $\\mathbb{Q}$. We use this construction to\ndescribe the differentials and filtration in the spectral sequence in terms of\nappropriate systems of higher cohomology operations.\n', 'title': 'Higher structure in the unstable Adams spectral sequence'}]","Based on the provided abstracts, I found that the paper ""A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization"" addresses the question.

Title: A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization

Abstract:   This paper aims to explore models based on the extreme gradient boosting (XGBoost) approach for business risk classification. Feature selection (FS) algorithms and hyper-parameter optimizations are simultaneously considered during model training. The five most commonly used FS methods including weight by Gini, weight by Chi-square, hierarchical variable clustering, weight by correlation, and weight by information are applied to alleviate the effect of redundant features. Two hyper-parameter optimization approaches, random search (RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in XGBoost. The effect of different FS and hyper-parameter optimization methods on the model performance are investigated by the Wilcoxon Signed Rank Test. The performance of XGBoost is compared to the traditionally utilized logistic regression (LR) model in terms of classification accuracy, area under the curve (AUC), recall, and F1 score obtained from the 10-fold cross validation. Results show that hierarchical clustering is the optimal FS method for LR while weight by Chi-square achieves the best performance in XG-Boost. Both TPE and RS optimization in XGBoost outperform LR significantly. TPE optimization shows a superiority over RS since it results in a significantly higher accuracy and a marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE tuning shows a lower variability than the RS method. Finally, the ranking of feature importance based on XGBoost enhances the model interpretation. Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an operative while powerful approach for business risk modeling."
"Under specific conditions, what equations does the susceptibility propagation method reduce to?",cfdce7e2,"[{'abstract': '  A susceptibility propagation that is constructed by combining a belief\npropagation and a linear response method is used for approximate computation\nfor Markov random fields. Herein, we formulate a new, improved susceptibility\npropagation by using the concept of a diagonal matching method that is based on\nmean-field approaches to inverse Ising problems. The proposed susceptibility\npropagation is robust for various network structures, and it is reduced to the\nordinary susceptibility propagation and to the adaptive\nThouless-Anderson-Palmer equation in special cases.\n', 'title': 'Susceptibility Propagation by Using Diagonal Consistency'}]","Based on the context, I found the answer to your question:

Title: Susceptibility Propagation by Using Diagonal Consistency

Abstract: A susceptibility propagation that is constructed by combining a belief propagation and a linear response method is used for approximate computation for Markov random fields. Herein, we formulate a new, improved susceptibility propagation by using the concept of a diagonal matching method that is based on mean-field approaches to inverse Ising problems. The proposed susceptibility propagation is robust for various network structures, and it is reduced to the ordinary susceptibility propagation and to the adaptive Thouless-Anderson-Palmer equation in special cases.

The susceptibility propagation method reduces to the ordinary susceptibility propagation and to the adaptive Thouless-Anderson-Palmer equation under specific conditions."
Describe a method for grouping user devices and their serving cloudlets in fog networks based on proximity and task popularity.,8f5d4b9a,"[{'abstract': ""  In this paper, the fundamental problem of distribution and proactive caching\nof computing tasks in fog networks is studied under latency and reliability\nconstraints. In the proposed scenario, computing can be executed either locally\nat the user device or offloaded to an edge cloudlet. Moreover, cloudlets\nexploit both their computing and storage capabilities by proactively caching\npopular task computation results to minimize computing latency. To this end, a\nclustering method to group spatially proximate user devices with mutual task\npopularity interests and their serving cloudlets is proposed. Then, cloudlets\ncan proactively cache the popular tasks' computations of their cluster members\nto minimize computing latency. Additionally, the problem of distributing tasks\nto cloudlets is formulated as a matching game in which a cost function of\ncomputing delay is minimized under latency and reliability constraints.\nSimulation results show that the proposed scheme guarantees reliable\ncomputations with bounded latency and achieves up to 91% decrease in computing\nlatency as compared to baseline schemes.\n"", 'title': 'Proactive Edge Computing in Latency-Constrained Fog Networks'}]","Title: Proactive Edge Computing in Latency-Constrained Fog Networks

Abstract:   In this paper, the fundamental problem of distribution and proactive caching
of computing tasks in fog networks is studied under latency and reliability
constraints. In the proposed scenario, computing can be executed either locally
at the user device or offloaded to an edge cloudlet. Moreover, cloudlets
exploit both their computing and storage capabilities by proactively caching
popular task computation results to minimize computing latency. To this end, a
clustering method to group spatially proximate user devices with mutual task
popularity interests and their serving cloudlets is proposed. Then, cloudlets
can proactively cache the popular tasks' computations of their cluster members
to minimize computing latency. Additionally, the problem of distributing tasks
to cloudlets is formulated as a matching game in which a cost function of
computing delay is minimized under latency and reliability constraints.
Simulation results show that the proposed scheme guarantees reliable
computations with bounded latency and achieves up to 91% decrease in computing
latency as compared to baseline schemes."
Can the Robertson-Webb query model be used to simulate cake cutting procedures? ,a898bbfc,"[{'abstract': '  We study the query complexity of cake cutting and give lower and upper bounds\nfor computing approximately envy-free, perfect, and equitable allocations with\nthe minimum number of cuts. The lower bounds are tight for computing connected\nenvy-free allocations among n=3 players and for computing perfect and equitable\nallocations with minimum number of cuts between n=2 players.\nWe also formalize moving knife procedures and show that a large subclass of\nthis family, which captures all the known moving knife procedures, can be\nsimulated efficiently with arbitrarily small error in the Robertson-Webb query\nmodel.\n', 'title': 'The Query Complexity of Cake Cutting'}]","Title: The Query Complexity of Cake Cutting

Abstract: We study the query complexity of cake cutting and give lower and upper bounds for computing approximately envy-free, perfect, and equitable allocations with the minimum number of cuts. The lower bounds are tight for computing connected envy-free allocations among n=3 players and for computing perfect and equitable allocations with minimum number of cuts between n=2 players.
We also formalize moving knife procedures and show that a large subclass of this family, which captures all the known moving knife procedures, can be simulated efficiently with arbitrarily small error in the Robertson-Webb query model."
What is the main focus of the analysis presented in an article about Girard Desargues's Brouillon Project?,b9ec9f4f,"[{'abstract': ""  Nous tentons dans cet article de proposer une thèse cohérente concernant\nla formation de la notion d'involution dans le Brouillon Project de Desargues.\nPour cela, nous donnons une analyse détaillée des dix premières pages\ndudit Brouillon, comprenant les développements de cas particuliers qui aident\nà comprendre l'intention de Desargues. Nous mettons cette analyse en regard\nde la lecture qu'en fait Jean de Beaugrand et que l'on trouve dans les Advis\nCharitables.\nThe purpose of this article is to propose a coherent thesis on how Girard\nDesargues arrived at the notion of involution in his Brouillon Project of 1639.\nTo this purpose we give a detailed analysis of the ten first pages of the\nBrouillon, including developments of particular cases which help to understand\nthe goal of Desargues, as well as to clarify the links between the notion of\ninvolution and that of harmonic division. We compare the conclusions of this\nanalysis with the very critical reading Jean de Beaugrand made of the Brouillon\nProject in the Advis Charitables of 1640.\n"", 'title': ""La notion d'involution dans le Brouillon Project de Girard Desargues""}]","Title: La notion d'involution dans le Brouillon Project de Girard Desargues

Abstract: Nous tentons dans cet article de proposer une thèse cohérente concernant la formation de la notion d'involution dans le Brouillon Project de Desargues. Pour cela, nous donnons une analyse détaillée des dix premières pages dudit Brouillon, comprenant les développements de cas particuliers qui aident à comprendre l'intention de Desargues. Nous mettons cette analyse en regard de la lecture qu'en fait Jean de Beaugrand et que l'on trouve dans les Advis Charitables.

The main focus of the analysis presented in this article is to propose a coherent thesis on how Girard Desargues arrived at the notion of involution in his Brouillon Project of 1639."
How does resonant inelastic x-ray scattering  characterize magnetic excitations in  (Sr₁₋ₓLaₓ)₂IrO₄?,69d987f5,"[{'abstract': '  The collective magnetic excitations in the spin-orbit Mott insulator\n(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\\,0.01,\\,0.04,\\, 0.1$) were investigated by\nmeans of resonant inelastic x-ray scattering. We report significant magnon\nenergy gaps at both the crystallographic and antiferromagnetic zone centers at\nall doping levels, along with a remarkably pronounced momentum-dependent\nlifetime broadening. The spin-wave gap is accounted for by a significant\nanisotropy in the interactions between $J_\\text{eff}=1/2$ isospins, thus\nmarking the departure of Sr$_2$IrO$_4$ from the essentially isotropic\nHeisenberg model appropriate for the superconducting cuprates.\n', 'title': 'Anisotropic exchange and spin-wave damping in pure and electron-doped Sr$_2$IrO$_4$'}]","Title: Anisotropic exchange and spin-wave damping in pure and electron-doped Sr$_2$IrO$_4$

Abstract: The collective magnetic excitations in the spin-orbit Mott insulator (Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\,0.01,\,0.04,\, 0.1$) were investigated by means of resonant inelastic x-ray scattering. We report significant magnon energy gaps at both the crystallographic and antiferromagnetic zone centers at all doping levels, along with a remarkably pronounced momentum-dependent lifetime broadening. The spin-wave gap is accounted for by a significant anisotropy in the interactions between $J_\text{eff}=1/2$ isospins, thus marking the departure of Sr$_2$IrO$_4$ from the essentially isotropic Heisenberg model appropriate for the superconducting cuprates."
How does the success of correlation-based variable screening depend on problem parameters?,7470dc5f,"[{'abstract': '  Statistical inference can be computationally prohibitive in\nultrahigh-dimensional linear models. Correlation-based variable screening, in\nwhich one leverages marginal correlations for removal of irrelevant variables\nfrom the model prior to statistical inference, can be used to overcome this\nchallenge. Prior works on correlation-based variable screening either impose\nstrong statistical priors on the linear model or assume specific post-screening\ninference methods. This paper first extends the analysis of correlation-based\nvariable screening to arbitrary linear models and post-screening inference\ntechniques. In particular, ($i$) it shows that a condition---termed the\nscreening condition---is sufficient for successful correlation-based screening\nof linear models, and ($ii$) it provides insights into the dependence of\nmarginal correlation-based screening on different problem parameters. Numerical\nexperiments confirm that these insights are not mere artifacts of analysis;\nrather, they are reflective of the challenges associated with marginal\ncorrelation-based variable screening. Second, the paper explicitly derives the\nscreening condition for two families of linear models, namely, sub-Gaussian\nlinear models and arbitrary (random or deterministic) linear models. In the\nprocess, it establishes that---under appropriate conditions---it is possible to\nreduce the dimension of an ultrahigh-dimensional, arbitrary linear model to\nalmost the sample size even when the number of active variables scales almost\nlinearly with the sample size.\n', 'title': 'ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional Linear Models'}]","Title: ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional Linear Models

Abstract: Statistical inference can be computationally prohibitive in ultrahigh-dimensional linear models. Correlation-based variable screening, in which one leverages marginal correlations for removal of irrelevant variables from the model prior to statistical inference, can be used to overcome this challenge. Prior works on correlation-based variable screening either impose strong statistical priors on the linear model or assume specific post-screening inference methods. This paper first extends the analysis of correlation-based variable screening to arbitrary linear models and post-screening inference techniques. In particular, (i) it shows that a condition---termed the screening condition---is sufficient for successful correlation-based screening of linear models, and (ii) it provides insights into the dependence of marginal correlation-based screening on different problem parameters. Numerical experiments confirm that these insights are not mere artifacts of analysis; rather, they are reflective of the challenges associated with marginal correlation-based variable screening."
What are two new Monte Carlo algorithms inspired by variable splitting and data augmentation?,39310759,"[{'abstract': '  This paper derives two new optimization-driven Monte Carlo algorithms\ninspired from variable splitting and data augmentation. In particular, the\nformulation of one of the proposed approaches is closely related to the\nalternating direction method of multipliers (ADMM) main steps. The proposed\nframework enables to derive faster and more efficient sampling schemes than the\ncurrent state-of-the-art methods and can embed the latter. By sampling\nefficiently the parameter to infer as well as the hyperparameters of the\nproblem, the generated samples can be used to approximate Bayesian estimators\nof the parameters to infer. Additionally, the proposed approach brings\nconfidence intervals at a low cost contrary to optimization methods.\nSimulations on two often-studied signal processing problems illustrate the\nperformance of the two proposed samplers. All results are compared to those\nobtained by recent state-of-the-art optimization and MCMC algorithms used to\nsolve these problems.\n', 'title': 'Split-and-augmented Gibbs sampler - Application to large-scale inference problems'}]","Title: Split-and-augmented Gibbs sampler - Application to large-scale inference problems

Abstract: This paper derives two new optimization-driven Monte Carlo algorithms inspired from variable splitting and data augmentation. In particular, the formulation of one of the proposed approaches is closely related to the alternating direction method of multipliers (ADMM) main steps. The proposed framework enables to derive faster and more efficient sampling schemes than the current state-of-the-art methods and can embed the latter. By sampling efficiently the parameter to infer as well as the hyperparameters of the problem, the generated samples can be used to approximate Bayesian estimators of the parameters to infer. Additionally, the proposed approach brings confidence intervals at a low cost contrary to optimization methods. Simulations on two often-studied signal processing problems illustrate the performance of the two proposed samplers. All results are compared to those obtained by recent state-of-the-art optimization and MCMC algorithms used to solve these problems."
How does Word2API address the challenge of acquiring a large dataset of words and APIs?,6460d63f,"[{'abstract': '  Developers increasingly rely on text matching tools to analyze the relation\nbetween natural language words and APIs. However, semantic gaps, namely textual\nmismatches between words and APIs, negatively affect these tools. Previous\nstudies have transformed words or APIs into low-dimensional vectors for\nmatching; however, inaccurate results were obtained due to the failure of\nmodeling words and APIs simultaneously. To resolve this problem, two main\nchallenges are to be addressed: the acquisition of massive words and APIs for\nmining and the alignment of words and APIs for modeling. Therefore, this study\nproposes Word2API to effectively estimate relatedness of words and APIs.\nWord2API collects millions of commonly used words and APIs from code\nrepositories to address the acquisition challenge. Then, a shuffling strategy\nis used to transform related words and APIs into tuples to address the\nalignment challenge. Using these tuples, Word2API models words and APIs\nsimultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness\nestimation in terms of precision and NDCG. Word2API is also effective on\nsolving typical software tasks, e.g., query expansion and API documents\nlinking. A simple system with Word2API-expanded queries recommends up to 21.4%\nmore related APIs for developers. Meanwhile, Word2API improves comparison\nalgorithms by 7.9%-17.4% in linking questions in Question&Answer communities to\nAPI documents.\n', 'title': 'Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding'}]","Title: Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding

Abstract:   Developers increasingly rely on text matching tools to analyze the relation
between natural language words and APIs. However, semantic gaps, namely textual
mismatches between words and APIs, negatively affect these tools. Previous
studies have transformed words or APIs into low-dimensional vectors for
matching; however, inaccurate results were obtained due to the failure of
modeling words and APIs simultaneously. To resolve this problem, two main
challenges are to be addressed: the acquisition of massive words and APIs for
mining and the alignment of words and APIs for modeling. Therefore, this study
proposes Word2API to effectively estimate relatedness of words and APIs.
Word2API collects millions of commonly used words and APIs from code
repositories to address the acquisition challenge. Then, a shuffling strategy
is used to transform related words and APIs into tuples to address the
alignment challenge. Using these tuples, Word2API models words and APIs
simultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness
estimation in terms of precision and NDCG. Word2API is also effective on
solving typical software tasks, e.g., query expansion and API documents
linking. A simple system with Word2API-expanded queries recommends up to 21.4%
more related APIs for developers. Meanwhile, Word2API improves comparison
algorithms by 7.9%-17.4% in linking questions in Question&Answer communities to
API documents.

According to the abstract, Word2API addresses the challenge of acquiring a large dataset of words and APIs by ""collecting millions of commonly used words and APIs from code repositories""."
At what germanium ion fluence is multifractality most pronounced after annealing?,91789eec,"[{'abstract': '  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy\nwere implanted into SiO2 matrix with Different fluences. The implanted samples\nwere annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of\nimplanted as well as annealed samples were captured by the atomic force\nmicroscopy (AFM). Two dimension (2D) multifractal detrended fluctuation\nanalysis (MFDFA) based on the partition function approach has been used to\nstudy the surfaces of ion implanted and annealed samples. The partition\nfunction is used to calculate generalized Hurst exponent with the segment size.\nMoreover, it is seen that the generalized Hurst exponents vary nonlinearly with\nthe moment, thereby exhibiting the multifractal nature. The multifractality of\nsurface is pronounced after annealing for the surface implanted with fluence\n7.5X1016 ions/cm^2.\n', 'title': 'Morphological characterization of Ge ion implanted SiO2 matrix using multifractal technique'}]","Title: Morphological characterization of Ge ion implanted SiO2 matrix using multifractal technique

Abstract: 200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy were implanted into SiO2 matrix with Different fluences. The implanted samples were annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of implanted as well as annealed samples were captured by the atomic force microscopy (AFM). Two dimension (2D) multifractal detrended fluctuation analysis (MFDFA) based on the partition function approach has been used to study the surfaces of ion implanted and annealed samples. The partition function is used to calculate generalized Hurst exponent with the segment size. Moreover, it is seen that the generalized Hurst exponents vary nonlinearly with the moment, thereby exhibiting the multifractal nature. The multifractality of surface is pronounced after annealing for the surface implanted with fluence 7.5X10^16 ions/cm^2.

Answer: The multifractality is most pronounced after annealing at a germanium ion fluence of 7.5X10^16 ions/cm^2."
What unique surface structures were observed in the low-temperature phase of EuFe2(As0.79P0.21)2?,e7c8bad5,"[{'abstract': '  Using low-temperature Magnetic Force Microscopy (MFM) we provide direct\nexperimental evidence for spontaneous vortex phase (SVP) formation in\nEuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting\n$T^{\\rm 0}_{\\rm SC}=23.6$~K and ferromagnetic $T_{\\rm FM}\\sim17.7$~K transition\ntemperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the\nvicinity of $T_{\\rm FM}$. Also, upon cooling cycle near $T_{\\rm FM}$ we observe\nthe first-order transition from the short period domain structure, which\nappears in the Meissner state, into the long period domain structure with\nspontaneous vortices. It is the first experimental observation of this scenario\nin the ferromagnetic superconductors. Low-temperature phase is characterized by\nmuch larger domains in V-AV state and peculiar branched striped structures at\nthe surface, which are typical for uniaxial ferromagnets with perpendicular\nmagnetic anisotropy (PMA). The domain wall parameters at various temperatures\nare estimated.\n', 'title': 'Direct Evidence of Spontaneous Abrikosov Vortex State in Ferromagnetic Superconductor EuFe$_2$(As$_{1-x}$P$_x$)$_2$ with $x=0.21$'}]","Title: Direct Evidence of Spontaneous Abrikosov Vortex State in Ferromagnetic Superconductor EuFe$_2$(As$_{1-x}$P$_x$)$_2$ with $x=0.21$

Abstract:   Using low-temperature Magnetic Force Microscopy (MFM) we provide direct
experimental evidence for spontaneous vortex phase (SVP) formation in
EuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting
$T^{\rm 0}_{\rm SC}=23.6$~K and ferromagnetic $T_{\rm FM}\sim17.7$~K transition
temperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the
vicinity of $T_{\rm FM}$. Also, upon cooling cycle near $T_{\rm FM}$ we observe
the first-order transition from the short period domain structure, which
appears in the Meissner state, into the long period domain structure with
spontaneous vortices. It is the first experimental observation of this scenario
in the ferromagnetic superconductors. Low-temperature phase is characterized by
much larger domains in V-AV state and peculiar branched striped structures at
the surface, which are typical for uniaxial ferromagnets with perpendicular
magnetic anisotropy (PMA). The domain wall parameters at various temperatures
are estimated.

The unique surface structures observed in the low-temperature phase of EuFe2(As0.79P0.21)2 are ""branched striped structures"" which are typical for uniaxial ferromagnets with perpendicular magnetic anisotropy (PMA)."
What are some of the issues discussed regarding adaptive credible intervals in the paper 'Uncertainty Quantification for the Horseshoe',c1cdbb5f,"[{'abstract': '  We begin by introducing the main ideas of the paper under discussion. We\ndiscuss some interesting issues regarding adaptive component-wise credible\nintervals. We then briefly touch upon the concepts of self-similarity and\nexcessive bias restriction. This is then followed by some comments on the\nextensive simulation study carried out in the paper.\n', 'title': 'Contributed Discussion to Uncertainty Quantification for the Horseshoe by Stéphanie van der Pas, Botond Szabó and Aad van der Vaart'}]","Title: Contributed Discussion to Uncertainty Quantification for the Horseshoe

Abstract: We begin by introducing the main ideas of the paper under discussion. We discuss some interesting issues regarding adaptive component-wise credible intervals. We then briefly touch upon the concepts of self-similarity and excessive bias restriction. This is then followed by some comments on the extensive simulation study carried out in the paper.

Unfortunately, the abstract does not provide more specific information about the issues discussed regarding adaptive credible intervals."
What is CC-CC and how is it related to CC?,2af5263a,"[{'abstract': '  Dependently typed languages such as Coq are used to specify and verify the\nfull functional correctness of source programs. Type-preserving compilation can\nbe used to preserve these specifications and proofs of correctness through\ncompilation into the generated target-language programs. Unfortunately,\ntype-preserving compilation of dependent types is hard. In essence, the problem\nis that dependent type systems are designed around high-level compositional\nabstractions to decide type checking, but compilation interferes with the\ntype-system rules for reasoning about run-time terms.\nWe develop a type-preserving closure-conversion translation from the Calculus\nof Constructions (CC) with strong dependent pairs ($\\Sigma$ types)---a subset\nof the core language of Coq---to a type-safe, dependently typed compiler\nintermediate language named CC-CC. The central challenge in this work is how to\ntranslate the source type-system rules for reasoning about functions into\ntarget type-system rules for reasoning about closures. To justify these rules,\nwe prove soundness of CC-CC by giving a model in CC. In addition to type\npreservation, we prove correctness of separate compilation.\n', 'title': 'Typed Closure Conversion for the Calculus of Constructions'}]","I apologize, but I couldn't find any information related to ""CC-CC"" in the provided context. The context consists of five research paper abstracts, and none of them mention ""CC-CC"" or ""CC"". If you meant to ask about a different term or concept, please feel free to ask, and I'll do my best to help."
What is a need for in research and education related to the analysis of longitudinal and multivariate data from factorial experiments?,7ee7399c,"[{'abstract': '  The numerical availability of statistical inference methods for a modern and\nrobust analysis of longitudinal- and multivariate data in factorial experiments\nis an essential element in research and education. While existing approaches\nthat rely on specific distributional assumptions of the data (multivariate\nnormality and/or characteristic covariance matrices) are implemented in\nstatistical software packages, there is a need for user-friendly software that\ncan be used for the analysis of data that do not fulfill the aforementioned\nassumptions and provide accurate p-value and confidence interval estimates.\nTherefore, newly developed statistical methods for the analysis of repeated\nmeasures designs and multivariate data that neither assume multivariate\nnormality nor specific covariance matrices have been implemented in the freely\navailable R-package MANOVA.RM. The package is equipped with a graphical user\ninterface for plausible applications in academia and other educational purpose.\nSeveral motivating examples illustrate the application of the methods.\n', 'title': 'Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM'}]","Title: Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM

Abstract: The numerical availability of statistical inference methods for a modern and 
robust analysis of longitudinal- and multivariate data in factorial experiments is 
an essential element in research and education. While existing approaches that rely 
on specific distributional assumptions of the data (multivariate normality and/or 
characteristic covariance matrices) are implemented in statistical software 
packages, there is a need for user-friendly software that can be used for the 
analysis of data that do not fulfill the aforementioned assumptions and 
provide accurate p-value and confidence interval estimates. Therefore, newly 
developed statistical methods for the analysis of repeated measures designs 
and multivariate data that neither assume multivariate normality nor specific 
covariance matrices have been implemented in the freely available R-package 
MANOVA.RM. The package is equipped with a graphical user interface for 
plausible applications in academia and other educational purpose. Several 
motivating examples illustrate the application of the methods."
Describe a type of basis function used in a new method for approximating functions of distributions,09808051,"[{'abstract': '  A number of fundamental quantities in statistical signal processing and\ninformation theory can be expressed as integral functions of two probability\ndensity functions. Such quantities are called density functionals as they map\ndensity functions onto the real line. For example, information divergence\nfunctions measure the dissimilarity between two probability density functions\nand are useful in a number of applications. Typically, estimating these\nquantities requires complete knowledge of the underlying distribution followed\nby multi-dimensional integration. Existing methods make parametric assumptions\nabout the data distribution or use non-parametric density estimation followed\nby high-dimensional integration. In this paper, we propose a new alternative.\nWe introduce the concept of ""data-driven basis functions"" - functions of\ndistributions whose value we can estimate given only samples from the\nunderlying distributions without requiring distribution fitting or direct\nintegration. We derive a new data-driven complete basis that is similar to the\ndeterministic Bernstein polynomial basis and develop two methods for performing\nbasis expansions of functionals of two distributions. We also show that the new\nbasis set allows us to approximate functions of distributions as closely as\ndesired. Finally, we evaluate the methodology by developing data driven\nestimators for the Kullback-Leibler divergences and the Hellinger distance and\nby constructing empirical estimates of tight bounds on the Bayes error rate.\n', 'title': 'Direct estimation of density functionals using a polynomial basis'}]","I think I can help you with that!

Title: Direct estimation of density functionals using a polynomial basis

Abstract: A number of fundamental quantities in statistical signal processing and information theory can be expressed as integral functions of two probability density functions. Such quantities are called density functionals as they map density functions onto the real line. For example, information divergence functions measure the dissimilarity between two probability density functions and are useful in a number of applications. Typically, estimating these quantities requires complete knowledge of the underlying distribution followed by multi-dimensional integration. Existing methods make parametric assumptions about the data distribution or use non-parametric density estimation followed by high-dimensional integration. In this paper, we propose a new alternative. We introduce the concept of ""data-driven basis functions"" - functions of distributions whose value we can estimate given only samples from the underlying distributions without requiring distribution fitting or direct integration. We derive a new data-driven complete basis that is similar to the deterministic Bernstein polynomial basis and develop two methods for performing basis expansions of functionals of two distributions. We also show that the new basis set allows us to approximate functions of distributions as closely as desired. Finally, we evaluate the methodology by developing data driven estimators for the Kullback-Leibler divergences and the Hellinger distance and by constructing empirical estimates of tight bounds on the Bayes error rate.

The type of basis function used in this new method is similar to the deterministic Bernstein polynomial basis."
What type of sensor measurements were used to demonstrate the benefits of sensor fusion in a flying robot?,477c7ab8,"[{'abstract': '  Self-supervised learning (SSL) is a reliable learning mechanism in which a\nrobot enhances its perceptual capabilities. Typically, in SSL a trusted,\nprimary sensor cue provides supervised training data to a secondary sensor cue.\nIn this article, a theoretical analysis is performed on the fusion of the\nprimary and secondary cue in a minimal model of SSL. A proof is provided that\ndetermines the specific conditions under which it is favorable to perform\nfusion. In short, it is favorable when (i) the prior on the target value is\nstrong or (ii) the secondary cue is sufficiently accurate. The theoretical\nfindings are validated with computational experiments. Subsequently, a\nreal-world case study is performed to investigate if fusion in SSL is also\nbeneficial when assumptions of the minimal model are not met. In particular, a\nflying robot learns to map pressure measurements to sonar height measurements\nand then fuses the two, resulting in better height estimation. Fusion is also\nbeneficial in the opposite case, when pressure is the primary cue. The analysis\nand results are encouraging to study SSL fusion also for other robots and\nsensors.\n', 'title': 'Self-supervised learning: When is fusion of the primary and secondary sensor cue useful?'}]","Title: Self-supervised learning: When is fusion of the primary and secondary sensor cue useful?

Abstract:   Self-supervised learning (SSL) is a reliable learning mechanism in which a
robot enhances its perceptual capabilities. Typically, in SSL a trusted,
primary sensor cue provides supervised training data to a secondary sensor cue.
In this article, a theoretical analysis is performed on the fusion of the
primary and secondary cue in a minimal model of SSL. A proof is provided that
determines the specific conditions under which it is favorable to perform
fusion. In short, it is favorable when (i) the prior on the target value is
strong or (ii) the secondary cue is sufficiently accurate. The theoretical
findings are validated with computational experiments. Subsequently, a
real-world case study is performed to investigate if fusion in SSL is also
beneficial when assumptions of the minimal model are not met. In particular, a
flying robot learns to map pressure measurements to sonar height measurements
and then fuses the two, resulting in better height estimation. Fusion is also
beneficial in the opposite case, when pressure is the primary cue. The analysis
and results are encouraging to study SSL fusion also for other robots and
sensors."
What method is proposed for classifying connected immersions into a given cell complex?,a8da760c,"[{'abstract': '  An immersion $f : {\\mathcal D} \\rightarrow \\mathcal C$ between cell complexes\nis a local homeomorphism onto its image that commutes with the characteristic\nmaps of the cell complexes. We study immersions between finite-dimensional\nconnected $\\Delta$-complexes by replacing the fundamental group of the base\nspace by an appropriate inverse monoid. We show how conjugacy classes of the\nclosed inverse submonoids of this inverse monoid may be used to classify\nconnected immersions into the complex. This extends earlier results of Margolis\nand Meakin for immersions between graphs and of Meakin and Szakács on\nimmersions into $2$-dimensional $CW$-complexes.\n', 'title': 'Inverse monoids and immersions of cell complexes'}]","Title: Inverse monoids and immersions of cell complexes

Abstract: An immersion $f : {\mathcal D} \rightarrow \mathcal C$ between cell complexes is a local homeomorphism onto its image that commutes with the characteristic maps of the cell complexes. We study immersions between finite-dimensional connected $\Delta$-complexes by replacing the fundamental group of the base space by an appropriate inverse monoid. We show how conjugacy classes of the closed inverse submonoids of this inverse monoid may be used to classify connected immersions into the complex. This extends earlier results of Margolis and Meakin for immersions between graphs and of Meakin and Szakács on immersions into $2$-dimensional $CW$-complexes."
How does the dark exciton splitting in WSe2 monolayers relate to theoretical predictions based on group theory?,ee29656b,"[{'abstract': '  The intricate interplay between optically dark and bright excitons governs\nthe light-matter interaction in transition metal dichalcogenide monolayers. We\nhave performed a detailed investigation of the ""spin-forbidden"" dark excitons\nin WSe2 monolayers by optical spectroscopy in an out-of-plane magnetic field\nBz. In agreement with the theoretical predictions deduced from group theory\nanalysis, magneto-photoluminescence experiments reveal a zero field splitting\n$\\delta=0.6 \\pm 0.1$ meV between two dark exciton states. The low energy state\nbeing strictly dipole forbidden (perfectly dark) at Bz=0 while the upper state\nis partially coupled to light with z polarization (""grey"" exciton). The first\ndetermination of the dark neutral exciton lifetime $\\tau_D$ in a transition\nmetal dichalcogenide monolayer is obtained by time-resolved photoluminescence.\nWe measure $\\tau_D \\sim 110 \\pm 10$ ps for the grey exciton state, i.e. two\norders of magnitude longer than the radiative lifetime of the bright neutral\nexciton at T=12 K.\n', 'title': 'Fine Structure and Lifetime of Dark Excitons in Transition Metal Dichalcogenide Monolayers'}]","Here's the answer:

Title: Fine Structure and Lifetime of Dark Excitons in Transition Metal Dichalcogenide Monolayers

Abstract: The intricate interplay between optically dark and bright excitons governs the light-matter interaction in transition metal dichalcogenide monolayers. We have performed a detailed investigation of the ""spin-forbidden"" dark excitons in WSe2 monolayers by optical spectroscopy in an out-of-plane magnetic field Bz. In agreement with the theoretical predictions deduced from group theory analysis, magneto-photoluminescence experiments reveal a zero field splitting $\delta=0.6 \pm 0.1$ meV between two dark exciton states. The low energy state being strictly dipole forbidden (perfectly dark) at Bz=0 while the upper state is partially coupled to light with z polarization (""grey"" exciton). The first determination of the dark neutral exciton lifetime $\tau_D$ in a transition metal dichalcogenide monolayer is obtained by time-resolved photoluminescence. We measure $\tau_D \sim 110 \pm 10$ ps for the grey exciton state, i.e. two orders of magnitude longer than the radiative lifetime of the bright neutral exciton at T=12 K."
How do researchers typically adjust for informative sampling in Bayesian estimation?,dc837bc0,"[{'abstract': '  Bayesian estimation is increasingly popular for performing model based\ninference to support policymaking. These data are often collected from surveys\nunder informative sampling designs where subject inclusion probabilities are\ndesigned to be correlated with the response variable of interest. Sampling\nweights constructed from marginal inclusion probabilities are typically used to\nform an exponentiated pseudo likelihood that adjusts the population likelihood\nfor estimation on the sample due to ease-of-estimation. We propose an\nalternative adjustment based on a Bayes rule construction that simultaneously\nperforms weight smoothing and estimates the population model parameters in a\nfully Bayesian construction. We formulate conditions on known marginal and\npairwise inclusion probabilities that define a class of sampling designs where\n$L_{1}$ consistency of the joint posterior is guaranteed. We compare\nperformances between the two approaches on synthetic data, which reveals that\nour fully Bayesian approach better estimates posterior uncertainty without a\nrequirement to calibrate the normalization of the sampling weights. We\ndemonstrate our method on an application concerning the National Health and\nNutrition Examination Survey exploring the relationship between caffeine\nconsumption and systolic blood pressure.\n', 'title': 'Fully Bayesian Estimation Under Informative Sampling'}]","Title: Fully Bayesian Estimation Under Informative Sampling

Abstract: Bayesian estimation is increasingly popular for performing model based inference to support policymaking. These data are often collected from surveys under informative sampling designs where subject inclusion probabilities are designed to be correlated with the response variable of interest. Sampling weights constructed from marginal inclusion probabilities are typically used to form an exponentiated pseudo likelihood that adjusts the population likelihood for estimation on the sample due to ease-of-estimation. We propose an alternative adjustment based on a Bayes rule construction that simultaneously performs weight smoothing and estimates the population model parameters in a fully Bayesian construction. We formulate conditions on known marginal and pairwise inclusion probabilities that define a class of sampling designs where $L_{1}$ consistency of the joint posterior is guaranteed. We compare performances between the two approaches on synthetic data, which reveals that our fully Bayesian approach better estimates posterior uncertainty without a requirement to calibrate the normalization of the sampling weights. We demonstrate our method on an application concerning the National Health and Nutrition Examination Survey exploring the relationship between caffeine consumption and systolic blood pressure.

In this paper, the researcher suggests that sampling weights constructed from marginal inclusion probabilities are typically used to form an exponentiated pseudo likelihood that adjusts the population likelihood for estimation on the sample due to ease-of-estimation. However, they propose an alternative adjustment based on a Bayes rule construction that simultaneously performs weight smoothing and estimates the population model parameters in a fully Bayesian construction."
What are the two elementary moduli spaces of meromorphic quadratic differentials with real periods studied in the paper?,c2bbd5a8,"[{'abstract': ""  We study special circle bundles over two elementary moduli spaces of\nmeromorphic quadratic differentials with real periods denoted by $\\mathcal\nQ_0^{\\mathbb R}(-7)$ and $\\mathcal Q^{\\mathbb R}_0([-3]^2)$. The space\n$\\mathcal Q_0^{\\mathbb R}(-7)$ is the moduli space of meromorphic quadratic\ndifferentials on the Riemann sphere with one pole of order 7 with real periods;\nit appears naturally in the study of a neighbourhood of the Witten's cycle\n$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic\ndifferentials of $\\mathcal M_{g,n}$. The space $\\mathcal Q^{\\mathbb\nR}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the\nRiemann sphere with two poles of order at most 3 with real periods; it appears\nin description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the\ncombinatorial model. The application of the formalism of the Bergman\ntau-function to the combinatorial model (with the goal of computing\nanalytically Poincare dual cycles to certain combinations of tautological\nclasses) requires the study of special sections of circle bundles over\n$\\mathcal Q_0^{\\mathbb R}(-7)$ and $\\mathcal Q^{\\mathbb R}_0([-3]^2)$; in the\ncase of the space $\\mathcal Q_0^{\\mathbb R}(-7)$ a section of this circle\nbundle is given by the argument of the modular discriminant. We study the\nspaces $\\mathcal Q_0^{\\mathbb R}(-7)$ and $\\mathcal Q^{\\mathbb R}_0([-3]^2)$,\nalso called the spaces of Boutroux curves, in detail, together with\ncorresponding circle bundles.\n"", 'title': 'Discriminant circle bundles over local models of Strebel graphs and Boutroux curves'}]","Based on the context provided, I can answer the question as follows:

Title: Discriminant circle bundles over local models of Strebel graphs and Boutroux curves

Abstract: We study special circle bundles over two elementary moduli spaces of meromorphic quadratic differentials with real periods denoted by $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$. The space $\mathcal Q_0^{\mathbb R}(-7)$ is the moduli space of meromorphic quadratic differentials on the Riemann sphere with one pole of order 7 with real periods; it appears naturally in the study of a neighbourhood of the Witten's cycle $W_1$ in the combinatorial model based on Jenkins-Strebel quadratic differentials of $\mathcal M_{g,n}$. The space $\mathcal Q^{\mathbb R}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the Riemann sphere with two poles of order at most 3 with real periods; it appears in description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the combinatorial model.

The two elementary moduli spaces of meromorphic quadratic differentials with real periods studied in the paper are $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$."
What is the relationship between Andreev reflection and valley polarization in graphene nanoribbons?,276785d5,"[{'abstract': '  Ballistic point contact (BPC) with zigzag edges in graphene is a main\ncandidate of a valley filter, in which the polarization of the valley degree of\nfreedom can be selected by using a local gate voltage. Here, we propose to\ndetect the valley filtering effect by Andreev reflection. Because electrons in\nthe lowest conduction band and the highest valence band of the BPC possess\nopposite chirality, the inter-band Andreev reflection is strongly suppressed,\nafter multiple scattering and interference. We draw this conclusion by both the\nscattering matrix analysis and the numerical simulation. The Andreev reflection\nas a function of the incident energy of electrons and the local gate voltage at\nthe BPC is obtained, by which the parameter region for a perfect valley filter\nand the direction of valley polarization can be determined. The Andreev\nreflection exhibits an oscillatory decay with the length of the BPC, indicating\na negative correlation to valley polarization.\n', 'title': 'Probing valley filtering effect by Andreev reflection in zigzag graphene nanoribbon'}]","Title: Probing valley filtering effect by Andreev reflection in zigzag graphene nanoribbon

Abstract: Ballistic point contact (BPC) with zigzag edges in graphene is a main candidate of a valley filter, in which the polarization of the valley degree of freedom can be selected by using a local gate voltage. Here, we propose to detect the valley filtering effect by Andreev reflection. Because electrons in the lowest conduction band and the highest valence band of the BPC possess opposite chirality, the inter-band Andreev reflection is strongly suppressed, after multiple scattering and interference. We draw this conclusion by both the scattering matrix analysis and the numerical simulation. The Andreev reflection as a function of the incident energy of electrons and the local gate voltage at the BPC is obtained, by which the parameter region for a perfect valley filter and the direction of valley polarization can be determined. The Andreev reflection exhibits an oscillatory decay with the length of the BPC, indicating a negative correlation to valley polarization.

The relationship between Andreev reflection and valley polarization in graphene nanoribbons is that Andreev reflection can be used to detect the valley filtering effect, and the parameter region for a perfect valley filter and the direction of valley polarization can be determined by the Andreev reflection as a function of the incident energy of electrons and the local gate voltage. Additionally, the Andreev reflection exhibits an oscillatory decay with the length of the BPC, which indicates a negative correlation to valley polarization."
How do two-tone spectroscopy measurements reveal energy level shifts in these systems?,f585d0c9,"[{'abstract': ""  We report on experimentally measured light shifts of superconducting flux\nqubits deep-strongly coupled to LC oscillators, where the coupling constants\nare comparable to the qubit and oscillator resonance frequencies. By using\ntwo-tone spectroscopy, the energies of the six lowest levels of each circuit\nare determined. We find huge Lamb shifts that exceed 90% of the bare qubit\nfrequencies and inversions of the qubits' ground and excited states when there\nare a finite number of photons in the oscillator. Our experimental results\nagree with theoretical predictions based on the quantum Rabi model.\n"", 'title': 'Inversion of Qubit Energy Levels in Qubit-Oscillator Circuits in the Deep-Strong-Coupling Regime'}]","I think I can help you with that!

Title: Inversion of Qubit Energy Levels in Qubit-Oscillator Circuits in the Deep-Strong-Coupling Regime

Abstract: We report on experimentally measured light shifts of superconducting flux qubits deep-strongly coupled to LC oscillators, where the coupling constants are comparable to the qubit and oscillator resonance frequencies. By using two-tone spectroscopy, the energies of the six lowest levels of each circuit are determined. We find huge Lamb shifts that exceed 90% of the bare qubit frequencies and inversions of the qubits' ground and excited states when there are a finite number of photons in the oscillator. Our experimental results agree with theoretical predictions based on the quantum Rabi model.

It seems that the two-tone spectroscopy measurements in this paper reveal energy level shifts in the qubit-oscillator systems by determining the energies of the six lowest levels of each circuit."
What is the main source of uncertainty identified in the posterior distribution of the dam breach model,9be0f336,"[{'abstract': '  Dam breach models are commonly used to predict outflow hydrographs of\npotentially failing dams and are key ingredients for evaluating flood risk. In\nthis paper a new dam breach modeling framework is introduced that shall improve\nthe reliability of hydrograph predictions of homogeneous earthen embankment\ndams. Striving for a small number of parameters, the simplified physics-based\nmodel describes the processes of failing embankment dams by breach enlargement,\ndriven by progressive surface erosion. Therein the erosion rate of dam material\nis modeled by empirical sediment transport formulations. Embedding the model\ninto a Bayesian multilevel framework allows for quantitative analysis of\ndifferent categories of uncertainties. To this end, data available in\nliterature of observed peak discharge and final breach width of historical dam\nfailures was used to perform model inversion by applying Markov Chain Monte\nCarlo simulation. Prior knowledge is mainly based on non-informative\ndistribution functions. The resulting posterior distribution shows that the\nmain source of uncertainty is a correlated subset of parameters, consisting of\nthe residual error term and the epistemic term quantifying the breach erosion\nrate. The prediction intervals of peak discharge and final breach width are\ncongruent with values known from literature. To finally predict the outflow\nhydrograph for real case applications, an alternative residual model was\nformulated that assumes perfect data and a perfect model. The fully\nprobabilistic fashion of hydrograph prediction has the potential to improve the\nadequate risk management of downstream flooding.\n', 'title': 'Development of probabilistic dam breach model using Bayesian inference'}]","Title: Development of probabilistic dam breach model using Bayesian inference

Abstract: Dam breach models are commonly used to predict outflow hydrographs of potentially failing dams and are key ingredients for evaluating flood risk. In this paper a new dam breach modeling framework is introduced that shall improve the reliability of hydrograph predictions of homogeneous earthen embankment dams. Striving for a small number of parameters, the simplified physics-based model describes the processes of failing embankment dams by breach enlargement, driven by progressive surface erosion. Therein the erosion rate of dam material is modeled by empirical sediment transport formulations. Embedding the model into a Bayesian multilevel framework allows for quantitative analysis of different categories of uncertainties. To this end, data available in literature of observed peak discharge and final breach width of historical dam failures was used to perform model inversion by applying Markov Chain Monte Carlo simulation. Prior knowledge is mainly based on non-informative distribution functions. The resulting posterior distribution shows that the main source of uncertainty is a correlated subset of parameters, consisting of the residual error term and the epistemic term quantifying the breach erosion rate. The prediction intervals of peak discharge and final breach width are congruent with values known from literature. To finally predict the outflow hydrograph for real case applications, an alternative residual model was formulated that assumes perfect data and a perfect model. The fully probabilistic fashion of hydrograph prediction has the potential to improve the adequate risk management of downstream flooding.

The main source of uncertainty identified in the posterior distribution of the dam breach model is a correlated subset of parameters, consisting of the residual error term and the epistemic term quantifying the breach erosion rate."
Does a paper by Diamond discuss conditions for asymptotic density of Beurling generalized integers?,24e23fbc,"[{'abstract': '  We give a short proof of the $L^{1}$ criterion for Beurling generalized\nintegers to have a positive asymptotic density. We actually prove the existence\nof density under a weaker hypothesis. We also discuss related sufficient\nconditions for the estimate $m(x)=\\sum_{n_{k}\\leq x} \\mu(n_k)/n_k=o(1)$, with\n$\\mu$ the Beurling analog of the Moebius function.\n', 'title': ""On Diamond's $L^1$ criterion for asymptotic density of Beurling generalized integers""}]","Title: On Diamond's $L^1$ criterion for asymptotic density of Beurling generalized integers

Abstract: We give a short proof of the $L^{1}$ criterion for Beurling generalized integers to have a positive asymptotic density. We actually prove the existence of density under a weaker hypothesis. We also discuss related sufficient conditions for the estimate $m(x)=\sum_{n_{k}\leq x} \mu(n_k)/n_k=o(1)$, with $\mu$ the Beurling analog of the Moebius function."
What types of sparse data exchange methods are discussed in the paper?,1d6719cd,"[{'abstract': ""  Reduction of communication and efficient partitioning are key issues for\nachieving scalability in hierarchical $N$-Body algorithms like FMM. In the\npresent work, we propose four independent strategies to improve partitioning\nand reduce communication. First of all, we show that the conventional wisdom of\nusing space-filling curve partitioning may not work well for boundary integral\nproblems, which constitute about 50% of FMM's application user base. We propose\nan alternative method which modifies orthogonal recursive bisection to solve\nthe cell-partition misalignment that has kept it from scaling previously.\nSecondly, we optimize the granularity of communication to find the optimal\nbalance between a bulk-synchronous collective communication of the local\nessential tree and an RDMA per task per cell. Finally, we take the dynamic\nsparse data exchange proposed by Hoefler et al. and extend it to a hierarchical\nsparse data exchange, which is demonstrated at scale to be faster than the MPI\nlibrary's MPI_Alltoallv that is commonly used.\n"", 'title': 'Communication Reducing Algorithms for Distributed Hierarchical N-Body Problems with Boundary Distributions'}]","Title: Communication Reducing Algorithms for Distributed Hierarchical N-Body Problems with Boundary Distributions

Abstract:   Reduction of communication and efficient partitioning are key issues for achieving scalability in hierarchical $N$-Body algorithms like FMM. In the present work, we propose four independent strategies to improve partitioning and reduce communication. First of all, we show that the conventional wisdom of using space-filling curve partitioning may not work well for boundary integral problems, which constitute about 50% of FMM's application user base. We propose an alternative method which modifies orthogonal recursive bisection to solve the cell-partition misalignment that has kept it from scaling previously. Secondly, we optimize the granularity of communication to find the optimal balance between a bulk-synchronous collective communication of the local essential tree and an RDMA per task per cell. Finally, we take the dynamic sparse data exchange proposed by Hoefler et al. and extend it to a hierarchical sparse data exchange, which is demonstrated at scale to be faster than the MPI library's MPI_Alltoallv that is commonly used."
What numerical methods are used to approximate the Weertman equation?,e5fe31fd,"[{'abstract': '  This work discusses the numerical approximation of a nonlinear\nreaction-advection-diffusion equation, which is a dimensionless form of the\nWeertman equation. This equation models steadily-moving dislocations in\nmaterials science. It reduces to the celebrated Peierls-Nabarro equation when\nits advection term is set to zero. The approach rests on considering a\ntime-dependent formulation, which admits the equation under study as its\nlong-time limit. Introducing a Preconditioned Collocation Scheme based on\nFourier transforms, the iterative numerical method presented solves the\ntime-dependent problem, delivering at convergence the desired numerical\nsolution to the Weertman equation. Although it rests on an explicit\ntime-evolution scheme, the method allows for large time steps, and captures the\nsolution in a robust manner. Numerical results illustrate the efficiency of the\napproach for several types of nonlinearities.\n', 'title': 'Fourier-based numerical approximation of the Weertman equation for moving dislocations'}]","I think I have found the answer you are looking for!

Title: Fourier-based numerical approximation of the Weertman equation for moving dislocations

Abstract: This work discusses the numerical approximation of a nonlinear reaction-advection-diffusion equation, which is a dimensionless form of the Weertman equation. This equation models steadily-moving dislocations in materials science. It reduces to the celebrated Peierls-Nabarro equation when its advection term is set to zero. The approach rests on considering a time-dependent formulation, which admits the equation under study as its long-time limit. Introducing a Preconditioned Collocation Scheme based on Fourier transforms, the iterative numerical method presented solves the time-dependent problem, delivering at convergence the desired numerical solution to the Weertman equation. Although it rests on an explicit time-evolution scheme, the method allows for large time steps, and captures the solution in a robust manner. Numerical results illustrate the efficiency of the approach for several types of nonlinearities.

The numerical method used to approximate the Weertman equation is a Preconditioned Collocation Scheme based on Fourier transforms."
What is the measured high-energy cut-off for the spectrum of 4C 74.26?,97c9bc44,"[{'abstract': '  The relativistic jets created by some active galactic nuclei are important\nagents of AGN feedback. In spite of this, our understanding of what produces\nthese jets is still incomplete. X-ray observations, which can probe the\nprocesses operating in the central regions in immediate vicinity of the\nsupermassive black hole, the presumed jet launching point, are potentially\nparticularly valuable in illuminating the jet formation process. Here, we\npresent the hard X-ray NuSTAR observations of the radio-loud quasar 4C 74.26 in\na joint analysis with quasi-simultaneous, soft X-ray Swift observations. Our\nspectral analysis reveals a high-energy cut-off of 183$_{-35}^{+51}$ keV and\nconfirms the presence of ionized reflection in the source. From the average\nspectrum we detect that the accretion disk is mildly recessed with an inner\nradius of $R_\\mathrm{in}=4-180\\,R_\\mathrm{g}$. However, no significant\nevolution of the inner radius is seen during the three months covered by our\nNuSTAR campaign. This lack of variation could mean that the jet formation in\nthis radio-loud quasar differs from what is observed in broad-line radio\ngalaxies.\n', 'title': 'The X-ray reflection spectrum of the radio-loud quasar 4C 74.26'}]","Title: The X-ray reflection spectrum of the radio-loud quasar 4C 74.26

Abstract: The relativistic jets created by some active galactic nuclei are important agents of AGN feedback. In spite of this, our understanding of what produces these jets is still incomplete. X-ray observations, which can probe the processes operating in the central regions in immediate vicinity of the supermassive black hole, the presumed jet launching point, are potentially particularly valuable in illuminating the jet formation process. Here, we present the hard X-ray NuSTAR observations of the radio-loud quasar 4C 74.26 in a joint analysis with quasi-simultaneous, soft X-ray Swift observations. Our spectral analysis reveals a high-energy cut-off of 183$_{-35}^{+51}$ keV and confirms the presence of ionized reflection in the source. From the average spectrum we detect that the accretion disk is mildly recessed with an inner radius of $R_\mathrm{in}=4-180\,R_\mathrm{g}$. However, no significant evolution of the inner radius is seen during the three months covered by our NuSTAR campaign. This lack of variation could mean that the jet formation in this radio-loud quasar differs from what is observed in broad-line radio galaxies.

The measured high-energy cut-off for the spectrum of 4C 74.26 is 183$_{-35}^{+51}$ keV."
What numerical methods exist for finding multiple roots of polynomial systems?,70d8e922,"[{'abstract': '  Given a polynomial system f associated with a simple multiple zero x of\nmultiplicity {\\mu}, we give a computable lower bound on the minimal distance\nbetween the simple multiple zero x and other zeros of f. If x is only given\nwith limited accuracy, we propose a numerical criterion that f is certified to\nhave {\\mu} zeros (counting multiplicities) in a small ball around x.\nFurthermore, for simple double zeros and simple triple zeros whose Jacobian is\nof normalized form, we define modified Newton iterations and prove the\nquantified quadratic convergence when the starting point is close to the exact\nsimple multiple zero. For simple multiple zeros of arbitrary multiplicity whose\nJacobian matrix may not have a normalized form, we perform unitary\ntransformations and modified Newton iterations, and prove its non-quantified\nquadratic convergence and its quantified convergence for simple triple zeros.\n', 'title': 'Computing Simple Multiple Zeros of Polynomial Systems'}]","Title: Computing Simple Multiple Zeros of Polynomial Systems

Abstract:   Given a polynomial system f associated with a simple multiple zero x of
multiplicity {\mu}, we give a computable lower bound on the minimal distance
between the simple multiple zero x and other zeros of f. If x is only given
with limited accuracy, we propose a numerical criterion that f is certified to
have {\mu} zeros (counting multiplicities) in a small ball around x.
Furthermore, for simple double zeros and simple triple zeros whose Jacobian is
of normalized form, we define modified Newton iterations and prove the
quantified quadratic convergence when the starting point is close to the exact
simple multiple zero. For simple multiple zeros of arbitrary multiplicity whose
Jacobian matrix may not have a normalized form, we perform unitary
transformations and modified Newton iterations, and prove its non-quantified
quadratic convergence and its quantified convergence for simple triple zeros."
Describe a novel prototype for a smart digital signage system that utilizes software-defined IoT and invisible image sensor communication.,ebade417,"[{'abstract': '  Everything in the world is being connected, and things are becoming\ninteractive. The future of the interactive world depends on the future Internet\nof Things (IoT). Software-defined networking (SDN) technology, a new paradigm\nin the networking area, can be useful in creating an IoT because it can handle\ninteractivity by controlling physical devices, transmission of data among them,\nand data acquisition. However, digital signage can be one of the promising\ntechnologies in this era of technology that is progressing toward the\ninteractive world, connecting users to the IoT network through device-to-device\ncommunication technology. This article illustrates a novel prototype that is\nmainly focused on a smart digital signage system comprised of software-defined\nIoT (SD-IoT) and invisible image sensor communication technology. We have\nproposed an SDN scheme with a view to initiating its flexibility and\ncompatibility for an IoT network-based smart digital signage system. The idea\nof invisible communication can make the users of the technology trendier to it,\nand the usage of unused resources such as images and videos can be ensured. In\naddition, this communication has paved the way for interactivity between the\nuser and digital signage, where the digital signage and the camera of a\nsmartphone can be operated as a transmitter and a receiver, respectively. The\nproposed scheme might be applicable to real-world applications because SDN has\nthe flexibility to adapt with the alteration of network status without any\nhardware modifications while displays and smartphones are available everywhere.\nA performance analysis of this system showed the advantages of an SD-IoT\nnetwork over an Internet protocol-based IoT network considering a queuing\nanalysis for a dynamic link allocation process in the case of user access to\nthe IoT network.\n', 'title': 'Performance analysis of smart digital signage system based on software-defined IoT and invisible image sensor communication'}]","I think I have found what you're looking for!

Title: Performance analysis of smart digital signage system based on software-defined IoT and invisible image sensor communication

Abstract: Everything in the world is being connected, and things are becoming interactive. The future of the interactive world depends on the future Internet of Things (IoT). Software-defined networking (SDN) technology, a new paradigm in the networking area, can be useful in creating an IoT because it can handle interactivity by controlling physical devices, transmission of data among them, and data acquisition. However, digital signage can be one of the promising technologies in this era of technology that is progressing toward the interactive world, connecting users to the IoT network through device-to-device communication technology. This article illustrates a novel prototype that is mainly focused on a smart digital signage system comprised of software-defined IoT (SD-IoT) and invisible image sensor communication technology. We have proposed an SDN scheme with a view to initiating its flexibility and compatibility for an IoT network-based smart digital signage system. The idea of invisible communication can make the users of the technology trendier to it, and the usage of unused resources such as images and videos can be ensured. In addition, this communication has paved the way for interactivity between the user and digital signage, where the digital signage and the camera of a smartphone can be operated as a transmitter and a receiver, respectively. The proposed scheme might be applicable to real-world applications because SDN has the flexibility to adapt with the alteration of network status without any hardware modifications while displays and smartphones are available everywhere. A performance analysis of this system showed the advantages of an SD-IoT network over an Internet protocol-based IoT network considering a queuing analysis for a dynamic link allocation process in the case of user access to the IoT network."
How has the advancement of low-level robot skills influenced the design of robot architectures?,91b60d6b,"[{'abstract': '  When developing general purpose robots, the overarching software architecture\ncan greatly affect the ease of accomplishing various tasks. Initial efforts to\ncreate unified robot systems in the 1990s led to hybrid architectures,\nemphasizing a hierarchy in which deliberative plans direct the use of reactive\nskills. However, since that time there has been significant progress in the\nlow-level skills available to robots, including manipulation and perception,\nmaking it newly feasible to accomplish many more tasks in real-world domains.\nThere is thus renewed optimism that robots will be able to perform a wide array\nof tasks while maintaining responsiveness to human operators. However, the top\nlayer in traditional hybrid architectures, designed to achieve long-term goals,\ncan make it difficult to react quickly to human interactions during goal-driven\nexecution. To mitigate this difficulty, we propose a novel architecture that\nsupports such transitions by adding a top-level reactive module which has\nflexible access to both reactive skills and a deliberative control module. To\nvalidate this architecture, we present a case study of its application on a\ndomestic service robot platform.\n', 'title': 'LAAIR: A Layered Architecture for Autonomous Interactive Robots'}]","Based on the context, I found a relevant paper that answers the question.

Title: LAAIR: A Layered Architecture for Autonomous Interactive Robots

Abstract: When developing general purpose robots, the overarching software architecture can greatly affect the ease of accomplishing various tasks. Initial efforts to create unified robot systems in the 1990s led to hybrid architectures, emphasizing a hierarchy in which deliberative plans direct the use of reactive skills. However, since that time there has been significant progress in the low-level skills available to robots, including manipulation and perception, making it newly feasible to accomplish many more tasks in real-world domains. There is thus renewed optimism that robots will be able to perform a wide array of tasks while maintaining responsiveness to human operators. However, the top layer in traditional hybrid architectures, designed to achieve long-term goals, can make it difficult to react quickly to human interactions during goal-driven execution. To mitigate this difficulty, we propose a novel architecture that supports such transitions by adding a top-level reactive module which has flexible access to both reactive skills and a deliberative control module. To validate this architecture, we present a case study of its application on a domestic service robot platform.

This paper explains how the advancement of low-level robot skills, specifically in manipulation and perception, has influenced the design of robot architectures."
What is computational memory and how does it differ from traditional memory?,34d9c119,"[{'abstract': '  For decades, conventional computers based on the von Neumann architecture\nhave performed computation by repeatedly transferring data between their\nprocessing and their memory units, which are physically separated. As\ncomputation becomes increasingly data-centric and as the scalability limits in\nterms of performance and power are being reached, alternative computing\nparadigms are searched for in which computation and storage are collocated. A\nfascinating new approach is that of computational memory where the physics of\nnanoscale memory devices are used to perform certain computational tasks within\nthe memory unit in a non-von Neumann manner. Here we present a large-scale\nexperimental demonstration using one million phase-change memory devices\norganized to perform a high-level computational primitive by exploiting the\ncrystallization dynamics. Also presented is an application of such a\ncomputational memory to process real-world data-sets. The results show that\nthis co-existence of computation and storage at the nanometer scale could be\nthe enabler for new, ultra-dense, low power, and massively parallel computing\nsystems.\n', 'title': 'Temporal correlation detection using computational phase-change memory'}]","Title: Temporal correlation detection using computational phase-change memory

Abstract: For decades, conventional computers based on the von Neumann architecture have performed computation by repeatedly transferring data between their processing and their memory units, which are physically separated. As computation becomes increasingly data-centric and as the scalability limits in terms of performance and power are being reached, alternative computing paradigms are searched for in which computation and storage are collocated. A fascinating new approach is that of computational memory where the physics of nanoscale memory devices are used to perform certain computational tasks within the memory unit in a non-von Neumann manner. Here we present a large-scale experimental demonstration using one million phase-change memory devices organized to perform a high-level computational primitive by exploiting the crystallization dynamics. Also presented is an application of such a computational memory to process real-world data-sets. The results show that this co-existence of computation and storage at the nanometer scale could be the enabler for new, ultra-dense, low power, and massively parallel computing systems.

This paper explains that traditional memory is based on the von Neumann architecture, where computation and storage are separate units. Computational memory, on the other hand, is a new approach that combines computation and storage within the memory unit itself, using the physics of nanoscale memory devices to perform certain computational tasks. This allows for a non-von Neumann manner of computation, which enables new possibilities for ultra-dense, low-power, and massively parallel computing systems."
How does the author argue that variance should be understood as a probability interval?,de234f2e,"[{'abstract': '  The existing measurement theory interprets the variance as the dispersion of\nmeasured value, which is actually contrary to a general mathematical knowledge\nthat the variance of a constant is 0. This paper will fully demonstrate that\nthe variance in measurement theory is actually the evaluation of probability\ninterval of an error instead of the dispersion of a measured value, point out\nthe key point of mistake in the existing interpretation, and fully interpret a\nseries of changes in conceptual logic and processing method brought about by\nthis new concept.\n', 'title': 'The Correct Application of Variance Concept in Measurement Theory'}]","Title: The Correct Application of Variance Concept in Measurement Theory

Abstract: The existing measurement theory interprets the variance as the dispersion of measured value, which is actually contrary to a general mathematical knowledge that the variance of a constant is 0. This paper will fully demonstrate that the variance in measurement theory is actually the evaluation of probability interval of an error instead of the dispersion of a measured value, point out the key point of mistake in the existing interpretation, and fully interpret a series of changes in conceptual logic and processing method brought about by this new concept."
Which paper examines Morita equivalences between blocks preserving defect groups and their local structures?,52a1595f,"[{'abstract': '  This survey is about old and new results about the modular representation\ntheory of finite reductive groups with a strong emphasis on local methods. This\nincludes subpairs, Brauer\'s Main Theorems, fusion, Rickard equivalences. In the\ndefining characteristic we describe the relation between $p$-local subgroups\nand parabolic subgroups, then give classical consequences on simple modules and\nblocks, including the Alperin weight conjecture in that case. In the\nnon-defining characteristics, we sketch a picture of the local methods\npioneered by Fong-Srinivasan in the determination of blocks and their ordinary\ncharacters. This includes the relationship with Lusztig\'s twisted induction and\nthe determination of defect groups. We conclude with a survey of the results\nand methods by Bonnafé-Dat-Rouquier giving Morita equivalences between blocks\nthat preserve defect groups and the local structures.\nThe text grew out of the course and talks given by the author in July and\nSeptember 2016 during the program ""Local representation theory and simple\ngroups"" at CIB Lausanne. Written Oct 2017, to appear in a proceedings volume\npublished by EMS.\n', 'title': 'Local methods for blocks of finite simple groups'}]","Title: Local methods for blocks of finite simple groups

Abstract: This survey is about old and new results about the modular representation theory of finite reductive groups with a strong emphasis on local methods. This includes subpairs, Brauer's Main Theorems, fusion, Rickard equivalences. In the defining characteristic we describe the relation between $p$-local subgroups and parabolic subgroups, then give classical consequences on simple modules and blocks, including the Alperin weight conjecture in that case. In the non-defining characteristics, we sketch a picture of the local methods pioneered by Fong-Srinivasan in the determination of blocks and their ordinary characters. This includes the relationship with Lusztig's twisted induction and the determination of defect groups. We conclude with a survey of the results and methods by Bonnafé-Dat-Rouquier giving Morita equivalences between blocks that preserve defect groups and the local structures."
At what ratio of couplings does the spin gap close in the DMRG calculations?,5987d4a6,"[{'abstract': '  We investigate the effect of dimensional crossover in the ground state of the\nantiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular\nlattice that interpolates between the regime of weakly coupled Haldane chains\n($J^{\\prime}\\! \\!\\ll\\!\\! J$) and the isotropic triangular lattice\n($J^{\\prime}\\!\\!=\\!\\!J$). We use the density-matrix renormalization group\n(DMRG) and Schwinger boson theory performed at the Gaussian correction level\nabove the saddle-point solution. Our DMRG results show an abrupt transition\nbetween decoupled spin chains and the spirally ordered regime at\n$(J^{\\prime}/J)_c\\sim 0.42$, signaled by the sudden closing of the spin gap.\nComing from the magnetically ordered side, the computation of the spin\nstiffness within Schwinger boson theory predicts the instability of the spiral\nmagnetic order toward a magnetically disordered phase with one-dimensional\nfeatures at $(J^{\\prime}/J)_c \\sim 0.43$. The agreement of these complementary\nmethods, along with the strong difference found between the intra- and the\ninterchain DMRG short spin-spin correlations; for sufficiently large values of\nthe interchain coupling, suggests that the interplay between the quantum\nfluctuations and the dimensional crossover effects gives rise to the\none-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.\n', 'title': 'One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice'}]","Title: One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice

Abstract: We investigate the effect of dimensional crossover in the ground state of the antiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular lattice that interpolates between the regime of weakly coupled Haldane chains ($J^{\prime} \! \ll \! J$) and the isotropic triangular lattice ($J^{\prime} \! = \! J$). We use the density-matrix renormalization group (DMRG) and Schwinger boson theory performed at the Gaussian correction level above the saddle-point solution. Our DMRG results show an abrupt transition between decoupled spin chains and the spirally ordered regime at $(J^{\prime}/J)_c \sim 0.42$, signaled by the sudden closing of the spin gap. Coming from the magnetically ordered side, the computation of the spin stiffness within Schwinger boson theory predicts the instability of the spiral magnetic order toward a magnetically disordered phase with one-dimensional features at $(J^{\prime}/J)_c \sim 0.43$. The agreement of these complementary methods, along with the strong difference found between the intra- and the interchain DMRG short spin-spin correlations; for sufficiently large values of the interchain coupling, suggests that the interplay between the quantum fluctuations and the dimensional crossover effects gives rise to the one-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.

Answer: The spin gap closes in the DMRG calculations at a ratio of couplings of $(J^{\prime}/J)_c \sim 0.42$."
What are the key parameters that determine the dynamics of a freely rising cylinder in a fluid?,60350b39,"[{'abstract': ""  In this Letter, we study the motion and wake-patterns of freely rising and\nfalling cylinders in quiescent fluid. We show that the amplitude of oscillation\nand the overall system-dynamics are intricately linked to two parameters: the\nparticle's mass-density relative to the fluid $m^* \\equiv \\rho_p/\\rho_f$ and\nits relative moment-of-inertia $I^* \\equiv {I}_p/{I}_f$. This supersedes the\ncurrent understanding that a critical mass density ($m^*\\approx$ 0.54) alone\ntriggers the sudden onset of vigorous vibrations. Using over 144 combinations\nof ${m}^*$ and $I^*$, we comprehensively map out the parameter space covering\nvery heavy ($m^* > 10$) to very buoyant ($m^* < 0.1$) particles. The entire\ndata collapses into two scaling regimes demarcated by a transitional Strouhal\nnumber, $St_t \\approx 0.17$. $St_t$ separates a mass-dominated regime from a\nregime dominated by the particle's moment of inertia. A shift from one regime\nto the other also marks a gradual transition in the wake-shedding pattern: from\nthe classical $2S$~(2-Single) vortex mode to a $2P$~(2-Pairs) vortex mode.\nThus, auto-rotation can have a significant influence on the trajectories and\nwakes of freely rising isotropic bodies.\n"", 'title': 'Mass and moment of inertia govern the transition in the dynamics and wakes of freely rising and falling cylinders'}]","The answer to your question can be found in the first research paper.

Title: Mass and moment of inertia govern the transition in the dynamics and wakes of freely rising and falling cylinders

Abstract: In this Letter, we study the motion and wake-patterns of freely rising and falling cylinders in quiescent fluid. We show that the amplitude of oscillation and the overall system-dynamics are intricately linked to two parameters: the particle's mass-density relative to the fluid $m^* \equiv \rho_p/\rho_f$ and its relative moment-of-inertia $I^* \equiv {I}_p/{I}_f$. This supersedes the current understanding that a critical mass density ($m^*\approx$ 0.54) alone triggers the sudden onset of vigorous vibrations. Using over 144 combinations of ${m}^*$ and $I^*$, we comprehensively map out the parameter space covering very heavy ($m^* > 10$) to very buoyant ($m^* < 0.1$) particles. The entire data collapses into two scaling regimes demarcated by a transitional Strouhal number, $St_t \approx 0.17$. $St_t$ separates a mass-dominated regime from a regime dominated by the particle's moment of inertia. A shift from one regime to the other also marks a gradual transition in the wake-shedding pattern: from the classical $2S$~(2-Single) vortex mode to a $2P$~(2-Pairs) vortex mode. Thus, auto-rotation can have a significant influence on the trajectories and wakes of freely rising isotropic bodies."
What is the relationship between the generalized Hurst exponent and the segment size in multifractal analysis?,91789eec,"[{'abstract': '  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy\nwere implanted into SiO2 matrix with Different fluences. The implanted samples\nwere annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of\nimplanted as well as annealed samples were captured by the atomic force\nmicroscopy (AFM). Two dimension (2D) multifractal detrended fluctuation\nanalysis (MFDFA) based on the partition function approach has been used to\nstudy the surfaces of ion implanted and annealed samples. The partition\nfunction is used to calculate generalized Hurst exponent with the segment size.\nMoreover, it is seen that the generalized Hurst exponents vary nonlinearly with\nthe moment, thereby exhibiting the multifractal nature. The multifractality of\nsurface is pronounced after annealing for the surface implanted with fluence\n7.5X1016 ions/cm^2.\n', 'title': 'Morphological characterization of Ge ion implanted SiO2 matrix using multifractal technique'}]","Title: Morphological characterization of Ge ion implanted SiO2 matrix using multifractal technique

Abstract: 200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy were implanted into SiO2 matrix with Different fluences. The implanted samples were annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of implanted as well as annealed samples were captured by the atomic force microscopy (AFM). Two dimension (2D) multifractal detrended fluctuation analysis (MFDFA) based on the partition function approach has been used to study the surfaces of ion implanted and annealed samples. The partition function is used to calculate generalized Hurst exponent with the segment size. Moreover, it is seen that the generalized Hurst exponents vary nonlinearly with the moment, thereby exhibiting the multifractal nature. The multifractality of surface is pronounced after annealing for the surface implanted with fluence 7.5X1016 ions/cm^2.

The relationship between the generalized Hurst exponent and the segment size in multifractal analysis is that the generalized Hurst exponents vary nonlinearly with the moment, which is calculated using the partition function approach with the segment size."
What type of competition was the proposed method for source localization evaluated on?,f312bd54,"[{'abstract': '  In this work we apply Amplitude Modulation Spectrum (AMS) features to the\nsource localization problem. Our approach computes 36 bilateral features for 2s\nlong signal segments and estimates the azimuthal directions of a sound source\nthrough a binaurally trained classifier. This directional information of a\nsound source could be e.g. used to steer the beamformer in a hearing aid to the\nsource of interest in order to increase the SNR. We evaluated our approach on\nthe development set of the IEEE-AASP Challenge on sound source localization and\ntracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline\napproach. Additionally, our approach is computationally less complex.\n', 'title': 'Binaural Source Localization based on Modulation-Domain Features and Decision Pooling'}]","Title: Binaural Source Localization based on Modulation-Domain Features and Decision Pooling

Abstract: In this work we apply Amplitude Modulation Spectrum (AMS) features to the source localization problem. Our approach computes 36 bilateral features for 2s long signal segments and estimates the azimuthal directions of a sound source through a binaurally trained classifier. This directional information of a sound source could be e.g. used to steer the beamformer in a hearing aid to the source of interest in order to increase the SNR. We evaluated our approach on the development set of the IEEE-AASP Challenge on sound source localization and tracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline approach. Additionally, our approach is computationally less complex."
How do multi-task neural network architectures improve obstacle detection and depth estimation for MAVs?,40913b42,"[{'abstract': '  In this work, we propose an end-to-end deep architecture that jointly learns\nto detect obstacles and estimate their depth for MAV flight applications. Most\nof the existing approaches either rely on Visual SLAM systems or on depth\nestimation models to build 3D maps and detect obstacles. However, for the task\nof avoiding obstacles this level of complexity is not required. Recent works\nhave proposed multi task architectures to both perform scene understanding and\ndepth estimation. We follow their track and propose a specific architecture to\njointly estimate depth and obstacles, without the need to compute a global map,\nbut maintaining compatibility with a global SLAM system if needed. The network\narchitecture is devised to exploit the joint information of the obstacle\ndetection task, that produces more reliable bounding boxes, with the depth\nestimation one, increasing the robustness of both to scenario changes. We call\nthis architecture J-MOD$^{2}$. We test the effectiveness of our approach with\nexperiments on sequences with different appearance and focal lengths and\ncompare it to SotA multi task methods that jointly perform semantic\nsegmentation and depth estimation. In addition, we show the integration in a\nfull system using a set of simulated navigation experiments where a MAV\nexplores an unknown scenario and plans safe trajectories by using our detection\nmodel.\n', 'title': 'J-MOD$^{2}$: Joint Monocular Obstacle Detection and Depth Estimation'}]","Title: J-MOD$^{2}$: Joint Monocular Obstacle Detection and Depth Estimation

Abstract:   In this work, we propose an end-to-end deep architecture that jointly learns
to detect obstacles and estimate their depth for MAV flight applications. Most
of the existing approaches either rely on Visual SLAM systems or on depth
estimation models to build 3D maps and detect obstacles. However, for the task
of avoiding obstacles this level of complexity is not required. Recent works
have proposed multi task architectures to both perform scene understanding and
depth estimation. We follow their track and propose a specific architecture to
jointly estimate depth and obstacles, without the need to compute a global map,
but maintaining compatibility with a global SLAM system if needed. The network
architecture is devised to exploit the joint information of the obstacle
detection task, that produces more reliable bounding boxes, with the depth
estimation one, increasing the robustness of both to scenario changes. We call
this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
experiments on sequences with different appearance and focal lengths and
compare it to SotA multi task methods that jointly perform semantic
segmentation and depth estimation. In addition, we show the integration in a
full system using a set of simulated navigation experiments where a MAV
explores an unknown scenario and plans safe trajectories by using our detection
model."
What are the reported results for arousal and valence recognition using this method?,f976f7f6,"[{'abstract': ""  This paper studies the emotion recognition from musical tracks in the\n2-dimensional valence-arousal (V-A) emotional space. We propose a method based\non convolutional (CNN) and recurrent neural networks (RNN), having\nsignificantly fewer parameters compared with the state-of-the-art method for\nthe same task. We utilize one CNN layer followed by two branches of RNNs\ntrained separately for arousal and valence. The method was evaluated using the\n'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for\narousal and 0.268 for valence, which is the best result reported on this\ndataset.\n"", 'title': 'Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition'}]","The question is about the reported results for arousal and valence recognition using a specific method.

Title: Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition

Abstract: This paper studies the emotion recognition from musical tracks in the 2-dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with the state-of-the-art method for the same task. We utilize one CNN layer followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the 'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset."
"How can a functor between categories preserve cofibrations, fibrations, and weak equivalences in the context of model structures?",db338d55,"[{'abstract': '  In this article, we develop a notion of Quillen bifibration which combines\nthe two notions of Grothendieck bifibration and of Quillen model structure. In\nparticular, given a bifibration $p:\\mathcal E\\to\\mathcal B$, we describe when a\nfamily of model structures on the fibers $\\mathcal E_A$ and on the basis\ncategory $\\mathcal B$ combines into a model structure on the total category\n$\\mathcal E$, such that the functor $p$ preserves cofibrations, fibrations and\nweak equivalences. Using this Grothendieck construction for model structures,\nwe revisit the traditional definition of Reedy model structures, and possible\ngeneralizations, and exhibit their bifibrational nature.\n', 'title': 'On bifibrations of model categories'}]","I think I can help you find the answer to your question!

Title: On bifibrations of model categories

Abstract:   In this article, we develop a notion of Quillen bifibration which combines
the two notions of Grothendieck bifibration and of Quillen model structure. In
particular, given a bifibration $p:\mathcal E\to\mathcal B$, we describe when a
family of model structures on the fibers $\mathcal E_A$ and on the basis
category $\mathcal B$ combines into a model structure on the total category
$\mathcal E$, such that the functor $p$ preserves cofibrations, fibrations and
weak equivalences. Using this Grothendieck construction for model structures,
we revisit the traditional definition of Reedy model structures, and possible
generalizations, and exhibit their bifibrational nature."
How is the standard Eppink and Parker design modified for this new spectrometer?,dd6f5818,"[{'abstract': '  We present a compact design for a velocity-map imaging spectrometer for\nenergetic electrons and ions. The standard geometry by Eppink and Parker [A. T.\nJ. B. Eppink and D. H. Parker, Rev. Sci. Instrum. 68, 3477 (1997)] is augmented\nby just two extended electrodes so as to realize an additional einzel lens. In\nthis way, for a maximum electrode voltage of 7 kV we experimentally demonstrate\nimaging of electrons with energies up to 65 eV. Simulations show that energy\nacceptances of <270 and <1,200 eV with an energy resolution of dE / E <5% are\nachievable for electrode voltages of <20 kV when using diameters of the\nposition-sensitive detector of 42 and 78 mm, respectively.\n', 'title': 'A compact design for velocity-map imaging energetic electrons and ions'}]","Title: A compact design for velocity-map imaging energetic electrons and ions

Abstract:   We present a compact design for a velocity-map imaging spectrometer for
energetic electrons and ions. The standard geometry by Eppink and Parker [A. T.
J. B. Eppink and D. H. Parker, Rev. Sci. Instrum. 68, 3477 (1997)] is augmented
by just two extended electrodes so as to realize an additional einzel lens. In
this way, for a maximum electrode voltage of 7 kV we experimentally demonstrate
imaging of electrons with energies up to 65 eV. Simulations show that energy
acceptances of <270 and <1,200 eV with an energy resolution of dE / E <5% are
achievable for electrode voltages of <20 kV when using diameters of the
position-sensitive detector of 42 and 78 mm, respectively.

The standard Eppink and Parker design is modified by adding two extended electrodes to create an additional einzel lens."
What is the optimal learning strategy for causal structure learning with a limited number of experiments?,4849fb2f,"[{'abstract': '  We study the problem of causal structure learning over a set of random\nvariables when the experimenter is allowed to perform at most $M$ experiments\nin a non-adaptive manner. We consider the optimal learning strategy in terms of\nminimizing the portions of the structure that remains unknown given the limited\nnumber of experiments in both Bayesian and minimax setting. We characterize the\ntheoretical optimal solution and propose an algorithm, which designs the\nexperiments efficiently in terms of time complexity. We show that for bounded\ndegree graphs, in the minimax case and in the Bayesian case with uniform\npriors, our proposed algorithm is a $\\rho$-approximation algorithm, where\n$\\rho$ is independent of the order of the underlying graph. Simulations on both\nsynthetic and real data show that the performance of our algorithm is very\nclose to the optimal solution.\n', 'title': 'Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments'}]","Title: Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments

Abstract: We study the problem of causal structure learning over a set of random variables when the experimenter is allowed to perform at most $M$ experiments in a non-adaptive manner. We consider the optimal learning strategy in terms of minimizing the portions of the structure that remains unknown given the limited number of experiments in both Bayesian and minimax setting. We characterize the theoretical optimal solution and propose an algorithm, which designs the experiments efficiently in terms of time complexity. We show that for bounded degree graphs, in the minimax case and in the Bayesian case with uniform priors, our proposed algorithm is a $\rho$-approximation algorithm, where $\rho$ is independent of the order of the underlying graph. Simulations on both synthetic and real data show that the performance of our algorithm is very close to the optimal solution."
Can existing designs effectively protect against a broad range of seismic frequencies?,3ac3e2dd,"[{'abstract': '  The regularity of earthquakes, their destructive power, and the nuisance of\nground vibration in urban environments, all motivate designs of defence\nstructures to lessen the impact of seismic and ground vibration waves on\nbuildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and\nup to a few tens of Hz for vibrations generated by human activities, cause a\nlarge amount of damage, or inconvenience, depending on the geological\nconditions they can travel considerable distances and may match the resonant\nfundamental frequency of buildings. The ultimate aim of any seismic\nmetamaterial, or any other seismic shield, is to protect over this entire range\nof frequencies, the long wavelengths involved, and low frequency, have meant\nthis has been unachievable to date.\nElastic flexural waves, applicable in the mechanical vibrations of thin\nelastic plates, can be designed to have a broad zero-frequency stop-band using\na periodic array of very small clamped circles. Inspired by this experimental\nand theoretical observation, all be it in a situation far removed from seismic\nwaves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)\nand body (pressure P and shear S) wave reflectors at very large wavelengths in\nstructured soils modelled as a fully elastic layer periodically clamped to\nbedrock.\nWe identify zero frequency stop-bands that only exist in the limit of columns\nof concrete clamped at their base to the bedrock. In a realistic configuration\nof a sedimentary basin 15 meters deep we observe a zero frequency stop-band\ncovering a broad frequency range of $0$ to $30$ Hz.\n', 'title': 'Clamped seismic metamaterials: Ultra-low broad frequency stop-bands'}]","Title: Clamped seismic metamaterials: Ultra-low broad frequency stop-bands

Abstract: The regularity of earthquakes, their destructive power, and the nuisance of ground vibration in urban environments, all motivate designs of defence structures to lessen the impact of seismic and ground vibration waves on buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and up to a few tens of Hz for vibrations generated by human activities, cause a large amount of damage, or inconvenience, depending on the geological conditions they can travel considerable distances and may match the resonant fundamental frequency of buildings. The ultimate aim of any seismic metamaterial, or any other seismic shield, is to protect over this entire range of frequencies, the long wavelengths involved, and low frequency, have meant this has been unachievable to date.
Elastic flexural waves, applicable in the mechanical vibrations of thin elastic plates, can be designed to have a broad zero-frequency stop-band using a periodic array of very small clamped circles. Inspired by this experimental and theoretical observation, all be it in a situation far removed from seismic waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh) and body (pressure P and shear S) wave reflectors at very large wavelengths in structured soils modelled as a fully elastic layer periodically clamped to bedrock.
We identify zero frequency stop-bands that only exist in the limit of columns of concrete clamped at their base to the bedrock. In a realistic configuration of a sedimentary basin 15 meters deep we observe a zero frequency stop-band covering a broad frequency range of $0$ to $30$ Hz."
How does the correlation between gradients change as depth increases in standard feedforward networks?,bb1aa03c,"[{'abstract': '  A long-standing obstacle to progress in deep learning is the problem of\nvanishing and exploding gradients. Although, the problem has largely been\novercome via carefully constructed initializations and batch normalization,\narchitectures incorporating skip-connections such as highway and resnets\nperform much better than standard feedforward architectures despite well-chosen\ninitialization and batch normalization. In this paper, we identify the\nshattered gradients problem. Specifically, we show that the correlation between\ngradients in standard feedforward networks decays exponentially with depth\nresulting in gradients that resemble white noise whereas, in contrast, the\ngradients in architectures with skip-connections are far more resistant to\nshattering, decaying sublinearly. Detailed empirical evidence is presented in\nsupport of the analysis, on both fully-connected networks and convnets.\nFinally, we present a new ""looks linear"" (LL) initialization that prevents\nshattering, with preliminary experiments showing the new initialization allows\nto train very deep networks without the addition of skip-connections.\n', 'title': 'The Shattered Gradients Problem: If resnets are the answer, then what is the question?'}]","The answer to your question can be found in the following research paper:

Title: The Shattered Gradients Problem: If resnets are the answer, then what is the question?

Abstract:   A long-standing obstacle to progress in deep learning is the problem of
vanishing and exploding gradients. Although, the problem has largely been
overcome via carefully constructed initializations and batch normalization,
architectures incorporating skip-connections such as highway and resnets
perform much better than standard feedforward architectures despite well-chosen
initialization and batch normalization. In this paper, we identify the
shattered gradients problem. Specifically, we show that the correlation between
gradients in standard feedforward networks decays exponentially with depth
resulting in gradients that resemble white noise whereas, in contrast, the
gradients in architectures with skip-connections are far more resistant to
shattering, decaying sublinearly. Detailed empirical evidence is presented in
support of the analysis, on both fully-connected networks and convnets.
Finally, we present a new ""looks linear"" (LL) initialization that prevents
shattering, with preliminary experiments showing the new initialization allows
to train very deep networks without the addition of skip-connections."
How do agent-based models replicate existing literature on tax evasion?,761c47ce,"[{'abstract': '  This paper presents a simple agent-based model of an economic system,\npopulated by agents playing different games according to their different view\nabout social cohesion and tax payment. After a first set of simulations,\ncorrectly replicating results of existing literature, a wider analysis is\npresented in order to study the effects of a dynamic-adaptation rule, in which\ncitizens may possibly decide to modify their individual tax compliance\naccording to individual criteria, such as, the strength of their ethical\ncommitment, the satisfaction gained by consumption of the public good and the\nperceived opinion of neighbors. Results show the presence of thresholds levels\nin the composition of society - between taxpayers and evaders - which explain\nthe extent of damages deriving from tax evasion.\n', 'title': 'A Game of Tax Evasion: evidences from an agent-based model'}]","Based on the provided abstracts, I think I can help you find the answer.

Title: A Game of Tax Evasion: evidences from an agent-based model

Abstract: This paper presents a simple agent-based model of an economic system, populated by agents playing different games according to their different view about social cohesion and tax payment. After a first set of simulations, correctly replicating results of existing literature, a wider analysis is presented in order to study the effects of a dynamic-adaptation rule, in which citizens may possibly decide to modify their individual tax compliance according to individual criteria, such as, the strength of their ethical commitment, the satisfaction gained by consumption of the public good and the perceived opinion of neighbors. Results show the presence of thresholds levels in the composition of society - between taxpayers and evaders - which explain the extent of damages deriving from tax evasion.

According to this abstract, the agent-based model ""correctly replicates results of existing literature"" on tax evasion, but it doesn't provide further details on how it does so."
What are the different implementations of fRG discussed in the context of the Fermi-edge singularity?,a28c7967,"[{'abstract': '  We study the Fermi-edge singularity, describing the response of a degenerate\nelectron system to optical excitation, in the framework of the functional\nrenormalization group (fRG). Results for the (interband) particle-hole\nsusceptibility from various implementations of fRG (one- and two-\nparticle-irreducible, multi-channel Hubbard-Stratonovich, flowing\nsusceptibility) are compared to the summation of all leading logarithmic (log)\ndiagrams, achieved by a (first-order) solution of the parquet equations. For\nthe (zero-dimensional) special case of the X-ray-edge singularity, we show that\nthe leading log formula can be analytically reproduced in a consistent way from\na truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic\nstructure, we show that this derivation relies on fortuitous partial\ncancellations special to the form of and accuracy applied to the X-ray-edge\nsingularity and does not generalize.\n', 'title': 'Fermi-edge singularity and the functional renormalization group'}]","Title: Fermi-edge singularity and the functional renormalization group

Abstract: We study the Fermi-edge singularity, describing the response of a degenerate electron system to optical excitation, in the framework of the functional renormalization group (fRG). Results for the (interband) particle-hole susceptibility from various implementations of fRG (one- and two- particle-irreducible, multi-channel Hubbard-Stratonovich, flowing susceptibility) are compared to the summation of all leading logarithmic (log) diagrams, achieved by a (first-order) solution of the parquet equations. For the (zero-dimensional) special case of the X-ray-edge singularity, we show that the leading log formula can be analytically reproduced in a consistent way from a truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic structure, we show that this derivation relies on fortuitous partial cancellations special to the form of and accuracy applied to the X-ray-edge singularity and does not generalize.

The different implementations of fRG discussed in the context of the Fermi-edge singularity are:

* one-particle-irreducible
* two-particle-irreducible
* multi-channel Hubbard-Stratonovich
* flowing susceptibility"
Can you describe the directional dependence of scattering of Cooper pairs observed in a Nb/Bi1.95Sb0.05Se3 hybrid structure?,d28ef8ad,"[{'abstract': '  We report the proximity induced anomalous transport behavior in a Nb\nBi1.95Sb0.05Se3 heterostructure. Mechanically Exfoliated single crystal of\nBi1.95Sb0.05Se3 topological insulator (TI) is partially covered with a 100 nm\nthick Niobium superconductor using DC magnetron sputtering by shadow masking\ntechnique. The magnetotransport (MR) measurements have been performed\nsimultaneously on the TI sample with and without Nb top layer in the\ntemperature,T, range of 3 to 8 K, and a magnetic field B up to 15 T. MR on TI\nregion shows Subnikov de Haas oscillation at fields greater than 5 T. Anomalous\nlinear change in resistance is observed in the field range of negative 4T to\npositive 4T at which Nb is superconducting. At 0 T field, the temperature\ndependence of resistance on the Nb covered region revealed a superconducting\ntransition (TC) at 8.2 K, whereas TI area showed similar TC with the absence of\nzero resistance states due to the additional resistance from superconductor\n(SC) TI interface. Interestingly below the TC the R vs T measured on TI showed\nan enhancement in resistance for positive field and prominent fall in\nresistance for negative field direction. This indicates the directional\ndependent scattering of the Cooper pairs on the surface of the TI due to the\nsuperposition of spin singlet and triplet states in the superconductor and TI\nrespectively.\n', 'title': 'Anomalous transport properties in Nb/Bi1.95Sb0.05Se3 hybrid structure'}]","Title: Anomalous transport properties in Nb/Bi1.95Sb0.05Se3 hybrid structure

Abstract:   We report the proximity induced anomalous transport behavior in a Nb
Bi1.95Sb0.05Se3 heterostructure. Mechanically Exfoliated single crystal of
Bi1.95Sb0.05Se3 topological insulator (TI) is partially covered with a 100 nm
thick Niobium superconductor using DC magnetron sputtering by shadow masking
technique. The magnetotransport (MR) measurements have been performed
simultaneously on the TI sample with and without Nb top layer in the
temperature,T, range of 3 to 8 K, and a magnetic field B up to 15 T. MR on TI
region shows Subnikov de Haas oscillation at fields greater than 5 T. Anomalous
linear change in resistance is observed in the field range of negative 4T to
positive 4T at which Nb is superconducting. At 0 T field, the temperature
dependence of resistance on the Nb covered region revealed a superconducting
transition (TC) at 8.2 K, whereas TI area showed similar TC with the absence of
zero resistance states due to the additional resistance from superconductor
(SC) TI interface. Interestingly below the TC the R vs T measured on TI showed
an enhancement in resistance for positive field and prominent fall in
resistance for negative field direction. This indicates the directional
dependent scattering of the Cooper pairs on the surface of the TI due to the
superposition of spin singlet and triplet states in the superconductor and TI
respectively."
Can this approach handle text documents of different lengths?,b380be21,"[{'abstract': '  We present a clustering-based language model using word embeddings for text\nreadability prediction. Presumably, an Euclidean semantic space hypothesis\nholds true for word embeddings whose training is done by observing word\nco-occurrences. We argue that clustering with word embeddings in the metric\nspace should yield feature representations in a higher semantic space\nappropriate for text regression. Also, by representing features in terms of\nhistograms, our approach can naturally address documents of varying lengths. An\nempirical evaluation using the Common Core Standards corpus reveals that the\nfeatures formed on our clustering-based language model significantly improve\nthe previously known results for the same corpus in readability prediction. We\nalso evaluate the task of sentence matching based on semantic relatedness using\nthe Wiki-SimpleWiki corpus and find that our features lead to superior matching\nperformance.\n', 'title': 'Language Modeling by Clustering with Word Embeddings for Text Readability Assessment'}]","The answer to the question can be found in the abstract of the paper ""Language Modeling by Clustering with Word Embeddings for Text Readability Assessment"".

Title: Language Modeling by Clustering with Word Embeddings for Text Readability Assessment

Abstract: We present a clustering-based language model using word embeddings for text readability prediction. Presumably, an Euclidean semantic space hypothesis holds true for word embeddings whose training is done by observing word co-occurrences. We argue that clustering with word embeddings in the metric space should yield feature representations in a higher semantic space appropriate for text regression. Also, by representing features in terms of histograms, our approach can naturally address documents of varying lengths. An empirical evaluation using the Common Core Standards corpus reveals that the features formed on our clustering-based language model significantly improve the previously known results for the same corpus in readability prediction. We also evaluate the task of sentence matching based on semantic relatedness using the Wiki-SimpleWiki corpus and find that our features lead to superior matching performance."
Can you describe a scenario where doves become dominant over hawks in an N-person Hawk-Dove game? ,125b89e9,"[{'abstract': '  In the animal world, the competition between individuals belonging to\ndifferent species for a resource often requires the cooperation of several\nindividuals in groups. This paper proposes a generalization of the Hawk-Dove\nGame for an arbitrary number of agents: the N-person Hawk-Dove Game. In this\nmodel, doves exemplify the cooperative behavior without intraspecies conflict,\nwhile hawks represent the aggressive behavior. In the absence of hawks, doves\nshare the resource equally and avoid conflict, but having hawks around lead to\ndoves escaping without fighting. Conversely, hawks fight for the resource at\nthe cost of getting injured. Nevertheless, if doves are present in sufficient\nnumber to expel the hawks, they can aggregate to protect the resource, and thus\navoid being plundered by hawks. We derive and numerically solve an exact\nequation for the evolution of the system in both finite and infinite well-mixed\npopulations, finding the conditions for stable coexistence between both\nspecies. Furthermore, by varying the different parameters, we found a scenario\nof bifurcations that leads the system from dominating hawks and coexistence to\nbi-stability, multiple interior equilibria and dominating doves.\n', 'title': 'Evolutionary dynamics of N-person Hawk-Dove games'}]","I can help you with that!

Title: Evolutionary dynamics of N-person Hawk-Dove games

Abstract: In the animal world, the competition between individuals belonging to different species for a resource often requires the cooperation of several individuals in groups. This paper proposes a generalization of the Hawk-Dove Game for an arbitrary number of agents: the N-person Hawk-Dove Game. In this model, doves exemplify the cooperative behavior without intraspecies conflict, while hawks represent the aggressive behavior. In the absence of hawks, doves share the resource equally and avoid conflict, but having hawks around lead to doves escaping without fighting. Conversely, hawks fight for the resource at the cost of getting injured. Nevertheless, if doves are present in sufficient number to expel the hawks, they can aggregate to protect the resource, and thus avoid being plundered by hawks. We derive and numerically solve an exact equation for the evolution of the system in both finite and infinite well-mixed populations, finding the conditions for stable coexistence between both species. Furthermore, by varying the different parameters, we found a scenario of bifurcations that leads the system from dominating hawks and coexistence to bi-stability, multiple interior equilibria and dominating doves.

In this scenario, doves become dominant over hawks when they are present in sufficient numbers to expel the hawks and aggregate to protect the resource, thereby avoiding being plundered by hawks."
Where can I find a unified analysis of the convergence of the forward-backward algorithm with simplified proofs?,e8aebeac,"[{'abstract': '  We provide a comprehensive study of the convergence of forward-backward\nalgorithm under suitable geometric conditions leading to fast rates. We present\nseveral new results and collect in a unified view a variety of results\nscattered in the literature, often providing simplified proofs. Novel\ncontributions include the analysis of infinite dimensional convex minimization\nproblems, allowing the case where minimizers might not exist. Further, we\nanalyze the relation between different geometric conditions, and discuss novel\nconnections with a priori conditions in linear inverse problems, including\nsource conditions, restricted isometry properties and partial smoothness.\n', 'title': 'Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry'}]","I think I can help you with that!

Title: Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry

Abstract: We provide a comprehensive study of the convergence of forward-backward
algorithm under suitable geometric conditions leading to fast rates. We present
several new results and collect in a unified view a variety of results
scattered in the literature, often providing simplified proofs. Novel
contributions include the analysis of infinite dimensional convex minimization
problems, allowing the case where minimizers might not exist. Further, we
analyze the relation between different geometric conditions, and discuss novel
connections with a priori conditions in linear inverse problems, including
source conditions, restricted isometry properties and partial smoothness."
What characteristic is demonstrated by the generated music across extended time periods? ,35f285fc,"[{'abstract': '  Realistic music generation is a challenging task. When building generative\nmodels of music that are learnt from data, typically high-level representations\nsuch as scores or MIDI are used that abstract away the idiosyncrasies of a\nparticular performance. But these nuances are very important for our perception\nof musicality and realism, so in this work we embark on modelling music in the\nraw audio domain. It has been shown that autoregressive models excel at\ngenerating raw audio waveforms of speech, but when applied to music, we find\nthem biased towards capturing local signal structure at the expense of\nmodelling long-range correlations. This is problematic because music exhibits\nstructure at many different timescales. In this work, we explore autoregressive\ndiscrete autoencoders (ADAs) as a means to enable autoregressive models to\ncapture long-range correlations in waveforms. We find that they allow us to\nunconditionally generate piano music directly in the raw audio domain, which\nshows stylistic consistency across tens of seconds.\n', 'title': 'The challenge of realistic music generation: modelling raw audio at scale'}]","The characteristic demonstrated by the generated music across extended time periods is ""stylistic consistency"".

Here is the relevant paper:

Title: The challenge of realistic music generation: modelling raw audio at scale

Abstract: 
Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds."
What is the asymptotic distribution of the likelihood ratio test  used to test for zero variance in random effects of mixed effects models?,f33e795a,"[{'abstract': '  Mixed effects models are widely used to describe heterogeneity in a\npopulation. A crucial issue when adjusting such a model to data consists in\nidentifying fixed and random effects. From a statistical point of view, it\nremains to test the nullity of the variances of a given subset of random\neffects. Some authors have proposed to use the likelihood ratio test and have\nestablished its asymptotic distribution in some particular cases. Nevertheless,\nto the best of our knowledge, no general variance components testing procedure\nhas been fully investigated yet. In this paper, we study the likelihood ratio\ntest properties to test that the variances of a general subset of the random\neffects are equal to zero in both linear and nonlinear mixed effects model,\nextending the existing results. We prove that the asymptotic distribution of\nthe test is a chi-bar-square distribution, that is to say a mixture of\nchi-square distributions, and we identify the corresponding weights. We\nhighlight in particular that the limiting distribution depends on the presence\nof correlations between the random effects but not on the linear or nonlinear\nstructure of the mixed effects model. We illustrate the finite sample size\nproperties of the test procedure through simulation studies and apply the test\nprocedure to two real datasets of dental growth and of coucal growth.\n', 'title': 'Likelihood ratio test for variance components in nonlinear mixed effects models'}]","Title: Likelihood ratio test for variance components in nonlinear mixed effects models

Abstract:   Mixed effects models are widely used to describe heterogeneity in a
population. A crucial issue when adjusting such a model to data consists in
identifying fixed and random effects. From a statistical point of view, it
remains to test the nullity of the variances of a given subset of random
effects. Some authors have proposed to use the likelihood ratio test and have
established its asymptotic distribution in some particular cases. Nevertheless,
to the best of our knowledge, no general variance components testing procedure
has been fully investigated yet. In this paper, we study the likelihood ratio
test properties to test that the variances of a general subset of the random
effects are equal to zero in both linear and nonlinear mixed effects model,
extending the existing results. We prove that the asymptotic distribution of
the test is a chi-bar-square distribution, that is to say a mixture of
chi-square distributions, and we identify the corresponding weights. We
highlight in particular that the limiting distribution depends on the presence
of correlations between the random effects but not on the linear or nonlinear
structure of the mixed effects model. We illustrate the finite sample size
properties of the test procedure through simulation studies and apply the test
procedure to two real datasets of dental growth and of coucal growth."
What is the role of virtual machine placement and replication in achieving high availability for VNFs?,73146369,"[{'abstract': '  Virtual Network Functions as a Service (VNFaaS) is currently under attentive\nstudy by telecommunications and cloud stakeholders as a promising business and\ntechnical direction consisting of providing network functions as a service on a\ncloud (NFV Infrastructure), instead of delivering standalone network\nappliances, in order to provide higher scalability and reduce maintenance\ncosts. However, the functioning of such NFVI hosting the VNFs is fundamental\nfor all the services and applications running on top of it, forcing to\nguarantee a high availability level. Indeed the availability of an VNFaaS\nrelies on the failure rate of its single components, namely the servers, the\nvirtualization software, and the communication network. The proper assignment\nof the virtual machines implementing network functions to NFVI servers and\ntheir protection is essential to guarantee high availability. We model the High\nAvailability Virtual Network Function Placement (HA-VNFP) as the problem of\nfinding the best assignment of virtual machines to servers guaranteeing\nprotection by replication. We propose a probabilistic approach to measure the\nreal availability of a system and design both efficient and effective\nalgorithms that can be used by stakeholders for both online and offline\nplanning.\n', 'title': 'Securing Virtual Network Function Placement with High Availability Guarantees'}]","Title: Securing Virtual Network Function Placement with High Availability Guarantees

Abstract: Virtual Network Functions as a Service (VNFaaS) is currently under attentive study by telecommunications and cloud stakeholders as a promising business and technical direction consisting of providing network functions as a service on a cloud (NFV Infrastructure), instead of delivering standalone network appliances, in order to provide higher scalability and reduce maintenance costs. However, the functioning of such NFVI hosting the VNFs is fundamental for all the services and applications running on top of it, forcing to guarantee a high availability level. Indeed the availability of an VNFaaS relies on the failure rate of its single components, namely the servers, the virtualization software, and the communication network. The proper assignment of the virtual machines implementing network functions to NFVI servers and their protection is essential to guarantee high availability. We model the High Availability Virtual Network Function Placement (HA-VNFP) as the problem of finding the best assignment of virtual machines to servers guaranteeing protection by replication. We propose a probabilistic approach to measure the real availability of a system and design both efficient and effective algorithms that can be used by stakeholders for both online and offline planning."
What are the limitations of using first-order and higher-order Markov chains to predict user behavior?,1a4e3129,"[{'abstract': '  Users form information trails as they browse the web, checkin with a\ngeolocation, rate items, or consume media. A common problem is to predict what\na user might do next for the purposes of guidance, recommendation, or\nprefetching. First-order and higher-order Markov chains have been widely used\nmethods to study such sequences of data. First-order Markov chains are easy to\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\nin contrast, have too many parameters and suffer from overfitting the training\ndata. Fitting these parameters with regularization and smoothing only offers\nmild improvements. In this paper we propose the retrospective higher-order\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\nis a special case of a higher-order Markov chain where the transitions depend\nretrospectively on a single history state instead of an arbitrary combination\nof history states. There are two immediate computational advantages: the number\nof parameters is linear in the order of the Markov chain and the model can be\nfit to large state spaces. Furthermore, by providing a specific structure to\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\nutilizing history states without risks of overfitting the data. We demonstrate\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\nmethod on various real application datasets spanning geolocation data, review\nsequences, and business locations. The RHOMP model uniformly outperforms\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\nfactorizations in terms of prediction accuracy.\n', 'title': 'Retrospective Higher-Order Markov Processes for User Trails'}]","Title: Retrospective Higher-Order Markov Processes for User Trails

Abstract: Users form information trails as they browse the web, checkin with a geolocation, rate items, or consume media. A common problem is to predict what a user might do next for the purposes of guidance, recommendation, or prefetching. First-order and higher-order Markov chains have been widely used methods to study such sequences of data. First-order Markov chains are easy to estimate, but lack accuracy when history matters. Higher-order Markov chains, in contrast, have too many parameters and suffer from overfitting the training data. Fitting these parameters with regularization and smoothing only offers mild improvements."
How are minima and transition states represented in networks constructed from molecular dynamics simulations?,f639bb6d,"[{'abstract': '  A database of minima and transition states corresponds to a network where the\nminima represent nodes and the transition states correspond to edges between\nthe pairs of minima they connect via steepest-descent paths. Here we construct\nnetworks for small clusters bound by the Morse potential for a selection of\nphysically relevant parameters, in two and three dimensions. The properties of\nthese unweighted and undirected networks are analysed to examine two features:\nwhether they are small-world, where the shortest path between nodes involves\nonly a small number or edges; and whether they are scale-free, having a degree\ndistribution that follows a power law. Small-world character is present, but\nstatistical tests show that a power law is not a good fit, so the networks are\nnot scale-free. These results for clusters are compared with the corresponding\nproperties for the molecular and atomic structural glass formers\northo-terphenyl and binary Lennard-Jones. These glassy systems do not show\nsmall-world properties, suggesting that such behaviour is linked to the\nstructure-seeking landscapes of the Morse clusters.\n', 'title': 'Properties of Kinetic Transition Networks for Atomic Clusters and Glassy Solids'}]","I think I found the answer you're looking for!

Title: Properties of Kinetic Transition Networks for Atomic Clusters and Glassy Solids

Abstract: A database of minima and transition states corresponds to a network where the minima represent nodes and the transition states correspond to edges between the pairs of minima they connect via steepest-descent paths. Here we construct networks for small clusters bound by the Morse potential for a selection of physically relevant parameters, in two and three dimensions. The properties of these unweighted and undirected networks are analysed to examine two features: whether they are small-world, where the shortest path between nodes involves only a small number or edges; and whether they are scale-free, having a degree distribution that follows a power law. Small-world character is present, but statistical tests show that a power law is not a good fit, so the networks are not scale-free. These results for clusters are compared with the corresponding properties for the molecular and atomic structural glass formers ortho-terphenyl and binary Lennard-Jones. These glassy systems do not show small-world properties, suggesting that such behaviour is linked to the structure-seeking landscapes of the Morse clusters.

The answer to your question is that minima are represented as nodes in the network, and transition states are represented as edges between pairs of minima that they connect via steepest-descent paths."
What happens to a Macdonald refined topological vertex in the limit where x approaches y?,6d5c13f7,"[{'abstract': ""  We consider the refined topological vertex of Iqbal et al, as a function of\ntwo parameters (x, y), and deform it by introducing Macdonald parameters (q,\nt), as in the work of Vuletic on plane partitions, to obtain 'a Macdonald\nrefined topological vertex'. In the limit q -> t, we recover the refined\ntopological vertex of Iqbal et al. In the limit x -> y, we obtain a\nqt-deformation of the topological vertex of Aganagic et al. Copies of the\nvertex can be glued to obtain qt-deformed 5D instanton partition functions that\nhave well-defined 4D limits and, for generic values of (q, t), contain\ninfinite-towers of poles for every pole in the limit q -> t.\n"", 'title': 'A Macdonald refined topological vertex'}]","I think I can help you with that!

Title: A Macdonald refined topological vertex

Abstract: We consider the refined topological vertex of Iqbal et al, as a function of two parameters (x, y), and deform it by introducing Macdonald parameters (q, t), as in the work of Vuletic on plane partitions, to obtain 'a Macdonald refined topological vertex'. In the limit q -> t, we recover the refined topological vertex of Iqbal et al. In the limit x -> y, we obtain a qt-deformation of the topological vertex of Aganagic et al. Copies of the vertex can be glued to obtain qt-deformed 5D instanton partition functions that have well-defined 4D limits and, for generic values of (q, t), contain infinite-towers of poles for every pole in the limit q -> t.

According to the abstract, in the limit where x approaches y, we obtain a qt-deformation of the topological vertex of Aganagic et al."
What type of deep learning model is used to guide the MCTS algorithm?,88e873a5,"[{'abstract': '  Retrosynthesis is a technique to plan the chemical synthesis of organic\nmolecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a\nsearch tree is built by analysing molecules recursively and dissecting them\ninto simpler molecular building blocks until one obtains a set of known\nbuilding blocks. The search space is intractably large, and it is difficult to\ndetermine the value of retrosynthetic positions. Here, we propose to model\nretrosynthesis as a Markov Decision Process. In combination with a Deep Neural\nNetwork policy learned from essentially the complete published knowledge of\nchemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In\nexploratory studies, we demonstrate that MCTS with neural network policies\noutperforms the traditionally used best-first search with hand-coded\nheuristics.\n', 'title': 'Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies'}]","Based on the provided context, the answer is:

Title: Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies

Abstract: Retrosynthesis is a technique to plan the chemical synthesis of organic molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a search tree is built by analysing molecules recursively and dissecting them into simpler molecular building blocks until one obtains a set of known building blocks. The search space is intractably large, and it is difficult to determine the value of retrosynthetic positions. Here, we propose to model retrosynthesis as a Markov Decision Process. In combination with a Deep Neural Network policy learned from essentially the complete published knowledge of chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In exploratory studies, we demonstrate that MCTS with neural network policies outperforms the traditionally used best-first search with hand-coded heuristics.

The deep learning model used to guide the MCTS algorithm is a Deep Neural Network policy."
How do the network properties of Morse clusters compare to those of glassy systems like ortho-terphenyl and binary Lennard-Jones?,f639bb6d,"[{'abstract': '  A database of minima and transition states corresponds to a network where the\nminima represent nodes and the transition states correspond to edges between\nthe pairs of minima they connect via steepest-descent paths. Here we construct\nnetworks for small clusters bound by the Morse potential for a selection of\nphysically relevant parameters, in two and three dimensions. The properties of\nthese unweighted and undirected networks are analysed to examine two features:\nwhether they are small-world, where the shortest path between nodes involves\nonly a small number or edges; and whether they are scale-free, having a degree\ndistribution that follows a power law. Small-world character is present, but\nstatistical tests show that a power law is not a good fit, so the networks are\nnot scale-free. These results for clusters are compared with the corresponding\nproperties for the molecular and atomic structural glass formers\northo-terphenyl and binary Lennard-Jones. These glassy systems do not show\nsmall-world properties, suggesting that such behaviour is linked to the\nstructure-seeking landscapes of the Morse clusters.\n', 'title': 'Properties of Kinetic Transition Networks for Atomic Clusters and Glassy Solids'}]","Title: Properties of Kinetic Transition Networks for Atomic Clusters and Glassy Solids

Abstract: A database of minima and transition states corresponds to a network where the minima represent nodes and the transition states correspond to edges between the pairs of minima they connect via steepest-descent paths. Here we construct networks for small clusters bound by the Morse potential for a selection of physically relevant parameters, in two and three dimensions. The properties of these unweighted and undirected networks are analysed to examine two features: whether they are small-world, where the shortest path between nodes involves only a small number or edges; and whether they are scale-free, having a degree distribution that follows a power law. Small-world character is present, but statistical tests show that a power law is not a good fit, so the networks are not scale-free. These results for clusters are compared with the corresponding properties for the molecular and atomic structural glass formers ortho-terphenyl and binary Lennard-Jones. These glassy systems do not show small-world properties, suggesting that such behaviour is linked to the structure-seeking landscapes of the Morse clusters."
"In what way does tuning parameters, particularly weight threshold (P) and number of iterations (I), improve the robustness of AdaBoostM1?",7409da1f,"[{'abstract': '  Machine Learning focuses on the construction and study of systems that can\nlearn from data. This is connected with the classification problem, which\nusually is what Machine Learning algorithms are designed to solve. When a\nmachine learning method is used by people with no special expertise in machine\nlearning, it is important that the method be robust in classification, in the\nsense that reasonable performance is obtained with minimal tuning of the\nproblem at hand. Algorithms are evaluated based on how robust they can classify\nthe given data. In this paper, we propose a quantifiable measure of robustness,\nand describe a particular learning method that is robust according to this\nmeasure in the context of classification problem. We proposed Adaptive Boosting\n(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold\n(P) and number of iterations (I) for boosting algorithm. To benchmark the\nperformance, we used the baseline classifier, AdaBoostM1 with Decision Stump as\nbase learner without tuning parameters. By tuning parameters and using J48 as\nbase learner, we are able to reduce the overall average error rate ratio\n(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2\nfor evaluation sets of data.\n', 'title': 'Enhanced version of AdaBoostM1 with J48 Tree learning method'}]","Title: Enhanced version of AdaBoostM1 with J48 Tree learning method

Abstract: Machine Learning focuses on the construction and study of systems that can
learn from data. This is connected with the classification problem, which
usually is what Machine Learning algorithms are designed to solve. When a
machine learning method is used by people with no special expertise in machine
learning, it is important that the method be robust in classification, in the
sense that reasonable performance is obtained with minimal tuning of the
problem at hand. Algorithms are evaluated based on how robust they can classify
the given data. In this paper, we propose a quantifiable measure of robustness,
and describe a particular learning method that is robust according to this
measure in the context of classification problem. We proposed Adaptive Boosting
(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold
(P) and number of iterations (I) for boosting algorithm. To benchmark the
performance, we used the baseline classifier, AdaBoostM1 with Decision Stump as
base learner without tuning parameters. By tuning parameters and using J48 as
base learner, we are able to reduce the overall average error rate ratio
(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2
for evaluation sets of data.

The answer can be found in the abstract of the first paper."
What methods were used to study gravitational wave production in post-inflationary preheating scenarios?,d42d4d86,"[{'abstract': '  Parametric resonance is among the most efficient phenomena generating\ngravitational waves (GWs) in the early Universe. The dynamics of parametric\nresonance, and hence of the GWs, depend exclusively on the resonance parameter\n$q$. The latter is determined by the properties of each scenario: the initial\namplitude and potential curvature of the oscillating field, and its coupling to\nother species. Previous works have only studied the GW production for fixed\nvalue(s) of $q$. We present an analytical derivation of the GW amplitude\ndependence on $q$, valid for any scenario, which we confront against numerical\nresults. By running lattice simulations in an expanding grid, we study for a\nwide range of $q$ values, the production of GWs in post-inflationary preheating\nscenarios driven by parametric resonance. We present simple fits for the final\namplitude and position of the local maxima in the GW spectrum. Our\nparametrization allows to predict the location and amplitude of the GW\nbackground today, for an arbitrary $q$. The GW signal can be rather large, as\n$h^2\\Omega_{\\rm GW}(f_p) \\lesssim 10^{-11}$, but it is always peaked at high\nfrequencies $f_p \\gtrsim 10^{7}$ Hz. We also discuss the case of\nspectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,\nor the Standard Model Higgs.\n', 'title': 'Gravitational wave production from preheating: parameter dependence'}]","Title: Gravitational wave production from preheating: parameter dependence

Abstract: Parametric resonance is among the most efficient phenomena generating gravitational waves (GWs) in the early Universe. The dynamics of parametric resonance, and hence of the GWs, depend exclusively on the resonance parameter $q$. The latter is determined by the properties of each scenario: the initial amplitude and potential curvature of the oscillating field, and its coupling to other species. Previous works have only studied the GW production for fixed value(s) of $q$. We present an analytical derivation of the GW amplitude dependence on $q$, valid for any scenario, which we confront against numerical results. By running lattice simulations in an expanding grid, we study for a wide range of $q$ values, the production of GWs in post-inflationary preheating scenarios driven by parametric resonance. We present simple fits for the final amplitude and position of the local maxima in the GW spectrum. Our parametrization allows to predict the location and amplitude of the GW background today, for an arbitrary $q$. The GW signal can be rather large, as $h^2\Omega_{\rm GW}(f_p) \lesssim 10^{-11}$, but it is always peaked at high frequencies $f_p \gtrsim 10^{7}$ Hz. We also discuss the case of spectator-field scenarios, where the oscillatory field can be e.g.~a curvaton, or the Standard Model Higgs.

The methods used to study gravitational wave production in post-inflationary preheating scenarios are:

* Analytical derivation of the GW amplitude dependence on $q$
* Numerical results obtained by running lattice simulations in an expanding grid
* Study of GW production for a wide range of $q$ values"
Under what statistical assumption can local integrals of motion be reliably constructed?,c66f8c4d,"[{'abstract': '  Rare regions with weak disorder (Griffiths regions) have the potential to\nspoil localization. We describe a non-perturbative construction of local\nintegrals of motion (LIOMs) for a weakly interacting spin chain in one\ndimension, under a physically reasonable assumption on the statistics of\neigenvalues. We discuss ideas about the situation in higher dimensions, where\none can no longer ensure that interactions involving the Griffiths regions are\nmuch smaller than the typical energy-level spacing for such regions. We argue\nthat ergodicity is restored in dimension d > 1, although equilibration should\nbe extremely slow, similar to the dynamics of glasses.\n', 'title': 'Many-Body Localization: Stability and Instability'}]","Title: Many-Body Localization: Stability and Instability

Abstract: Rare regions with weak disorder (Griffiths regions) have the potential to spoil localization. We describe a non-perturbative construction of local integrals of motion (LIOMs) for a weakly interacting spin chain in one dimension, under a physically reasonable assumption on the statistics of eigenvalues. We discuss ideas about the situation in higher dimensions, where one can no longer ensure that interactions involving the Griffiths regions are much smaller than the typical energy-level spacing for such regions. We argue that ergodicity is restored in dimension d > 1, although equilibration should be extremely slow, similar to the dynamics of glasses."
What type of strength model is developed based on pore collapse simulations?,b93fb0b4,"[{'abstract': '  Shock wave interactions with defects, such as pores, are known to play a key\nrole in the chemical initiation of energetic materials. The shock response of\nhexanitrostilbene is studied through a combination of large scale reactive\nmolecular dynamics and mesoscale hydrodynamic simulations. In order to extend\nour simulation capability at the mesoscale to include weak shock conditions (<\n6 GPa), atomistic simulations of pore collapse are used to define a strain rate\ndependent strength model. Comparing these simulation methods allows us to\nimpose physically-reasonable constraints on the mesoscale model parameters. In\ndoing so, we have been able to study shock waves interacting with pores as a\nfunction of this viscoplastic material response. We find that the pore collapse\nbehavior of weak shocks is characteristically different to that of strong\nshocks.\n', 'title': 'Multiscale Modeling of Shock Wave Localization in Porous Energetic Material'}]","Title: Multiscale Modeling of Shock Wave Localization in Porous Energetic Material

Abstract:   Shock wave interactions with defects, such as pores, are known to play a key
role in the chemical initiation of energetic materials. The shock response of
hexanitrostilbene is studied through a combination of large scale reactive
molecular dynamics and mesoscale hydrodynamic simulations. In order to extend
our simulation capability at the mesoscale to include weak shock conditions (<
6 GPa), atomistic simulations of pore collapse are used to define a strain rate
dependent strength model. Comparing these simulation methods allows us to
impose physically-reasonable constraints on the mesoscale model parameters. In
doing so, we have been able to study shock waves interacting with pores as a
function of this viscoplastic material response. We find that the pore collapse
behavior of weak shocks is characteristically different to that of strong
shocks.

The type of strength model developed is a strain rate dependent strength model."
What is the impact of optional laboratory courses on student performance in introductory science courses?,de76883b,"[{'abstract': '  Instructional labs are widely seen as a unique, albeit expensive, way to\nteach scientific content. We measured the effectiveness of introductory lab\ncourses at achieving this educational goal across nine different lab courses at\nthree very different institutions. These institutions and courses encompassed a\nbroad range of student populations and instructional styles. The nine courses\nstudied had two key things in common: the labs aimed to reinforce the content\npresented in lectures, and the labs were optional. By comparing the performance\nof students who did and did not take the labs (with careful normalization for\nselection effects), we found universally and precisely no added value to\nlearning from taking the labs as measured by course exam performance. This work\nshould motivate institutions and departments to reexamine the goals and conduct\nof their lab courses, given their resource-intensive nature. We show why these\nresults make sense when looking at the comparative mental processes of students\ninvolved in research and instructional labs, and offer alternative goals and\ninstructional approaches that would make lab courses more educationally\nvaluable.\n', 'title': 'Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content'}]","I think I can help you with that!

Title: Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content

Abstract: Instructional labs are widely seen as a unique, albeit expensive, way to teach scientific content. We measured the effectiveness of introductory lab courses at achieving this educational goal across nine different lab courses at three very different institutions. These institutions and courses encompassed a broad range of student populations and instructional styles. The nine courses studied had two key things in common: the labs aimed to reinforce the content presented in lectures, and the labs were optional. By comparing the performance of students who did and did not take the labs (with careful normalization for selection effects), we found universally and precisely no added value to learning from taking the labs as measured by course exam performance. This work should motivate institutions and departments to reexamine the goals and conduct of their lab courses, given their resource-intensive nature. We show why these results make sense when looking at the comparative mental processes of students involved in research and instructional labs, and offer alternative goals and instructional approaches that would make lab courses more educationally valuable."
"Besides the standard FLRW model, what other cosmological model should be considered when analyzing gravitational waves from merging binaries?",f52e95ca,"[{'abstract': '  Waveforms of gravitational waves provide information about a variety of\nparameters for the binary system merging. However, standard calculations have\nbeen performed assuming a FLRW universe with no perturbations. In reality this\nassumption should be dropped: we show that the inclusion of cosmological\nperturbations translates into corrections to the estimate of astrophysical\nparameters derived for the merging binary systems. We compute corrections to\nthe estimate of the luminosity distance due to velocity, volume, lensing and\ngravitational potential effects. Our results show that the amplitude of the\ncorrections will be negligible for current instruments, mildly important for\nexperiments like the planned DECIGO, and very important for future ones such as\nthe Big Bang Observer.\n', 'title': 'Cosmological perturbation effects on gravitational-wave luminosity distance estimates'}]","Title: Stellar streams as gravitational experiments I. The case of Sagittarius

Abstract: Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy offer a unique way to constrain the shape of galactic gravitational potentials. Such streams can be used as leaning tower gravitational experiments on galactic scales. The most well motivated modification of gravity proposed as an alternative to dark matter on galactic scales is Milgromian dynamics (MOND), and we present here the first ever N-body simulations of the dynamical evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a realistic baryonic mass model for the Milky Way, we attempt to reproduce the present-day spatial and kinematic structure of the Sagittarius dwarf and its immense tidal stream that wraps around the Milky Way. With very little freedom on the original structure of the progenitor, constrained by the total luminosity of the Sagittarius structure and by the observed stellar mass-size relation for isolated dwarf galaxies, we find reasonable agreement between our simulations and observations of this system. The observed stellar velocities in the leading arm can be reproduced if we include a massive hot gas corona around the Milky Way that is flattened in the direction of the principal plane of its satellites. This is the first time that tidal dissolution in MOND has been tested rigorously at these mass and acceleration scales."
What methods can be used to approximate Kolmogorov complexity for causal inference?,663a6d9c,"[{'abstract': '  The algorithmic Markov condition states that the most likely causal direction\nbetween two random variables X and Y can be identified as that direction with\nthe lowest Kolmogorov complexity. Due to the halting problem, however, this\nnotion is not computable.\nWe hence propose to do causal inference by stochastic complexity. That is, we\npropose to approximate Kolmogorov complexity via the Minimum Description Length\n(MDL) principle, using a score that is mini-max optimal with regard to the\nmodel class under consideration. This means that even in an adversarial\nsetting, such as when the true distribution is not in this class, we still\nobtain the optimal encoding for the data relative to the class.\nWe instantiate this framework, which we call CISC, for pairs of univariate\ndiscrete variables, using the class of multinomial distributions. Experiments\nshow that CISC is highly accurate on synthetic, benchmark, as well as\nreal-world data, outperforming the state of the art by a margin, and scales\nextremely well with regard to sample and domain sizes.\n', 'title': 'Causal Inference by Stochastic Complexity'}]","I think I can help you with that!

The method that can be used to approximate Kolmogorov complexity for causal inference is the Minimum Description Length (MDL) principle, as mentioned in the abstract of the paper:

Title: Causal Inference by Stochastic Complexity

Abstract: The algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identified as that direction with the lowest Kolmogorov complexity. Due to the halting problem, however, this notion is not computable. We hence propose to do causal inference by stochastic complexity. That is, we propose to approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle, using a score that is mini-max optimal with regard to the model class under consideration. This means that even in an adversarial setting, such as when the true distribution is not in this class, we still obtain the optimal encoding for the data relative to the class. We instantiate this framework, which we call CISC, for pairs of univariate discrete variables, using the class of multinomial distributions. Experiments show that CISC is highly accurate on synthetic, benchmark, as well as real-world data, outperforming the state of the art by a margin, and scales extremely well with regard to sample and domain sizes."
Is there an upper bound on the reduction in mutual information achievable by degrading a channel?,aa995c73,"[{'abstract': '  Consider a channel with a given input distribution. Our aim is to degrade it\nto a channel with at most L output letters. One such degradation method is the\nso called ""greedy-merge"" algorithm. We derive an upper bound on the reduction\nin mutual information between input and output. For fixed input alphabet size\nand variable L, the upper bound is within a constant factor of an\nalgorithm-independent lower bound. Thus, we establish that greedy-merge is\noptimal in the power-law sense.\n', 'title': 'Greedy-Merge Degrading has Optimal Power-Law'}]","Title: Greedy-Merge Degrading has Optimal Power-Law

Abstract:   Consider a channel with a given input distribution. Our aim is to degrade it to a channel with at most L output letters. One such degradation method is the so called ""greedy-merge"" algorithm. We derive an upper bound on the reduction in mutual information between input and output. For fixed input alphabet size and variable L, the upper bound is within a constant factor of an algorithm-independent lower bound. Thus, we establish that greedy-merge is optimal in the power-law sense."
What are methods for incorporating  extra information about features into a machine learning model?,83274a30,"[{'abstract': '  Very often features come with their own vectorial descriptions which provide\ndetailed information about their properties. We refer to these vectorial\ndescriptions as feature side-information. In the standard learning scenario,\ninput is represented as a vector of features and the feature side-information\nis most often ignored or used only for feature selection prior to model\nfitting. We believe that feature side-information which carries information\nabout features intrinsic property will help improve model prediction if used in\na proper way during learning process. In this paper, we propose a framework\nthat allows for the incorporation of the feature side-information during the\nlearning of very general model families to improve the prediction performance.\nWe control the structures of the learned models so that they reflect features\nsimilarities as these are defined on the basis of the side-information. We\nperform experiments on a number of benchmark datasets which show significant\npredictive performance gains, over a number of baselines, as a result of the\nexploitation of the side-information.\n', 'title': 'Regularising Non-linear Models Using Feature Side-information'}]","After reviewing the research papers, I found two methods for incorporating extra information about features into a machine learning model:

**Method 1: Using feature side-information**

Title: Regularising Non-linear Models Using Feature Side-information

Abstract: Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.

**Method 2: Using PULSE (feature discovery and learning method)**

Title: Learning a Predictive Model for Music Using PULSE

Abstract: Predictive models for music are studied by researchers of algorithmic composition, the cognitive sciences and machine learning. They serve as base models for composition, can simulate human prediction and provide a multidisciplinary application domain for learning algorithms. A particularly well established and constantly advanced subtask is the prediction of monophonic melodies. As melodies typically involve non-Markovian dependencies their prediction requires a capable learning algorithm. In this thesis, I apply the recent feature discovery and learning method PULSE to the realm of symbolic music modeling. PULSE is comprised of a feature generating operation and L1-regularized optimization. These are used to iteratively expand and cull the feature set, effectively exploring feature spaces that are too large for common feature selection approaches. I design a general Python framework for PULSE, propose task-optimized feature generating operations and various music-theoretically motivated features that are evaluated on a standard corpus of monophonic folk and chorale melodies. The proposed method significantly outperforms comparable state-of-the-art models. I further discuss the free parameters of the learning algorithm and analyze the feature composition of the learned models. The models learned by PULSE afford an easy inspection and are musicologically interpreted for the first time."
What is the context dependent measurement demonstrated by a coin flip in zero gravity?,3064a54c,"[{'abstract': '  Playing the game of heads or tails in zero gravity demonstrates that there\nexists a contextual ""measurement"" in classical mechanics. When the coin is\nflipped, its orientation is a continuous variable. However, the ""measurement""\nthat occurs when the coin is caught by clapping two hands together gives a\ndiscrete value (heads or tails) that depends on the context (orientation of the\nhands). It is then shown that there is a strong analogy with the spin\nmeasurement of the Stern-Gerlach experiment, and in particular with Stern and\nGerlach\'s sequential measurements. Finally, we clarify the analogy by recalling\nhow the de Broglie-Bohm interpretation simply explains the spin ""measurement"".\n', 'title': 'Heads or tails in zero gravity: an example of a classical contextual ""measurement""'}]","Title: Heads or tails in zero gravity: an example of a classical contextual ""measurement""

Abstract: Playing the game of heads or tails in zero gravity demonstrates that there exists a contextual ""measurement"" in classical mechanics. When the coin is flipped, its orientation is a continuous variable. However, the ""measurement"" that occurs when the coin is caught by clapping two hands together gives a discrete value (heads or tails) that depends on the context (orientation of the hands). It is then shown that there is a strong analogy with the spin measurement of the Stern-Gerlach experiment, and in particular with Stern and Gerlach's sequential measurements. Finally, we clarify the analogy by recalling how the de Broglie-Bohm interpretation simply explains the spin ""measurement""."
How is neutron scattering used to identify the presence of  a quantum spin liquid?,bd7de381,"[{'abstract': '  Conventional crystalline magnets are characterized by symmetry breaking and\nnormal modes of excitation called magnons with quantized angular momentum\n$\\hbar$. Neutron scattering correspondingly features extra magnetic Bragg\ndiffraction at low temperatures and dispersive inelastic scattering associated\nwith single magnon creation and annihilation. Exceptions are anticipated in\nso-called quantum spin liquids as exemplified by the one-dimensional spin-1/2\nchain which has no magnetic order and where magnons accordingly fractionalize\ninto spinons with angular momentum $\\hbar/2$. This is spectacularly revealed by\na continuum of inelastic neutron scattering associated with two-spinon\nprocesses and the absence of magnetic Bragg diffraction. Here, we report\nevidence for these same key features of a quantum spin liquid in the\nthree-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific\nheat and neutron scattering measurements, Monte Carlo simulations, and analytic\napproximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is\nan almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on\na pyrochlore lattice with weak connectivity and frustrated interactions.\nMagnetic Bragg diffraction is absent and 90\\% of the spectral weight forms a\ncontinuum of magnetic scattering not dissimilar to that of the spin-1/2 chain\nbut with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb\nphase. The residual entropy and diffuse elastic scattering points to an exotic\nstate of matter driven by frustration, quantum fluctuations and weak exchange\ndisorder.\n', 'title': 'Continuum of quantum fluctuations in a three-dimensional $S\\!=\\!1$ Heisenberg magnet'}]","Title: Continuum of quantum fluctuations in a three-dimensional $S\!=\!1$ Heisenberg magnet

Abstract: Conventional crystalline magnets are characterized by symmetry breaking and normal modes of excitation called magnons with quantized angular momentum $\hbar$. Neutron scattering correspondingly features extra magnetic Bragg diffraction at low temperatures and dispersive inelastic scattering associated with single magnon creation and annihilation. Exceptions are anticipated in so-called quantum spin liquids as exemplified by the one-dimensional spin-1/2 chain which has no magnetic order and where magnons accordingly fractionalize into spinons with angular momentum $\hbar/2$. This is spectacularly revealed by a continuum of inelastic neutron scattering associated with two-spinon processes and the absence of magnetic Bragg diffraction. Here, we report evidence for these same key features of a quantum spin liquid in the three-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific heat and neutron scattering measurements, Monte Carlo simulations, and analytic approximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is an almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on a pyrochlore lattice with weak connectivity and frustrated interactions. Magnetic Bragg diffraction is absent and 90% of the spectral weight forms a continuum of magnetic scattering not dissimilar to that of the spin-1/2 chain but with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb phase. The residual entropy and diffuse elastic scattering points to an exotic state of matter driven by frustration, quantum fluctuations and weak exchange disorder."
How can stochastic geometry be used to analyze the performance of multi-hop data transmission in massive IoT networks?,ff1d86de,"[{'abstract': '  Mission critical data dissemination in massive Internet of things (IoT)\nnetworks imposes constraints on the message transfer delay between devices. Due\nto low power and communication range of IoT devices, data is foreseen to be\nrelayed over multiple device-to-device (D2D) links before reaching the\ndestination. The coexistence of a massive number of IoT devices poses a\nchallenge in maximizing the successful transmission capacity of the overall\nnetwork alongside reducing the multi-hop transmission delay in order to support\nmission critical applications. There is a delicate interplay between the\ncarrier sensing threshold of the contention based medium access protocol and\nthe choice of packet forwarding strategy selected at each hop by the devices.\nThe fundamental problem in optimizing the performance of such networks is to\nbalance the tradeoff between conflicting performance objectives such as the\nspatial frequency reuse, transmission quality, and packet progress towards the\ndestination. In this paper, we use a stochastic geometry approach to quantify\nthe performance of multi-hop massive IoT networks in terms of the spatial\nfrequency reuse and the transmission quality under different packet forwarding\nschemes. We also develop a comprehensive performance metric that can be used to\noptimize the system to achieve the best performance. The results can be used to\nselect the best forwarding scheme and tune the carrier sensing threshold to\noptimize the performance of the network according to the delay constraints and\ntransmission quality requirements.\n', 'title': 'Optimizing Mission Critical Data Dissemination in Massive IoT Networks'}]","Title: Optimizing Mission Critical Data Dissemination in Massive IoT Networks

Abstract: Mission critical data dissemination in massive Internet of things (IoT) networks imposes constraints on the message transfer delay between devices. Due to low power and communication range of IoT devices, data is foreseen to be relayed over multiple device-to-device (D2D) links before reaching the destination. The coexistence of a massive number of IoT devices poses a challenge in maximizing the successful transmission capacity of the overall network alongside reducing the multi-hop transmission delay in order to support mission critical applications. There is a delicate interplay between the carrier sensing threshold of the contention based medium access protocol and the choice of packet forwarding strategy selected at each hop by the devices. The fundamental problem in optimizing the performance of such networks is to balance the tradeoff between conflicting performance objectives such as the spatial frequency reuse, transmission quality, and packet progress towards the destination. In this paper, we use a stochastic geometry approach to quantify the performance of multi-hop massive IoT networks in terms of the spatial frequency reuse and the transmission quality under different packet forwarding schemes. We also develop a comprehensive performance metric that can be used to optimize the system to achieve the best performance. The results can be used to select the best forwarding scheme and tune the carrier sensing threshold to optimize the performance of the network according to the delay constraints and transmission quality requirements."
How can statistical inference be used to compare and contrast the classical beta and the weighted-Gini beta?,81ff7fb0,"[{'abstract': ""  The `beta' is one of the key quantities in the capital asset pricing model\n(CAPM). In statistical language, the beta can be viewed as the slope of the\nregression line fitted to financial returns on the market against the returns\non the asset under consideration. The insurance counterpart of CAPM, called the\nweighted insurance pricing model (WIPM), gives rise to the so-called\nweighted-Gini beta. The aforementioned two betas may or may not coincide,\ndepending on the form of the underlying regression function, and this has\nprofound implications when designing portfolios and allocating risk capital. To\nfacilitate these tasks, in this paper we develop large-sample statistical\ninference results that, in a straightforward fashion, imply confidence\nintervals for, and hypothesis tests about, the equality of the two betas.\n"", 'title': 'Statistical foundations for assessing the difference between the classical and weighted-Gini betas'}]","Based on the provided context, I found a relevant research paper that addresses your question:

Title: Statistical foundations for assessing the difference between the classical and weighted-Gini betas

Abstract: The `beta' is one of the key quantities in the capital asset pricing model (CAPM). In statistical language, the beta can be viewed as the slope of the regression line fitted to financial returns on the market against the returns on the asset under consideration. The insurance counterpart of CAPM, called the weighted insurance pricing model (WIPM), gives rise to the so-called weighted-Gini beta. The aforementioned two betas may or may not coincide, depending on the form of the underlying regression function, and this has profound implications when designing portfolios and allocating risk capital. To facilitate these tasks, in this paper we develop large-sample statistical inference results that, in a straightforward fashion, imply confidence intervals for, and hypothesis tests about, the equality of the two betas."
What are the conditions for a family of model structures on fibers and a basis category to combine into a model structure on a total category?,db338d55,"[{'abstract': '  In this article, we develop a notion of Quillen bifibration which combines\nthe two notions of Grothendieck bifibration and of Quillen model structure. In\nparticular, given a bifibration $p:\\mathcal E\\to\\mathcal B$, we describe when a\nfamily of model structures on the fibers $\\mathcal E_A$ and on the basis\ncategory $\\mathcal B$ combines into a model structure on the total category\n$\\mathcal E$, such that the functor $p$ preserves cofibrations, fibrations and\nweak equivalences. Using this Grothendieck construction for model structures,\nwe revisit the traditional definition of Reedy model structures, and possible\ngeneralizations, and exhibit their bifibrational nature.\n', 'title': 'On bifibrations of model categories'}]","Title: On bifibrations of model categories

Abstract: In this article, we develop a notion of Quillen bifibration which combines the two notions of Grothendieck bifibration and of Quillen model structure. In particular, given a bifibration $p:\mathcal E\to\mathcal B$, we describe when a family of model structures on the fibers $\mathcal E_A$ and on the basis category $\mathcal B$ combines into a model structure on the total category $\mathcal E$, such that the functor $p$ preserves cofibrations, fibrations and weak equivalences. Using this Grothendieck construction for model structures, we revisit the traditional definition of Reedy model structures, and possible generalizations, and exhibit their bifibrational nature."
How was the performance of the proposed audio-based AU recognition framework evaluated?,23d32d82,"[{'abstract': '  Extensive efforts have been devoted to recognizing facial action units (AUs).\nHowever, it is still challenging to recognize AUs from spontaneous facial\ndisplays especially when they are accompanied with speech. Different from all\nprior work that utilized visual observations for facial AU recognition, this\npaper presents a novel approach that recognizes speech-related AUs exclusively\nfrom audio signals based on the fact that facial activities are highly\ncorrelated with voice during speech. Specifically, dynamic and physiological\nrelationships between AUs and phonemes are modeled through a continuous time\nBayesian network (CTBN); then AU recognition is performed by probabilistic\ninference via the CTBN model.\nA pilot audiovisual AU-coded database has been constructed to evaluate the\nproposed audio-based AU recognition framework. The database consists of a\n""clean"" subset with frontal and neutral faces and a challenging subset\ncollected with large head movements and occlusions. Experimental results on\nthis database show that the proposed CTBN model achieves promising recognition\nperformance for 7 speech-related AUs and outperforms the state-of-the-art\nvisual-based methods especially for those AUs that are activated at low\nintensities or ""hardly visible"" in the visual channel. Furthermore, the CTBN\nmodel yields more impressive recognition performance on the challenging subset,\nwhere the visual-based approaches suffer significantly.\n', 'title': 'Listen to Your Face: Inferring Facial Action Units from Audio Channel'}]","Title: Listen to Your Face: Inferring Facial Action Units from Audio Channel

Abstract: Extensive efforts have been devoted to recognizing facial action units (AUs). However, it is still challenging to recognize AUs from spontaneous facial displays especially when they are accompanied with speech. Different from all prior work that utilized visual observations for facial AU recognition, this paper presents a novel approach that recognizes speech-related AUs exclusively from audio signals based on the fact that facial activities are highly correlated with voice during speech. Specifically, dynamic and physiological relationships between AUs and phonemes are modeled through a continuous time Bayesian network (CTBN); then AU recognition is performed by probabilistic inference via the CTBN model.
A pilot audiovisual AU-coded database has been constructed to evaluate the proposed audio-based AU recognition framework. The database consists of a ""clean"" subset with frontal and neutral faces and a challenging subset collected with large head movements and occlusions. Experimental results on this database show that the proposed CTBN model achieves promising recognition performance for 7 speech-related AUs and outperforms the state-of-the-art visual-based methods especially for those AUs that are activated at low intensities or ""hardly visible"" in the visual channel. Furthermore, the CTBN model yields more impressive recognition performance on the challenging subset, where the visual-based approaches suffer significantly.

The performance of the proposed audio-based AU recognition framework was evaluated on a pilot audiovisual AU-coded database, which consists of a ""clean"" subset and a challenging subset."
What is a 'pre-basis' and how is it used in the construction of an exponent lattice basis?,5f1f4d73,"[{'abstract': '  Computing a basis for the exponent lattice of algebraic numbers is a basic\nproblem in the field of computational number theory with applications to many\nother areas. The main cost of a well-known algorithm\n\\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on\ncomputing the primitive element of the extended field generated by the given\nalgebraic numbers. When the extended field is of large degree, the problem\nseems intractable by the tool implementing the algorithm. In this paper, a\nspecial kind of exponent lattice basis is introduced. An important feature of\nthe basis is that it can be inductively constructed, which allows us to deal\nwith the given algebraic numbers one by one when computing the basis. Based on\nthis, an effective framework for constructing exponent lattice basis is\nproposed. Through computing a so-called pre-basis first and then solving some\nlinear Diophantine equations, the basis can be efficiently constructed. A new\ncertificate for multiplicative independence and some techniques for decreasing\ndegrees of algebraic numbers are provided to speed up the computation. The new\nalgorithm has been implemented with Mathematica and its effectiveness is\nverified by testing various examples. Moreover, the algorithm is applied to\nprogram verification for finding invariants of linear loops.\n', 'title': 'An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers'}]","Title: An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers

Abstract: Computing a basis for the exponent lattice of algebraic numbers is a basic
problem in the field of computational number theory with applications to many
other areas. The main cost of a well-known algorithm
\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
computing the primitive element of the extended field generated by the given
algebraic numbers. When the extended field is of large degree, the problem
seems intractable by the tool implementing the algorithm. In this paper, a
special kind of exponent lattice basis is introduced. An important feature of
the basis is that it can be inductively constructed, which allows us to deal
with the given algebraic numbers one by one when computing the basis. Based on
this, an effective framework for constructing exponent lattice basis is
proposed. Through computing a so-called pre-basis first and then solving some
linear Diophantine equations, the basis can be efficiently constructed. A new
certificate for multiplicative independence and some techniques for decreasing
degrees of algebraic numbers are provided to speed up the computation. The new
algorithm has been implemented with Mathematica and its effectiveness is
verified by testing various examples. Moreover, the algorithm is applied to
program verification for finding invariants of linear loops.

The answer can be found in the abstract of the paper ""An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers"". According to the abstract, a ""pre-basis"" is computed first, and then some linear Diophantine equations are solved to efficiently construct the exponent lattice basis."
How does a robot achieve fixtureless regrasping?,737f6d89,"[{'abstract': '  This paper presents a fixturing strategy for regrasping that does not require\na physical fixture. To regrasp an object in a gripper, a robot pushes the\nobject against external contact/s in the environment such that the external\ncontact keeps the object stationary while the fingers slide over the object. We\ncall this manipulation technique fixtureless fixturing. Exploiting the\nmechanics of pushing, we characterize a convex polyhedral set of pushes that\nresults in fixtureless fixturing. These pushes are robust against uncertainty\nin the object inertia, grasping force, and the friction at the contacts. We\npropose a sampling-based planner that uses the sets of robust pushes to rapidly\nbuild a tree of reachable grasps. A path in this tree is a pushing strategy,\npossibly involving pushes from different sides, to regrasp the object. We\ndemonstrate the experimental validity and robustness of the proposed\nmanipulation technique with different regrasp examples on a manipulation\nplatform. Such a fast and flexible regrasp planner facilitates versatile and\nflexible automation solutions.\n', 'title': 'Regrasping by Fixtureless Fixturing'}]","I think I can help you with that!

Title: Regrasping by Fixtureless Fixturing

Abstract: This paper presents a fixturing strategy for regrasping that does not require a physical fixture. To regrasp an object in a gripper, a robot pushes the object against external contact/s in the environment such that the external contact keeps the object stationary while the fingers slide over the object. We call this manipulation technique fixtureless fixturing. Exploiting the mechanics of pushing, we characterize a convex polyhedral set of pushes that results in fixtureless fixturing. These pushes are robust against uncertainty in the object inertia, grasping force, and the friction at the contacts. We propose a sampling-based planner that uses the sets of robust pushes to rapidly build a tree of reachable grasps. A path in this tree is a pushing strategy, possibly involving pushes from different sides, to regrasp the object. We demonstrate the experimental validity and robustness of the proposed manipulation technique with different regrasp examples on a manipulation platform. Such a fast and flexible regrasp planner facilitates versatile and flexible automation solutions."
How does the mass-sheet degeneracy affect measurements of the Hubble constant using time-delay lensing?,9dc57430,"[{'abstract': '  Cosmological parameter constraints from observations of time-delay lenses are\nbecoming increasingly precise. However, there may be significant bias and\nscatter in these measurements due to, among other things, the so-called\nmass-sheet degeneracy. To estimate these uncertainties, we analyze strong\nlenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet\ntransformation to the radial density profiles of lenses, and by selecting\nlenses near isothermality, we find that the bias on H0 can be reduced to 5%\nwith an intrinsic scatter of 10%, confirming previous results performed on a\ndifferent simulation data set. We further investigate whether combining lensing\nobservables with kinematic constraints helps to minimize this bias. We do not\ndetect any significant dependence of the bias on lens model parameters or\nobservational properties of the galaxy, but depending on the source--lens\nconfiguration, a bias may still exist. Cross lenses provide an accurate\nestimate of the Hubble constant, while fold (double) lenses tend to be biased\nlow (high). With kinematic constraints, double lenses show bias and intrinsic\nscatter of 6% and 10%, respectively, while quad lenses show bias and intrinsic\nscatter of 0.5% and 10%, respectively. For lenses with a reduced $\\chi^2 > 1$,\na power-law dependence of the $\\chi^2$ on the lens environment (number of\nnearby galaxies) is seen. Lastly, we model, in greater detail, the cases of two\ndouble lenses that are significantly biased. We are able to remove the bias,\nsuggesting that the remaining biases could also be reduced by carefully taking\ninto account additional sources of systematic uncertainty.\n', 'title': 'Reducing biases on $H_0$ measurements using strong lensing and galaxy dynamics: results from the EAGLE simulation'}]","I think I can help you with that!

Title: Reducing biases on $H_0$ measurements using strong lensing and galaxy dynamics: results from the EAGLE simulation

Abstract: Cosmological parameter constraints from observations of time-delay lenses are becoming increasingly precise. However, there may be significant bias and scatter in these measurements due to, among other things, the so-called mass-sheet degeneracy. To estimate these uncertainties, we analyze strong lenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet transformation to the radial density profiles of lenses, and by selecting lenses near isothermality, we find that the bias on H0 can be reduced to 5% with an intrinsic scatter of 10%, confirming previous results performed on a different simulation data set. We further investigate whether combining lensing observables with kinematic constraints helps to minimize this bias."
Does the presence of circumstellar gas in WD1145+017 potentially explain the observed change in color during transits? ,9bc0b2c8,"[{'abstract': ""  The first transiting planetesimal orbiting a white dwarf was recently\ndetected in K2 data of WD1145+017 and has been followed up intensively. The\nmultiple, long, and variable transits suggest the transiting objects are dust\nclouds, probably produced by a disintegrating asteroid. In addition, the system\ncontains circumstellar gas, evident by broad absorption lines, mostly in the\nu'-band, and a dust disc, indicated by an infrared excess. Here we present the\nfirst detection of a change in colour of WD1145+017 during transits, using\nsimultaneous multi-band fast-photometry ULTRACAM measurements over the\nu'g'r'i'-bands. The observations reveal what appears to be 'bluing' during\ntransits; transits are deeper in the redder bands, with a u'-r' colour\ndifference of up to ~-0.05 mag. We explore various possible explanations for\nthe bluing. 'Spectral' photometry obtained by integrating over bandpasses in\nthe spectroscopic data in- and out-of-transit, compared to the photometric\ndata, shows that the observed colour difference is most likely the result of\nreduced circumstellar absorption in the spectrum during transits. This\nindicates that the transiting objects and the gas share the same line-of-sight,\nand that the gas covers the white dwarf only partially, as would be expected if\nthe gas, the transiting debris, and the dust emitting the infrared excess, are\npart of the same general disc structure (although possibly at different radii).\nIn addition, we present the results of a week-long monitoring campaign of the\nsystem.\n"", 'title': ""Once in a blue moon: detection of 'bluing' during debris transits in the white dwarf WD1145+017""}]","Title: Once in a blue moon: detection of 'bluing' during debris transits in the white dwarf WD1145+017

Abstract: The first transiting planetesimal orbiting a white dwarf was recently detected in K2 data of WD1145+017 and has been followed up intensively. The multiple, long, and variable transits suggest the transiting objects are dust clouds, probably produced by a disintegrating asteroid. In addition, the system contains circumstellar gas, evident by broad absorption lines, mostly in the u'-band, and a dust disc, indicated by an infrared excess. Here we present the first detection of a change in colour of WD1145+017 during transits, using simultaneous multi-band fast-photometry ULTRACAM measurements over the u'g'r'i'-bands. The observations reveal what appears to be 'bluing' during transits; transits are deeper in the redder bands, with a u'-r' colour difference of up to ~-0.05 mag. We explore various possible explanations for the bluing. 'Spectral' photometry obtained by integrating over bandpasses in the spectroscopic data in- and out-of-transit, compared to the photometric data, shows that the observed colour difference is most likely the result of reduced circumstellar absorption in the spectrum during transits. This indicates that the transiting objects and the gas share the same line-of-sight, and that the gas covers the white dwarf only partially, as would be expected if the gas, the transiting debris, and the dust emitting the infrared excess, are part of the same general disc structure (although possibly at different radii). In addition, we present the results of a week-long monitoring campaign of the system."
For which values of  'n' does Hegarty conjecture the existence of a permutation that destroys all arithmetic progressions modulo 'n'?,e5636c57,"[{'abstract': '  Hegarty conjectured for $n\\neq 2, 3, 5, 7$ that $\\mathbb{Z}/n\\mathbb{Z}$ has\na permutation which destroys all arithmetic progressions mod $n$. For $n\\ge\nn_0$, Hegarty and Martinsson demonstrated that $\\mathbb{Z}/n\\mathbb{Z}$ has an\narithmetic-progression destroying permutation. However $n_0\\approx 1.4\\times\n10^{14}$ and thus resolving the conjecture in full remained out of reach of any\ncomputational techniques. However, this paper using constructions modeled after\nthose used by Elkies and Swaminathan for the case of $\\mathbb{Z}/p\\mathbb{Z}$\nwith $p$ being prime, establish the conjecture in full. Furthermore our results\ndo not rely on the fact that it suffices to study when $n<n_0$ and thus our\nresults completely independent of the proof given by Hegarty and Martinsson.\n', 'title': 'On A Conjecture Regarding Permutations Which Destroy Arithmetic Progressions'}]","The answer to your question can be found in the first abstract.

Title: On A Conjecture Regarding Permutations Which Destroy Arithmetic Progressions

Abstract: Hegarty conjectured for $n\neq 2, 3, 5, 7$ that $\mathbb{Z}/n\mathbb{Z}$ has a permutation which destroys all arithmetic progressions mod $n$. For $n\ge n_0$, Hegarty and Martinsson demonstrated that $\mathbb{Z}/n\mathbb{Z}$ has an arithmetic-progression destroying permutation. However $n_0\approx 1.4\times 10^{14}$ and thus resolving the conjecture in full remained out of reach of any computational techniques. However, this paper using constructions modeled after those used by Elkies and Swaminathan for the case of $\mathbb{Z}/p\mathbb{Z}$ with $p$ being prime, establish the conjecture in full. Furthermore our results do not rely on the fact that it suffices to study when $n<n_0$ and thus our results completely independent of the proof given by Hegarty and Martinsson.

So, according to the abstract, Hegarty conjectured the existence of a permutation that destroys all arithmetic progressions modulo 'n' for all values of 'n' except 2, 3, 5, and 7."
What is the relationship between the 'greedy-merge' algorithm and the lower bound on mutual information reduction?,aa995c73,"[{'abstract': '  Consider a channel with a given input distribution. Our aim is to degrade it\nto a channel with at most L output letters. One such degradation method is the\nso called ""greedy-merge"" algorithm. We derive an upper bound on the reduction\nin mutual information between input and output. For fixed input alphabet size\nand variable L, the upper bound is within a constant factor of an\nalgorithm-independent lower bound. Thus, we establish that greedy-merge is\noptimal in the power-law sense.\n', 'title': 'Greedy-Merge Degrading has Optimal Power-Law'}]","After reviewing the provided abstracts, I think I have found the relevant information.

Title: Greedy-Merge Degrading has Optimal Power-Law

Abstract: Consider a channel with a given input distribution. Our aim is to degrade it to a channel with at most L output letters. One such degradation method is the so called ""greedy-merge"" algorithm. We derive an upper bound on the reduction in mutual information between input and output. For fixed input alphabet size and variable L, the upper bound is within a constant factor of an algorithm-independent lower bound. Thus, we establish that greedy-merge is optimal in the power-law sense.

According to this abstract, the ""greedy-merge"" algorithm has an upper bound on the reduction in mutual information that is within a constant factor of an algorithm-independent lower bound. This means that the ""greedy-merge"" algorithm is optimal in the power-law sense, implying a strong relationship between the algorithm and the lower bound on mutual information reduction."
How is the stiffness of materials represented in a topology optimization framework for dynamic problems?,c0feab14,"[{'abstract': '  This paper presents a topology optimization framework for structural problems\nsubjected to transient loading. The mechanical model assumes a linear elastic\nisotropic material, infinitesimal strains, and a dynamic response. The\noptimization problem is solved using the gradient-based optimizer Method of\nMoving Asymptotes (MMA) with time-dependent sensitivities provided via the\nadjoint method. The stiffness of materials is interpolated using the Solid\nIsotropic Material with Penalization (SIMP) method and the Heaviside Projection\nMethod (HPM) is used to stabilize the problem numerically and improve the\nmanufacturability of the topology-optimized designs. Both static and dynamic\noptimization examples are considered here. The resulting optimized designs\ndemonstrate the ability of topology optimization to tailor the transient\nresponse of structures.\n', 'title': 'Topology optimization for transient response of structures subjected to dynamic loads'}]","Title: Topology optimization for transient response of structures subjected to dynamic loads

Abstract: This paper presents a topology optimization framework for structural problems subjected to transient loading. The mechanical model assumes a linear elastic isotropic material, infinitesimal strains, and a dynamic response. The optimization problem is solved using the gradient-based optimizer Method of Moving Asymptotes (MMA) with time-dependent sensitivities provided via the adjoint method. The stiffness of materials is interpolated using the Solid Isotropic Material with Penalization (SIMP) method and the Heaviside Projection Method (HPM) is used to stabilize the problem numerically and improve the manufacturability of the topology-optimized designs. Both static and dynamic optimization examples are considered here. The resulting optimized designs demonstrate the ability of topology optimization to tailor the transient response of structures.

The answer to the question is that the stiffness of materials is represented using the Solid Isotropic Material with Penalization (SIMP) method."
What is ionization-parameter mapping and how is it used to study galaxies,d0b10541,"[{'abstract': '  Identifying the mechanism by which high energy Lyman continuum (LyC) photons\nescaped from early galaxies is one of the most pressing questions in cosmic\nevolution. Haro 11 is the best known local LyC leaking galaxy, providing an\nimportant opportunity to test our understanding of LyC escape. The observed LyC\nemission in this galaxy presumably originates from one of the three bright,\nphotoionizing knots known as A, B, and C. It is known that Knot C has strong\nLy$\\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray\nsource, which may be a low-luminosity AGN. To clarify the LyC source, we carry\nout ionization-parameter mapping (IPM) by obtaining narrow-band imaging from\nthe Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved\nratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization\nstructure of the interstellar medium and allows us to identify optically thin\nregions. To optimize the continuum subtraction, we introduce a new method for\ndetermining the best continuum scale factor derived from the mode of the\ncontinuum-subtracted, image flux distribution. We find no conclusive evidence\nof LyC escape from Knots B or C, but instead, we identify a high-ionization\nregion extending over at least 1 kpc from Knot A. Knot A shows evidence of an\nextremely young age ($\\lesssim 1$ Myr), perhaps containing very massive stars\n($>100$ M$_\\odot$). It is weak in Ly$\\alpha$, so if it is confirmed as the LyC\nsource, our results imply that LyC emission may be independent of Ly$\\alpha$\nemission.\n', 'title': 'Haro 11: Where is the Lyman continuum source?'}]","I'm sorry, but I couldn't find any information related to ""ionization-parameter mapping"" or its use in studying galaxies in the provided context. The papers seem to be related to various topics such as velocity-map imaging, brain mapping, radio variability, amortized inference regularization, and magnetic particle imaging, but none of them mention ionization-parameter mapping.

If you could provide more context or information about what ionization-parameter mapping is or what you're looking for, I'd be happy to try and assist you further."
Is there a relationship between the number of nonlinear one-variable wave solutions and the integrability of a system?,6c97c1da,"[{'abstract': '  The KdV equation can be derived in the shallow water limit of the Euler\nequations. Over the last few decades, this equation has been extended to\ninclude higher order effects. Although this equation has only one conservation\nlaw, exact periodic and solitonic solutions exist. Khare and Saxena\n\\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact\nsolutions by combining known ones for several fundamental equations (e.g.,\nKorteweg - de Vries, Nonlinear Schrödinger). Here we find that this\nconstruction can be repeated for higher order, non-integrable extensions of\nthese equations. Contrary to many statements in the literature, there seems to\nbe no correlation between integrability and the number of nonlinear one\nvariable wave solutions.\n', 'title': 'Superposition solutions to the extended KdV equation for water surface waves'}]","Title: Superposition solutions to the extended KdV equation for water surface waves

Abstract:  The KdV equation can be derived in the shallow water limit of the Euler
equations. Over the last few decades, this equation has been extended to
include higher order effects. Although this equation has only one conservation
law, exact periodic and solitonic solutions exist. Khare and Saxena
\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact
solutions by combining known ones for several fundamental equations (e.g.,
Korteweg - de Vries, Nonlinear Schrödinger). Here we find that this
construction can be repeated for higher order, non-integrable extensions of
these equations. Contrary to many statements in the literature, there seems to
be no correlation between integrability and the number of nonlinear one
variable wave solutions."
Is there an optimal lower bound that can be calculated using only the trace of the inverse and the inverse squared of a symmetric positive definite matrix?,6fcfed81,"[{'abstract': ""  Lower bounds on the smallest eigenvalue of a symmetric positive definite\nmatrices $A\\in\\mathbb{R}^{m\\times m}$ play an important role in condition\nnumber estimation and in iterative methods for singular value computation. In\nparticular, the bounds based on ${\\rm Tr}(A^{-1})$ and ${\\rm Tr}(A^{-2})$\nattract attention recently because they can be computed in $O(m)$ work when $A$\nis tridiagonal. In this paper, we focus on these bounds and investigate their\nproperties in detail. First, we consider the problem of finding the optimal\nbound that can be computed solely from ${\\rm Tr}(A^{-1})$ and ${\\rm\nTr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one\nin terms of sharpness. Next, we study the gap between the Laguerre bound and\nthe smallest eigenvalue. We characterize the situation in which the gap becomes\nlargest in terms of the eigenvalue distribution of $A$ and show that the gap\nbecomes smallest when ${\\rm Tr}(A^{-2})/\\{{\\rm Tr}(A^{-1})\\}^2$ approaches 1 or\n$\\frac{1}{m}$. These results will be useful, for example, in designing\nefficient shift strategies for singular value computation algorithms.\n"", 'title': ""On the optimality and sharpness of Laguerre's lower bound on the smallest eigenvalue of a symmetric positive definite matrix""}]","Title: On the optimality and sharpness of Laguerre's lower bound on the smallest eigenvalue of a symmetric positive definite matrix

Abstract: Lower bounds on the smallest eigenvalue of a symmetric positive definite matrices $A\in\mathbb{R}^{m\times m}$ play an important role in condition number estimation and in iterative methods for singular value computation. In particular, the bounds based on ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$ attract attention recently because they can be computed in $O(m)$ work when $A$ is tridiagonal. In this paper, we focus on these bounds and investigate their properties in detail. First, we consider the problem of finding the optimal bound that can be computed solely from ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one in terms of sharpness. Next, we study the gap between the Laguerre bound and the smallest eigenvalue. We characterize the situation in which the gap becomes largest in terms of the eigenvalue distribution of $A$ and show that the gap becomes smallest when ${\rm Tr}(A^{-2})/\{{\rm Tr}(A^{-1})\}^2$ approaches 1 or $\frac{1}{m}$. These results will be useful, for example, in designing efficient shift strategies for singular value computation algorithms."
How does transient absorption spectroscopy help explain the photophysics of transition metal dichalcogenides?,1500de3f,"[{'abstract': '  Transition metal dichalcogenides (TMDs) are emerging as promising\ntwo-dimensional (2d) semiconductors for optoelectronic and flexible devices.\nHowever, a microscopic explanation of their photophysics -- of pivotal\nimportance for the understanding and optimization of device operation -- is\nstill lacking. Here we use femtosecond transient absorption spectroscopy, with\npump pulse tunability and broadband probing, to monitor the relaxation dynamics\nof single-layer MoS2 over the entire visible range, upon photoexcitation of\ndifferent excitonic transitions. We find that, irrespective of excitation\nphoton energy, the transient absorption spectrum shows the simultaneous\nbleaching of all excitonic transitions and corresponding red-shifted\nphotoinduced absorption bands. First-principle modeling of the ultrafast\noptical response reveals that a transient bandgap renormalization, caused by\nthe presence of photo-excited carriers, is primarily responsible for the\nobserved features. Our results demonstrate the strong impact of many-body\neffects in the transient optical response of TMDs even in the\nlow-excitation-density regime.\n', 'title': 'Photo-Induced Bandgap Renormalization Governs the Ultrafast Response of Single-Layer MoS2'}]","Title: Photo-Induced Bandgap Renormalization Governs the Ultrafast Response of Single-Layer MoS2

Abstract: Transition metal dichalcogenides (TMDs) are emerging as promising two-dimensional (2d) semiconductors for optoelectronic and flexible devices. However, a microscopic explanation of their photophysics -- of pivotal importance for the understanding and optimization of device operation -- is still lacking. Here we use femtosecond transient absorption spectroscopy, with pump pulse tunability and broadband probing, to monitor the relaxation dynamics of single-layer MoS2 over the entire visible range, upon photoexcitation of different excitonic transitions. We find that, irrespective of excitation photon energy, the transient absorption spectrum shows the simultaneous bleaching of all excitonic transitions and corresponding red-shifted photoinduced absorption bands. First-principle modeling of the ultrafast optical response reveals that a transient bandgap renormalization, caused by the presence of photo-excited carriers, is primarily responsible for the observed features. Our results demonstrate the strong impact of many-body effects in the transient optical response of TMDs even in the low-excitation-density regime."
What is the range of Reynolds numbers and turbulence intensity at which the best performing machine learning model is accurate?,8b6823e9,"[{'abstract': '  Numerous studies have been carried out to measure wind pressures around\ncircular cylinders since the early 20th century due to its engineering\nsignificance. Consequently, a large amount of wind pressure data sets have\naccumulated, which presents an excellent opportunity for using machine learning\n(ML) techniques to train models to predict wind pressures around circular\ncylinders. Wind pressures around smooth circular cylinders are a function of\nmainly the Reynolds number (Re), turbulence intensity (Ti) of the incident\nwind, and circumferential angle of the cylinder. Considering these three\nparameters as the inputs, this study trained two ML models to predict mean and\nfluctuating pressures respectively. Three machine learning algorithms including\ndecision tree regressor, random forest, and gradient boosting regression trees\n(GBRT) were tested. The GBRT models exhibited the best performance for\npredicting both mean and fluctuating pressures, and they are capable of making\naccurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to\n15%. It is believed that the GBRT models provide very efficient and economical\nalternative to traditional wind tunnel tests and computational fluid dynamic\nsimulations for determining wind pressures around smooth circular cylinders\nwithin the studied Re and Ti range.\n', 'title': 'Predicting wind pressures around circular cylinders using machine learning techniques'}]","Title: Predicting wind pressures around circular cylinders using machine learning techniques

Abstract: Numerous studies have been carried out to measure wind pressures around circular cylinders since the early 20th century due to its engineering significance. Consequently, a large amount of wind pressure data sets have accumulated, which presents an excellent opportunity for using machine learning (ML) techniques to train models to predict wind pressures around circular cylinders. Wind pressures around smooth circular cylinders are a function of mainly the Reynolds number (Re), turbulence intensity (Ti) of the incident wind, and circumferential angle of the cylinder. Considering these three parameters as the inputs, this study trained two ML models to predict mean and fluctuating pressures respectively. Three machine learning algorithms including decision tree regressor, random forest, and gradient boosting regression trees (GBRT) were tested. The GBRT models exhibited the best performance for predicting both mean and fluctuating pressures, and they are capable of making accurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to 15%. It is believed that the GBRT models provide very efficient and economical alternative to traditional wind tunnel tests and computational fluid dynamic simulations for determining wind pressures around smooth circular cylinders within the studied Re and Ti range.

The answer to the question is that the best performing machine learning model (GBRT) is accurate for Reynolds numbers ranging from 10^4 to 10^6 and turbulence intensities ranging from 0% to 15%."
How does a new method for estimating vaccination uptake compare to methods using clinical data?,7b1d3b2e,"[{'abstract': '  Estimating vaccination uptake is an integral part of ensuring public health.\nIt was recently shown that vaccination uptake can be estimated automatically\nfrom web data, instead of slowly collected clinical records or population\nsurveys. All prior work in this area assumes that features of vaccination\nuptake collected from the web are temporally regular. We present the first ever\nmethod to remove this assumption from vaccination uptake estimation: our method\ndynamically adapts to temporal fluctuations in time series web data used to\nestimate vaccination uptake. We show our method to outperform the state of the\nart compared to competitive baselines that use not only web data but also\ncurated clinical data. This performance improvement is more pronounced for\nvaccines whose uptake has been irregular due to negative media attention (HPV-1\nand HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of\n12 years old (whose vaccination is more irregular compared to younger\nchildren).\n', 'title': 'Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search Queries'}]","Based on the context, I found the relevant research paper that answers the question.

Title: Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search Queries

Abstract: Estimating vaccination uptake is an integral part of ensuring public health.
It was recently shown that vaccination uptake can be estimated automatically
from web data, instead of slowly collected clinical records or population
surveys. All prior work in this area assumes that features of vaccination
uptake collected from the web are temporally regular. We present the first ever
method to remove this assumption from vaccination uptake estimation: our method
dynamically adapts to temporal fluctuations in time series web data used to
estimate vaccination uptake. We show our method to outperform the state of the
art compared to competitive baselines that use not only web data but also
curated clinical data. This performance improvement is more pronounced for
vaccines whose uptake has been irregular due to negative media attention (HPV-1
and HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of
12 years old (whose vaccination is more irregular compared to younger
children)."
What effect does a decrease in temperature have on the length scale of glassy clusters? ,b2d25d3f,"[{'abstract': '  In glass forming liquids close to the glass transition point, even a very\nslight increase in the macroscopic density results in a dramatic slowing down\nof the macroscopic relaxation. Concomitantly, the local density itself\nfluctuates in space. Therefore, one can imagine that even very small local\ndensity variations control the local glassy nature. Based on this perspective,\na model for describing growing length scale accompanying the vitrification is\nintroduced, in which we assume that in a subsystem whose density is above a\ncertain threshold value, $\\rho_{\\rm c}$, owing to steric constraints, particle\nrearrangements are highly suppressed for a sufficiently long time period\n($\\sim$ structural relaxation time). We regard such a subsystem as a glassy\ncluster. Then, based on the statistics of the subsystem-density, we predict\nthat with compression (increasing average density $\\rho$) at a fixed\ntemperature $T$ in supercooled states, the characteristic length of the\nclusters, $\\xi$, diverges as $\\xi\\sim(\\rho_{\\rm c}-\\rho)^{-2/d}$, where $d$ is\nthe spatial dimensionality. This $\\xi$ measures the average persistence length\nof the steric constraints in blocking the rearrangement motions and is\ndetermined by the subsystem density. Additionally, with decreasing $T$ at a\nfixed $\\rho$, the length scale diverges in the same manner as $\\xi\\sim(T-T_{\\rm\nc})^{-2/d}$, for which $\\rho$ is identical to $\\rho_{\\rm c}$ at $T=T_{\\rm c}$.\nThe exponent describing the diverging length scale is the same as the one\npredicted by some theoretical models and indeed has been observed in some\nsimulations and experiments. However, the basic mechanism for this divergence\nis different; that is, we do not invoke thermodynamic anomalies associated with\nthe thermodynamic phase transition as the origin of the growing length scale.\nWe further present arguements for the cooperative properties based on the\nclusters.\n', 'title': 'Growing length scale accompanying the vitrification: A perspective based on non-singular density fluctuations'}]","Title: Growing length scale accompanying the vitrification: A perspective based on non-singular density fluctuations

Abstract: In glass forming liquids close to the glass transition point, even a very slight increase in the macroscopic density results in a dramatic slowing down of the macroscopic relaxation. Concomitantly, the local density itself fluctuates in space. Therefore, one can imagine that even very small local density variations control the local glassy nature. Based on this perspective, a model for describing growing length scale accompanying the vitrification is introduced, in which we assume that in a subsystem whose density is above a certain threshold value, $\rho_{\rm c}$, owing to steric constraints, particle rearrangements are highly suppressed for a sufficiently long time period ($\sim$ structural relaxation time). We regard such a subsystem as a glassy cluster. Then, based on the statistics of the subsystem-density, we predict that with compression (increasing average density $\rho$) at a fixed temperature $T$ in supercooled states, the characteristic length of the clusters, $\xi$, diverges as $\xi\sim(\rho_{\rm c}-\rho)^{-2/d}$, where $d$ is the spatial dimensionality. This $\xi$ measures the average persistence length of the steric constraints in blocking the rearrangement motions and is determined by the subsystem density. Additionally, with decreasing $T$ at a fixed $\rho$, the length scale diverges in the same manner as $\xi\sim(T-T_{\rm c})^{-2/d}$, for which $\rho$ is identical to $\rho_{\rm c}$ at $T=T_{\rm c}$. The exponent describing the diverging length scale is the same as the one predicted by some theoretical models and indeed has been observed in some simulations and experiments. However, the basic mechanism for this divergence is different; that is, we do not invoke thermodynamic anomalies associated with the thermodynamic phase transition as the origin of the growing length scale. We further present arguments for the cooperative properties based on the clusters.

The answer to your question is that with decreasing temperature $T$ at a fixed average density $\rho$, the length scale of the glassy clusters diverges as $\xi\sim(T-T_{\rm c})^{-2/d}$."
"Which mathematical transformation does this paper extend, related to Lambert series?",f07cb101,"[{'abstract': ""  It is pointed out that the generalized Lambert series\n$\\displaystyle\\sum_{n=1}^{\\infty}\\frac{n^{N-2h}}{e^{n^{N}x}-1}$ studied by\nKanemitsu, Tanigawa and Yoshimoto can be found on page $332$ of Ramanujan's\nLost Notebook in a slightly more general form. We extend an important\ntransformation of this series obtained by Kanemitsu, Tanigawa and Yoshimoto by\nremoving restrictions on the parameters $N$ and $h$ that they impose. From our\nextension we deduce a beautiful new generalization of Ramanujan's famous\nformula for odd zeta values which, for $N$ odd and $m>0$, gives a relation\nbetween $\\zeta(2m+1)$ and $\\zeta(2Nm+1)$. A result complementary to the\naforementioned generalization is obtained for any even $N$ and\n$m\\in\\mathbb{Z}$. It generalizes a transformation of Wigert and can be regarded\nas a formula for $\\zeta\\left(2m+1-\\frac{1}{N}\\right)$. Applications of these\ntransformations include a generalization of the transformation for the\nlogarithm of Dedekind eta-function $\\eta(z)$, Zudilin- and Rivoal-type results\non transcendence of certain values, and a transcendence criterion for Euler's\nconstant $\\gamma$.\n"", 'title': 'Generalized Lambert series and arithmetic nature of odd zeta values'}]","Title: Generalized Lambert series and arithmetic nature of odd zeta values

Abstract: It is pointed out that the generalized Lambert series $\displaystyle\sum_{n=1}^{\infty}\frac{n^{N-2h}}{e^{n^{N}x}-1}$ studied by Kanemitsu, Tanigawa and Yoshimoto can be found on page $332$ of Ramanujan's Lost Notebook in a slightly more general form. We extend an important transformation of this series obtained by Kanemitsu, Tanigawa and Yoshimoto by removing restrictions on the parameters $N$ and $h$ that they impose. From our extension we deduce a beautiful new generalization of Ramanujan's famous formula for odd zeta values which, for $N$ odd and $m>0$, gives a relation between $\zeta(2m+1)$ and $\zeta(2Nm+1)$. A result complementary to the aforementioned generalization is obtained for any even $N$ and $m\in\mathbb{Z}$. It generalizes a transformation of Wigert and can be regarded as a formula for $\zeta\left(2m+1-\frac{1}{N}\right)$. Applications of these transformations include a generalization of the transformation for the logarithm of Dedekind eta-function $\eta(z)$, Zudilin- and Rivoal-type results on transcendence of certain values, and a transcendence criterion for Euler's constant $\gamma$."
Can the direction of a magnetic field influence the focusing of magnetic particles in a channel?,1bb320fe,"[{'abstract': ""  We report a method to control the positions of ellipsoidal magnets in flowing\nchannels of rectangular or circular cross section at low Reynolds number.A\nstatic uniform magnetic field is used to pin the particle orientation, and the\nparticles move with translational drift velocities resulting from hydrodynamic\ninteractions with the channel walls which can be described using Blake's image\ntensor.Building on his insights, we are able to present a far-field theory\npredicting the particle motion in rectangular channels, and validate the\naccuracy of the theory by comparing to numerical solutions using the boundary\nelement method.We find that, by changing the direction of the applied magnetic\nfield, the motion can be controlled so that particles move either to a curved\nfocusing region or to the channel walls.We also use simulations to show that\nthe particles are focused to a single line in a circular channel.Our results\nsuggest ways to focus and segregate magnetic particles in lab-on-a-chip\ndevices.\n"", 'title': 'Far-field theory for trajectories of magnetic ellipsoids in rectangular and circular channels'}]","I think I can help you with that!

Title: Far-field theory for trajectories of magnetic ellipsoids in rectangular and circular channels

Abstract:   We report a method to control the positions of ellipsoidal magnets in flowing
channels of rectangular or circular cross section at low Reynolds number.A
static uniform magnetic field is used to pin the particle orientation, and the
particles move with translational drift velocities resulting from hydrodynamic
interactions with the channel walls which can be described using Blake's image
tensor.Building on his insights, we are able to present a far-field theory
predicting the particle motion in rectangular channels, and validate the
accuracy of the theory by comparing to numerical solutions using the boundary
element method.We find that, by changing the direction of the applied magnetic
field, the motion can be controlled so that particles move either to a curved
focusing region or to the channel walls.We also use simulations to show that
the particles are focused to a single line in a circular channel.Our results
suggest ways to focus and segregate magnetic particles in lab-on-a-chip
devices.

According to this paper, the direction of the applied magnetic field can indeed influence the focusing of magnetic particles in a channel. By changing the direction of the magnetic field, the motion of the particles can be controlled to move either to a curved focusing region or to the channel walls."
What are the benefits of using a Riemannian extension of stochastic variance reduction for gradient descent?,b06c0299,"[{'abstract': '  Stochastic variance reduction algorithms have recently become popular for\nminimizing the average of a large but finite number of loss functions. In this\npaper, we propose a novel Riemannian extension of the Euclidean stochastic\nvariance reduced gradient algorithm (R-SVRG) to a manifold search space. The\nkey challenges of averaging, adding, and subtracting multiple gradients are\naddressed with retraction and vector transport. We present a global convergence\nanalysis of the proposed algorithm with a decay step size and a local\nconvergence rate analysis under a fixed step size under some natural\nassumptions. The proposed algorithm is applied to problems on the Grassmann\nmanifold, such as principal component analysis, low-rank matrix completion, and\ncomputation of the Karcher mean of subspaces, and outperforms the standard\nRiemannian stochastic gradient descent algorithm in each case.\n', 'title': 'Riemannian stochastic variance reduced gradient'}]","Title: Riemannian stochastic variance reduced gradient

Abstract:   Stochastic variance reduction algorithms have recently become popular for
minimizing the average of a large but finite number of loss functions. In this
paper, we propose a novel Riemannian extension of the Euclidean stochastic
variance reduced gradient algorithm (R-SVRG) to a manifold search space. The
key challenges of averaging, adding, and subtracting multiple gradients are
addressed with retraction and vector transport. We present a global convergence
analysis of the proposed algorithm with a decay step size and a local
convergence rate analysis under a fixed step size under some natural
assumptions. The proposed algorithm is applied to problems on the Grassmann
manifold, such as principal component analysis, low-rank matrix completion, and
computation of the Karcher mean of subspaces, and outperforms the standard
Riemannian stochastic gradient descent algorithm in each case.

The benefits of using a Riemannian extension of stochastic variance reduction for gradient descent are that it outperforms the standard Riemannian stochastic gradient descent algorithm in problems on the Grassmann manifold, such as principal component analysis, low-rank matrix completion, and computation of the Karcher mean of subspaces."
What is the challenge of classifying degenerate critical points of multivariate polynomial functions? ,b5333ed7,"[{'abstract': '  In this paper, we consider the problem of identifying the type (local\nminimizer, maximizer or saddle point) of a given isolated real critical point\n$c$, which is degenerate, of a multivariate polynomial function $f$. To this\nend, we introduce the definition of faithful radius of $c$ by means of the\ncurve of tangency of $f$. We show that the type of $c$ can be determined by the\nglobal extrema of $f$ over the Euclidean ball centered at $c$ with a faithful\nradius.We propose algorithms to compute a faithful radius of $c$ and determine\nits type.\n', 'title': 'On types of degenerate critical points of real polynomial functions'}]","I think I can help you with that!

Title: On types of degenerate critical points of real polynomial functions

Abstract: In this paper, we consider the problem of identifying the type (local minimizer, maximizer or saddle point) of a given isolated real critical point $c$, which is degenerate, of a multivariate polynomial function $f$. To this end, we introduce the definition of faithful radius of $c$ by means of the curve of tangency of $f$. We show that the type of $c$ can be determined by the global extrema of $f$ over the Euclidean ball centered at $c$ with a faithful radius. We propose algorithms to compute a faithful radius of $c$ and determine its type.

The challenge lies in determining the type of a degenerate critical point, which is not straightforward. The researchers have proposed a solution by introducing the concept of a faithful radius and developing algorithms to compute it, ultimately identifying the type of the critical point."
What is the primary advantage of using a deep learning approach for obtaining ternary projections in compressed sensing?,a22b9d6e,"[{'abstract': '  Compressed sensing (CS) is a sampling theory that allows reconstruction of\nsparse (or compressible) signals from an incomplete number of measurements,\nusing of a sensing mechanism implemented by an appropriate projection matrix.\nThe CS theory is based on random Gaussian projection matrices, which satisfy\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\nprojections are more suitable for hardware implementation. In this paper, we\npresent a deep learning approach to obtain very sparse ternary projections for\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\nexperimental results on real images demonstrate the effectiveness of the\nproposed approach compared to state-of-the-art methods, with significant\nadvantage in terms of complexity.\n', 'title': 'Deep Learning Sparse Ternary Projections for Compressed Sensing of Images'}]","Title: Deep Learning Sparse Ternary Projections for Compressed Sensing of Images

Abstract:   Compressed sensing (CS) is a sampling theory that allows reconstruction of
sparse (or compressible) signals from an incomplete number of measurements,
using of a sensing mechanism implemented by an appropriate projection matrix.
The CS theory is based on random Gaussian projection matrices, which satisfy
recovery guarantees with high probability; however, sparse ternary {0, -1, +1}
projections are more suitable for hardware implementation. In this paper, we
present a deep learning approach to obtain very sparse ternary projections for
compressed sensing. Our deep learning architecture jointly learns a pair of a
projection matrix and a reconstruction operator in an end-to-end fashion. The
experimental results on real images demonstrate the effectiveness of the
proposed approach compared to state-of-the-art methods, with significant
advantage in terms of complexity.

The primary advantage of using a deep learning approach for obtaining ternary projections in compressed sensing is a significant advantage in terms of complexity."
Are there any techniques for patterning surface monolayers to control carbon nanotube binding?,d1408eac,"[{'abstract': '  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel\nmaterial are used in a variety of electronics applications. However, a\ncompetitive CNT-based technology requires the precise placement of CNTs at\npredefined locations of a substrate. One promising placement approach is to use\nchemical recognition to bind CNTs from solution at the desired locations on a\nsurface. Producing the chemical pattern on the substrate is challenging. Here\nwe describe a one-step patterning approach based on a highly photosensitive\nsurface monolayer. The monolayer contains chromophopric group as light\nsensitive body with heteroatoms as high quantum yield photolysis center. As\ndeposited, the layer will bind CNTs from solution. However, when exposed to\nultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for\nconventional photoresists, the monolayer cleaves and no longer binds CNTs.\nThese features allow standard, wafer-scale UV lithography processes to be used\nto form a patterned chemical monolayer without the need for complex substrate\npatterning or monolayer stamping.\n', 'title': 'Photo-Chemically Directed Self-Assembly of Carbon Nanotubes on Surfaces'}]","Title: Photo-Chemically Directed Self-Assembly of Carbon Nanotubes on Surfaces

Abstract: Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel material are used in a variety of electronics applications. However, a competitive CNT-based technology requires the precise placement of CNTs at predefined locations of a substrate. One promising placement approach is to use chemical recognition to bind CNTs from solution at the desired locations on a surface. Producing the chemical pattern on the substrate is challenging. Here we describe a one-step patterning approach based on a highly photosensitive surface monolayer. The monolayer contains chromophopric group as light sensitive body with heteroatoms as high quantum yield photolysis center. As deposited, the layer will bind CNTs from solution. However, when exposed to ultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for conventional photoresists, the monolayer cleaves and no longer binds CNTs. These features allow standard, wafer-scale UV lithography processes to be used to form a patterned chemical monolayer without the need for complex substrate patterning or monolayer stamping."
"Besides improving generalization, what other benefits does mixup offer for neural networks?",29bdd097,"[{'abstract': '  Large deep neural networks are powerful, but exhibit undesirable behaviors\nsuch as memorization and sensitivity to adversarial examples. In this work, we\npropose mixup, a simple learning principle to alleviate these issues. In\nessence, mixup trains a neural network on convex combinations of pairs of\nexamples and their labels. By doing so, mixup regularizes the neural network to\nfavor simple linear behavior in-between training examples. Our experiments on\nthe ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show\nthat mixup improves the generalization of state-of-the-art neural network\narchitectures. We also find that mixup reduces the memorization of corrupt\nlabels, increases the robustness to adversarial examples, and stabilizes the\ntraining of generative adversarial networks.\n', 'title': 'mixup: Beyond Empirical Risk Minimization'}]","Title: mixup: Beyond Empirical Risk Minimization

Abstract: Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.

Besides improving generalization, mixup offers two other benefits for neural networks: it reduces the memorization of corrupt labels and increases the robustness to adversarial examples."
What novel characterization of the dual solution space is presented in the research?,421a2f14,"[{'abstract': '  One of the popular approaches for low-rank tensor completion is to use the\nlatent trace norm regularization. However, most existing works in this\ndirection learn a sparse combination of tensors. In this work, we fill this gap\nby proposing a variant of the latent trace norm that helps in learning a\nnon-sparse combination of tensors. We develop a dual framework for solving the\nlow-rank tensor completion problem. We first show a novel characterization of\nthe dual solution space with an interesting factorization of the optimal\nsolution. Overall, the optimal solution is shown to lie on a Cartesian product\nof Riemannian manifolds. Furthermore, we exploit the versatile Riemannian\noptimization framework for proposing computationally efficient trust region\nalgorithm. The experiments illustrate the efficacy of the proposed algorithm on\nseveral real-world datasets across applications.\n', 'title': 'A dual framework for low-rank tensor completion'}]","Title: A dual framework for low-rank tensor completion

Abstract: One of the popular approaches for low-rank tensor completion is to use the latent trace norm regularization. However, most existing works in this direction learn a sparse combination of tensors. In this work, we fill this gap by proposing a variant of the latent trace norm that helps in learning a non-sparse combination of tensors. We develop a dual framework for solving the low-rank tensor completion problem. We first show a novel characterization of the dual solution space with an interesting factorization of the optimal solution. Overall, the optimal solution is shown to lie on a Cartesian product of Riemannian manifolds. Furthermore, we exploit the versatile Riemannian optimization framework for proposing computationally efficient trust region algorithm. The experiments illustrate the efficacy of the proposed algorithm on several real-world datasets across applications.

The novel characterization of the dual solution space presented in this research is that the optimal solution lies on a Cartesian product of Riemannian manifolds with an interesting factorization of the optimal solution."
What types of problems were used to demonstrate the performance of these algorithms?,39310759,"[{'abstract': '  This paper derives two new optimization-driven Monte Carlo algorithms\ninspired from variable splitting and data augmentation. In particular, the\nformulation of one of the proposed approaches is closely related to the\nalternating direction method of multipliers (ADMM) main steps. The proposed\nframework enables to derive faster and more efficient sampling schemes than the\ncurrent state-of-the-art methods and can embed the latter. By sampling\nefficiently the parameter to infer as well as the hyperparameters of the\nproblem, the generated samples can be used to approximate Bayesian estimators\nof the parameters to infer. Additionally, the proposed approach brings\nconfidence intervals at a low cost contrary to optimization methods.\nSimulations on two often-studied signal processing problems illustrate the\nperformance of the two proposed samplers. All results are compared to those\nobtained by recent state-of-the-art optimization and MCMC algorithms used to\nsolve these problems.\n', 'title': 'Split-and-augmented Gibbs sampler - Application to large-scale inference problems'}]","Based on the provided abstracts, I found the following answers to the question:

Title: Exponential Stability Analysis via Integral Quadratic Constraints

Abstract: The theory of integral quadratic constraints (IQCs) allows verification of stability and gain-bound properties of systems containing nonlinear or uncertain elements. Gain bounds often imply exponential stability, but it can be challenging to compute useful numerical bounds on the exponential decay rate. This work presents a generalization of the classical IQC results of Megretski and Rantzer that leads to a tractable computational procedure for finding exponential rate certificates that are far less conservative than ones computed from $L_2$ gain bounds alone. An expanded library of IQCs for certifying exponential stability is also provided and the effectiveness of the technique is demonstrated via numerical examples.

The type of problem used to demonstrate the performance of this algorithm is numerical examples.

Title: Failures of Gradient-Based Deep Learning

Abstract: In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.

The type of problem used to demonstrate the performance of this algorithm is four types of simple problems.

Title: Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry

Abstract: We provide a comprehensive study of the convergence of forward-backward algorithm under suitable geometric conditions leading to fast rates. We present several new results and collect in a unified view a variety of results scattered in the literature, often providing simplified proofs. Novel contributions include the analysis of infinite dimensional convex minimization problems, allowing the case where minimizers might not exist. Further, we analyze the relation between different geometric conditions, and discuss novel connections with a priori conditions in linear inverse problems, including source conditions, restricted isometry properties and partial smoothness.

No specific problem type is mentioned for demonstrating the performance of this algorithm.

Title: Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks

Abstract: The life of the modern world essentially depends on the work of the large artificial homogeneous networks, such as wired and wireless communication systems, networks of roads and pipelines. The support of their effective continuous functioning requires automatic screening and permanent optimization with processing of the huge amount of data by high-performance distributed systems. We propose new meta-algorithm of large homogeneous network analysis, its decomposition into alternative sets of loosely connected subnets, and parallel optimization of the most independent elements. This algorithm is based on a network-specific correlation function, Simulated Annealing technique, and is adapted to work in the computer cluster. On the example of large wireless network, we show that proposed algorithm essentially increases speed of parallel optimization. The elaborated general approach can be used for analysis and optimization of the wide range of networks, including such specific types as artificial neural networks or organized in networks physiological systems of living organisms.

The type of problem used to demonstrate the performance of this algorithm is a large wireless network.

Title: Asynchronous Byzantine Machine Learning (the case of SGD)

Abstract: Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce \emph{Kardam}, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than $f/n$, where $f$ is the number of Byzantine failures tolerated and $n$ the total number of workers.

The types of problems used to demonstrate the performance of this algorithm are the CIFAR-100 and EMNIST datasets."
How does the extension complexity of the stable set polytope of a bipartite graph relate to the number of vertices and edges in the graph?,d34ea03d,"[{'abstract': '  The extension complexity $\\mathsf{xc}(P)$ of a polytope $P$ is the minimum\nnumber of facets of a polytope that affinely projects to $P$. Let $G$ be a\nbipartite graph with $n$ vertices, $m$ edges, and no isolated vertices. Let\n$\\mathsf{STAB}(G)$ be the convex hull of the stable sets of $G$. It is easy to\nsee that $n \\leqslant \\mathsf{xc} (\\mathsf{STAB}(G)) \\leqslant n+m$. We improve\nboth of these bounds. For the upper bound, we show that $\\mathsf{xc}\n(\\mathsf{STAB}(G))$ is $O(\\frac{n^2}{\\log n})$, which is an improvement when\n$G$ has quadratically many edges. For the lower bound, we prove that\n$\\mathsf{xc} (\\mathsf{STAB}(G))$ is $\\Omega(n \\log n)$ when $G$ is the\nincidence graph of a finite projective plane. We also provide examples of\n$3$-regular bipartite graphs $G$ such that the edge vs stable set matrix of $G$\nhas a fooling set of size $|E(G)|$.\n', 'title': 'Extension complexity of stable set polytopes of bipartite graphs'}]","Title: Extension complexity of stable set polytopes of bipartite graphs

Abstract: The extension complexity $\mathsf{xc}(P)$ of a polytope $P$ is the minimum number of facets of a polytope that affinely projects to $P$. Let $G$ be a bipartite graph with $n$ vertices, $m$ edges, and no isolated vertices. Let $\mathsf{STAB}(G)$ be the convex hull of the stable sets of $G$. It is easy to see that $n \leqslant \mathsf{xc} (\mathsf{STAB}(G)) \leqslant n+m$. We improve both of these bounds. For the upper bound, we show that $\mathsf{xc} (\mathsf{STAB}(G))$ is $O(\frac{n^2}{\log n})$, which is an improvement when $G$ has quadratically many edges. For the lower bound, we prove that $\mathsf{xc} (\mathsf{STAB}(G))$ is $\Omega(n \log n)$ when $G$ is the incidence graph of a finite projective plane. We also provide examples of $3$-regular bipartite graphs $G$ such that the edge vs stable set matrix of $G$ has a fooling set of size $|E(G)|$."
How does the described framework learn the structure of neural networks?,c2bb4cbd,"[{'abstract': '  We present E NERGY N ET , a new framework for analyzing and building\nartificial neural network architectures. Our approach adaptively learns the\nstructure of the networks in an unsupervised manner. The methodology is based\nupon the theoretical guarantees of the energy function of restricted Boltzmann\nmachines (RBM) of infinite number of nodes. We present experimental results to\nshow that the final network adapts to the complexity of a given problem.\n', 'title': 'EnergyNet: Energy-based Adaptive Structural Learning of Artificial Neural Network Architectures'}]","Title: EnergyNet: Energy-based Adaptive Structural Learning of Artificial Neural Network Architectures

Abstract: We present E NERGY N ET , a new framework for analyzing and building artificial neural network architectures. Our approach adaptively learns the structure of the networks in an unsupervised manner. The methodology is based upon the theoretical guarantees of the energy function of restricted Boltzmann machines (RBM) of infinite number of nodes. We present experimental results to show that the final network adapts to the complexity of a given problem."
How does vortex formation impact the movement of an electron through a superfluid?,966a736a,"[{'abstract': '  We study the motion of an electron bubble in the zero temperature limit where\nneither phonons nor rotons provide a significant contribution to the drag\nexerted on an ion moving within the superfluid. By using the Gross-Clark model,\nin which a Gross-Pitaevskii equation for the superfluid wavefunction is coupled\nto a Schrödinger equation for the electron wavefunction, we study how\nvortex nucleation affects the measured drift velocity of the ion. We use\nparameters that give realistic values of the ratio of the radius of the bubble\nwith respect to the healing length in superfluid $^4$He at a pressure of one\nbar. By performing fully 3D spatio-temporal simulations of the superfluid\ncoupled to an electron, that is modelled within an adiabatic approximation and\nmoving under the influence of an applied electric field, we are able to recover\nthe key dynamics of the ion-vortex interactions that arise and the subsequent\nion-vortex complexes that can form. Using the numerically computed drift\nvelocity of the ion as a function of the applied electric field, we determine\nthe vortex-nucleation limited mobility of the ion to recover values in\nreasonable agreement with measured data.\n', 'title': 'Vortex Nucleation Limited Mobility of Free Electron Bubbles in the Gross-Pitaevskii Model of a Superfluid'}]","Based on the context, I found a relevant research paper that addresses the question.

Title: Vortex Nucleation Limited Mobility of Free Electron Bubbles in the Gross-Pitaevskii Model of a Superfluid

Abstract: We study the motion of an electron bubble in the zero temperature limit where neither phonons nor rotons provide a significant contribution to the drag exerted on an ion moving within the superfluid. By using the Gross-Clark model, in which a Gross-Pitaevskii equation for the superfluid wavefunction is coupled to a Schrödinger equation for the electron wavefunction, we study how vortex nucleation affects the measured drift velocity of the ion. We use parameters that give realistic values of the ratio of the radius of the bubble with respect to the healing length in superfluid $^4$He at a pressure of one bar. By performing fully 3D spatio-temporal simulations of the superfluid coupled to an electron, that is modelled within an adiabatic approximation and moving under the influence of an applied electric field, we are able to recover the key dynamics of the ion-vortex interactions that arise and the subsequent ion-vortex complexes that can form. Using the numerically computed drift velocity of the ion as a function of the applied electric field, we determine the vortex-nucleation limited mobility of the ion to recover values in reasonable agreement with measured data."
What are the challenges of sharing phenotyping algorithms used in Electronic Health Records?,c6909af1,"[{'abstract': '  Electronic Health Records (EHR) are data generated during routine clinical\ncare. EHR offer researchers unprecedented phenotypic breadth and depth and have\nthe potential to accelerate the pace of precision medicine at scale. A main EHR\nuse-case is creating phenotyping algorithms to define disease status, onset and\nseverity. Currently, no common machine-readable standard exists for defining\nphenotyping algorithms which often are stored in human-readable formats. As a\nresult, the translation of algorithms to implementation code is challenging and\nsharing across the scientific community is problematic. In this paper, we\nevaluate openEHR, a formal EHR data specification, for computable\nrepresentations of EHR phenotyping algorithms.\n', 'title': 'Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms'}]","Based on the context, I found an answer to your question.

Title: Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms

Abstract:   Electronic Health Records (EHR) are data generated during routine clinical
care. EHR offer researchers unprecedented phenotypic breadth and depth and have
the potential to accelerate the pace of precision medicine at scale. A main EHR
use-case is creating phenotyping algorithms to define disease status, onset and
severity. Currently, no common machine-readable standard exists for defining
phenotyping algorithms which often are stored in human-readable formats. As a
result, the translation of algorithms to implementation code is challenging and
sharing across the scientific community is problematic. In this paper, we
evaluate openEHR, a formal EHR data specification, for computable
representations of EHR phenotyping algorithms.

The challenge of sharing phenotyping algorithms used in Electronic Health Records is that there is no common machine-readable standard for defining these algorithms, which are often stored in human-readable formats. This makes the translation of algorithms to implementation code challenging and sharing across the scientific community problematic."
"How can the Chern number, a topological invariant, be represented using polarization fields?",1212ea3c,"[{'abstract': '  Topology has appeared in different physical contexts. The most prominent\napplication is topologically protected edge transport in condensed matter\nphysics. The Chern number, the topological invariant of gapped Bloch\nHamiltonians, is an important quantity in this field. Another example of\ntopology, in polarization physics, are polarization singularities, called L\nlines and C points. By establishing a connection between these two theories, we\ndevelop a novel technique to visualize and potentially measure the Chern\nnumber: it can be expressed either as the winding of the polarization azimuth\nalong L lines in reciprocal space, or in terms of the handedness and the index\nof the C points. For mechanical systems, this is directly connected to the\nvisible motion patterns.\n', 'title': 'L lines, C points and Chern numbers: understanding band structure topology using polarization fields'}]","Title: L lines, C points and Chern numbers: understanding band structure topology using polarization fields

Abstract: Topology has appeared in different physical contexts. The most prominent application is topologically protected edge transport in condensed matter physics. The Chern number, the topological invariant of gapped Bloch Hamiltonians, is an important quantity in this field. Another example of topology, in polarization physics, are polarization singularities, called L lines and C points. By establishing a connection between these two theories, we develop a novel technique to visualize and potentially measure the Chern number: it can be expressed either as the winding of the polarization azimuth along L lines in reciprocal space, or in terms of the handedness and the index of the C points. For mechanical systems, this is directly connected to the visible motion patterns."
What new method was used to analyze the eMID interbank market?,4f903d8a,"[{'abstract': '  Interbank markets are often characterised in terms of a core-periphery\nnetwork structure, with a highly interconnected core of banks holding the\nmarket together, and a periphery of banks connected mostly to the core but not\ninternally. This paradigm has recently been challenged for short time scales,\nwhere interbank markets seem better characterised by a bipartite structure with\nmore core-periphery connections than inside the core. Using a novel\ncore-periphery detection method on the eMID interbank market, we enrich this\npicture by showing that the network is actually characterised by multiple\ncore-periphery pairs. Moreover, a transition from core-periphery to bipartite\nstructures occurs by shortening the temporal scale of data aggregation. We\nfurther show how the global financial crisis transformed the market, in terms\nof composition, multiplicity and internal organisation of core-periphery pairs.\nBy unveiling such a fine-grained organisation and transformation of the\ninterbank market, our method can find important applications in the\nunderstanding of how distress can propagate over financial networks.\n', 'title': 'Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis'}]","Title: Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis

Abstract: Interbank markets are often characterised in terms of a core-periphery network structure, with a highly interconnected core of banks holding the market together, and a periphery of banks connected mostly to the core but not internally. This paradigm has recently been challenged for short time scales, where interbank markets seem better characterised by a bipartite structure with more core-periphery connections than inside the core. Using a novel core-periphery detection method on the eMID interbank market, we enrich this picture by showing that the network is actually characterised by multiple core-periphery pairs. Moreover, a transition from core-periphery to bipartite structures occurs by shortening the temporal scale of data aggregation. We further show how the global financial crisis transformed the market, in terms of composition, multiplicity and internal organisation of core-periphery pairs. By unveiling such a fine-grained organisation and transformation of the interbank market, our method can find important applications in the understanding of how distress can propagate over financial networks.

The new method used to analyze the eMID interbank market is a novel core-periphery detection method."
What types of forecasts (short and long-term) are used in systems designed for network safety and security monitoring?,ace2482f,"[{'abstract': '  Real time large scale streaming data pose major challenges to forecasting, in\nparticular defying the presence of human experts to perform the corresponding\nanalysis. We present here a class of models and methods used to develop an\nautomated, scalable and versatile system for large scale forecasting oriented\ntowards safety and security monitoring. Our system provides short and long term\nforecasts and uses them to detect safety and security issues in relation with\nmultiple internet connected devices well in advance they might take place.\n', 'title': 'Large Scale Automated Forecasting for Monitoring Network Safety and Security'}]","Based on the context, I found the answer to your question:

Title: Large Scale Automated Forecasting for Monitoring Network Safety and Security

Abstract: Real time large scale streaming data pose major challenges to forecasting, in particular defying the presence of human experts to perform the corresponding analysis. We present here a class of models and methods used to develop an automated, scalable and versatile system for large scale forecasting oriented towards safety and security monitoring. Our system provides short and long term forecasts and uses them to detect safety and security issues in relation with multiple internet connected devices well in advance they might take place.

This paper mentions that the system provides both short-term and long-term forecasts for network safety and security monitoring."
Describe challenges faced by the standard Hamiltonian Monte Carlo (HMC) method with multimodal target distributions.,a43bd05b,"[{'abstract': '  Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)\nmethod for performing approximate inference in complex probabilistic models of\ncontinuous variables. In common with many MCMC methods, however, the standard\nHMC approach performs poorly in distributions with multiple isolated modes. We\npresent a method for augmenting the Hamiltonian system with an extra continuous\ntemperature control variable which allows the dynamic to bridge between\nsampling a complex target distribution and a simpler unimodal base\ndistribution. This augmentation both helps improve mixing in multimodal targets\nand allows the normalisation constant of the target distribution to be\nestimated. The method is simple to implement within existing HMC code,\nrequiring only a standard leapfrog integrator. We demonstrate experimentally\nthat the method is competitive with annealed importance sampling and simulating\ntempering methods at sampling from challenging multimodal distributions and\nestimating their normalising constants.\n', 'title': 'Continuously tempered Hamiltonian Monte Carlo'}]","Title: Continuously tempered Hamiltonian Monte Carlo

Abstract: Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC) method for performing approximate inference in complex probabilistic models of continuous variables. In common with many MCMC methods, however, the standard HMC approach performs poorly in distributions with multiple isolated modes.

The challenge faced by the standard Hamiltonian Monte Carlo (HMC) method is that it performs poorly in distributions with multiple isolated modes."
Describe methods for incorporating real-time sensing and collision avoidance into the motion planning of a humanoid robot.,f1f8bf18,"[{'abstract': '  A high redundant non-holonomic humanoid mobile dual-arm manipulator system is\npresented in this paper where the motion planning to realize ""human-like""\nautonomous navigation and manipulation tasks is studied. Firstly, an improved\nMaxiMin NSGA-II algorithm, which optimizes five objective functions to solve\nthe problems of singularity, redundancy, and coupling between mobile base and\nmanipulator simultaneously, is proposed to design the optimal pose to\nmanipulate the target object. Then, in order to link the initial pose and that\noptimal pose, an off-line motion planning algorithm is designed. In detail, an\nefficient direct-connect bidirectional RRT and gradient descent algorithm is\nproposed to reduce the sampled nodes largely, and a geometric optimization\nmethod is proposed for path pruning. Besides, head forward behaviors are\nrealized by calculating the reasonable orientations and assigning them to the\nmobile base to improve the quality of human-robot interaction. Thirdly, the\nextension to on-line planning is done by introducing real-time sensing,\ncollision-test and control cycles to update robotic motion in dynamic\nenvironments. Fourthly, an EEs\' via-point-based multi-objective genetic\nalgorithm is proposed to design the ""human-like"" via-poses by optimizing four\nobjective functions. Finally, numerous simulations are presented to validate\nthe effectiveness of proposed algorithms.\n', 'title': 'Motion Planning for a Humanoid Mobile Manipulator System'}]","Title: Motion Planning for a Humanoid Mobile Manipulator System

Abstract: A high redundant non-holonomic humanoid mobile dual-arm manipulator system is presented in this paper where the motion planning to realize ""human-like"" autonomous navigation and manipulation tasks is studied. Firstly, an improved MaxiMin NSGA-II algorithm, which optimizes five objective functions to solve the problems of singularity, redundancy, and coupling between mobile base and manipulator simultaneously, is proposed to design the optimal pose to manipulate the target object. Then, in order to link the initial pose and that optimal pose, an off-line motion planning algorithm is designed. In detail, an efficient direct-connect bidirectional RRT and gradient descent algorithm is proposed to reduce the sampled nodes largely, and a geometric optimization method is proposed for path pruning. Besides, head forward behaviors are realized by calculating the reasonable orientations and assigning them to the mobile base to improve the quality of human-robot interaction. Thirdly, the extension to on-line planning is done by introducing real-time sensing, collision-test and control cycles to update robotic motion in dynamic environments. Fourthly, an EEs' via-point-based multi-objective genetic algorithm is proposed to design the ""human-like"" via-poses by optimizing four objective functions. Finally, numerous simulations are presented to validate the effectiveness of proposed algorithms."
What is the relationship between the isoperimetric quotient and the isoperimetric inequality in Euclidean space on a smooth compact Riemannian manifold?,c0050835,"[{'abstract': '  Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with\nsmooth boundary $\\partial M$. Suppose that $(M,g)$ admits a scalar-flat\nconformal metric. We prove that the supremum of the isoperimetric quotient over\nthe scalar-flat conformal class is strictly larger than the best constant of\nthe isoperimetric inequality in the Euclidean space, and consequently is\nachieved, if either (i) $n\\ge 12$ and $\\partial M$ has a nonumbilic point; or\n(ii) $n\\ge 10$, $\\partial M$ is umbilic and the Weyl tensor does not vanish at\nsome boundary point.\n', 'title': 'On the isoperimetric quotient over scalar-flat conformal classes'}]","Title: On the isoperimetric quotient over scalar-flat conformal classes

Abstract: Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with smooth boundary $\partial M$. Suppose that $(M,g)$ admits a scalar-flat conformal metric. We prove that the supremum of the isoperimetric quotient over the scalar-flat conformal class is strictly larger than the best constant of the isoperimetric inequality in the Euclidean space, and consequently is achieved, if either (i) $n\ge 12$ and $\partial M$ has a nonumbilic point; or (ii) $n\ge 10$, $\partial M$ is umbilic and the Weyl tensor does not vanish at some boundary point."
What advantage does the proposed method offer compared to estimating pairwise Kullback-Leibler divergence of empirical distributions?,360f22b7,"[{'abstract': '  This paper presents a distance-based discriminative framework for learning\nwith probability distributions. Instead of using kernel mean embeddings or\ngeneralized radial basis kernels, we introduce embeddings based on\ndissimilarity of distributions to some reference distributions denoted as\ntemplates. Our framework extends the theory of similarity of Balcan et al.\n(2008) to the population distribution case and we show that, for some learning\nproblems, some dissimilarity on distribution achieves low-error linear decision\nfunctions with high probability. Our key result is to prove that the theory\nalso holds for empirical distributions. Algorithmically, the proposed approach\nconsists in computing a mapping based on pairwise dissimilarity where learning\na linear decision function is amenable. Our experimental results show that the\nWasserstein distance embedding performs better than kernel mean embeddings and\ncomputing Wasserstein distance is far more tractable than estimating pairwise\nKullback-Leibler divergence of empirical distributions.\n', 'title': 'Distance Measure Machines'}]","Title: Distance Measure Machines

Abstract:   This paper presents a distance-based discriminative framework for learning
with probability distributions. Instead of using kernel mean embeddings or
generalized radial basis kernels, we introduce embeddings based on
dissimilarity of distributions to some reference distributions denoted as
templates. Our framework extends the theory of similarity of Balcan et al.
(2008) to the population distribution case and we show that, for some learning
problems, some dissimilarity on distribution achieves low-error linear decision
functions with high probability. Our key result is to prove that the theory
also holds for empirical distributions. Algorithmically, the proposed approach
consists in computing a mapping based on pairwise dissimilarity where learning
a linear decision function is amenable. Our experimental results show that the
Wasserstein distance embedding performs better than kernel mean embeddings and
computing Wasserstein distance is far more tractable than estimating pairwise
Kullback-Leibler divergence of empirical distributions.

The advantage of the proposed method is that computing Wasserstein distance is far more tractable than estimating pairwise Kullback-Leibler divergence of empirical distributions."
What is the potential benefit of natural language person retrieval for searching video footage?,e1b37929,"[{'abstract': '  Following the recent progress in image classification and captioning using\ndeep learning, we develop a novel natural language person retrieval system\nbased on an attention mechanism. More specifically, given the description of a\nperson, the goal is to localize the person in an image. To this end, we first\nconstruct a benchmark dataset for natural language person retrieval. To do so,\nwe generate bounding boxes for persons in a public image dataset from the\nsegmentation masks, which are then annotated with descriptions and attributes\nusing the Amazon Mechanical Turk. We then adopt a region proposal network in\nFaster R-CNN as a candidate region generator. The cropped images based on the\nregion proposals as well as the whole images with attention weights are fed\ninto Convolutional Neural Networks for visual feature extraction, while the\nnatural language expression and attributes are input to Bidirectional Long\nShort- Term Memory (BLSTM) models for text feature extraction. The visual and\ntext features are integrated to score region proposals, and the one with the\nhighest score is retrieved as the output of our system. The experimental\nresults show significant improvement over the state-of-the-art method for\ngeneric object retrieval and this line of research promises to benefit search\nin surveillance video footage.\n', 'title': 'Attention-based Natural Language Person Retrieval'}]","Title: Attention-based Natural Language Person Retrieval

Abstract: Following the recent progress in image classification and captioning using deep learning, we develop a novel natural language person retrieval system based on an attention mechanism. More specifically, given the description of a person, the goal is to localize the person in an image. To this end, we first construct a benchmark dataset for natural language person retrieval. To do so, we generate bounding boxes for persons in a public image dataset from the segmentation masks, which are then annotated with descriptions and attributes using the Amazon Mechanical Turk. We then adopt a region proposal network in Faster R-CNN as a candidate region generator. The cropped images based on the region proposals as well as the whole images with attention weights are fed into Convolutional Neural Networks for visual feature extraction, while the natural language expression and attributes are input to Bidirectional Long Short-Term Memory (BLSTM) models for text feature extraction. The visual and text features are integrated to score region proposals, and the one with the highest score is retrieved as the output of our system. The experimental results show significant improvement over the state-of-the-art method for generic object retrieval and this line of research promises to benefit search in surveillance video footage.

The potential benefit of natural language person retrieval for searching video footage is that it can significantly improve the search process in surveillance video footage."
How does a study  using an anti-spherical category  relate to calculations of the p-canonical basis?,d653f3a6,"[{'abstract': '  We study a diagrammatic categorification (the ""anti-spherical category"") of\nthe anti-spherical module for any Coxeter group. We deduce that Deodhar\'s\n(sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients,\nand that a monotonicity conjecture of Brenti\'s holds. The main technical\nobservation is a localisation procedure for the anti-spherical category, from\nwhich we construct a ""light leaves"" basis of morphisms. Our techniques may be\nused to calculate many new elements of the $p$-canonical basis in the\nanti-spherical module. The results use generators and relations for Soergel\nbimodules (""Soergel calculus"") in a crucial way.\n', 'title': 'The anti-spherical category'}]","Title: The anti-spherical category

Abstract: We study a diagrammatic categorification (the ""anti-spherical category"") of the anti-spherical module for any Coxeter group. We deduce that Deodhar's (sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients, and that a monotonicity conjecture of Brenti's holds. The main technical observation is a localisation procedure for the anti-spherical category, from which we construct a ""light leaves"" basis of morphisms. Our techniques may be used to calculate many new elements of the $p$-canonical basis in the anti-spherical module. The results use generators and relations for Soergel bimodules (""Soergel calculus"") in a crucial way."
How can sequential convex programming be used for control policy synthesis?,e2c58b8a,"[{'abstract': '  Learning to make decisions from observed data in dynamic environments remains\na problem of fundamental importance in a number of fields, from artificial\nintelligence and robotics, to medicine and finance. This paper concerns the\nproblem of learning control policies for unknown linear dynamical systems so as\nto maximize a quadratic reward function. We present a method to optimize the\nexpected value of the reward over the posterior distribution of the unknown\nsystem parameters, given data. The algorithm involves sequential convex\nprograming, and enjoys reliable local convergence and robust stability\nguarantees. Numerical simulations and stabilization of a real-world inverted\npendulum are used to demonstrate the approach, with strong performance and\nrobustness properties observed in both.\n', 'title': 'Learning convex bounds for linear quadratic control policy synthesis'}]","Title: Learning convex bounds for linear quadratic control policy synthesis

Abstract:   Learning to make decisions from observed data in dynamic environments remains
a problem of fundamental importance in a number of fields, from artificial
intelligence and robotics, to medicine and finance. This paper concerns the
problem of learning control policies for unknown linear dynamical systems so as
to maximize a quadratic reward function. We present a method to optimize the
expected value of the reward over the posterior distribution of the unknown
system parameters, given data. The algorithm involves sequential convex
programing, and enjoys reliable local convergence and robust stability
guarantees. Numerical simulations and stabilization of a real-world inverted
pendulum are used to demonstrate the approach, with strong performance and
robustness properties observed in both.

(Note: The abstract mentions that the algorithm involves sequential convex programming, which is the answer to the question)"
What are the computational models studied in a paper investigating width hierarchies for Boolean function complexities?,6881cf9d,"[{'abstract': '  We consider quantum, nondterministic and probabilistic versions of known\ncomputational model Ordered Read-$k$-times Branching Programs or Ordered Binary\nDecision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We\nshow width hierarchy for complexity classes of Boolean function computed by\nthese models and discuss relation between different variants of $k$-OBDD.\n', 'title': 'Width Hierarchies for Quantum and Classical Ordered Binary Decision Diagrams with Repeated Test'}]","Title: Width Hierarchies for Quantum and Classical Ordered Binary Decision Diagrams with Repeated Test

Abstract: We consider quantum, nondeterministic and probabilistic versions of known computational model Ordered Read-$k$-times Branching Programs or Ordered Binary Decision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We show width hierarchy for complexity classes of Boolean function computed by these models and discuss relation between different variants of $k$-OBDD."
How does the temperature of a host star influence the characteristics of a transiting giant planet's atmosphere? ,ed061da3,"[{'abstract': ""  The amount of ultraviolet irradiation and ablation experienced by a planet\ndepends strongly on the temperature of its host star. Of the thousands of\nextra-solar planets now known, only four giant planets have been found that\ntransit hot, A-type stars (temperatures of 7300-10,000K), and none are known to\ntransit even hotter B-type stars. WASP-33 is an A-type star with a temperature\nof ~7430K, which hosts the hottest known transiting planet; the planet is\nitself as hot as a red dwarf star of type M. The planet displays a large heat\ndifferential between its day-side and night-side, and is highly inflated,\ntraits that have been linked to high insolation. However, even at the\ntemperature of WASP-33b's day-side, its atmosphere likely resembles the\nmolecule-dominated atmospheres of other planets, and at the level of\nultraviolet irradiation it experiences, its atmosphere is unlikely to be\nsignificantly ablated over the lifetime of its star. Here we report\nobservations of the bright star HD 195689, which reveal a close-in (orbital\nperiod ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star\nis at the dividing line between stars of type A and B, and we measure the\nKELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar\ntype K4. The molecules in K stars are entirely dissociated, and thus the\nprimary sources of opacity in the day-side atmosphere of KELT-9b are likely\natomic metals. Furthermore, KELT-9b receives ~700 times more extreme\nultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b,\nleading to a predicted range of mass-loss rates that could leave the planet\nlargely stripped of its envelope during the main-sequence lifetime of the host\nstar.\n"", 'title': 'A giant planet undergoing extreme ultraviolet irradiation by its hot massive-star host'}]","Title: A giant planet undergoing extreme ultraviolet irradiation by its hot massive-star host

Abstract: The amount of ultraviolet irradiation and ablation experienced by a planet depends strongly on the temperature of its host star. Of the thousands of extra-solar planets now known, only four giant planets have been found that transit hot, A-type stars (temperatures of 7300-10,000K), and none are known to transit even hotter B-type stars. WASP-33 is an A-type star with a temperature of ~7430K, which hosts the hottest known transiting planet; the planet is itself as hot as a red dwarf star of type M. The planet displays a large heat differential between its day-side and night-side, and is highly inflated, traits that have been linked to high insolation. However, even at the temperature of WASP-33b's day-side, its atmosphere likely resembles the molecule-dominated atmospheres of other planets, and at the level of ultraviolet irradiation it experiences, its atmosphere is unlikely to be significantly ablated over the lifetime of its star. Here we report observations of the bright star HD 195689, which reveal a close-in (orbital period ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star is at the dividing line between stars of type A and B, and we measure the KELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar type K4. The molecules in K stars are entirely dissociated, and thus the primary sources of opacity in the day-side atmosphere of KELT-9b are likely atomic metals. Furthermore, KELT-9b receives ~700 times more extreme ultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b, leading to a predicted range of mass-loss rates that could leave the planet largely stripped of its envelope during the main-sequence lifetime of the host star."
What are the different mechanisms proposed for the coating of a ball with fluid as it exits a reservoir?,2eb8a03e,"[{'abstract': ""  We consider the withdrawal of a ball from a fluid reservoir to understand the\nlongevity of the connection between that ball and the fluid it breaks away\nfrom, at intermediate Reynolds numbers. Scaling arguments based on the\nprocesses observed as the ball interacts with the fluid surface were applied to\nthe `pinch-off time', when the ball breaks its connection with the fluid from\nwhich it has been withdrawn, measured experimentally. At the lowest Reynolds\nnumbers tested, pinch-off occurs in a `surface seal' close to the reservoir\nsurface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal'\nclose to the ball. Our scaling analysis shows that the connection between ball\nand fluid is controlled by the fluid film draining from the ball as it\ncontinues to be winched away from the fluid reservoir. The draining flow itself\ndepends on the amount of fluid coating the ball on exit from the reservoir. We\nconsider the possibilities that this coating was created through: a surface\ntension driven Landau Levitch Derjaguin wetting of the surface; a\nvisco-inertial quick coating; or alternatively through the inertia of the fluid\nmoving with the ball through the reservoir. We show that although the pinch-off\nmechanism is controlled by viscosity, the coating mechanism is governed by a\ndifferent length and timescale, dictated by the inertial added mass of the ball\nwhen submersed.\n"", 'title': 'A Ball Breaking Away from a Fluid'}]","Title: A Ball Breaking Away from a Fluid

Abstract: We consider the withdrawal of a ball from a fluid reservoir to understand the
longevity of the connection between that ball and the fluid it breaks away
from, at intermediate Reynolds numbers. Scaling arguments based on the
processes observed as the ball interacts with the fluid surface were applied to
the `pinch-off time', when the ball breaks its connection with the fluid from
which it has been withdrawn, measured experimentally. At the lowest Reynolds
numbers tested, pinch-off occurs in a `surface seal' close to the reservoir
surface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal'
close to the ball. Our scaling analysis shows that the connection between ball
and fluid is controlled by the fluid film draining from the ball as it
continues to be winched away from the fluid reservoir. The draining flow itself
depends on the amount of fluid coating the ball on exit from the reservoir. We
consider the possibilities that this coating was created through: a surface
tension driven Landau Levitch Derjaguin wetting of the surface; a
visco-inertial quick coating; or alternatively through the inertia of the fluid
moving with the ball through the reservoir."
How does a coherence theorem contribute to the use of diagrammatic reasoning in compositional game theory?,56dac11b,"[{'abstract': ""  Categories of polymorphic lenses in computer science, and of open games in\ncompositional game theory, have a curious structure that is reminiscent of\ncompact closed categories, but differs in some crucial ways. Specifically they\nhave a family of morphisms that behave like the counits of a compact closed\ncategory, but have no corresponding units; and they have a `partial' duality\nthat behaves like transposition in a compact closed category when it is\ndefined. We axiomatise this structure, which we refer to as a `teleological\ncategory'. We precisely define a diagrammatic language suitable for these\ncategories, and prove a coherence theorem for them. This underpins the use of\ndiagrammatic reasoning in compositional game theory, which has previously been\nused only informally.\n"", 'title': 'Coherence for lenses and open games'}]","Title: Coherence for lenses and open games

Abstract: Categories of polymorphic lenses in computer science, and of open games in compositional game theory, have a curious structure that is reminiscent of compact closed categories, but differs in some crucial ways. Specifically they have a family of morphisms that behave like the counits of a compact closed category, but have no corresponding units; and they have a `partial' duality that behaves like transposition in a compact closed category when it is defined. We axiomatise this structure, which we refer to as a `teleological category'. We precisely define a diagrammatic language suitable for these categories, and prove a coherence theorem for them. This underpins the use of diagrammatic reasoning in compositional game theory, which has previously been used only informally."
Find a chapter discussing the attentional sequence-to-sequence model in machine translation.,7ac18b32,"[{'abstract': '  Draft of textbook chapter on neural machine translation. a comprehensive\ntreatment of the topic, ranging from introduction to neural networks,\ncomputation graphs, description of the currently dominant attentional\nsequence-to-sequence model, recent refinements, alternative architectures and\nchallenges. Written as chapter for the textbook Statistical Machine\nTranslation. Used in the JHU Fall 2017 class on machine translation.\n', 'title': 'Neural Machine Translation'}]","I think I can help you with that!

Title: Neural Machine Translation

Abstract: Draft of textbook chapter on neural machine translation. a comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges. Written as chapter for the textbook Statistical Machine Translation. Used in the JHU Fall 2017 class on machine translation."
Can solutions to the KdV equation and the Nonlinear Schrodinger equation be combined to create new solutions?,6c97c1da,"[{'abstract': '  The KdV equation can be derived in the shallow water limit of the Euler\nequations. Over the last few decades, this equation has been extended to\ninclude higher order effects. Although this equation has only one conservation\nlaw, exact periodic and solitonic solutions exist. Khare and Saxena\n\\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact\nsolutions by combining known ones for several fundamental equations (e.g.,\nKorteweg - de Vries, Nonlinear Schrödinger). Here we find that this\nconstruction can be repeated for higher order, non-integrable extensions of\nthese equations. Contrary to many statements in the literature, there seems to\nbe no correlation between integrability and the number of nonlinear one\nvariable wave solutions.\n', 'title': 'Superposition solutions to the extended KdV equation for water surface waves'}]","Title: Superposition solutions to the extended KdV equation for water surface waves

Abstract: The KdV equation can be derived in the shallow water limit of the Euler
equations. Over the last few decades, this equation has been extended to
include higher order effects. Although this equation has only one conservation
law, exact periodic and solitonic solutions exist. Khare and Saxena
\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact
solutions by combining known ones for several fundamental equations (e.g.,
Korteweg - de Vries, Nonlinear Schrödinger). Here we find that this
construction can be repeated for higher order, non-integrable extensions of
these equations. Contrary to many statements in the literature, there seems to
be no correlation between integrability and the number of nonlinear one
variable wave solutions."
Which parameters are chosen as decision variables in the  Buoyant Airborne Turbine case study? ,9f89b2c8,"[{'abstract': '  We present a novel data-driven nested optimization framework that addresses\nthe problem of coupling between plant and controller optimization. This\noptimization strategy is tailored towards instances where a closed-form\nexpression for the system dynamic response is unobtainable and simulations or\nexperiments are necessary. Specifically, Bayesian Optimization, which is a\ndata-driven technique for finding the optimum of an unknown and\nexpensive-to-evaluate objective function, is employed to solve a nested\noptimization problem. The underlying objective function is modeled by a\nGaussian Process (GP); then, Bayesian Optimization utilizes the predictive\nuncertainty information from the GP to determine the best subsequent control or\nplant parameters. The proposed framework differs from the majority of co-design\nliterature where there exists a closed-form model of the system dynamics.\nFurthermore, we utilize the idea of Batch Bayesian Optimization at the plant\noptimization level to generate a set of plant designs at each iteration of the\noverall optimization process, recognizing that there will exist economies of\nscale in running multiple experiments in each iteration of the plant design\nprocess. We validate the proposed framework for a Buoyant Airborne Turbine\n(BAT). We choose the horizontal stabilizer area, longitudinal center of mass\nrelative to center of buoyancy (plant parameters), and the pitch angle\nset-point (controller parameter) as our decision variables. Our results\ndemonstrate that these plant and control parameters converge to their\nrespective optimal values within only a few iterations.\n', 'title': 'Economically Efficient Combined Plant and Controller Design Using Batch Bayesian Optimization: Mathematical Framework and Airborne Wind Energy Case Study'}]","Here is the answer to your question:

Title: Economically Efficient Combined Plant and Controller Design Using Batch Bayesian Optimization: Mathematical Framework and Airborne Wind Energy Case Study

Abstract: We present a novel data-driven nested optimization framework that addresses the problem of coupling between plant and controller optimization. This optimization strategy is tailored towards instances where a closed-form expression for the system dynamic response is unobtainable and simulations or experiments are necessary. Specifically, Bayesian Optimization, which is a data-driven technique for finding the optimum of an unknown and expensive-to-evaluate objective function, is employed to solve a nested optimization problem. The underlying objective function is modeled by a Gaussian Process (GP); then, Bayesian Optimization utilizes the predictive uncertainty information from the GP to determine the best subsequent control or plant parameters. The proposed framework differs from the majority of co-design literature where there exists a closed-form model of the system dynamics. Furthermore, we utilize the idea of Batch Bayesian Optimization at the plant optimization level to generate a set of plant designs at each iteration of the overall optimization process, recognizing that there will exist economies of scale in running multiple experiments in each iteration of the plant design process. We validate the proposed framework for a Buoyant Airborne Turbine (BAT). We choose the horizontal stabilizer area, longitudinal center of mass relative to center of buoyancy (plant parameters), and the pitch angle set-point (controller parameter) as our decision variables. Our results demonstrate that these plant and control parameters converge to their respective optimal values within only a few iterations.

The parameters chosen as decision variables in the Buoyant Airborne Turbine case study are:

1. Horizontal stabilizer area (plant parameter)
2. Longitudinal center of mass relative to center of buoyancy (plant parameter)
3. Pitch angle set-point (controller parameter)"
What is the name of the new algebraic technique that provides shorter proofs for these two theorems?,1aca6203,"[{'abstract': ""  This paper provides short proofs of two fundamental theorems of finite\nsemigroup theory whose previous proofs were significantly longer, namely the\ntwo-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike\ntheorem, using a new algebraic technique that we call the merge decomposition.\nA prototypical application of this technique decomposes a semigroup $T$ into a\ntwo-sided semidirect product whose components are built from two subsemigroups\n$T_1,T_2$, which together generate $T$, and the subsemigroup generated by their\nsetwise product $T_1T_2$. In this sense we decompose $T$ by merging the\nsubsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup\nhomomorphisms from free semigroups.\n"", 'title': 'Merge decompositions, two-sided Krohn-Rhodes, and aperiodic pointlikes'}]","Title: Merge decompositions, two-sided Krohn-Rhodes, and aperiodic pointlikes

Abstract: This paper provides short proofs of two fundamental theorems of finite semigroup theory whose previous proofs were significantly longer, namely the two-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike theorem, using a new algebraic technique that we call the merge decomposition. A prototypical application of this technique decomposes a semigroup $T$ into a two-sided semidirect product whose components are built from two subsemigroups $T_1,T_2$, which together generate $T$, and the subsemigroup generated by their setwise product $T_1T_2$. In this sense we decompose $T$ by merging the subsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup homomorphisms from free semigroups."
Name three research areas covered in Luke P. Lee's review of optofluidics.,2a67343e,"[{'abstract': '  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National\nUniversity of Singapore. In this contribution he describes the power of\noptofluidics as a research tool and reviews new insights within the areas of\nsingle cell analysis, microphysiological analysis, and integrated systems.\n', 'title': 'When Streams of Optofluidics Meet the Sea of Life'}]","Based on the context, I can help you with your question!

The three research areas covered in Luke P. Lee's review of optofluidics are:

1. Single cell analysis
2. Microphysiological analysis
3. Integrated systems"
What is the topological description of Gelfand-Cetlin fibers?,9b931a93,"[{'abstract': '  Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of\nGelfand-Cetlin systems over complex partial flag manifolds, we provide a\ncomplete description of the topology of Gelfand-Cetlin fibers. We prove that\nall fibers are \\emph{smooth} isotropic submanifolds and give a complete\ndescription of the fiber to be Lagrangian in terms of combinatorics of\nGelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian\nfibers. After a few combinatorial and numercal tests for the displaceability,\nusing the bulk-deformation of Floer cohomology by Schubert cycles, we prove\nthat every full flag manifold $\\mathcal{F}(n)$ ($n \\geq 3$) with a monotone\nKirillov-Kostant-Souriau symplectic form carries a continuum of\nnon-displaceable Lagrangian tori which degenerates to a non-torus fiber in the\nHausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\\mathcal{F}(3)$\nis non-displaceable the question of which was raised by Nohara-Ueda who\ncomputed its Floer cohomology to be vanishing.\n', 'title': 'Lagrangian fibers of Gelfand-Cetlin systems'}]","Based on the context, I think I have found the answer to your question.

Title: Lagrangian fibers of Gelfand-Cetlin systems

Abstract: Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of
Gelfand-Cetlin systems over complex partial flag manifolds, we provide a
complete description of the topology of Gelfand-Cetlin fibers. We prove that
all fibers are \emph{smooth} isotropic submanifolds and give a complete
description of the fiber to be Lagrangian in terms of combinatorics of
Gelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian
fibers. After a few combinatorial and numercal tests for the displaceability,
using the bulk-deformation of Floer cohomology by Schubert cycles, we prove
that every full flag manifold $\mathcal{F}(n)$ ($n \geq 3$) with a monotone
Kirillov-Kostant-Souriau symplectic form carries a continuum of
non-displaceable Lagrangian tori which degenerates to a non-torus fiber in the
Hausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\mathcal{F}(3)$
is non-displaceable the question of which was raised by Nohara-Ueda who
computed its Floer cohomology to be vanishing.

Let me know if this is what you were looking for!"
Are there any papers that explore applications of Hermite-Hadamard's inequalities to special means?,d590ac57,"[{'abstract': ""  We have recently established some integral inequalities for convex functions\nvia the Hermite-Hadamard's inequalities. In continuation here, we also\nestablish some interesting new integral inequalities for convex functions via\nthe Hermite--Hadamard's inequalities and Jensen's integral inequality. Useful\napplications involving special means are also included.\n"", 'title': 'New type integral inequalities for convex functions with applications II'}]","I think I can help you with that!

Title: New type integral inequalities for convex functions with applications II

Abstract: We have recently established some integral inequalities for convex functions via the Hermite-Hadamard's inequalities. In continuation here, we also establish some interesting new integral inequalities for convex functions via the Hermite--Hadamard's inequalities and Jensen's integral inequality. Useful applications involving special means are also included.

This paper seems to be exploring applications of Hermite-Hadamard's inequalities to special means, which is exactly what you're looking for!"
What type of convolutional layer is proposed in this paper to address the limitations of traditional CNNs for recommendation?,13456342,"[{'abstract': '  Convolutional Neural Networks (CNNs) have been recently introduced in the\ndomain of session-based next item recommendation. An ordered collection of past\nitems the user has interacted with in a session (or sequence) are embedded into\na 2-dimensional latent matrix, and treated as an image. The convolution and\npooling operations are then applied to the mapped item embeddings. In this\npaper, we first examine the typical session-based CNN recommender and show that\nboth the generative model and network architecture are suboptimal when modeling\nlong-range dependencies in the item sequence. To address the issues, we\nintroduce a simple, but very effective generative model that is capable of\nlearning high-level representation from both short- and long-range item\ndependencies. The network architecture of the proposed model is formed of a\nstack of \\emph{holed} convolutional layers, which can efficiently increase the\nreceptive fields without relying on the pooling operation. Another contribution\nis the effective use of residual block structure in recommender systems, which\ncan ease the optimization for much deeper networks. The proposed generative\nmodel attains state-of-the-art accuracy with less training time in the next\nitem recommendation task. It accordingly can be used as a powerful\nrecommendation baseline to beat in future, especially when there are long\nsequences of user feedback.\n', 'title': 'A Simple Convolutional Generative Network for Next Item Recommendation'}]","Title: A Simple Convolutional Generative Network for Next Item Recommendation

Abstract: Convolutional Neural Networks (CNNs) have been recently introduced in the domain of session-based next item recommendation. An ordered collection of past items the user has interacted with in a session (or sequence) are embedded into a 2-dimensional latent matrix, and treated as an image. The convolution and pooling operations are then applied to the mapped item embeddings. In this paper, we first examine the typical session-based CNN recommender and show that both the generative model and network architecture are suboptimal when modeling long-range dependencies in the item sequence. To address the issues, we introduce a simple, but very effective generative model that is capable of learning high-level representation from both short- and long-range item dependencies. The network architecture of the proposed model is formed of a stack of \emph{holed} convolutional layers, which can efficiently increase the receptive fields without relying on the pooling operation. Another contribution is the effective use of residual block structure in recommender systems, which can ease the optimization for much deeper networks. The proposed generative model attains state-of-the-art accuracy with less training time in the next item recommendation task. It accordingly can be used as a powerful recommendation baseline to beat in future, especially when there are long sequences of user feedback.

The type of convolutional layer proposed in this paper is a ""holed"" convolutional layer."
What kind of queries can be used with ElasticSearch like systems?,2f6fbf5e,"[{'abstract': '  Finding patterns in data and being able to retrieve information from those\npatterns is an important task in Information retrieval. Complex search\nrequirements which are not fulfilled by simple string matching and require\nexploring certain patterns in data demand a better query engine that can\nsupport searching via structured queries. In this article, we built a\nstructured query engine which supports searching data through structured\nqueries on the lines of ElasticSearch. We will show how we achieved real time\nindexing and retrieving of data through a RESTful API and how complex queries\ncan be created and processed using efficient data structures we created for\nstoring the data in structured way. Finally, we will conclude with an example\nof movie recommendation system built on top of this query engine.\n', 'title': 'Building a Structured Query Engine'}]","Based on the context, I think I have found the answer to your question.

Title: Building a Structured Query Engine

Abstract: Finding patterns in data and being able to retrieve information from those patterns is an important task in Information retrieval. Complex search requirements which are not fulfilled by simple string matching and require exploring certain patterns in data demand a better query engine that can support searching via structured queries. In this article, we built a structured query engine which supports searching data through structured queries on the lines of ElasticSearch. We will show how we achieved real time indexing and retrieving of data through a RESTful API and how complex queries can be created and processed using efficient data structures we created for storing the data in structured way. Finally, we will conclude with an example of movie recommendation system built on top of this query engine."
Can a quasi-plurisubharmonic function provide a solution to the Monge-Ampere equation on compact Hermitian manifolds?,66d91004,"[{'abstract': '  We show that a positive Borel measure of positive finite total mass, on\ncompact Hermitian manifolds, admits a Holder continuous quasi-plurisubharmonic\nsolution to the Monge-Ampere equation if and only if it is dominated locally by\nMonge-Ampere measures of Holder continuous plurisubharmonic functions.\n', 'title': 'Hölder continuous solutions of the Monge-Ampère equation on compact Hermitian manifolds'}]","Title: Hölder continuous solutions of the Monge-Ampère equation on compact Hermitian manifolds

Abstract: We show that a positive Borel measure of positive finite total mass, on compact Hermitian manifolds, admits a Hölder continuous quasi-plurisubharmonic solution to the Monge-Ampere equation if and only if it is dominated locally by Monge-Ampere measures of Hölder continuous plurisubharmonic functions."
What type of result categorizes   Lipschitz nonlocal minimal subgraphs outside a ball? ,264dc421,"[{'abstract': '  We show that nonlocal minimal cones which are non-singular subgraphs outside\nthe origin are necessarily halfspaces.\nThe proof is based on classical ideas of~\\cite{DG1} and on the computation of\nthe linearized nonlocal mean curvature operator, which is proved to satisfy a\nsuitable maximum principle.\nWith this, we obtain new, and somehow simpler, proofs of the Bernstein-type\nresults for nonlocal minimal surfaces which have been recently established\nin~\\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type\nresult which classifies Lipschitz nonlocal minimal subgraphs outside a ball.\n', 'title': 'Flatness results for nonlocal minimal cones and subgraphs'}]","Based on the context, I found that the result categorizing Lipschitz nonlocal minimal subgraphs outside a ball is a nonlocal Bernstein-Moser-type result.

Here is the relevant paper:

Title: Flatness results for nonlocal minimal cones and subgraphs

Abstract: We show that nonlocal minimal cones which are non-singular subgraphs outside the origin are necessarily halfspaces. The proof is based on classical ideas of ~\cite{DG1} and on the computation of the linearized nonlocal mean curvature operator, which is proved to satisfy a suitable maximum principle. With this, we obtain new, and somehow simpler, proofs of the Bernstein-type results for nonlocal minimal surfaces which have been recently established in ~\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type result which classifies Lipschitz nonlocal minimal subgraphs outside a ball."
How do motor-cargo and motor-motor interactions affect the stall force?,64291e32,"[{'abstract': '  We study a generic one-dimensional model for an intracellular cargo driven by\nN motor proteins against an external applied force. The model includes\nmotor-cargo and motor-motor interactions. The cargo motion is described by an\nover-damped Langevin equation, while motor dynamics is specified by hopping\nrates which follow a local detailed balance condition with respect to change in\nenergy per hopping event. Based on this model, we show that the stall force,\nthe mean external force corresponding to zero mean cargo velocity, is\ncompletely independent of the details of the interactions and is, therefore,\nalways equal to the sum of the stall forces of the individual motors. This\nexact result is arrived on the basis of a simple assumption: the (macroscopic)\nstate of stall of the cargo is analogous to a state of thermodynamic\nequilibrium, and is characterized by vanishing net probability current between\nany two microstates, with the latter specified by motor positions relative to\nthe cargo. The corresponding probability distribution of the microstates under\nstall is also determined. These predictions are in complete agreement with\nnumerical simulations, carried out using specific forms of interaction\npotentials.\n', 'title': 'Stall force of a cargo driven by N interacting motor proteins'}]","Title: Stall force of a cargo driven by N interacting motor proteins

Abstract: We study a generic one-dimensional model for an intracellular cargo driven by N motor proteins against an external applied force. The model includes motor-cargo and motor-motor interactions. The cargo motion is described by an over-damped Langevin equation, while motor dynamics is specified by hopping rates which follow a local detailed balance condition with respect to change in energy per hopping event. Based on this model, we show that the stall force, the mean external force corresponding to zero mean cargo velocity, is completely independent of the details of the interactions and is, therefore, always equal to the sum of the stall forces of the individual motors. This exact result is arrived on the basis of a simple assumption: the (macroscopic) state of stall of the cargo is analogous to a state of thermodynamic equilibrium, and is characterized by vanishing net probability current between any two microstates, with the latter specified by motor positions relative to the cargo. The corresponding probability distribution of the microstates under stall is also determined. These predictions are in complete agreement with numerical simulations, carried out using specific forms of interaction potentials."
What are the optimal parameters for a DA-PB based WPT system to maximize efficiency?,90303ad3,"[{'abstract': '  The extremely low efficiency is regarded as the bottleneck of Wireless Power\nTransfer (WPT) technology. To tackle this problem, either enlarging the\ntransfer power or changing the infrastructure of WPT system could be an\nintuitively proposed way. However, the drastically important issue on the user\nexposure of electromagnetic radiation is rarely considered while we try to\nimprove the efficiency of WPT. In this paper, a Distributed Antenna Power\nBeacon (DA-PB) based WPT system where these antennas are uniformly distributed\non a circle is analyzed and optimized with the safety electromagnetic radiation\nlevel (SERL) requirement. In this model, three key questions are intended to be\nanswered: 1) With the SERL, what is the performance of the harvested power at\nthe users ? 2) How do we configure the parameters to maximize the efficiency of\nWPT? 3) Under the same constraints, does the DA-PB still have performance gain\nthan the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of\nDA-PB is derived to make the radio frequency (RF) electromagnetic radiation\npower density at any location of the charging cell lower than the SERL\npublished by the Federal Communications Commission (FCC). Second, the\nclosed-form expressions of average harvested Direct Current (DC) power per user\nin the charging cell for pass-loss exponent 2 and 4 are also provided. In order\nto maximize the average efficiency of WPT, the optimal radii for distributed\nantennas elements (DAEs) are derived when the pass-loss exponent takes the\ntypical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a\nbenchmark. Simulation results verify our derived theoretical results. And it is\nshown that the proposed DA-PB indeed achieves larger average harvested DC power\nthan CA-PB and can improve the efficiency of WPT.\n', 'title': 'On the Deployment of Distributed Antennas for Wireless Power Transfer with Safety Electromagnetic Radiation Level Requirement'}]","I think I have the answer you're looking for!

Title: On the Deployment of Distributed Antennas for Wireless Power Transfer with Safety Electromagnetic Radiation Level Requirement

Abstract: ... (full abstract omitted for brevity)

According to this paper, the optimal radii for distributed antennas elements (DAEs) are derived when the pass-loss exponent takes the typical value of 2 and 4, in order to maximize the average efficiency of Wireless Power Transfer (WPT)."
How have researchers evaluated the performance of assortativity-based features for network classification tasks?,f916d301,"[{'abstract': ""  Several social, medical, engineering and biological challenges rely on\ndiscovering the functionality of networks from their structure and node\nmetadata, when it is available. For example, in chemoinformatics one might want\nto detect whether a molecule is toxic based on structure and atomic types, or\ndiscover the research field of a scientific collaboration network. Existing\ntechniques rely on counting or measuring structural patterns that are known to\nshow large variations from network to network, such as the number of triangles,\nor the assortativity of node metadata. We introduce the concept of multi-hop\nassortativity, that captures the similarity of the nodes situated at the\nextremities of a randomly selected path of a given length. We show that\nmulti-hop assortativity unifies various existing concepts and offers a\nversatile family of 'fingerprints' to characterize networks. These fingerprints\nallow in turn to recover the functionalities of a network, with the help of the\nmachine learning toolbox. Our method is evaluated empirically on established\nsocial and chemoinformatic network benchmarks. Results reveal that our\nassortativity based features are competitive providing highly accurate results\noften outperforming state of the art methods for the network classification\ntask.\n"", 'title': 'Multi-hop assortativities for networks classification'}]","Title: Multi-hop assortativities for networks classification

Abstract: Several social, medical, engineering and biological challenges rely on discovering the functionality of networks from their structure and node metadata, when it is available. For example, in chemoinformatics one might want to detect whether a molecule is toxic based on structure and atomic types, or discover the research field of a scientific collaboration network. Existing techniques rely on counting or measuring structural patterns that are known to show large variations from network to network, such as the number of triangles, or the assortativity of node metadata. We introduce the concept of multi-hop assortativity, that captures the similarity of the nodes situated at the extremities of a randomly selected path of a given length. We show that multi-hop assortativity unifies various existing concepts and offers a versatile family of 'fingerprints' to characterize networks. These fingerprints allow in turn to recover the functionalities of a network, with the help of the machine learning toolbox. Our method is evaluated empirically on established social and chemoinformatic network benchmarks. Results reveal that our assortativity based features are competitive providing highly accurate results often outperforming state of the art methods for the network classification task."
What is the relationship between Total Conductance and Path Integrated Gradients?,e9b9395e,"[{'abstract': '  The challenge of assigning importance to individual neurons in a network is\nof interest when interpreting deep learning models. In recent work, Dhamdhere\net al. proposed Total Conductance, a ""natural refinement of Integrated\nGradients"" for attributing importance to internal neurons. Unfortunately, the\nauthors found that calculating conductance in tensorflow required the addition\nof several custom gradient operators and did not scale well. In this work, we\nshow that the formula for Total Conductance is mathematically equivalent to\nPath Integrated Gradients computed on a hidden layer in the network. We provide\na scalable implementation of Total Conductance using standard tensorflow\ngradient operators that we call Neuron Integrated Gradients. We compare Neuron\nIntegrated Gradients to DeepLIFT, a pre-existing computationally efficient\napproach that is applicable to calculating internal neuron importance. We find\nthat DeepLIFT produces strong empirical results and is faster to compute, but\nbecause it lacks the theoretical properties of Neuron Integrated Gradients, it\nmay not always be preferred in practice. Colab notebook reproducing results:\nthis http URL\n', 'title': 'Computationally Efficient Measures of Internal Neuron Importance'}]","Title: Computationally Efficient Measures of Internal Neuron Importance

Abstract: The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a ""natural refinement of Integrated Gradients"" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: this http URL"
What are the implications of decidability of semantics and truth for formulas without nested common knowledge operators in a logic of gossips?,29944bff,"[{'abstract': '  Gossip protocols aim at arriving, by means of point-to-point or group\ncommunications, at a situation in which all the agents know each other secrets.\nRecently a number of authors studied distributed epistemic gossip protocols.\nThese protocols use as guards formulas from a simple epistemic logic, which\nmakes their analysis and verification substantially easier.\nWe study here common knowledge in the context of such a logic. First, we\nanalyze when it can be reduced to iterated knowledge. Then we show that the\nsemantics and truth for formulas without nested common knowledge operator are\ndecidable. This implies that implementability, partial correctness and\ntermination of distributed epistemic gossip protocols that use non-nested\ncommon knowledge operator is decidable, as well. Given that common knowledge is\nequivalent to an infinite conjunction of nested knowledge, these results are\nnon-trivial generalizations of the corresponding decidability results for the\noriginal epistemic logic, established in (Apt & Wojtczak, 2016).\nK. R. Apt & D. Wojtczak (2016): On Decidability of a Logic of Gossips. In\nProc. of JELIA 2016, pp. 18-33, doi:10.1007/ 978-3-319-48758-8_2.\n', 'title': 'Common Knowledge in a Logic of Gossips'}]","I think I can help you with that!

Title: Common Knowledge in a Logic of Gossips

Abstract: Gossip protocols aim at arriving, by means of point-to-point or group communications, at a situation in which all the agents know each other secrets. Recently a number of authors studied distributed epistemic gossip protocols. These protocols use as guards formulas from a simple epistemic logic, which makes their analysis and verification substantially easier. We study here common knowledge in the context of such a logic. First, we analyze when it can be reduced to iterated knowledge. Then we show that the semantics and truth for formulas without nested common knowledge operator are decidable. This implies that implementability, partial correctness and termination of distributed epistemic gossip protocols that use non-nested common knowledge operator is decidable, as well. Given that common knowledge is equivalent to an infinite conjunction of nested knowledge, these results are non-trivial generalizations of the corresponding decidability results for the original epistemic logic, established in (Apt & Wojtczak, 2016)."
"What three NLP tasks, involving both unsupervised and supervised settings, was the model evaluated on?",f304705a,"[{'abstract': '  We describe a neural network model that jointly learns distributed\nrepresentations of texts and knowledge base (KB) entities. Given a text in the\nKB, we train our proposed model to predict entities that are relevant to the\ntext. Our model is designed to be generic with the ability to address various\nNLP tasks with ease. We train the model using a large corpus of texts and their\nentity annotations extracted from Wikipedia. We evaluated the model on three\nimportant NLP tasks (i.e., sentence textual similarity, entity linking, and\nfactoid question answering) involving both unsupervised and supervised\nsettings. As a result, we achieved state-of-the-art results on all three of\nthese tasks. Our code and trained models are publicly available for further\nacademic research.\n', 'title': 'Learning Distributed Representations of Texts and Entities from Knowledge Base'}]","Title: Learning Distributed Representations of Texts and Entities from Knowledge Base

Abstract: We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research.

The answer is: sentence textual similarity, entity linking, and factoid question answering."
What are the limits of temperature measurement for interacting quantum systems?,ebaaaff9,"[{'abstract': '  We study the ultimate bounds on the estimation of temperature for an\ninteracting quantum system. We consider two coupled bosonic modes that are\nassumed to be thermal and using quantum estimation theory establish the role\nthe Hamiltonian parameters play in thermometry. We show that in the case of a\nconserved particle number the interaction between the modes leads to a decrease\nin the overall sensitivity to temperature, while interestingly, if particle\nexchange is allowed with the thermal bath the converse is true. We explain this\ndichotomy by examining the energy spectra. Finally, we devise experimentally\nimplementable thermometry schemes that rely only on locally accessible\ninformation from the total system, showing that almost Heisenberg limited\nprecision can still be achieved, and we address the (im)possibility for\nmultiparameter estimation in the system.\n', 'title': 'Global and local thermometry schemes in coupled quantum systems'}]","I think I can help you with that!

Title: Global and local thermometry schemes in coupled quantum systems

Abstract: We study the ultimate bounds on the estimation of temperature for an interacting quantum system. We consider two coupled bosonic modes that are assumed to be thermal and using quantum estimation theory establish the role the Hamiltonian parameters play in thermometry. We show that in the case of a conserved particle number the interaction between the modes leads to a decrease in the overall sensitivity to temperature, while interestingly, if particle exchange is allowed with the thermal bath the converse is true. We explain this dichotomy by examining the energy spectra. Finally, we devise experimentally implementable thermometry schemes that rely only on locally accessible information from the total system, showing that almost Heisenberg limited precision can still be achieved, and we address the (im)possibility for multiparameter estimation in the system."
A study investigates the educational benefits of lab courses designed to reinforce lecture content at various institutions. What were the key findings?,de76883b,"[{'abstract': '  Instructional labs are widely seen as a unique, albeit expensive, way to\nteach scientific content. We measured the effectiveness of introductory lab\ncourses at achieving this educational goal across nine different lab courses at\nthree very different institutions. These institutions and courses encompassed a\nbroad range of student populations and instructional styles. The nine courses\nstudied had two key things in common: the labs aimed to reinforce the content\npresented in lectures, and the labs were optional. By comparing the performance\nof students who did and did not take the labs (with careful normalization for\nselection effects), we found universally and precisely no added value to\nlearning from taking the labs as measured by course exam performance. This work\nshould motivate institutions and departments to reexamine the goals and conduct\nof their lab courses, given their resource-intensive nature. We show why these\nresults make sense when looking at the comparative mental processes of students\ninvolved in research and instructional labs, and offer alternative goals and\ninstructional approaches that would make lab courses more educationally\nvaluable.\n', 'title': 'Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content'}]","Title: Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content

Abstract:   Instructional labs are widely seen as a unique, albeit expensive, way to
teach scientific content. We measured the effectiveness of introductory lab
courses at achieving this educational goal across nine different lab courses at
three very different institutions. These institutions and courses encompassed a
broad range of student populations and instructional styles. The nine courses
studied had two key things in common: the labs aimed to reinforce the content
presented in lectures, and the labs were optional. By comparing the performance
of students who did and did not take the labs (with careful normalization for
selection effects), we found universally and precisely no added value to
learning from taking the labs as measured by course exam performance. This work
should motivate institutions and departments to reexamine the goals and conduct
of their lab courses, given their resource-intensive nature. We show why these
results make sense when looking at the comparative mental processes of students
involved in research and instructional labs, and offer alternative goals and
instructional approaches that would make lab courses more educationally
valuable."
What are the main contributors to changes in pore pressure and stress near hydraulic fractures according to this study?,d568929c,"[{'abstract': '  We present a systematic global sensitivity analysis using the Sobol method\nwhich can be utilized to rank the variables that affect two quantity of\ninterests -- pore pressure depletion and stress change -- around a\nhydraulically-fractured horizontal well based on their degree of importance.\nThese variables include rock properties and stimulation design variables. A\nfully-coupled poroelastic hydraulic fracture model is used to account for pore\npressure and stress changes due to production. To ease the computational cost\nof a simulator, we also provide reduced order models (ROMs), which can be used\nto replace the complex numerical model with a rather simple analytical model,\nfor calculating the pore pressure and stresses at different locations around\nhydraulic fractures. The main findings of this research are: (i) mobility,\nproduction pressure, and fracture half-length are the main contributors to the\nchanges in the quantities of interest. The percentage of the contribution of\neach parameter depends on the location with respect to pre-existing hydraulic\nfractures and the quantity of interest. (ii) As the time progresses, the effect\nof mobility decreases and the effect of production pressure increases. (iii)\nThese two variables are also dominant for horizontal stresses at large\ndistances from hydraulic fractures. (iv) At zones close to hydraulic fracture\ntips or inside the spacing area, other parameters such as fracture spacing and\nhalf-length are the dominant factors that affect the minimum horizontal stress.\nThe results of this study will provide useful guidelines for the stimulation\ndesign of legacy wells and secondary operations such as refracturing and infill\ndrilling.\n', 'title': 'A global sensitivity analysis and reduced order models for hydraulically-fractured horizontal wells'}]","The answer to your question can be found in the first paper:

Title: A global sensitivity analysis and reduced order models for hydraulically-fractured horizontal wells

Abstract: We present a systematic global sensitivity analysis using the Sobol method which can be utilized to rank the variables that affect two quantity of interests -- pore pressure depletion and stress change -- around a hydraulically-fractured horizontal well based on their degree of importance. These variables include rock properties and stimulation design variables. A fully-coupled poroelastic hydraulic fracture model is used to account for pore pressure and stress changes due to production. To ease the computational cost of a simulator, we also provide reduced order models (ROMs), which can be used to replace the complex numerical model with a rather simple analytical model, for calculating the pore pressure and stresses at different locations around hydraulic fractures. The main findings of this research are: (i) mobility, production pressure, and fracture half-length are the main contributors to the changes in the quantities of interest. The percentage of the contribution of each parameter depends on the location with respect to pre-existing hydraulic fractures and the quantity of interest. (ii) As the time progresses, the effect of mobility decreases and the effect of production pressure increases. (iii) These two variables are also dominant for horizontal stresses at large distances from hydraulic fractures. (iv) At zones close to hydraulic fracture tips or inside the spacing area, other parameters such as fracture spacing and half-length are the dominant factors that affect the minimum horizontal stress. The results of this study will provide useful guidelines for the stimulation design of legacy wells and secondary operations such as refracturing and infill drilling.

According to this study, the main contributors to changes in pore pressure and stress near hydraulic fractures are mobility, production pressure, and fracture half-length."
What type of random infection model is used in the paper focused on permutation tests for infection graphs?,9fb280d5,"[{'abstract': '  We formulate and analyze a novel hypothesis testing problem for inferring the\nedge structure of an infection graph. In our model, a disease spreads over a\nnetwork via contagion or random infection, where the random variables governing\nthe rates of contracting the disease from neighbors or random infection are\nindependent exponential random variables with unknown rate parameters. A subset\nof nodes is also censored uniformly at random. Given the statuses of nodes in\nthe network, the goal is to determine the underlying graph. We present a\nprocedure based on permutation testing, and we derive sufficient conditions for\nthe validity of our test in terms of automorphism groups of the graphs\ncorresponding to the null and alternative hypotheses. Further, the test is\nvalid more generally for infection processes satisfying a basic symmetry\ncondition. Our test is easy to compute and does not involve estimating unknown\nparameters governing the process. We also derive risk bounds for our\npermutation test in a variety of settings, and motivate our test statistic in\nterms of approximate equivalence to likelihood ratio testing and maximin tests.\nWe conclude with an application to real data from an HIV infection network.\n', 'title': 'Permutation Tests for Infection Graphs'}]","Title: Permutation Tests for Infection Graphs

Abstract: We formulate and analyze a novel hypothesis testing problem for inferring the edge structure of an infection graph. In our model, a disease spreads over a network via contagion or random infection, where the random variables governing the rates of contracting the disease from neighbors or random infection are independent exponential random variables with unknown rate parameters. A subset of nodes is also censored uniformly at random. Given the statuses of nodes in the network, the goal is to determine the underlying graph. We present a procedure based on permutation testing, and we derive sufficient conditions for the validity of our test in terms of automorphism groups of the graphs corresponding to the null and alternative hypotheses. Further, the test is valid more generally for infection processes satisfying a basic symmetry condition. Our test is easy to compute and does not involve estimating unknown parameters governing the process. We also derive risk bounds for our permutation test in a variety of settings, and motivate our test statistic in terms of approximate equivalence to likelihood ratio testing and maximin tests. We conclude with an application to real data from an HIV infection network.

The type of random infection model used is one where the random variables governing the rates of contracting the disease from neighbors or random infection are independent exponential random variables with unknown rate parameters."
What is the relationship between the dimension of the joint space and a kinematic map?,73ad209e,"[{'abstract': '  In this paper we combine a survey of the most important topological\nproperties of kinematic maps that appear in robotics, with the exposition of\nsome basic results regarding the topological complexity of a map. In\nparticular, we discuss mechanical devices that consist of rigid parts connected\nby joints and show how the geometry of the joints determines the forward\nkinematic map that relates the configuration of joints with the pose of the\nend-effector of the device. We explain how to compute the dimension of the\njoint space and describe topological obstructions for a kinematic map to be a\nfibration or to admit a continuous section. In the second part of the paper we\ndefine the complexity of a continuous map and show how the concept can be\nviewed as a measure of the difficulty to find a robust manipulation plan for a\ngiven mechanical device. We also derive some basic estimates for the complexity\nand relate it to the degree of instability of a manipulation plan.\n', 'title': ""A Topologist's View of Kinematic Maps and Manipulation Complexity""}]","Based on the provided context, I think I can help you with your question.

The relationship between the dimension of the joint space and a kinematic map is discussed in the paper:

Title: A Topologist's View of Kinematic Maps and Manipulation Complexity

Abstract: In this paper we combine a survey of the most important topological properties of kinematic maps that appear in robotics, with the exposition of some basic results regarding the topological complexity of a map. In particular, we discuss mechanical devices that consist of rigid parts connected by joints and show how the geometry of the joints determines the forward kinematic map that relates the configuration of joints with the pose of the end-effector of the device. We explain how to compute the dimension of the joint space and describe topological obstructions for a kinematic map to be a fibration or to admit a continuous section. In the second part of the paper we define the complexity of a continuous map and show how the concept can be viewed as a measure of the difficulty to find a robust manipulation plan for a given mechanical device. We also derive some basic estimates for the complexity and relate it to the degree of instability of a manipulation plan."
Does the Lefschetz duality theorem impose any restrictions on the coefficient ring?,d3822449,"[{'abstract': '  We prove the Lefschetz duality for intersection (co)homology in the framework\nof $\\partial$-pesudomanifolds. We work with general perversities and without\nrestriction on the coefficient ring.\n', 'title': 'Lefschetz duality for intersection (co)homology'}]","The answer to your question can be found in the first abstract.

Title: Lefschetz duality for intersection (co)homology

Abstract: We prove the Lefschetz duality for intersection (co)homology in the framework of $\partial$-pesudomanifolds. We work with general perversities and without restriction on the coefficient ring.

According to this abstract, the answer is no, the Lefschetz duality theorem does not impose any restrictions on the coefficient ring."
What type of estimates are used to prove the existence of the Calabi flow?,a2c4eaac,"[{'abstract': '  In this paper, we prove that there exists a dimensional constant $\\delta > 0$\nsuch that given any background Kähler metric $\\omega$, the Calabi flow with\ninitial data $u_0$ satisfying \\begin{equation*} \\partial \\bar \\partial u_0 \\in\nL^\\infty (M) \\text{ and } (1- \\delta )\\omega < \\omega_{u_0} < (1+\\delta\n)\\omega, \\end{equation*} admits a unique short time solution and it becomes\nsmooth immediately, where $\\omega_{u_0} : = \\omega +\\sqrt{-1}\\partial\n\\bar\\partial u_0$. The existence time depends on initial data $u_0$ and the\nmetric $\\omega$. As a corollary, we get that Calabi flow has short time\nexistence for any initial data satisfying \\begin{equation*} \\partial \\bar\n\\partial u_0 \\in C^0(M) \\text{ and } \\omega_{u_0} > 0, \\end{equation*} which\nshould be interpreted as a ""continuous Kähler metric"". A main technical\ningredient is Schauder-type estimates for biharmonic heat equation on\nRiemannian manifolds with time weighted Hölder norms.\n', 'title': 'The Calabi flow with rough initial data'}]","Title: The Calabi flow with rough initial data

Abstract:   In this paper, we prove that there exists a dimensional constant $\delta > 0$
such that given any background Kähler metric $\omega$, the Calabi flow with
initial data $u_0$ satisfying \begin{equation*} \partial \bar \partial u_0 \in
L^\infty (M) \text{ and } (1- \delta )\omega < \omega_{u_0} < (1+\delta
)\omega, \end{equation*} admits a unique short time solution and it becomes
smooth immediately, where $\omega_{u_0} : = \omega +\sqrt{-1}\partial
\bar\partial u_0$. The existence time depends on initial data $u_0$ and the
metric $\omega$. As a corollary, we get that Calabi flow has short time
existence for any initial data satisfying \begin{equation*} \partial \bar
\partial u_0 \in C^0(M) \text{ and } \omega_{u_0} > 0, \end{equation*} which
should be interpreted as a ""continuous Kähler metric"". A main technical
ingredient is Schauder-type estimates for biharmonic heat equation on
Riemannian manifolds with time weighted Hölder norms.

The estimates used to prove the existence of the Calabi flow are Schauder-type estimates for biharmonic heat equation on Riemannian manifolds with time weighted Hölder norms."
What are the computational trade-offs associated with using different contact models in trajectory optimization? ,7875ff89,"[{'abstract': '  In this paper, we analyze the effects of contact models on contact-implicit\ntrajectory optimization for manipulation. We consider three different\napproaches: (1) a contact model that is based on complementarity constraints,\n(2) a smooth contact model, and our proposed method (3) a variable smooth\ncontact model. We compare these models in simulation in terms of physical\naccuracy, quality of motions, and computation time. In each case, the\noptimization process is initialized by setting all torque variables to zero,\nnamely, without a meaningful initial guess. For simulations, we consider a\npushing task with varying complexity for a 7 degrees-of-freedom robot arm. Our\nresults demonstrate that the optimization based on the proposed variable smooth\ncontact model provides a good trade-off between the physical fidelity and\nquality of motions at the cost of increased computation time.\n', 'title': 'A Comparative Analysis of Contact Models in Trajectory Optimization for Manipulation'}]","Title: A Comparative Analysis of Contact Models in Trajectory Optimization for Manipulation

Abstract: In this paper, we analyze the effects of contact models on contact-implicit trajectory optimization for manipulation. We consider three different approaches: (1) a contact model that is based on complementarity constraints, (2) a smooth contact model, and our proposed method (3) a variable smooth contact model. We compare these models in simulation in terms of physical accuracy, quality of motions, and computation time. In each case, the optimization process is initialized by setting all torque variables to zero, namely, without a meaningful initial guess. For simulations, we consider a pushing task with varying complexity for a 7 degrees-of-freedom robot arm. Our results demonstrate that the optimization based on the proposed variable smooth contact model provides a good trade-off between the physical fidelity and quality of motions at the cost of increased computation time."
Can an h-process representation be used to obtain contraction estimates for filtering distributions conditioned on observations? ,51257d6a,"[{'abstract': '  For a class of partially observed diffusions, sufficient conditions are given\nfor the map from initial condition of the signal to filtering distribution to\nbe contractive with respect to Wasserstein distances, with rate which has no\ndependence on the dimension of the state-space and is stable under tensor\nproducts of the model. The main assumptions are that the signal has affine\ndrift and constant diffusion coefficient, and that the likelihood functions are\nlog-concave. Contraction estimates are obtained from an $h$-process\nrepresentation of the transition probabilities of the signal reweighted so as\nto condition on the observations.\n', 'title': 'Dimension-free Wasserstein contraction of nonlinear filters'}]","I think I can help you with that!

Title: Dimension-free Wasserstein contraction of nonlinear filters

Abstract: For a class of partially observed diffusions, sufficient conditions are given for the map from initial condition of the signal to filtering distribution to be contractive with respect to Wasserstein distances, with rate which has no dependence on the dimension of the state-space and is stable under tensor products of the model. The main assumptions are that the signal has affine drift and constant diffusion coefficient, and that the likelihood functions are log-concave. Contraction estimates are obtained from an $h$-process representation of the transition probabilities of the signal reweighted so as to condition on the observations."
What programming model did Cray and the University of Minnesota collaborate on for a new astrophysical code?,8e341192,"[{'abstract': '  We present a new code for astrophysical magneto-hydrodynamics specifically\ndesigned and optimized for high performance and scaling on modern and future\nsupercomputers. We describe a novel hybrid OpenMP/MPI programming model that\nemerged from a collaboration between Cray, Inc. and the University of\nMinnesota. This design utilizes MPI-RMA optimized for thread scaling, which\nallows the code to run extremely efficiently at very high thread counts ideal\nfor the latest generation of the multi-core and many-core architectures. Such\nperformance characteristics are needed in the era of ""exascale"" computing. We\ndescribe and demonstrate our high-performance design in detail with the intent\nthat it may be used as a model for other, future astrophysical codes intended\nfor applications demanding exceptional performance.\n', 'title': 'WOMBAT: A Scalable and High Performance Astrophysical MHD Code'}]","Here is the answer to your question:

Title: WOMBAT: A Scalable and High Performance Astrophysical MHD Code

Abstract: We present a new code for astrophysical magneto-hydrodynamics specifically designed and optimized for high performance and scaling on modern and future supercomputers. We describe a novel hybrid OpenMP/MPI programming model that emerged from a collaboration between Cray, Inc. and the University of Minnesota. This design utilizes MPI-RMA optimized for thread scaling, which allows the code to run extremely efficiently at very high thread counts ideal for the latest generation of the multi-core and many-core architectures. Such performance characteristics are needed in the era of ""exascale"" computing. We describe and demonstrate our high-performance design in detail with the intent that it may be used as a model for other, future astrophysical codes intended for applications demanding exceptional performance."
What type of  calculations are these new methods particularly efficient for on modern supercomputers?,65c50b33,"[{'abstract': '  Motivated by the recently proposed parallel orbital-updating approach in real\nspace method, we propose a parallel orbital-updating based plane-wave basis\nmethod for electronic structure calculations, for solving the corresponding\neigenvalue problems. In addition, we propose two new modified parallel\norbital-updating methods. Compared to the traditional plane-wave methods, our\nmethods allow for two-level parallelization, which is particularly interesting\nfor large scale parallelization. Numerical experiments show that these new\nmethods are more reliable and efficient for large scale calculations on modern\nsupercomputers\n', 'title': 'A parallel orbital-updating based plane-wave basis method for electronic structure calculations'}]","Title: A parallel orbital-updating based plane-wave basis method for electronic structure calculations

Abstract: Motivated by the recently proposed parallel orbital-updating approach in real space method, we propose a parallel orbital-updating based plane-wave basis method for electronic structure calculations, for solving the corresponding eigenvalue problems. In addition, we propose two new modified parallel orbital-updating methods. Compared to the traditional plane-wave methods, our methods allow for two-level parallelization, which is particularly interesting for large scale parallelization. Numerical experiments show that these new methods are more reliable and efficient for large scale calculations on modern supercomputers."
What are some effective algorithms for selecting influential nodes in online social networks with minimal cost?,7dd015ff,"[{'abstract': '  We study the min-cost seed selection problem in online social networks, where\nthe goal is to select a set of seed nodes with the minimum total cost such that\nthe expected number of influenced nodes in the network exceeds a predefined\nthreshold. We propose several algorithms that outperform the previous studies\nboth on the theoretical approximation ratios and on the experimental\nperformance. Under the case where the nodes have heterogeneous costs, our\nalgorithms are the first bi- criteria approximation algorithms with polynomial\nrunning time and provable logarithmic performance bounds using a general\ncontagion model. Under the case where the users have uniform costs, our\nalgorithms achieve logarithmic approximation ratio and provable time complexity\nwhich is smaller than that of existing algorithms in orders of magnitude. We\nconduct extensive experiments using real social networks. The experimental\nresults show that, our algorithms significantly outperform the existing\nalgorithms both on the total cost and on the running time, and also scale well\nto billion-scale networks.\n', 'title': 'Cost-Effective Seed Selection in Online Social Networks'}]","I think I can help you with that!

Title: Cost-Effective Seed Selection in Online Social Networks

Abstract: We study the min-cost seed selection problem in online social networks, where the goal is to select a set of seed nodes with the minimum total cost such that the expected number of influenced nodes in the network exceeds a predefined threshold. We propose several algorithms that outperform the previous studies both on the theoretical approximation ratios and on the experimental performance. Under the case where the nodes have heterogeneous costs, our algorithms are the first bi-criteria approximation algorithms with polynomial running time and provable logarithmic performance bounds using a general contagion model. Under the case where the users have uniform costs, our algorithms achieve logarithmic approximation ratio and provable time complexity which is smaller than that of existing algorithms in orders of magnitude. We conduct extensive experiments using real social networks. The experimental results show that, our algorithms significantly outperform the existing algorithms both on the total cost and on the running time, and also scale well to billion-scale networks."
primitive element,691fd7f8,"[{'abstract': '  With $\\Fq$ the finite field of $q$ elements, we investigate the following\nquestion. If $\\gamma$ generates $\\Fqn$ over $\\Fq$ and $\\beta$ is a non-zero\nelement of $\\Fqn$, is there always an $a \\in \\Fq$ such that $\\beta(\\gamma + a)$\nis a primitive element? We resolve this case when $n=3$, thereby proving a\nconjecture by Cohen. We also improve substantially on what is known when $n=4$.\n', 'title': 'Existence results for primitive elements in cubic and quartic extensions of a finite field'}]","I think I can help you with that!

Title: Existence results for primitive elements in cubic and quartic extensions of a finite field

Abstract: With $\Fq$ the finite field of $q$ elements, we investigate the following question. If $\gamma$ generates $\Fqn$ over $\Fq$ and $\beta$ is a non-zero element of $\Fqn$, is there always an $a \in \Fq$ such that $\beta(\gamma + a)$ is a primitive element? We resolve this case when $n=3$, thereby proving a conjecture by Cohen. We also improve substantially on what is known when $n=4$."
How is the radio frequency (RF) electromagnetic radiation power density calculated in a DA-PB based WPT system?,90303ad3,"[{'abstract': '  The extremely low efficiency is regarded as the bottleneck of Wireless Power\nTransfer (WPT) technology. To tackle this problem, either enlarging the\ntransfer power or changing the infrastructure of WPT system could be an\nintuitively proposed way. However, the drastically important issue on the user\nexposure of electromagnetic radiation is rarely considered while we try to\nimprove the efficiency of WPT. In this paper, a Distributed Antenna Power\nBeacon (DA-PB) based WPT system where these antennas are uniformly distributed\non a circle is analyzed and optimized with the safety electromagnetic radiation\nlevel (SERL) requirement. In this model, three key questions are intended to be\nanswered: 1) With the SERL, what is the performance of the harvested power at\nthe users ? 2) How do we configure the parameters to maximize the efficiency of\nWPT? 3) Under the same constraints, does the DA-PB still have performance gain\nthan the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of\nDA-PB is derived to make the radio frequency (RF) electromagnetic radiation\npower density at any location of the charging cell lower than the SERL\npublished by the Federal Communications Commission (FCC). Second, the\nclosed-form expressions of average harvested Direct Current (DC) power per user\nin the charging cell for pass-loss exponent 2 and 4 are also provided. In order\nto maximize the average efficiency of WPT, the optimal radii for distributed\nantennas elements (DAEs) are derived when the pass-loss exponent takes the\ntypical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a\nbenchmark. Simulation results verify our derived theoretical results. And it is\nshown that the proposed DA-PB indeed achieves larger average harvested DC power\nthan CA-PB and can improve the efficiency of WPT.\n', 'title': 'On the Deployment of Distributed Antennas for Wireless Power Transfer with Safety Electromagnetic Radiation Level Requirement'}]","Title: On the Deployment of Distributed Antennas for Wireless Power Transfer with Safety Electromagnetic Radiation Level Requirement

Abstract: ... 

The minimum antenna height of DA-PB is derived to make the radio frequency (RF) electromagnetic radiation power density at any location of the charging cell lower than the SERL published by the Federal Communications Commission (FCC).

However, the exact calculation method of RF electromagnetic radiation power density is not provided in this abstract. If you are looking for the calculation method, you might need to refer to the full paper or other related resources."
What type of model is used to address the uncertainty of positive instance labels in multiple instance learning?,21d71280,"[{'abstract': '  We describe a novel weakly supervised deep learning framework that combines\nboth the discriminative and generative models to learn meaningful\nrepresentation in the multiple instance learning (MIL) setting. MIL is a weakly\nsupervised learning problem where labels are associated with groups of\ninstances (referred as bags) instead of individual instances. To address the\nessential challenge in MIL problems raised from the uncertainty of positive\ninstances label, we use a discriminative model regularized by variational\nautoencoders (VAEs) to maximize the differences between latent representations\nof all instances and negative instances. As a result, the hidden layer of the\nvariational autoencoder learns meaningful representation. This representation\ncan effectively be used for MIL problems as illustrated by better performance\non the standard benchmark datasets comparing to the state-of-the-art\napproaches. More importantly, unlike most related studies, the proposed\nframework can be easily scaled to large dataset problems, as illustrated by the\naudio event detection and segmentation task. Visualization also confirms the\neffectiveness of the latent representation in discriminating positive and\nnegative classes.\n', 'title': 'Deep Multiple Instance Feature Learning via Variational Autoencoder'}]","Title: Deep Multiple Instance Feature Learning via Variational Autoencoder

Abstract: We describe a novel weakly supervised deep learning framework that combines both the discriminative and generative models to learn meaningful representation in the multiple instance learning (MIL) setting. MIL is a weakly supervised learning problem where labels are associated with groups of instances (referred as bags) instead of individual instances. To address the essential challenge in MIL problems raised from the uncertainty of positive instances label, we use a discriminative model regularized by variational autoencoders (VAEs) to maximize the differences between latent representations of all instances and negative instances. As a result, the hidden layer of the variational autoencoder learns meaningful representation. This representation can effectively be used for MIL problems as illustrated by better performance on the standard benchmark datasets comparing to the state-of-the-art approaches. More importantly, unlike most related studies, the proposed framework can be easily scaled to large dataset problems, as illustrated by the audio event detection and segmentation task. Visualization also confirms the effectiveness of the latent representation in discriminating positive and negative classes.

The model used to address the uncertainty of positive instance labels in multiple instance learning is a Variational Autoencoder (VAE)."
What type of model is used to extract text features from natural language descriptions of people?,e1b37929,"[{'abstract': '  Following the recent progress in image classification and captioning using\ndeep learning, we develop a novel natural language person retrieval system\nbased on an attention mechanism. More specifically, given the description of a\nperson, the goal is to localize the person in an image. To this end, we first\nconstruct a benchmark dataset for natural language person retrieval. To do so,\nwe generate bounding boxes for persons in a public image dataset from the\nsegmentation masks, which are then annotated with descriptions and attributes\nusing the Amazon Mechanical Turk. We then adopt a region proposal network in\nFaster R-CNN as a candidate region generator. The cropped images based on the\nregion proposals as well as the whole images with attention weights are fed\ninto Convolutional Neural Networks for visual feature extraction, while the\nnatural language expression and attributes are input to Bidirectional Long\nShort- Term Memory (BLSTM) models for text feature extraction. The visual and\ntext features are integrated to score region proposals, and the one with the\nhighest score is retrieved as the output of our system. The experimental\nresults show significant improvement over the state-of-the-art method for\ngeneric object retrieval and this line of research promises to benefit search\nin surveillance video footage.\n', 'title': 'Attention-based Natural Language Person Retrieval'}]","Title: Attention-based Natural Language Person Retrieval

Abstract: Following the recent progress in image classification and captioning using deep learning, we develop a novel natural language person retrieval system based on an attention mechanism. More specifically, given the description of a person, the goal is to localize the person in an image. To this end, we first construct a benchmark dataset for natural language person retrieval. To do so, we generate bounding boxes for persons in a public image dataset from the segmentation masks, which are then annotated with descriptions and attributes using the Amazon Mechanical Turk. We then adopt a region proposal network in Faster R-CNN as a candidate region generator. The cropped images based on the region proposals as well as the whole images with attention weights are fed into Convolutional Neural Networks for visual feature extraction, while the natural language expression and attributes are input to Bidirectional Long Short-Term Memory (BLSTM) models for text feature extraction. The visual and text features are integrated to score region proposals, and the one with the highest score is retrieved as the output of our system. The experimental results show significant improvement over the state-of-the-art method for generic object retrieval and this line of research promises to benefit search in surveillance video footage."
Can you find research on applying deformation theory to pseudorepresentations?,cfff3c6c,"[{'abstract': ""  Given a property of representations satisfying a basic stability condition,\nRamakrishna developed a variant of Mazur's Galois deformation theory for\nrepresentations with that property. We introduce an axiomatic definition of\npseudorepresentations with such a property. Among other things, we show that\npseudorepresentations with a property enjoy a good deformation theory,\ngeneralizing Ramakrishna's theory to pseudorepresentations.\n"", 'title': 'Deformation conditions for pseudorepresentations'}]","I think I can help you with that!

Title: Deformation conditions for pseudorepresentations

Abstract: Given a property of representations satisfying a basic stability condition, Ramakrishna developed a variant of Mazur's Galois deformation theory for representations with that property. We introduce an axiomatic definition of pseudorepresentations with such a property. Among other things, we show that pseudorepresentations with a property enjoy a good deformation theory, generalizing Ramakrishna's theory to pseudorepresentations."
In what kind of quantum system does the smallest eigenvalue of the fixed-trace ensemble carry information about entanglement? ,8f701419,"[{'abstract': ""  The statistical behaviour of the smallest eigenvalue has important\nimplications for systems which can be modeled using a Wishart-Laguerre\nensemble, the regular one or the fixed trace one. For example, the density of\nthe smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role\nin characterizing multiple channel telecommunication systems. Similarly, in the\nquantum entanglement problem, the smallest eigenvalue of the fixed trace\nensemble carries information regarding the nature of entanglement.\nFor real Wishart-Laguerre matrices, there exists an elegant recurrence scheme\nsuggested by Edelman to directly obtain the exact expression for the smallest\neigenvalue density. In the case of complex Wishart-Laguerre matrices, for\nfinding exact and explicit expressions for the smallest eigenvalue density,\nexisting results based on determinants become impractical when the determinants\ninvolve large-size matrices. In this work, we derive a recurrence scheme for\nthe complex case which is analogous to that of Edelman's for the real case.\nThis is used to obtain exact results for the smallest eigenvalue density for\nboth the regular, and the fixed trace complex Wishart-Laguerre ensembles. We\nvalidate our analytical results using Monte Carlo simulations. We also study\nscaled Wishart-Laguerre ensemble and investigate its efficacy in approximating\nthe fixed-trace ensemble. Eventually, we apply our result for the fixed-trace\nensemble to investigate the behaviour of the smallest eigenvalue in the\nparadigmatic system of coupled kicked tops.\n"", 'title': 'Smallest eigenvalue density for regular or fixed-trace complex Wishart-Laguerre ensemble and entanglement in coupled kicked tops'}]","Title: Smallest eigenvalue density for regular or fixed-trace complex Wishart-Laguerre ensemble and entanglement in coupled kicked tops

Abstract: The statistical behaviour of the smallest eigenvalue has important implications for systems which can be modeled using a Wishart-Laguerre ensemble, the regular one or the fixed trace one. For example, the density of the smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role in characterizing multiple channel telecommunication systems. Similarly, in the quantum entanglement problem, the smallest eigenvalue of the fixed trace ensemble carries information regarding the nature of entanglement."
What parallel is drawn between a coin flip in zero gravity and the Stern-Gerlach experiment?,3064a54c,"[{'abstract': '  Playing the game of heads or tails in zero gravity demonstrates that there\nexists a contextual ""measurement"" in classical mechanics. When the coin is\nflipped, its orientation is a continuous variable. However, the ""measurement""\nthat occurs when the coin is caught by clapping two hands together gives a\ndiscrete value (heads or tails) that depends on the context (orientation of the\nhands). It is then shown that there is a strong analogy with the spin\nmeasurement of the Stern-Gerlach experiment, and in particular with Stern and\nGerlach\'s sequential measurements. Finally, we clarify the analogy by recalling\nhow the de Broglie-Bohm interpretation simply explains the spin ""measurement"".\n', 'title': 'Heads or tails in zero gravity: an example of a classical contextual ""measurement""'}]","I'd be happy to help you with that question.

Title: Heads or tails in zero gravity: an example of a classical contextual ""measurement""

Abstract:  Playing the game of heads or tails in zero gravity demonstrates that there exists a contextual ""measurement"" in classical mechanics. When the coin is flipped, its orientation is a continuous variable. However, the ""measurement"" that occurs when the coin is caught by clapping two hands together gives a discrete value (heads or tails) that depends on the context (orientation of the hands). It is then shown that there is a strong analogy with the spin measurement of the Stern-Gerlach experiment, and in particular with Stern and Gerlach's sequential measurements. Finally, we clarify the analogy by recalling how the de Broglie-Bohm interpretation simply explains the spin ""measurement"".

So, the parallel drawn between a coin flip in zero gravity and the Stern-Gerlach experiment is a strong analogy with the spin measurement of the Stern-Gerlach experiment, particularly with Stern and Gerlach's sequential measurements."
For what type of algebraic variety is an efficient method for calculating Frobenius action representation matrices proposed?,275af301,"[{'abstract': '  This paper is concerned with the computation of representation matrices for\nthe action of Frobenius to the cohomology groups of algebraic varieties.\nSpecifically we shall give an algorithm to compute the matrices for arbitrary\nalgebraic varieties with defining equations over perfect fields of positive\ncharacteristic, and estimate its complexity. Moreover, we propose a specific\nefficient method, which works for complete intersections.\n', 'title': 'Computing representation matrices for the action of Frobenius to cohomology groups'}]","Title: Computing representation matrices for the action of Frobenius to cohomology groups

Abstract: This paper is concerned with the computation of representation matrices for the action of Frobenius to the cohomology groups of algebraic varieties. Specifically we shall give an algorithm to compute the matrices for arbitrary algebraic varieties with defining equations over perfect fields of positive characteristic, and estimate its complexity. Moreover, we propose a specific efficient method, which works for complete intersections."
What is the polar factorization of Borel maps in optimal mass transport?,73dde11f,"[{'abstract': '  Let M be a compact Riemannian manifold and let $\\mu$,d be the associated\nmeasure and distance on M. Robert McCann obtained, generalizing results for the\nEuclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->\nM pushing forward $\\mu$ to a measure $\\nu$: each S factors uniquely a.e. into\nthe composition S = T \\circ U, where U : M -> M is volume preserving and T : M\n-> M is the optimal map transporting $\\mu$ to $\\nu$ with respect to the cost\nfunction d^2/2.\nIn this article we study the polar factorization of conformal and projective\nmaps of the sphere S^n. For conformal maps, which may be identified with\nelements of the identity component of O(1,n+1), we prove that the polar\nfactorization in the sense of optimal mass transport coincides with the\nalgebraic polar factorization (Cartan decomposition) of this Lie group. For the\nprojective case, where the group GL_+(n+1) is involved, we find necessary and\nsufficient conditions for these two factorizations to agree.\n', 'title': 'Polar factorization of conformal and projective maps of the sphere in the sense of optimal mass transport'}]","Title: Polar factorization of conformal and projective maps of the sphere in the sense of optimal mass transport

Abstract:   Let M be a compact Riemannian manifold and let $\mu$,d be the associated
measure and distance on M. Robert McCann obtained, generalizing results for the
Euclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->
M pushing forward $\mu$ to a measure $\nu$: each S factors uniquely a.e. into
the composition S = T \circ U, where U : M -> M is volume preserving and T : M
-> M is the optimal map transporting $\mu$ to $\nu$ with respect to the cost
function d^2/2.
In this article we study the polar factorization of conformal and projective
maps of the sphere S^n. For conformal maps, which may be identified with
elements of the identity component of O(1,n+1), we prove that the polar
factorization in the sense of optimal mass transport coincides with the
algebraic polar factorization (Cartan decomposition) of this Lie group. For the
projective case, where the group GL_+(n+1) is involved, we find necessary and
sufficient conditions for these two factorizations to agree."
In what fields are large-scale computational experiments widely used to make critical decisions?,73ab7d0c,"[{'abstract': '  Large-scale computational experiments, often running over weeks and over\nlarge datasets, are used extensively in fields such as epidemiology,\nmeteorology, computational biology, and healthcare to understand phenomena, and\ndesign high-stakes policies affecting everyday health and economy. For\ninstance, the OpenMalaria framework is a computationally-intensive simulation\nused by various non-governmental and governmental agencies to understand\nmalarial disease spread and effectiveness of intervention strategies, and\nsubsequently design healthcare policies. Given that such shared results form\nthe basis of inferences drawn, technological solutions designed, and day-to-day\npolicies drafted, it is essential that the computations are validated and\ntrusted. In particular, in a multi-agent environment involving several\nindependent computing agents, a notion of trust in results generated by peers\nis critical in facilitating transparency, accountability, and collaboration.\nUsing a novel combination of distributed validation of atomic computation\nblocks and a blockchain-based immutable audits mechanism, this work proposes a\nuniversal framework for distributed trust in computations. In particular we\naddress the scalaibility problem by reducing the storage and communication\ncosts using a lossy compression scheme. This framework guarantees not only\nverifiability of final results, but also the validity of local computations,\nand its cost-benefit tradeoffs are studied using a synthetic example of\ntraining a neural network.\n', 'title': 'Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach'}]","Title: Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach

Abstract:   Large-scale computational experiments, often running over weeks and over
large datasets, are used extensively in fields such as epidemiology,
meteorology, computational biology, and healthcare to understand phenomena, and
design high-stakes policies affecting everyday health and economy. For
instance, the OpenMalaria framework is a computationally-intensive simulation
used by various non-governmental and governmental agencies to understand
malarial disease spread and effectiveness of intervention strategies, and
subsequently design healthcare policies. Given that such shared results form
the basis of inferences drawn, technological solutions designed, and day-to-day
policies drafted, it is essential that the computations are validated and
trusted. In particular, in a multi-agent environment involving several
independent computing agents, a notion of trust in results generated by peers
is critical in facilitating transparency, accountability, and collaboration.
Using a novel combination of distributed validation of atomic computation
blocks and a blockchain-based immutable audits mechanism, this work proposes a
universal framework for distributed trust in computations. In particular we
address the scalaibility problem by reducing the storage and communication
costs using a lossy compression scheme. This framework guarantees not only
verifiability of final results, but also the validity of local computations,
and its cost-benefit tradeoffs are studied using a synthetic example of
training a neural network."
What are the observational constraints on deuterium fractions in quiescent prestellar cores?,8b0b8af9,"[{'abstract': '  High-mass stars are expected to form from dense prestellar cores. Their\nprecise formation conditions are widely discussed, including their virial\ncondition, which results in slow collapse for super-virial cores with strong\nsupport by turbulence or magnetic fields, or fast collapse for sub-virial\nsources. To disentangle their formation processes, measurements of the\ndeuterium fractions are frequently employed to approximately estimate the ages\nof these cores and to obtain constraints on their dynamical evolution. We here\npresent 3D magneto-hydrodynamical simulations including for the first time an\naccurate non-equilibrium chemical network with 21 gas-phase species plus dust\ngrains and 213 reactions. With this network we model the deuteration process in\nfully depleted prestellar cores in great detail and determine its response to\nvariations in the initial conditions. We explore the dependence on the initial\ngas column density, the turbulent Mach number, the mass-to-magnetic flux ratio\nand the distribution of the magnetic field, as well as the initial\northo-to-para ratio of H2. We find excellent agreement with recent observations\nof deuterium fractions in quiescent sources. Our results show that deuteration\nis rather efficient, even when assuming a conservative ortho-to-para ratio of 3\nand highly sub-virial initial conditions, leading to large deuterium fractions\nalready within roughly a free-fall time. We discuss the implications of our\nresults and give an outlook to relevant future investigations.\n', 'title': 'Deuterium fractionation and H2D+ evolution in turbulent and magnetized cloud cores'}]","Title: Deuterium fractionation and H2D+ evolution in turbulent and magnetized cloud cores

Abstract: High-mass stars are expected to form from dense prestellar cores. Their precise formation conditions are widely discussed, including their virial condition, which results in slow collapse for super-virial cores with strong support by turbulence or magnetic fields, or fast collapse for sub-virial sources. To disentangle their formation processes, measurements of the deuterium fractions are frequently employed to approximately estimate the ages of these cores and to obtain constraints on their dynamical evolution. We here present 3D magneto-hydrodynamical simulations including for the first time an accurate non-equilibrium chemical network with 21 gas-phase species plus dust grains and 213 reactions. With this network we model the deuteration process in fully depleted prestellar cores in great detail and determine its response to variations in the initial conditions. We explore the dependence on the initial gas column density, the turbulent Mach number, the mass-to-magnetic flux ratio and the distribution of the magnetic field, as well as the initial ortho-to-para ratio of H2. We find excellent agreement with recent observations of deuterium fractions in quiescent sources. Our results show that deuteration is rather efficient, even when assuming a conservative ortho-to-para ratio of 3 and highly sub-virial initial conditions, leading to large deuterium fractions already within roughly a free-fall time. We discuss the implications of our results and give an outlook to relevant future investigations."
How does DeepFense address the challenge of adversarial attacks on Deep Learning models?,ffdaedd9,"[{'abstract': '  Recent advances in adversarial Deep Learning (DL) have opened up a largely\nunexplored surface for malicious attacks jeopardizing the integrity of\nautonomous DL systems. With the wide-spread usage of DL in critical and\ntime-sensitive applications, including unmanned vehicles, drones, and video\nsurveillance systems, online detection of malicious inputs is of utmost\nimportance. We propose DeepFense, the first end-to-end automated framework that\nsimultaneously enables efficient and safe execution of DL models. DeepFense\nformalizes the goal of thwarting adversarial attacks as an optimization problem\nthat minimizes the rarely observed regions in the latent feature space spanned\nby a DL network. To solve the aforementioned minimization problem, a set of\ncomplementary but disjoint modular redundancies are trained to validate the\nlegitimacy of the input samples in parallel with the victim DL model. DeepFense\nleverages hardware/software/algorithm co-design and customized acceleration to\nachieve just-in-time performance in resource-constrained settings. The proposed\ncountermeasure is unsupervised, meaning that no adversarial sample is leveraged\nto train modular redundancies. We further provide an accompanying API to reduce\nthe non-recurring engineering cost and ensure automated adaptation to various\nplatforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders\nof magnitude performance improvement while enabling online adversarial sample\ndetection.\n', 'title': 'DeepFense: Online Accelerated Defense Against Adversarial Deep Learning'}]","Title: DeepFense: Online Accelerated Defense Against Adversarial Deep Learning

Abstract: Recent advances in adversarial Deep Learning (DL) have opened up a largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. With the wide-spread usage of DL in critical and time-sensitive applications, including unmanned vehicles, drones, and video surveillance systems, online detection of malicious inputs is of utmost importance. We propose DeepFense, the first end-to-end automated framework that simultaneously enables efficient and safe execution of DL models. DeepFense formalizes the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples in parallel with the victim DL model. DeepFense leverages hardware/software/algorithm co-design and customized acceleration to achieve just-in-time performance in resource-constrained settings. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. We further provide an accompanying API to reduce the non-recurring engineering cost and ensure automated adaptation to various platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders of magnitude performance improvement while enabling online adversarial sample detection.

DeepFense addresses the challenge of adversarial attacks on Deep Learning models by formalizing the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. It uses a set of complementary but disjoint modular redundancies that are trained to validate the legitimacy of the input samples in parallel with the victim DL model."
"Apart from visual observations, what novel approach is proposed for recognizing speech-related AUs?",23d32d82,"[{'abstract': '  Extensive efforts have been devoted to recognizing facial action units (AUs).\nHowever, it is still challenging to recognize AUs from spontaneous facial\ndisplays especially when they are accompanied with speech. Different from all\nprior work that utilized visual observations for facial AU recognition, this\npaper presents a novel approach that recognizes speech-related AUs exclusively\nfrom audio signals based on the fact that facial activities are highly\ncorrelated with voice during speech. Specifically, dynamic and physiological\nrelationships between AUs and phonemes are modeled through a continuous time\nBayesian network (CTBN); then AU recognition is performed by probabilistic\ninference via the CTBN model.\nA pilot audiovisual AU-coded database has been constructed to evaluate the\nproposed audio-based AU recognition framework. The database consists of a\n""clean"" subset with frontal and neutral faces and a challenging subset\ncollected with large head movements and occlusions. Experimental results on\nthis database show that the proposed CTBN model achieves promising recognition\nperformance for 7 speech-related AUs and outperforms the state-of-the-art\nvisual-based methods especially for those AUs that are activated at low\nintensities or ""hardly visible"" in the visual channel. Furthermore, the CTBN\nmodel yields more impressive recognition performance on the challenging subset,\nwhere the visual-based approaches suffer significantly.\n', 'title': 'Listen to Your Face: Inferring Facial Action Units from Audio Channel'}]","Title: Listen to Your Face: Inferring Facial Action Units from Audio Channel

Abstract: Extensive efforts have been devoted to recognizing facial action units (AUs). However, it is still challenging to recognize AUs from spontaneous facial displays especially when they are accompanied with speech. Different from all prior work that utilized visual observations for facial AU recognition, this paper presents a novel approach that recognizes speech-related AUs exclusively from audio signals based on the fact that facial activities are highly correlated with voice during speech. Specifically, dynamic and physiological relationships between AUs and phonemes are modeled through a continuous time Bayesian network (CTBN); then AU recognition is performed by probabilistic inference via the CTBN model."
In what city was a congestion barcode method initially demonstrated? ,aadfb609,"[{'abstract': '  This work presents a new method to quantify connectivity in transportation\nnetworks. Inspired by the field of topological data analysis, we propose a\nnovel approach to explore the robustness of road network connectivity in the\npresence of congestion on the roadway. The robustness of the pattern is\nsummarized in a congestion barcode, which can be constructed directly from\ntraffic datasets commonly used for navigation. As an initial demonstration, we\nillustrate the main technique on a publicly available traffic dataset in a\nneighborhood in New York City.\n', 'title': 'Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology'}]","The answer to your question is:

Title: Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology

Abstract: This work presents a new method to quantify connectivity in transportation networks. Inspired by the field of topological data analysis, we propose a novel approach to explore the robustness of road network connectivity in the presence of congestion on the roadway. The robustness of the pattern is summarized in a congestion barcode, which can be constructed directly from traffic datasets commonly used for navigation. As an initial demonstration, we illustrate the main technique on a publicly available traffic dataset in a neighborhood in New York City."
Are all Leavitt path algebras that are graded Sigma-V rings also graded directly-finite?,4e1f8f9b,"[{'abstract': '  In this paper, we give a complete characterization of Leavitt path algebras\nwhich are graded $\\Sigma $-$V$ rings, that is, rings over which a direct sum of\narbitrary copies of any graded simple module is graded injective. Specifically,\nwe show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded\n$\\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of\narbitrary size but with finitely many non-zero entries over $K$ or\n$K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical\ncharacterization of such a graded $\\Sigma $-$V$ ring $L$% . When the graph $E$\nis finite, we show that $L$ is a graded $\\Sigma $-$V$ ring $\\Longleftrightarrow\nL$ is graded directly-finite $\\Longleftrightarrow L $ has bounded index of\nnilpotence $\\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that\nthe equivalence of these properties in the preceding statement no longer holds\nwhen the graph $E$ is infinite. Following this, we also characterize Leavitt\npath algebras $L$ which are non-graded $\\Sigma $-$V$ rings. Graded rings which\nare graded directly-finite are explored and it is shown that if a Leavitt path\nalgebra $L$ is a graded $\\Sigma$-$V$ ring, then $L$ is always graded\ndirectly-finite. Examples show the subtle differences between graded and\nnon-graded directly-finite rings. Leavitt path algebras which are graded\ndirectly-finite are shown to be directed unions of graded semisimple rings.\nUsing this, we give an alternative proof of a theorem of Vaš \\cite{V} on\ndirectly-finite Leavitt path algebras.\n', 'title': 'Leavitt path algebras: Graded direct-finiteness and graded $Σ$-injective simple modules'}]","Title: Leavitt path algebras: Graded direct-finiteness and graded $Σ$-injective simple modules

Abstract: In this paper, we give a complete characterization of Leavitt path algebras which are graded $\Sigma $-$V$ rings, that is, rings over which a direct sum of arbitrary copies of any graded simple module is graded injective. Specifically, we show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded $\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of arbitrary size but with finitely many non-zero entries over $K$ or $K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical characterization of such a graded $\Sigma $-$V$ ring $L$%. When the graph $E$ is finite, we show that $L$ is a graded $\Sigma $-$V$ ring $\Longleftrightarrow L$ is graded directly-finite $\Longleftrightarrow L $ has bounded index of nilpotence $\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that the equivalence of these properties in the preceding statement no longer holds when the graph $E$ is infinite. Following this, we also characterize Leavitt path algebras $L$ which are non-graded $\Sigma $-$V$ rings. Graded rings which are graded directly-finite are explored and it is shown that if a Leavitt path algebra $L$ is a graded $\Sigma$-$V$ ring, then $L$ is always graded directly-finite. Examples show the subtle differences between graded and non-graded directly-finite rings. Leavitt path algebras which are graded directly-finite are shown to be directed unions of graded semisimple rings. Using this, we give an alternative proof of a theorem of Vaš \cite{V} on directly-finite Leavitt path algebras."
What is the significance of  Rickart C*-algebras and AW*-algebras in the context of synaptic algebra research?,fb7133a7,"[{'abstract': '  Using a representation theorem of Erik Alfsen, Frederic Schultz, and Erling\nStormer for special JB-algebras, we prove that a synaptic algebra is norm\ncomplete (i.e., Banach) if and only if it is isomorphic to the self-adjoint\npart of a Rickart C*-algebra. Also, we give conditions on a Banach synaptic\nalgebra that are equivalent to the condition that it is isomorphic to the\nself-adjoint part of an AW*-algebra. Moreover, we study some relationships\nbetween synaptic algebras and so-called generalized Hermitian algebras.\n', 'title': 'Banach synaptic algebras'}]","Title: Banach synaptic algebras

Abstract: Using a representation theorem of Erik Alfsen, Frederic Schultz, and Erling Stormer for special JB-algebras, we prove that a synaptic algebra is norm complete (i.e., Banach) if and only if it is isomorphic to the self-adjoint part of a Rickart C*-algebra. Also, we give conditions on a Banach synaptic algebra that are equivalent to the condition that it is isomorphic to the self-adjoint part of an AW*-algebra. Moreover, we study some relationships between synaptic algebras and so-called generalized Hermitian algebras.

According to this abstract, Rickart C*-algebras and AW*-algebras are significant in the context of synaptic algebra research because they are directly related to the norm completeness (Banach property) of synaptic algebras. Specifically, a synaptic algebra is Banach if and only if it is isomorphic to the self-adjoint part of a Rickart C*-algebra, and certain conditions on a Banach synaptic algebra are equivalent to it being isomorphic to the self-adjoint part of an AW*-algebra."
How does NASNet compare to human-designed architectures in terms of accuracy and computational efficiency on benchmark datasets like CIFAR-10 and ImageNet?,94261d14,"[{'abstract': '  Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the ""NASNet search space"") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or ""cell"") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named ""NASNet architecture"". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset.\n', 'title': 'Learning Transferable Architectures for Scalable Image Recognition'}]","Title: Learning Transferable Architectures for Scalable Image Recognition

Abstract: Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the ""NASNet search space"") which enables transferability. In our experiments, we search for the best convolutional layer (or ""cell"") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named ""NASNet architecture"". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms."
"What solar telescope was used to observe an emerging flux region with an arch filament system on April 17, 2015?",46bbd08d,"[{'abstract': ""  A new generation of solar instruments provides improved spectral, spatial,\nand temporal resolution, thus facilitating a better understanding of dynamic\nprocesses on the Sun. High-resolution observations often reveal\nmultiple-component spectral line profiles, e.g., in the near-infrared He I\n10830 \\AA\\ triplet, which provides information about the chromospheric velocity\nand magnetic fine structure. We observed an emerging flux region, including two\nsmall pores and an arch filament system, on 2015 April 17 with the 'very fast\nspectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the\n1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We\ndiscuss this method of obtaining fast (one per minute) spectral scans of the\nsolar surface and its potential to follow dynamic processes on the Sun. We\ndemonstrate the performance of the 'very fast spectroscopic mode' by tracking\nchromospheric high-velocity features in the arch filament system.\n"", 'title': ""Flows along arch filaments observed in the GRIS 'very fast spectroscopic mode'""}]","Title: Flows along arch filaments observed in the GRIS 'very fast spectroscopic mode'

Abstract: A new generation of solar instruments provides improved spectral, spatial, and temporal resolution, thus facilitating a better understanding of dynamic processes on the Sun. High-resolution observations often reveal multiple-component spectral line profiles, e.g., in the near-infrared He I 10830 Å triplet, which provides information about the chromospheric velocity and magnetic fine structure. We observed an emerging flux region, including two small pores and an arch filament system, on 2015 April 17 with the 'very fast spectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the 1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We discuss this method of obtaining fast (one per minute) spectral scans of the solar surface and its potential to follow dynamic processes on the Sun. We demonstrate the performance of the 'very fast spectroscopic mode' by tracking chromospheric high-velocity features in the arch filament system.

The answer is the 1.5-meter GREGOR solar telescope."
"Based on first-principles calculations, what guidelines can be established for designing adatoms with minimal quantum fluctuations?",b464b44f,"[{'abstract': '  Stabilizing the magnetic signal of single adatoms is a crucial step towards\ntheir successful usage in widespread technological applications such as\nhigh-density magnetic data storage devices. The quantum mechanical nature of\nthese tiny objects, however, introduces intrinsic zero-point spin-fluctuations\nthat tend to destabilize the local magnetic moment of interest by dwindling the\nmagnetic anisotropy potential barrier even at absolute zero temperature. Here,\nwe elucidate the origins and quantify the effect of the fundamental ingredients\ndetermining the magnitude of the fluctuations, namely the ($i$) local magnetic\nmoment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner\nexcitations. Based on a systematic first-principles study of 3d and 4d adatoms,\nwe demonstrate that the transverse contribution of the fluctuations is\ncomparable in size to the magnetic moment itself, leading to a remarkable\n$\\gtrsim$50$\\%$ reduction of the magnetic anisotropy energy. Our analysis gives\nrise to a comprehensible diagram relating the fluctuation magnitude to\ncharacteristic features of adatoms, providing practical guidelines for\ndesigning magnetically stable nanomagnets with minimal quantum fluctuations.\n', 'title': 'Zero-point spin-fluctuations of single adatoms'}]","Title: Zero-point spin-fluctuations of single adatoms

Abstract:  Stabilizing the magnetic signal of single adatoms is a crucial step towards
their successful usage in widespread technological applications such as
high-density magnetic data storage devices. The quantum mechanical nature of
these tiny objects, however, introduces intrinsic zero-point spin-fluctuations
that tend to destabilize the local magnetic moment of interest by dwindling the
magnetic anisotropy potential barrier even at absolute zero temperature. Here,
we elucidate the origins and quantify the effect of the fundamental ingredients
determining the magnitude of the fluctuations, namely the ($i$) local magnetic
moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner
excitations. Based on a systematic first-principles study of 3d and 4d adatoms,
we demonstrate that the transverse contribution of the fluctuations is
comparable in size to the magnetic moment itself, leading to a remarkable
$\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives
rise to a comprehensible diagram relating the fluctuation magnitude to
characteristic features of adatoms, providing practical guidelines for
designing magnetically stable nanomagnets with minimal quantum fluctuations."
Describe a novel hybrid architecture that combines neural network embedding and a dot product layer for accurate and interpretable nonlinear modeling.,e7f550a8,"[{'abstract': '  Machine learning algorithms such as linear regression, SVM and neural network\nhave played an increasingly important role in the process of scientific\ndiscovery. However, none of them is both interpretable and accurate on\nnonlinear datasets. Here we present contextual regression, a method that joins\nthese two desirable properties together using a hybrid architecture of neural\nnetwork embedding and dot product layer. We demonstrate its high prediction\naccuracy and sensitivity through the task of predictive feature selection on a\nsimulated dataset and the application of predicting open chromatin sites in the\nhuman genome. On the simulated data, our method achieved high fidelity recovery\nof feature contributions under random noise levels up to 200%. On the open\nchromatin dataset, the application of our method not only outperformed the\nstate of the art method in terms of accuracy, but also unveiled two previously\nunfound open chromatin related histone marks. Our method can fill the blank of\naccurate and interpretable nonlinear modeling in scientific data mining tasks.\n', 'title': 'Contextual Regression: An Accurate and Conveniently Interpretable Nonlinear Model for Mining Discovery from Scientific Data'}]","After searching through the provided abstracts, I couldn't find a perfect match for the description of a novel hybrid architecture that combines neural network embedding and a dot product layer for accurate and interpretable nonlinear modeling.

However, I'd like to suggest that the closest match is not among the provided abstracts. It's possible that the researcher is looking for a different paper. If you'd like, I can try to help you search for the paper or provide guidance on how to find it.

If you'd like to explore the provided abstracts further, I can suggest some papers that might be related to neural networks or hybrid architectures, but they don't exactly match the description:

* HybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning (not exactly a hybrid architecture combining neural network embedding and a dot product layer, but it does involve a two-branch encoder-decoder architecture)
* EnergyNet: Energy-based Adaptive Structural Learning of Artificial Neural Network Architectures (this paper is more focused on adaptively learning the structure of neural networks, but it doesn't seem to involve a hybrid architecture combining neural network embedding and a dot product layer)
* LAAIR: A Layered Architecture for Autonomous Interactive Robots (this paper is more focused on robotics and doesn't seem to involve neural networks or hybrid architectures)

Let me know if you'd like to explore further or if I can assist you in any other way!"
How do researchers ensure efficient beacon broadcasting in a decentralized VLC system?,1d1a2392,"[{'abstract': '  Indoor localization based on Visible Light Communication (VLC) has been in\nfavor with both the academia and industry for years. In this paper, we present\na prototyping photodiode-based VLC system towards large-scale localization.\nSpecially, we give in-depth analysis of the design constraints and\nconsiderations for large-scale indoor localization research. After that we\nidentify the key enablers for such systems: 1) distributed architecture, 2)\none-way communication, and 3) random multiple access. Accordingly, we propose\nPlugo -- a photodiode-based VLC system conforming to the aforementioned\ncriteria. We present a compact design of the VLC-compatible LED bulbs featuring\nplug-and-go use-cases. The basic framed slotted Additive Links On-line Hawaii\nArea (ALOHA) is exploited to achieve random multiple access over the shared\noptical medium. We show its effectiveness in beacon broadcasting by\nexperiments, and further demonstrate its scalability to large-scale scenarios\nthrough simulations. Finally, preliminary localization experiments are\nconducted using fingerprinting-based methods in a customized testbed, achieving\nan average accuracy of 0.14 m along with a 90-percentile accuracy of 0.33 m.\n', 'title': 'Plugo: a VLC Systematic Perspective of Large-scale Indoor Localization'}]","Title: Plugo: a VLC Systematic Perspective of Large-scale Indoor Localization

Abstract: Indoor localization based on Visible Light Communication (VLC) has been in favor with both the academia and industry for years. In this paper, we present a prototyping photodiode-based VLC system towards large-scale localization. Specially, we give in-depth analysis of the design constraints and considerations for large-scale indoor localization research. After that we identify the key enablers for such systems: 1) distributed architecture, 2) one-way communication, and 3) random multiple access. Accordingly, we propose Plugo -- a photodiode-based VLC system conforming to the aforementioned criteria. We present a compact design of the VLC-compatible LED bulbs featuring plug-and-go use-cases. The basic framed slotted Additive Links On-line Hawaii Area (ALOHA) is exploited to achieve random multiple access over the shared optical medium. We show its effectiveness in beacon broadcasting by experiments, and further demonstrate its scalability to large-scale scenarios through simulations. Finally, preliminary localization experiments are conducted using fingerprinting-based methods in a customized testbed, achieving an average accuracy of 0.14 m along with a 90-percentile accuracy of 0.33 m.

In this paper, the researchers ensure efficient beacon broadcasting in a decentralized VLC system by using the basic framed slotted ALOHA to achieve random multiple access over the shared optical medium."
What is the inductive formula for the structure constants in the torus-equivariant Grothendieck group of the affine Grassmannian associated to SL2?,11bef1ff,"[{'abstract': '  Let $G:=\\widehat{SL_2}$ denote the affine Kac-Moody group associated to\n$SL_2$ and $\\bar{\\mathcal{X}}$ the associated affine Grassmannian. We determine\nan inductive formula for the Schubert basis structure constants in the\ntorus-equivariant Grothendieck group of $\\bar{\\mathcal{X}}$. In the case of\nordinary (non-equivariant) $K$-theory we find an explicit closed form for the\nstructure constants. We also determine an inductive formula for the structure\nconstants in the torus-equivariant cohomology ring, and use this formula to\nfind closed forms for some of the structure constants.\n', 'title': 'An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$'}]","Title: An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$

Abstract: Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to $SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine an inductive formula for the Schubert basis structure constants in the torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of ordinary (non-equivariant) $K$-theory we find an explicit closed form for the structure constants. We also determine an inductive formula for the structure constants in the torus-equivariant cohomology ring, and use this formula to find closed forms for some of the structure constants."
Are there any known examples of groups containing Morse geodesics but not Morse elements?,10c473bb,"[{'abstract': '  In this paper we exhibit Morse geodesics, often called ""hyperbolic\ndirections"", in infinite unbounded torsion groups. The groups studied are\nlacunary hyperbolic groups and constructed using graded small cancellation\nconditions. In all previously known examples, Morse geodesics were found in\ngroups which also contained Morse elements, infinite order elements whose\ncyclic subgroup gives a Morse quasi-geodesic. Our result presents the first\nexample of a group which contains Morse geodesics but no Morse elements. In\nfact, we show that there is an isometrically embedded $7$-regular tree inside\nsuch groups where every infinite, simple path is a Morse geodesic.\n', 'title': 'Morse geodesics in torsion groups'}]","Here's the answer:

Title: Morse geodesics in torsion groups

Abstract:   In this paper we exhibit Morse geodesics, often called ""hyperbolic
directions"", in infinite unbounded torsion groups. The groups studied are
lacunary hyperbolic groups and constructed using graded small cancellation
conditions. In all previously known examples, Morse geodesics were found in
groups which also contained Morse elements, infinite order elements whose
cyclic subgroup gives a Morse quasi-geodesic. Our result presents the first
example of a group which contains Morse geodesics but no Morse elements. In
fact, we show that there is an isometrically embedded $7$-regular tree inside
such groups where every infinite, simple path is a Morse geodesic."
Do star clusters form more easily in the center of a galaxy with a cored dark matter halo?,aba7d3ed,"[{'abstract': '  The recent detection of two faint and extended star clusters in the central\nregions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV,\nraises the question of whether clusters with such low densities can survive the\ntidal field of cold dark matter haloes with central density cusps. Using both\nanalytic arguments and a suite of collisionless N-body simulations, I show that\nthese clusters are extremely fragile and quickly disrupted in the presence of\ncentral cusps $\\rho\\sim r^{-\\alpha}$ with $\\alpha\\gtrsim 0.2$. Furthermore, the\nscenario in which the clusters where originally more massive and sank to the\ncenter of the halo requires extreme fine tuning and does not naturally\nreproduce the observed systems. In turn, these clusters are long lived in cored\nhaloes, whose central regions are safe shelters for $\\alpha\\lesssim 0.2$. The\nonly viable scenario for hosts that have preserved their primoridal cusp to the\npresent time is that the clusters formed at rest at the bottom of the\npotential, which is easily tested by measurement of the clusters proper\nvelocity within the host. This offers means to readily probe the central\ndensity profile of two dwarf galaxies as faint as $L_V\\sim5\\times 10^5 L_\\odot$\nand $L_V\\sim6\\times10^4 L_\\odot$, in which stellar feedback is unlikely to be\neffective.\n', 'title': 'Deadly dark matter cusps vs faint and extended star clusters: Eridanus II and Andromeda XXV'}]","Title: Deadly dark matter cusps vs faint and extended star clusters: Eridanus II and Andromeda XXV

Abstract: The recent detection of two faint and extended star clusters in the central regions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV, raises the question of whether clusters with such low densities can survive the tidal field of cold dark matter haloes with central density cusps. Using both analytic arguments and a suite of collisionless N-body simulations, I show that these clusters are extremely fragile and quickly disrupted in the presence of central cusps $\rho\sim r^{-\alpha}$ with $\alpha\gtrsim 0.2$. Furthermore, the scenario in which the clusters where originally more massive and sank to the center of the halo requires extreme fine tuning and does not naturally reproduce the observed systems. In turn, these clusters are long lived in cored haloes, whose central regions are safe shelters for $\alpha\lesssim 0.2$. The only viable scenario for hosts that have preserved their primoridal cusp to the present time is that the clusters formed at rest at the bottom of the potential, which is easily tested by measurement of the clusters proper velocity within the host. This offers means to readily probe the central density profile of two dwarf galaxies as faint as $L_V\sim5\times 10^5 L_\odot$ and $L_V\sim6\times10^4 L_\odot$, in which stellar feedback is unlikely to be effective."
How can well-defined use models and threat models improve information sharing?,1da77640,"[{'abstract': '  In the past decade, the information security and threat landscape has grown\nsignificantly making it difficult for a single defender to defend against all\nattacks at the same time. This called for introduc- ing information sharing, a\nparadigm in which threat indicators are shared in a community of trust to\nfacilitate defenses. Standards for representation, exchange, and consumption of\nindicators are pro- posed in the literature, although various issues are\nundermined. In this paper, we rethink information sharing for actionable\nintelli- gence, by highlighting various issues that deserve further explo-\nration. We argue that information sharing can benefit from well- defined use\nmodels, threat models, well-understood risk by mea- surement and robust\nscoring, well-understood and preserved pri- vacy and quality of indicators and\nrobust mechanism to avoid free riding behavior of selfish agent. We call for\nusing the differential nature of data and community structures for optimizing\nsharing.\n', 'title': 'Rethinking Information Sharing for Actionable Threat Intelligence'}]","Title: Rethinking Information Sharing for Actionable Threat Intelligence

Abstract: In the past decade, the information security and threat landscape has grown significantly making it difficult for a single defender to defend against all attacks at the same time. This called for introducing information sharing, a paradigm in which threat indicators are shared in a community of trust to facilitate defenses. Standards for representation, exchange, and consumption of indicators are proposed in the literature, although various issues are undermined. In this paper, we rethink information sharing for actionable intelligence, by highlighting various issues that deserve further exploration. We argue that information sharing can benefit from well-defined use models, threat models, well-understood risk by measurement and robust scoring, well-understood and preserved privacy and quality of indicators and robust mechanism to avoid free riding behavior of selfish agents. We call for using the differential nature of data and community structures for optimizing sharing."
How does the size of a parent asteroid body potentially influence the observed size distribution of its fragments?,7b1156fb,"[{'abstract': '  Young asteroid families are unique sources of information about fragmentation\nphysics and the structure of their parent bodies, since their physical\nproperties have not changed much since their birth. Families have different\nproperties such as age, size, taxonomy, collision severity and others, and\nunderstanding the effect of those properties on our observations of the\nsize-frequency distribution (SFD) of family fragments can give us important\ninsights into the hypervelocity collision processes at scales we cannot achieve\nin our laboratories. Here we take as an example the very young Datura family,\nwith a small 8-km parent body, and compare its size distribution to other\nfamilies, with both large and small parent bodies, and created by both\ncatastrophic and cratering formation events. We conclude that most likely\nexplanation for the shallower size distribution compared to larger families is\na more pronounced observational bias because of its small size. Its size\ndistribution is perfectly normal when its parent body size is taken into\naccount. We also discuss some other possibilities. In addition, we study\nanother common feature: an offset or ""bump"" in the distribution occurring for a\nfew of the larger elements. We hypothesize that it can be explained by a newly\ndescribed regime of cratering, ""spall cratering"", which controls the majority\nof impact craters on the surface of small asteroids like Datura.\n', 'title': 'Interpretations of family size distributions: The Datura example'}]","I think I can help you with that!

Title: Interpretations of family size distributions: The Datura example

Abstract: Young asteroid families are unique sources of information about fragmentation physics and the structure of their parent bodies, since their physical properties have not changed much since their birth. Families have different properties such as age, size, taxonomy, collision severity and others, and understanding the effect of those properties on our observations of the size-frequency distribution (SFD) of family fragments can give us important insights into the hypervelocity collision processes at scales we cannot achieve in our laboratories. Here we take as an example the very young Datura family, with a small 8-km parent body, and compare its size distribution to other families, with both large and small parent bodies, and created by both catastrophic and cratering formation events. We conclude that the most likely explanation for the shallower size distribution compared to larger families is a more pronounced observational bias because of its small size. Its size distribution is perfectly normal when its parent body size is taken into account. We also discuss some other possibilities. In addition, we study another common feature: an offset or ""bump"" in the distribution occurring for a few of the larger elements. We hypothesize that it can be explained by a newly described regime of cratering, ""spall cratering"", which controls the majority of impact craters on the surface of small asteroids like Datura.

This research paper suggests that the size of the parent asteroid body can influence the observed size distribution of its fragments, and that a smaller parent body can lead to a shallower size distribution due to observational bias."
Are there specific  conditions on the separation of points that lead to  approximations of the exponential sum?,ba643293,"[{'abstract': '  We bound an exponential sum that appears in the study of irregularities of\ndistribution (the low-frequency Fourier energy of the sum of several Dirac\nmeasures) by geometric quantities: a special case is that for all $\\left\\{ x_1,\n\\dots, x_N\\right\\} \\subset \\mathbb{T}^2$, $X \\geq 1$ and a universal $c>0$ $$\n\\sum_{i,j=1}^{N}{ \\frac{X^2}{1 + X^4 \\|x_i -x_j\\|^4}} \\lesssim \\sum_{k \\in\n\\mathbb{Z}^2 \\atop \\|k\\| \\leq X}{ \\left| \\sum_{n=1}^{N}{ e^{2 \\pi i\n\\left\\langle k, x_n \\right\\rangle}}\\right|^2} \\lesssim \\sum_{i,j=1}^{N}{ X^2\ne^{-c X^2\\|x_i -x_j\\|^2}}.$$ Since this exponential sum is intimately tied to\nrather subtle distribution properties of the points, we obtain nonlocal\nstructural statements for near-minimizers of the Riesz-type energy. In the\nregime $X \\gtrsim N^{1/2}$ both upper and lower bound match for\nmaximally-separated point sets satisfying $\\|x_i -x_j\\| \\gtrsim N^{-1/2}$.\n', 'title': 'Exponential Sums and Riesz energies'}]","Title: Exponential Sums and Riesz energies

Abstract: We bound an exponential sum that appears in the study of irregularities of
distribution (the low-frequency Fourier energy of the sum of several Dirac
measures) by geometric quantities: a special case is that for all $\left\{ x_1,
\dots, x_N\right\} \subset \mathbb{T}^2$, $X \geq 1$ and a universal $c>0$ $$
\sum_{i,j=1}^{N}{ \frac{X^2}{1 + X^4 \|x_i -x_j\|^4}} \lesssim \sum_{k \in
\mathbb{Z}^2 \atop \|k\| \leq X}{ \left| \sum_{n=1}^{N}{ e^{2 \pi i
\left\langle k, x_n \right\rangle}}\right|^2} \lesssim \sum_{i,j=1}^{N}{ X^2
e^{-c X^2\|x_i -x_j\|^2}}.$$ Since this exponential sum is intimately tied to
rather subtle distribution properties of the points, we obtain nonlocal
structural statements for near-minimizers of the Riesz-type energy. In the
regime $X \gtrsim N^{1/2}$ both upper and lower bound match for
maximally-separated point sets satisfying $\|x_i -x_j\| \gtrsim N^{-1/2}$."
"In robotics, what are some methods for handling the challenge of predicting the effects of physical actions based on sensory input?",e91ae589,"[{'abstract': '  One of the most basic skills a robot should possess is predicting the effect\nof physical interactions with objects in the environment. This enables optimal\naction selection to reach a certain goal state. Traditionally, dynamics are\napproximated by physics-based analytical models. These models rely on specific\nstate representations that may be hard to obtain from raw sensory data,\nespecially if no knowledge of the object shape is assumed. More recently, we\nhave seen learning approaches that can predict the effect of complex physical\ninteractions directly from sensory input. It is however an open question how\nfar these models generalize beyond their training data. In this work, we\ninvestigate the advantages and limitations of neural network based learning\napproaches for predicting the effects of actions based on sensory input and\nshow how analytical and learned models can be combined to leverage the best of\nboth worlds. As physical interaction task, we use planar pushing, for which\nthere exists a well-known analytical model and a large real-world dataset. We\npropose to use a convolutional neural network to convert raw depth images or\norganized point clouds into a suitable representation for the analytical model\nand compare this approach to using neural networks for both, perception and\nprediction. A systematic evaluation of the proposed approach on a very large\nreal-world dataset shows two main advantages of the hybrid architecture.\nCompared to a pure neural network, it significantly (i) reduces required\ntraining data and (ii) improves generalization to novel physical interaction.\n', 'title': 'Combining learned and analytical models for predicting action effects'}]","Based on the provided context, I found a research paper that addresses the challenge of predicting the effects of physical actions based on sensory input.

Title: Combining learned and analytical models for predicting action effects

Abstract: One of the most basic skills a robot should possess is predicting the effect of physical interactions with objects in the environment. This enables optimal action selection to reach a certain goal state. Traditionally, dynamics are approximated by physics-based analytical models. These models rely on specific state representations that may be hard to obtain from raw sensory data, especially if no knowledge of the object shape is assumed. More recently, we have seen learning approaches that can predict the effect of complex physical interactions directly from sensory input. It is however an open question how far these models generalize beyond their training data. In this work, we investigate the advantages and limitations of neural network based learning approaches for predicting the effects of actions based on sensory input and show how analytical and learned models can be combined to leverage the best of both worlds. As physical interaction task, we use planar pushing, for which there exists a well-known analytical model and a large real-world dataset. We propose to use a convolutional neural network to convert raw depth images or organized point clouds into a suitable representation for the analytical model and compare this approach to using neural networks for both, perception and prediction. A systematic evaluation of the proposed approach on a very large real-world dataset shows two main advantages of the hybrid architecture. Compared to a pure neural network, it significantly (i) reduces required training data and (ii) improves generalization to novel physical interaction."
"What communication method allows divers to control an AUV underwater, particularly in dangerous situations?",b6a0c3c5,"[{'abstract': '  We propose a robust gesture-based communication pipeline for divers to\ninstruct an Autonomous Underwater Vehicle (AUV) to assist them in performing\nhigh-risk tasks and helping in case of emergency. A gesture communication\nlanguage (CADDIAN) is developed, based on consolidated and standardized diver\ngestures, including an alphabet, syntax and semantics, ensuring a logical\nconsistency. A hierarchical classification approach is introduced for hand\ngesture recognition based on stereo imagery and multi-descriptor aggregation to\nspecifically cope with underwater image artifacts, e.g. light backscatter or\ncolor attenuation. Once the classification task is finished, a syntax check is\nperformed to filter out invalid command sequences sent by the diver or\ngenerated by errors in the classifier. Throughout this process, the diver\nreceives constant feedback from an underwater tablet to acknowledge or abort\nthe mission at any time. The objective is to prevent the AUV from executing\nunnecessary, infeasible or potentially harmful motions. Experimental results\nunder different environmental conditions in archaeological exploration and\nbridge inspection applications show that the system performs well in the field.\n', 'title': 'Robust Gesture-Based Communication for Underwater Human-Robot Interaction in the context of Search and Rescue Diver Missions'}]","Title: Robust Gesture-Based Communication for Underwater Human-Robot Interaction in the context of Search and Rescue Diver Missions

Abstract: We propose a robust gesture-based communication pipeline for divers to instruct an Autonomous Underwater Vehicle (AUV) to assist them in performing high-risk tasks and helping in case of emergency. A gesture communication language (CADDIAN) is developed, based on consolidated and standardized diver gestures, including an alphabet, syntax and semantics, ensuring a logical consistency. A hierarchical classification approach is introduced for hand gesture recognition based on stereo imagery and multi-descriptor aggregation to specifically cope with underwater image artifacts, e.g. light backscatter or color attenuation. Once the classification task is finished, a syntax check is performed to filter out invalid command sequences sent by the diver or generated by errors in the classifier. Throughout this process, the diver receives constant feedback from an underwater tablet to acknowledge or abort the mission at any time. The objective is to prevent the AUV from executing unnecessary, infeasible or potentially harmful motions. Experimental results under different environmental conditions in archaeological exploration and bridge inspection applications show that the system performs well in the field."
What is the specific type of grading used when completing the quantum affine gl(n) algebra?,e35bd685,"[{'abstract': '  We present an affine analog of the evaluation map for quantum groups. Namely\nwe introduce a surjective homomorphism from the quantum toroidal gl(n) algebra\nto the quantum affine gl(n) algebra completed with respect to the homogeneous\ngrading. We give a brief discussion of evaluation modules.\n', 'title': 'An evaluation homomorphism for quantum toroidal gl(n) algebras'}]","Title: An evaluation homomorphism for quantum toroidal gl(n) algebras

Abstract: We present an affine analog of the evaluation map for quantum groups. Namely we introduce a surjective homomorphism from the quantum toroidal gl(n) algebra to the quantum affine gl(n) algebra completed with respect to the homogeneous grading. We give a brief discussion of evaluation modules.

In this paper, the specific type of grading used when completing the quantum affine gl(n) algebra is the homogeneous grading."
In what ways can a simple model of social influence explain the success of anti-establishment ideas?,ecf1c3c3,"[{'abstract': '  The spread of opinions, memes, diseases, and ""alternative facts"" in a\npopulation depends both on the details of the spreading process and on the\nstructure of the social and communication networks on which they spread. In\nthis paper, we explore how \\textit{anti-establishment} nodes (e.g.,\n\\textit{hipsters}) influence the spreading dynamics of two competing products.\nWe consider a model in which spreading follows a deterministic rule for\nupdating node states (which describe which product has been adopted) in which\nan adjustable fraction $p_{\\rm Hip}$ of the nodes in a network are hipsters,\nwho choose to adopt the product that they believe is the less popular of the\ntwo. The remaining nodes are conformists, who choose which product to adopt by\nconsidering which products their immediate neighbors have adopted. We simulate\nour model on both synthetic and real networks, and we show that the hipsters\nhave a major effect on the final fraction of people who adopt each product:\neven when only one of the two products exists at the beginning of the\nsimulations, a very small fraction of hipsters in a network can still cause the\nother product to eventually become the more popular one. To account for this\nbehavior, we construct an approximation for the steady-state adoption fraction\non $k$-regular trees in the limit of few hipsters. Additionally, our\nsimulations demonstrate that a time delay $\\tau$ in the knowledge of the\nproduct distribution in a population, as compared to immediate knowledge of\nproduct adoption among nearest neighbors, can have a large effect on the final\ndistribution of product adoptions. Our simple model and analysis may help shed\nlight on the road to success for anti-establishment choices in elections, as\nsuch success can arise rather generically in our model from a small number of\nanti-establishment individuals and ordinary processes of social influence on\nnormal individuals.\n', 'title': 'Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority'}]","Title: Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority

Abstract: The spread of opinions, memes, diseases, and ""alternative facts"" in a population depends both on the details of the spreading process and on the structure of the social and communication networks on which they spread. In this paper, we explore how anti-establishment nodes (e.g., hipsters) influence the spreading dynamics of two competing products. We consider a model in which spreading follows a deterministic rule for updating node states (which describe which product has been adopted) in which an adjustable fraction p_Hip of the nodes in a network are hipsters, who choose to adopt the product that they believe is the less popular of the two. The remaining nodes are conformists, who choose which product to adopt by considering which products their immediate neighbors have adopted. We simulate our model on both synthetic and real networks, and we show that the hipsters have a major effect on the final fraction of people who adopt each product: even when only one of the two products exists at the beginning of the simulations, a very small fraction of hipsters in a network can still cause the other product to eventually become the more popular one. To account for this behavior, we construct an approximation for the steady-state adoption fraction on k-regular trees in the limit of few hipsters. Additionally, our simulations demonstrate that a time delay τ in the knowledge of the product distribution in a population, as compared to immediate knowledge of product adoption among nearest neighbors, can have a large effect on the final distribution of product adoptions. Our simple model and analysis may help shed light on the road to success for anti-establishment choices in elections, as such success can arise rather generically in our model from a small number of anti-establishment individuals and ordinary processes of social influence on normal individuals."
How does the proposed algorithm compare to the NUP algorithm in terms of performance on BA and ER network models?,1919ce1a,"[{'abstract': '  With the tremendous increase of the Internet traffic, achieving the best\nperformance with limited resources is becoming an extremely urgent problem. In\norder to address this concern, in this paper, we build an optimization problem\nwhich aims to maximize the total utility of traffic flows with the capacity\nconstraint of nodes and links in the network. Based on Duality Theory, we\npropose an iterative algorithm which adjusts the rates of traffic flows and\ncapacity of nodes and links simultaneously to maximize the total utility.\nSimulation results show that our algorithm performs better than the NUP\nalgorithm on BA and ER network models, which has shown to get the best\nperformance so far. Since our research combines the topology information with\ncapacity constraint, it may give some insights for resource allocation in real\ncommunication networks.\n', 'title': 'Optimal Resource Allocation with Node and Link Capacity Constraints in Complex Networks'}]","Title: Optimal Resource Allocation with Node and Link Capacity Constraints in Complex Networks

Abstract: With the tremendous increase of the Internet traffic, achieving the best performance with limited resources is becoming an extremely urgent problem. In order to address this concern, in this paper, we build an optimization problem which aims to maximize the total utility of traffic flows with the capacity constraint of nodes and links in the network. Based on Duality Theory, we propose an iterative algorithm which adjusts the rates of traffic flows and capacity of nodes and links simultaneously to maximize the total utility. Simulation results show that our algorithm performs better than the NUP algorithm on BA and ER network models, which has shown to get the best performance so far. Since our research combines the topology information with capacity constraint, it may give some insights for resource allocation in real communication networks."
What are some graph-theoretical measures used to assess alterations in brain network structure in Alzheimer's disease?,61fd91b0,"[{'abstract': ""  A sequence of pathological changes takes place in Alzheimer's disease, which\ncan be assessed in vivo using various brain imaging methods. Currently, there\nis no appropriate statistical model available that can easily integrate\nmultiple imaging modalities, being able to utilize the additional information\nprovided from the combined data. We applied Gaussian graphical models (GGMs)\nfor analyzing the conditional dependency networks of multimodal neuroimaging\ndata and assessed alterations of the network structure in mild cognitive\nimpairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy\ncontrols.\nData from N=667 subjects were obtained from the Alzheimer's Disease\nNeuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism\n(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.\nSeparate GGMs were estimated using a Bayesian framework for the combined\nmultimodal data for each diagnostic category. Graph-theoretical statistics were\ncalculated to determine network alterations associated with disease severity.\nNetwork measures clustering coefficient, path length and small-world\ncoefficient were significantly altered across diagnostic groups, with a\nbiphasic u-shape trajectory, i.e. increased small-world coefficient in early\nMCI, intermediate values in late MCI, and decreased values in AD patients\ncompared to controls. In contrast, no group differences were found for\nclustering coefficient and small-world coefficient when estimating conditional\ndependency networks on single imaging modalities.\nGGMs provide a useful methodology to analyze the conditional dependency\nnetworks of multimodal neuroimaging data.\n"", 'title': ""Assessing inter-modal and inter-regional dependencies in prodromal Alzheimer's disease using multimodal MRI/PET and Gaussian graphical models""}]","Title: Assessing inter-modal and inter-regional dependencies in prodromal Alzheimer's disease using multimodal MRI/PET and Gaussian graphical models

Abstract: A sequence of pathological changes takes place in Alzheimer's disease, which can be assessed in vivo using various brain imaging methods. Currently, there is no appropriate statistical model available that can easily integrate multiple imaging modalities, being able to utilize the additional information provided from the combined data. We applied Gaussian graphical models (GGMs) for analyzing the conditional dependency networks of multimodal neuroimaging data and assessed alterations of the network structure in mild cognitive impairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy controls.

Data from N=667 subjects were obtained from the Alzheimer's Disease Neuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism (FDG-PET), and gray matter volume (MRI) was calculated for each brain region. Separate GGMs were estimated using a Bayesian framework for the combined multimodal data for each diagnostic category. Graph-theoretical statistics were calculated to determine network alterations associated with disease severity. Network measures clustering coefficient, path length, and small-world coefficient were significantly altered across diagnostic groups, with a biphasic u-shape trajectory, i.e. increased small-world coefficient in early MCI, intermediate values in late MCI, and decreased values in AD patients compared to controls.

The graph-theoretical measures used to assess alterations in brain network structure in Alzheimer's disease are:

* Clustering coefficient
* Path length
* Small-world coefficient"
What type of metadata is helpful for finding specific research software?,14db7edd,"[{'abstract': '  Broad efforts are underway to capture metadata about research software and\nretain it across services; notable in this regard is the CodeMeta project. What\nmetadata are important to have about (research) software? What metadata are\nuseful for searching for codes? What would you like to learn about astronomy\nsoftware? This BoF sought to gather information on metadata most desired by\nresearchers and users of astro software and others interested in registering,\nindexing, capturing, and doing research on this software. Information from this\nBoF could conceivably result in changes to the Astrophysics Source Code Library\n(ASCL) or other resources for the benefit of the community or provide input\ninto other projects concerned with software metadata.\n', 'title': 'Software metadata: How much is enough?'}]","Title: Software metadata: How much is enough?

Abstract: Broad efforts are underway to capture metadata about research software and retain it across services; notable in this regard is the CodeMeta project. What metadata are important to have about (research) software? What metadata are useful for searching for codes? What would you like to learn about astronomy software? This BoF sought to gather information on metadata most desired by researchers and users of astro software and others interested in registering, indexing, capturing, and doing research on this software. Information from this BoF could conceivably result in changes to the Astrophysics Source Code Library (ASCL) or other resources for the benefit of the community or provide input into other projects concerned with software metadata."
Can you explain  generalized  varieties and pseudovarieties in automata theory?,cc7c5e89,"[{'abstract': '  The classical Eilenberg correspondence, based on the concept of the syntactic\nmonoid, relates varieties of regular languages with pseudovarieties of finite\nmonoids. Various modifications of this correspondence appeared, with more\ngeneral classes of regular languages on one hand and classes of more complex\nalgebraic structures on the other hand. For example, classes of languages need\nnot be closed under complementation or all preimages under homomorphisms, while\nmonoids can be equipped with a compatible order or they can have a\ndistinguished set of generators. Such generalized varieties and pseudovarieties\nalso have natural counterparts formed by classes of finite (ordered) automata.\nIn this paper the previous approaches are combined. The notion of positive\n$\\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final\nstates are specified) is introduced and their correspondence with positive\n$\\mathcal C$-varieties of languages is proved.\n', 'title': 'On Varieties of Ordered Automata'}]","I think I can help you with that!

Title: On Varieties of Ordered Automata

Abstract: The classical Eilenberg correspondence, based on the concept of the syntactic monoid, relates varieties of regular languages with pseudovarieties of finite monoids. Various modifications of this correspondence appeared, with more general classes of regular languages on one hand and classes of more complex algebraic structures on the other hand. For example, classes of languages need not be closed under complementation or all preimages under homomorphisms, while monoids can be equipped with a compatible order or they can have a distinguished set of generators. Such generalized varieties and pseudovarieties also have natural counterparts formed by classes of finite (ordered) automata. In this paper the previous approaches are combined. The notion of positive $\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final states are specified) is introduced and their correspondence with positive $\mathcal C$-varieties of languages is proved."
What happens to the edge modes in this system when the topological classification is reduced?,7c1718e2,"[{'abstract': '  One of the most challenging problems in correlated topological systems is a\nrealization of the reduction of topological classification, but very few\nexperimental platforms have been proposed so far. We here demonstrate that\nultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded\nin an optical lattice of two-leg ladder geometry can be the first promising\ntestbed for the reduction $\\mathbb{Z}\\to\\mathbb{Z}_4$, where solid evidence for\nthe reduction is available thanks to their high controllability. We further\ngive a detailed account of how to experimentally access this phenomenon; around\nthe edges, the destruction of one-particle gapless excitations can be observed\nby the local radio frequency spectroscopy, while that of gapless spin\nexcitations can be observed by a time-dependent spin expectation value of a\nsuperposed state of the ground state and the first excited state. We clarify\nthat even when the reduction occurs, a gapless edge mode is recovered around a\ndislocation, which can be another piece of evidence for the reduction.\n', 'title': 'Reduction of topological $\\mathbb{Z}$ classification in cold atomic systems'}]","Title: Reduction of topological $\mathbb{Z}$ classification in cold atomic systems

Abstract:   One of the most challenging problems in correlated topological systems is a
realization of the reduction of topological classification, but very few
experimental platforms have been proposed so far. We here demonstrate that
ultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded
in an optical lattice of two-leg ladder geometry can be the first promising
testbed for the reduction $\mathbb{Z}\to\mathbb{Z}_4$, where solid evidence for
the reduction is available thanks to their high controllability. We further
give a detailed account of how to experimentally access this phenomenon; around
the edges, the destruction of one-particle gapless excitations can be observed
by the local radio frequency spectroscopy, while that of gapless spin
excitations can be observed by a time-dependent spin expectation value of a
superposed state of the ground state and the first excited state. We clarify
that even when the reduction occurs, a gapless edge mode is recovered around a
dislocation, which can be another piece of evidence for the reduction."
What are some historical examples of the desire for intelligent machines?,50d0200a,"[{'abstract': ""  Our desire and fascination with intelligent machines dates back to the\nantiquity's mythical automaton Talos, Aristotle's mode of mechanical thought\n(syllogism) and Heron of Alexandria's mechanical machines and automata.\nHowever, the quest for Artificial General Intelligence (AGI) is troubled with\nrepeated failures of strategies and approaches throughout the history. This\ndecade has seen a shift in interest towards bio-inspired software and hardware,\nwith the assumption that such mimicry entails intelligence. Though these steps\nare fruitful in certain directions and have advanced automation, their singular\ndesign focus renders them highly inefficient in achieving AGI. Which set of\nrequirements have to be met in the design of AGI? What are the limits in the\ndesign of the artificial? Here, a careful examination of computation in\nbiological systems hints that evolutionary tinkering of contextual processing\nof information enabled by a hierarchical architecture is the key to build AGI.\n"", 'title': 'Design of the Artificial: lessons from the biological roots of general intelligence'}]","A historical context!

Here's the answer:

Title: Design of the Artificial: lessons from the biological roots of general intelligence

Abstract: Our desire and fascination with intelligent machines dates back to the antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought (syllogism) and Heron of Alexandria's mechanical machines and automata. However, the quest for Artificial General Intelligence (AGI) is troubled with repeated failures of strategies and approaches throughout the history. This decade has seen a shift in interest towards bio-inspired software and hardware, with the assumption that such mimicry entails intelligence. Though these steps are fruitful in certain directions and have advanced automation, their singular design focus renders them highly inefficient in achieving AGI. Which set of requirements have to be met in the design of AGI? What are the limits in the design of the artificial? Here, a careful examination of computation in biological systems hints that evolutionary tinkering of contextual processing of information enabled by a hierarchical architecture is the key to build AGI."
How does the authors' analysis help identify suitable stars for searching for planets using radial velocity measurements?,7b1a2368,"[{'abstract': '  Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we\npresent new measurements of activity and magnetic field proxies of 442 low-mass\nK5-M7 dwarfs. The objects were analysed as potential targets to search for\nplanetary-mass companions with the new spectropolarimeter and high-precision\nvelocimeter, SPIRou. We have analysed their high-resolution spectra in an\nhomogeneous way: circular polarisation, chromospheric features, and Zeeman\nbroadening of the FeH infrared line. The complex relationship between these\nactivity indicators is analysed: while no strong connection is found between\nthe large-scale and small-scale magnetic fields, the latter relates with the\nnon-thermal flux originating in the chromosphere.\nWe then examine the relationship between various activity diagnostics and the\noptical radial-velocity jitter available in the literature, especially for\nplanet host stars. We use this to derive for all stars an activity merit\nfunction (higher for quieter stars) with the goal of identifying the most\nfavorable stars where the radial-velocity jitter is low enough for planet\nsearches. We find that the main contributors to the RV jitter are the\nlarge-scale magnetic field and the chromospheric non-thermal emission.\nIn addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed\nalong their rotation using the spectropolarimetric mode, and we derive their\nmagnetic topology. These very slow rotators are good representatives of future\nSPIRou targets. They are compared to other stars where the magnetic topology is\nalso known. The poloidal component of the magnetic field is predominent in all\nthree stars.\n', 'title': 'SPIRou Input Catalog: Activity, Rotation and Magnetic Field of Cool Dwarfs'}]","Title: SPIRou Input Catalog: Activity, Rotation and Magnetic Field of Cool Dwarfs

Abstract:   Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we present new measurements of activity and magnetic field proxies of 442 low-mass K5-M7 dwarfs. The objects were analysed as potential targets to search for planetary-mass companions with the new spectropolarimeter and high-precision velocimeter, SPIRou. We have analysed their high-resolution spectra in an homogeneous way: circular polarisation, chromospheric features, and Zeeman broadening of the FeH infrared line. The complex relationship between these activity indicators is analysed: while no strong connection is found between the large-scale and small-scale magnetic fields, the latter relates with the non-thermal flux originating in the chromosphere.
We then examine the relationship between various activity diagnostics and the optical radial-velocity jitter available in the literature, especially for planet host stars. We use this to derive for all stars an activity merit function (higher for quieter stars) with the goal of identifying the most favorable stars where the radial-velocity jitter is low enough for planet searches. We find that the main contributors to the RV jitter are the large-scale magnetic field and the chromospheric non-thermal emission.
In addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed along their rotation using the spectropolarimetric mode, and we derive their magnetic topology. These very slow rotators are good representatives of future SPIRou targets. They are compared to other stars where the magnetic topology is also known. The poloidal component of the magnetic field is predominent in all three stars.

The authors' analysis helps identify suitable stars for searching for planets using radial velocity measurements by providing an activity merit function that ranks stars based on their radial-velocity jitter, allowing researchers to focus on the most favorable stars for planet searches."
What novel applications arise from combining transformation optics with nonlinear materials?,5905cb28,"[{'abstract': ""  The advances in geometric approaches to optical devices due to transformation\noptics has led to the development of cloaks, concentrators, and other devices.\nIt has also been shown that transformation optics can be used to gravitational\nfields from general relativity. However, the technique is currently constrained\nto linear devices, as a consistent approach to nonlinearity (including both the\ncase of a nonlinear background medium and a nonlinear transformation) remains\nan open question. Here we show that nonlinearity can be incorporated into\ntransformation optics in a consistent way. We use this to illustrate a number\nof novel effects, including cloaking an optical soliton, modeling nonlinear\nsolutions to Einstein's field equations, controlling transport in a Debye\nsolid, and developing a set of constitutive to relations for relativistic\ncloaks in arbitrary nonlinear backgrounds.\n"", 'title': 'A Unified Approach to Nonlinear Transformation Materials'}]","Based on the provided abstracts, I found a relevant paper that discusses the combination of transformation optics with nonlinear materials.

Title: A Unified Approach to Nonlinear Transformation Materials

Abstract: The advances in geometric approaches to optical devices due to transformation optics has led to the development of cloaks, concentrators, and other devices. It has also been shown that transformation optics can be used to gravitational fields from general relativity. However, the technique is currently constrained to linear devices, as a consistent approach to nonlinearity (including both the case of a nonlinear background medium and a nonlinear transformation) remains an open question. Here we show that nonlinearity can be incorporated into transformation optics in a consistent way. We use this to illustrate a number of novel effects, including cloaking an optical soliton, modeling nonlinear solutions to Einstein's field equations, controlling transport in a Debye solid, and developing a set of constitutive to relations for relativistic cloaks in arbitrary nonlinear backgrounds.

In this paper, the authors demonstrate that nonlinearity can be incorporated into transformation optics, which leads to novel effects such as cloaking an optical soliton, modeling nonlinear solutions to Einstein's field equations, controlling transport in a Debye solid, and developing constitutive relations for relativistic cloaks in arbitrary nonlinear backgrounds. These applications arise from combining transformation optics with nonlinear materials."
What is the formation process of barchan dunes underwater?,fa338c57,"[{'abstract': '  Barchan dunes are crescentic shape dunes with horns pointing downstream. The\npresent paper reports the formation of subaqueous barchan dunes from initially\nconical heaps in a rectangular channel. Because the most unique feature of a\nbarchan dune is its horns, we associate the timescale for the appearance of\nhorns to the formation of a barchan dune. A granular heap initially conical was\nplaced on the bottom wall of a closed conduit and it was entrained by a water\nflow in turbulent regime. After a certain time, horns appear and grow, until an\nequilibrium length is reached. Our results show the existence of the timescales\n$0.5t_c$ and $2.5t_c$ for the appearance and equilibrium of horns,\nrespectively, where $t_c$ is a characteristic time that scales with the grains\ndiameter, gravity acceleration, densities of the fluid and grains, and shear\nand threshold velocities.\n', 'title': 'Birth of a subaqueous barchan dune'}]","I think I can help you with that!

Title: Birth of a subaqueous barchan dune

Abstract: Barchan dunes are crescentic shape dunes with horns pointing downstream. The present paper reports the formation of subaqueous barchan dunes from initially conical heaps in a rectangular channel. Because the most unique feature of a barchan dune is its horns, we associate the timescale for the appearance of horns to the formation of a barchan dune. A granular heap initially conical was placed on the bottom wall of a closed conduit and it was entrained by a water flow in turbulent regime. After a certain time, horns appear and grow, until an equilibrium length is reached. Our results show the existence of the timescales $0.5t_c$ and $2.5t_c$ for the appearance and equilibrium of horns, respectively, where $t_c$ is a characteristic time that scales with the grains diameter, gravity acceleration, densities of the fluid and grains, and shear and threshold velocities."
What two new modified parallel orbital-updating methods are proposed in this paper?,65c50b33,"[{'abstract': '  Motivated by the recently proposed parallel orbital-updating approach in real\nspace method, we propose a parallel orbital-updating based plane-wave basis\nmethod for electronic structure calculations, for solving the corresponding\neigenvalue problems. In addition, we propose two new modified parallel\norbital-updating methods. Compared to the traditional plane-wave methods, our\nmethods allow for two-level parallelization, which is particularly interesting\nfor large scale parallelization. Numerical experiments show that these new\nmethods are more reliable and efficient for large scale calculations on modern\nsupercomputers\n', 'title': 'A parallel orbital-updating based plane-wave basis method for electronic structure calculations'}]","Unfortunately, I couldn't find the answer to your question in the provided context. The abstract of the paper ""A parallel orbital-updating based plane-wave basis method for electronic structure calculations"" mentions that two new modified parallel orbital-updating methods are proposed, but it doesn't specify what these methods are.

If you're looking for more information about these methods, I suggest searching for the full paper or contacting the authors directly."
How does the use of a quarter-car linear suspension model contribute to the design of an H-infinity filter for a cloud-aided semiactive suspension system?,a1ccda61,"[{'abstract': '  This chapter presents an H-infinity filtering framework for cloud-aided\nsemiactive suspension system with time-varying delays. In this system, road\nprofile information is downloaded from a cloud database to facilitate onboard\nestimation of suspension states. Time-varying data transmission delays are\nconsidered and assumed to be bounded. A quarter-car linear suspension model is\nused and an H-infinity filter is designed with both onboard sensor measurements\nand delayed road profile information from the cloud. The filter design\nprocedure is designed based on linear matrix inequalities (LMIs). Numerical\nsimulation results are reported that illustrates the fusion of cloud-based and\non-board information that can be achieved in Vehicleto- Cloud-to-Vehicle\n(V2C2V) implementation.\n', 'title': 'H-infinity Filtering for Cloud-Aided Semi-active Suspension with Delayed Information'}]","Title: H-infinity Filtering for Cloud-Aided Semi-active Suspension with Delayed Information

Abstract: This chapter presents an H-infinity filtering framework for cloud-aided semiactive suspension system with time-varying delays. In this system, road profile information is downloaded from a cloud database to facilitate onboard estimation of suspension states. Time-varying data transmission delays are considered and assumed to be bounded. A quarter-car linear suspension model is used and an H-infinity filter is designed with both onboard sensor measurements and delayed road profile information from the cloud. The filter design procedure is designed based on linear matrix inequalities (LMIs). Numerical simulation results are reported that illustrates the fusion of cloud-based and on-board information that can be achieved in Vehicle-to-Cloud-to-Vehicle (V2C2V) implementation.

The use of a quarter-car linear suspension model contributes to the design of an H-infinity filter for a cloud-aided semiactive suspension system by providing a simplified and tractable representation of the system, allowing for the development of an effective filtering framework that can handle time-varying delays and integrate cloud-based and onboard information."
What are the limitations of existing methods for finding exact expressions of the smallest eigenvalue density for complex Wishart-Laguerre matrices?,8f701419,"[{'abstract': ""  The statistical behaviour of the smallest eigenvalue has important\nimplications for systems which can be modeled using a Wishart-Laguerre\nensemble, the regular one or the fixed trace one. For example, the density of\nthe smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role\nin characterizing multiple channel telecommunication systems. Similarly, in the\nquantum entanglement problem, the smallest eigenvalue of the fixed trace\nensemble carries information regarding the nature of entanglement.\nFor real Wishart-Laguerre matrices, there exists an elegant recurrence scheme\nsuggested by Edelman to directly obtain the exact expression for the smallest\neigenvalue density. In the case of complex Wishart-Laguerre matrices, for\nfinding exact and explicit expressions for the smallest eigenvalue density,\nexisting results based on determinants become impractical when the determinants\ninvolve large-size matrices. In this work, we derive a recurrence scheme for\nthe complex case which is analogous to that of Edelman's for the real case.\nThis is used to obtain exact results for the smallest eigenvalue density for\nboth the regular, and the fixed trace complex Wishart-Laguerre ensembles. We\nvalidate our analytical results using Monte Carlo simulations. We also study\nscaled Wishart-Laguerre ensemble and investigate its efficacy in approximating\nthe fixed-trace ensemble. Eventually, we apply our result for the fixed-trace\nensemble to investigate the behaviour of the smallest eigenvalue in the\nparadigmatic system of coupled kicked tops.\n"", 'title': 'Smallest eigenvalue density for regular or fixed-trace complex Wishart-Laguerre ensemble and entanglement in coupled kicked tops'}]","Title: Smallest eigenvalue density for regular or fixed-trace complex Wishart-Laguerre ensemble and entanglement in coupled kicked tops

Abstract: ...In the case of complex Wishart-Laguerre matrices, for finding exact and explicit expressions for the smallest eigenvalue density, existing results based on determinants become impractical when the determinants involve large-size matrices...

The limitation of existing methods is that they become impractical when dealing with large-size matrices."
How does the scheduling policy discussed in the paper relate to the implementation on routers or multi-layer switches?,42e17545,"[{'abstract': '  Networked control systems (NCS) have attracted considerable attention in\nrecent years. While the stabilizability and optimal control of NCS for a given\ncommunication system has already been studied extensively, the design of the\ncommunication system for NCS has recently seen an increase in more thorough\ninvestigation. In this paper, we address an optimal scheduling problem for a\nset of NCS sharing a dedicated communication channel, providing performance\nbounds and asymptotic stability. We derive a suboptimal scheduling policy with\ndynamic state-based priorities calculated at the sensors, which are then used\nfor stateless priority queuing in the network, making it both scalable and\nefficient to implement on routers or multi-layer switches. These properties are\nbeneficial towards leveraging existing IP networks for control, which will be a\ncrucial factor for the proliferation of wide-area NCS applications. By allowing\nfor an arbitrary number of concurrent transmissions, we are able to investigate\nthe relationship between available bandwidth, transmission rate, and delay. To\ndemonstrate the feasibility of our approach, we provide a proof-of-concept\nimplementation of the priority scheduler using real networking hardware.\n', 'title': 'State-dependent Priority Scheduling for Networked Control Systems'}]","Title: State-dependent Priority Scheduling for Networked Control Systems

Abstract:   Networked control systems (NCS) have attracted considerable attention in
recent years. While the stabilizability and optimal control of NCS for a given
communication system has already been studied extensively, the design of the
communication system for NCS has recently seen an increase in more thorough
investigation. In this paper, we address an optimal scheduling problem for a
set of NCS sharing a dedicated communication channel, providing performance
bounds and asymptotic stability. We derive a suboptimal scheduling policy with
dynamic state-based priorities calculated at the sensors, which are then used
for stateless priority queuing in the network, making it both scalable and
efficient to implement on routers or multi-layer switches. These properties are
beneficial towards leveraging existing IP networks for control, which will be a
crucial factor for the proliferation of wide-area NCS applications. By allowing
for an arbitrary number of concurrent transmissions, we are able to investigate
the relationship between available bandwidth, transmission rate, and delay. To
demonstrate the feasibility of our approach, we provide a proof-of-concept
implementation of the priority scheduler using real networking hardware.

The scheduling policy discussed in this paper relates to the implementation on routers or multi-layer switches in that the suboptimal scheduling policy with dynamic state-based priorities is designed to be scalable and efficient to implement on these devices, making it possible to leverage existing IP networks for control."
How many classification datasets are included in the OpenML100 benchmark suite?,623f37bf,"[{'abstract': '  We advocate the use of curated, comprehensive benchmark suites of machine\nlearning datasets, backed by standardized OpenML-based interfaces and\ncomplementary software toolkits written in Python, Java and R. Major\ndistinguishing features of OpenML benchmark suites are (a) ease of use through\nstandardized data formats, APIs, and existing client libraries; (b)\nmachine-readable meta-information regarding the contents of the suite; and (c)\nonline sharing of results, enabling large scale comparisons. As a first such\nsuite, we propose the OpenML100, a machine learning benchmark suite of\n100~classification datasets carefully curated from the thousands of datasets\navailable on OpenML.org.\n', 'title': 'OpenML Benchmarking Suites and the OpenML100'}]","Title: OpenML Benchmarking Suites and the OpenML100

Abstract: We advocate the use of curated, comprehensive benchmark suites of machine learning datasets, backed by standardized OpenML-based interfaces and complementary software toolkits written in Python, Java and R. Major distinguishing features of OpenML benchmark suites are (a) ease of use through standardized data formats, APIs, and existing client libraries; (b) machine-readable meta-information regarding the contents of the suite; and (c) online sharing of results, enabling large scale comparisons. As a first such suite, we propose the OpenML100, a machine learning benchmark suite of 100~classification datasets carefully curated from the thousands of datasets available on OpenML.org.

Answer: 100"
What are the key characteristics that distinguish quantum spin liquids from conventional crystalline magnets?,bd7de381,"[{'abstract': '  Conventional crystalline magnets are characterized by symmetry breaking and\nnormal modes of excitation called magnons with quantized angular momentum\n$\\hbar$. Neutron scattering correspondingly features extra magnetic Bragg\ndiffraction at low temperatures and dispersive inelastic scattering associated\nwith single magnon creation and annihilation. Exceptions are anticipated in\nso-called quantum spin liquids as exemplified by the one-dimensional spin-1/2\nchain which has no magnetic order and where magnons accordingly fractionalize\ninto spinons with angular momentum $\\hbar/2$. This is spectacularly revealed by\na continuum of inelastic neutron scattering associated with two-spinon\nprocesses and the absence of magnetic Bragg diffraction. Here, we report\nevidence for these same key features of a quantum spin liquid in the\nthree-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific\nheat and neutron scattering measurements, Monte Carlo simulations, and analytic\napproximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is\nan almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on\na pyrochlore lattice with weak connectivity and frustrated interactions.\nMagnetic Bragg diffraction is absent and 90\\% of the spectral weight forms a\ncontinuum of magnetic scattering not dissimilar to that of the spin-1/2 chain\nbut with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb\nphase. The residual entropy and diffuse elastic scattering points to an exotic\nstate of matter driven by frustration, quantum fluctuations and weak exchange\ndisorder.\n', 'title': 'Continuum of quantum fluctuations in a three-dimensional $S\\!=\\!1$ Heisenberg magnet'}]","Title: Continuum of quantum fluctuations in a three-dimensional $S\!=\!1$ Heisenberg magnet

Abstract: Conventional crystalline magnets are characterized by symmetry breaking and normal modes of excitation called magnons with quantized angular momentum $\hbar$. Neutron scattering correspondingly features extra magnetic Bragg diffraction at low temperatures and dispersive inelastic scattering associated with single magnon creation and annihilation. Exceptions are anticipated in so-called quantum spin liquids as exemplified by the one-dimensional spin-1/2 chain which has no magnetic order and where magnons accordingly fractionalize into spinons with angular momentum $\hbar/2$. This is spectacularly revealed by a continuum of inelastic neutron scattering associated with two-spinon processes and the absence of magnetic Bragg diffraction."
Has a general testing procedure for variance components in mixed effects models been fully investigated?,f33e795a,"[{'abstract': '  Mixed effects models are widely used to describe heterogeneity in a\npopulation. A crucial issue when adjusting such a model to data consists in\nidentifying fixed and random effects. From a statistical point of view, it\nremains to test the nullity of the variances of a given subset of random\neffects. Some authors have proposed to use the likelihood ratio test and have\nestablished its asymptotic distribution in some particular cases. Nevertheless,\nto the best of our knowledge, no general variance components testing procedure\nhas been fully investigated yet. In this paper, we study the likelihood ratio\ntest properties to test that the variances of a general subset of the random\neffects are equal to zero in both linear and nonlinear mixed effects model,\nextending the existing results. We prove that the asymptotic distribution of\nthe test is a chi-bar-square distribution, that is to say a mixture of\nchi-square distributions, and we identify the corresponding weights. We\nhighlight in particular that the limiting distribution depends on the presence\nof correlations between the random effects but not on the linear or nonlinear\nstructure of the mixed effects model. We illustrate the finite sample size\nproperties of the test procedure through simulation studies and apply the test\nprocedure to two real datasets of dental growth and of coucal growth.\n', 'title': 'Likelihood ratio test for variance components in nonlinear mixed effects models'}]","Title: Likelihood ratio test for variance components in nonlinear mixed effects models

Abstract: Mixed effects models are widely used to describe heterogeneity in a population. A crucial issue when adjusting such a model to data consists in identifying fixed and random effects. From a statistical point of view, it remains to test the nullity of the variances of a given subset of random effects. Some authors have proposed to use the likelihood ratio test and have established its asymptotic distribution in some particular cases. Nevertheless, to the best of our knowledge, no general variance components testing procedure has been fully investigated yet. In this paper, we study the likelihood ratio test properties to test that the variances of a general subset of the random effects are equal to zero in both linear and nonlinear mixed effects model, extending the existing results. We prove that the asymptotic distribution of the test is a chi-bar-square distribution, that is to say a mixture of chi-square distributions, and we identify the corresponding weights. We highlight in particular that the limiting distribution depends on the presence of correlations between the random effects but not on the linear or nonlinear structure of the mixed effects model. We illustrate the finite sample size properties of the test procedure through simulation studies and apply the test procedure to two real datasets of dental growth and of coucal growth."
What is the affine Grassmannian associated to  SL2 called?,11bef1ff,"[{'abstract': '  Let $G:=\\widehat{SL_2}$ denote the affine Kac-Moody group associated to\n$SL_2$ and $\\bar{\\mathcal{X}}$ the associated affine Grassmannian. We determine\nan inductive formula for the Schubert basis structure constants in the\ntorus-equivariant Grothendieck group of $\\bar{\\mathcal{X}}$. In the case of\nordinary (non-equivariant) $K$-theory we find an explicit closed form for the\nstructure constants. We also determine an inductive formula for the structure\nconstants in the torus-equivariant cohomology ring, and use this formula to\nfind closed forms for some of the structure constants.\n', 'title': 'An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$'}]","Based on the context, I found an answer to your question:

Title: An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$

Abstract: Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to $SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine an inductive formula for the Schubert basis structure constants in the torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of ordinary (non-equivariant) $K$-theory we find an explicit closed form for the structure constants. We also determine an inductive formula for the structure constants in the torus-equivariant cohomology ring, and use this formula to find closed forms for some of the structure constants.

The affine Grassmannian associated to $SL_2$ is denoted as $\bar{\mathcal{X}}$."
Are there established theoretical bounds comparable to those found for linear combinations of sigmoids in previous work? ,97eca714,"[{'abstract': '  We develop a new approach to learn the parameters of regression models with\nhidden variables. In a nutshell, we estimate the gradient of the regression\nfunction at a set of random points, and cluster the estimated gradients. The\ncenters of the clusters are used as estimates for the parameters of hidden\nunits. We justify this approach by studying a toy model, whereby the regression\nfunction is a linear combination of sigmoids. We prove that indeed the\nestimated gradients concentrate around the parameter vectors of the hidden\nunits, and provide non-asymptotic bounds on the number of required samples. To\nthe best of our knowledge, no comparable guarantees have been proven for linear\ncombinations of sigmoids.\n', 'title': 'Learning Combinations of Sigmoids Through Gradient Estimation'}]","Title: Concentration of weakly dependent Banach-valued sums and applications to statistical learning methods

Abstract: We obtain a Bernstein-type inequality for sums of Banach-valued random variables satisfying a weak dependence assumption of general type and under certain smoothness assumptions of the underlying Banach norm. We use this inequality in order to investigate in the asymptotical regime the error upper bounds for the broad family of spectral regularization methods for reproducing kernel decision rules, when trained on a sample coming from a $\tau-$mixing process."
How does a social robot provide feedback to a user in a brain-machine interaction system?,54fcc6a8,"[{'abstract': ""  Brain-Machine Interaction (BMI) system motivates interesting and promising\nresults in forward/feedback control consistent with human intention. It holds\ngreat promise for advancements in patient care and applications to\nneurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic\nplatform using a personalized social robot in order to assist patients having\ncognitive deficits through bilateral rehabilitation and mental training. For\ninitial testing of the platform, electroencephalography (EEG) brainwaves of a\nhuman user were collected in real time during tasks of imaginary movements.\nFirst, the brainwaves associated with imagined body kinematics parameters were\ndecoded to control a cursor on a computer screen in training protocol. Then,\nthe experienced subject was able to interact with a social robot via our\nreal-time BMI robotic platform. Corresponding to subject's imagery performance,\nhe/she received specific gesture movements and eye color changes as\nneural-based feedback from the robot. This hands-free neurofeedback interaction\nnot only can be used for mind control of a social robot's movements, but also\nsets the stage for application to enhancing and recovering mental abilities\nsuch as attention via training in humans by providing real-time neurofeedback\nfrom a social robot.\n"", 'title': 'Real-time brain machine interaction via social robot gesture control'}]","Title: Real-time brain machine interaction via social robot gesture control

Abstract: Brain-Machine Interaction (BMI) system motivates interesting and promising results in forward/feedback control consistent with human intention. It holds great promise for advancements in patient care and applications to neurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic platform using a personalized social robot in order to assist patients having cognitive deficits through bilateral rehabilitation and mental training. For initial testing of the platform, electroencephalography (EEG) brainwaves of a human user were collected in real time during tasks of imaginary movements. First, the brainwaves associated with imagined body kinematics parameters were decoded to control a cursor on a computer screen in training protocol. Then, the experienced subject was able to interact with a social robot via our real-time BMI robotic platform. Corresponding to subject's imagery performance, he/she received specific gesture movements and eye color changes as neural-based feedback from the robot. This hands-free neurofeedback interaction not only can be used for mind control of a social robot's movements, but also sets the stage for application to enhancing and recovering mental abilities such as attention via training in humans by providing real-time neurofeedback from a social robot."
In what types of datasets has the RHOMP model demonstrated superior predictive accuracy compared to other methods?,1a4e3129,"[{'abstract': '  Users form information trails as they browse the web, checkin with a\ngeolocation, rate items, or consume media. A common problem is to predict what\na user might do next for the purposes of guidance, recommendation, or\nprefetching. First-order and higher-order Markov chains have been widely used\nmethods to study such sequences of data. First-order Markov chains are easy to\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\nin contrast, have too many parameters and suffer from overfitting the training\ndata. Fitting these parameters with regularization and smoothing only offers\nmild improvements. In this paper we propose the retrospective higher-order\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\nis a special case of a higher-order Markov chain where the transitions depend\nretrospectively on a single history state instead of an arbitrary combination\nof history states. There are two immediate computational advantages: the number\nof parameters is linear in the order of the Markov chain and the model can be\nfit to large state spaces. Furthermore, by providing a specific structure to\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\nutilizing history states without risks of overfitting the data. We demonstrate\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\nmethod on various real application datasets spanning geolocation data, review\nsequences, and business locations. The RHOMP model uniformly outperforms\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\nfactorizations in terms of prediction accuracy.\n', 'title': 'Retrospective Higher-Order Markov Processes for User Trails'}]","Title: Retrospective Higher-Order Markov Processes for User Trails

Abstract: Users form information trails as they browse the web, checkin with a geolocation, rate items, or consume media. A common problem is to predict what a user might do next for the purposes of guidance, recommendation, or prefetching. First-order and higher-order Markov chains have been widely used methods to study such sequences of data. First-order Markov chains are easy to estimate, but lack accuracy when history matters. Higher-order Markov chains, in contrast, have too many parameters and suffer from overfitting the training data. Fitting these parameters with regularization and smoothing only offers mild improvements. In this paper we propose the retrospective higher-order Markov process (RHOMP) as a low-parameter model for such sequences. This model is a special case of a higher-order Markov chain where the transitions depend retrospectively on a single history state instead of an arbitrary combination of history states. There are two immediate computational advantages: the number of parameters is linear in the order of the Markov chain and the model can be fit to large state spaces. Furthermore, by providing a specific structure to the higher-order chain, RHOMPs improve the model accuracy by efficiently utilizing history states without risks of overfitting the data. We demonstrate how to estimate a RHOMP from data and we demonstrate the effectiveness of our method on various real application datasets spanning geolocation data, review sequences, and business locations. The RHOMP model uniformly outperforms higher-order Markov chains, Kneser-Ney regularization, and tensor factorizations in terms of prediction accuracy.

The RHOMP model has demonstrated superior predictive accuracy compared to other methods on datasets including geolocation data, review sequences, and business locations."
What are some methods for object localization in images with limited labeled data?,2292955c,"[{'abstract': '  We address the problem of localisation of objects as bounding boxes in images\nwith weak labels. This weakly supervised object localisation problem has been\ntackled in the past using discriminative models where each object class is\nlocalised independently from other classes. We propose a novel framework based\non Bayesian joint topic modelling. Our framework has three distinctive\nadvantages over previous works: (1) All object classes and image backgrounds\nare modelled jointly together in a single generative model so that ""explaining\naway"" inference can resolve ambiguity and lead to better learning and\nlocalisation. (2) The Bayesian formulation of the model enables easy\nintegration of prior knowledge about object appearance to compensate for\nlimited supervision. (3) Our model can be learned with a mixture of weakly\nlabelled and unlabelled data, allowing the large volume of unlabelled images on\nthe Internet to be exploited for learning. Extensive experiments on the\nchallenging VOC dataset demonstrate that our approach outperforms the\nstate-of-the-art competitors.\n', 'title': 'Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation'}]","Based on the context, I found two papers that address object localization in images with limited labeled data.

Here are the results:

**1. Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation**

Abstract: We address the problem of localisation of objects as bounding boxes in images with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. We propose a novel framework based on Bayesian joint topic modelling. Our framework has three distinctive advantages over previous works: (1) All object classes and image backgrounds are modelled jointly together in a single generative model so that ""explaining away"" inference can resolve ambiguity and lead to better learning and localisation. (2) The Bayesian formulation of the model enables easy integration of prior knowledge about object appearance to compensate for limited supervision. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Extensive experiments on the challenging VOC dataset demonstrate that our approach outperforms the state-of-the-art competitors.

**2. Attention-based Natural Language Person Retrieval**

Abstract: Following the recent progress in image classification and captioning using deep learning, we develop a novel natural language person retrieval system based on an attention mechanism. More specifically, given the description of a person, the goal is to localize the person in an image. To this end, we first construct a benchmark dataset for natural language person retrieval. To do so, we generate bounding boxes for persons in a public image dataset from the segmentation masks, which are then annotated with descriptions and attributes using the Amazon Mechanical Turk. We then adopt a region proposal network in Faster R-CNN as a candidate region generator. The cropped images based on the region proposals as well as the whole images with attention weights are fed into Convolutional Neural Networks for visual feature extraction, while the natural language expression and attributes are input to Bidirectional Long Short-Term Memory (BLSTM) models for text feature extraction. The visual and text features are integrated to score region proposals, and the one with the highest score is retrieved as the output of our system. The experimental results show significant improvement over the state-of-the-art method for generic object retrieval and this line of research promises to benefit search in surveillance video footage."
What are some recent advancements in the study of homogenization for elliptic systems that relax traditional function space conditions?,9bb106ee,"[{'abstract': '  We study the homogenization process for families of strongly nonlinear\nelliptic systems with the homogeneous Dirichlet boundary conditions. The growth\nand the coercivity of the elliptic operator is assumed to be indicated by a\ngeneral inhomogeneous anisotropic $N-$function, which may be possibly also\ndependent on the spatial variable, i.e., the homogenization process will change\nthe characteristic function spaces at each step. Such a problem is well known\nand there exists many positive results for the function satisfying $\\Delta_2$\nand $\\nabla_2$ conditions an being in addition Hölder continuous with\nrespect to the spatial variable. We shall show that cases these conditions can\nbe neglected and will deal with a rather general problem in general function\nspace setting.\n', 'title': 'Homogenization of nonlinear elliptic systems in nonreflexive Musielak-Orlicz spaces'}]","Title: Homogenization of nonlinear elliptic systems in nonreflexive Musielak-Orlicz spaces

Abstract: We study the homogenization process for families of strongly nonlinear
elliptic systems with the homogeneous Dirichlet boundary conditions. The growth
and the coercivity of the elliptic operator is assumed to be indicated by a
general inhomogeneous anisotropic $N-$function, which may be possibly also
dependent on the spatial variable, i.e., the homogenization process will change
the characteristic function spaces at each step. Such a problem is well known
and there exists many positive results for the function satisfying $\Delta_2$
and $\nabla_2$ conditions an being in addition Hölder continuous with
respect to the spatial variable. We shall show that cases these conditions can
be neglected and will deal with a rather general problem in general function
space setting."
What type of phase transition occurs between the drift-BCS state and the Fulde-Ferrell-Larkin-Ovchinnikov state?,1adb8417,"[{'abstract': '  We show that in the presence of magnetic field, two superconducting phases\nwith the center-of-mass momentum of Cooper pair parallel to the magnetic field\nare induced in spin-orbit-coupled superconductor Li$_2$Pd$_3$B. Specifically,\nat small magnetic field, the center-of-mass momentum is induced due to the\nenergy-spectrum distortion and no unpairing region with vanishing singlet\ncorrelation appears. We refer to this superconducting state as the drift-BCS\nstate. By further increasing the magnetic field, the superconducting state\nfalls into the Fulde-Ferrell-Larkin-Ovchinnikov state with the emergence of the\nunpairing regions. The observed abrupt enhancement of the center-of-mass\nmomenta and suppression on the order parameters during the crossover indicate\nthe first-order phase transition. Enhanced Pauli limit and hence enlarged\nmagnetic-field regime of the Fulde-Ferrell-Larkin-Ovchinnikov state, due to the\nspin-flip terms of the spin-orbit coupling, are revealed. We also address the\ntriplet correlations induced by the spin-orbit coupling, and show that the\nCooper-pair spin polarizations, generated by the magnetic field and\ncenter-of-mass momentum with the triplet correlations, exhibit totally\ndifferent magnetic-field dependences between the drift-BCS and\nFulde-Ferrell-Larkin-Ovchinnikov states.\n', 'title': 'Fulde-Ferrell-Larkin-Ovchinnikov state in spin-orbit-coupled superconductors'}]","Title: Fulde-Ferrell-Larkin-Ovchinnikov state in spin-orbit-coupled superconductors

Abstract:   We show that in the presence of magnetic field, two superconducting phases
with the center-of-mass momentum of Cooper pair parallel to the magnetic field
are induced in spin-orbit-coupled superconductor Li$_2$Pd$_3$B. Specifically,
at small magnetic field, the center-of-mass momentum is induced due to the
energy-spectrum distortion and no unpairing region with vanishing singlet
correlation appears. We refer to this superconducting state as the drift-BCS
state. By further increasing the magnetic field, the superconducting state
falls into the Fulde-Ferrell-Larkin-Ovchinnikov state with the emergence of the
unpairing regions. The observed abrupt enhancement of the center-of-mass
momenta and suppression on the order parameters during the crossover indicate
the first-order phase transition."
What are the limitations of idealizing porous media as bundles of straight cylindrical tubes when studying rarefied gas flows?,1affc023,"[{'abstract': ""  The apparent gas permeability of the porous medium is an important parameter\nin the prediction of unconventional gas production, which was first\ninvestigated systematically by Klinkenberg in 1941 and found to increase with\nthe reciprocal mean gas pressure (or equivalently, the Knudsen number).\nAlthough the underlying rarefaction effects are well-known, the reason that the\ncorrection factor in Klinkenberg's famous equation decreases when the Knudsen\nnumber increases has not been fully understood. Most of the studies idealize\nthe porous medium as a bundle of straight cylindrical tubes, however, according\nto the gas kinetic theory, this only results in an increase of the correction\nfactor with the Knudsen number, which clearly contradicts Klinkenberg's\nexperimental observations. Here, by solving the Bhatnagar-Gross-Krook equation\nin simplified (but not simple) porous media, we identify, for the first time,\ntwo key factors that can explain Klinkenberg's experimental results: the\ntortuous flow path and the non-unitary tangential momentum accommodation\ncoefficient for the gas-surface interaction. Moreover, we find that\nKlinkenberg's results can only be observed when the ratio between the apparent\nand intrinsic permeabilities is $\\lesssim30$; at large ratios (or Knudsen\nnumbers) the correction factor increases with the Knudsen number. Our numerical\nresults could also serve as benchmarking cases to assess the accuracy of\nmacroscopic models and/or numerical schemes for the modeling/simulation of\nrarefied gas flows in complex geometries over a wide range of gas rarefaction.\n"", 'title': 'On the apparent permeability of porous media in rarefied gas flows'}]","Title: On the apparent permeability of porous media in rarefied gas flows

Abstract: The apparent gas permeability of the porous medium is an important parameter
in the prediction of unconventional gas production, which was first
investigated systematically by Klinkenberg in 1941 and found to increase with
the reciprocal mean gas pressure (or equivalently, the Knudsen number).
Although the underlying rarefaction effects are well-known, the reason that the
correction factor in Klinkenberg's famous equation decreases when the Knudsen
number increases has not been fully understood. Most of the studies idealize
the porous medium as a bundle of straight cylindrical tubes, however, according
to the gas kinetic theory, this only results in an increase of the correction
factor with the Knudsen number, which clearly contradicts Klinkenberg's
experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
in simplified (but not simple) porous media, we identify, for the first time,
two key factors that can explain Klinkenberg's experimental results: the
tortuous flow path and the non-unitary tangential momentum accommodation
coefficient for the gas-surface interaction. Moreover, we find that
Klinkenberg's results can only be observed when the ratio between the apparent
and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
numbers) the correction factor increases with the Knudsen number. Our numerical
results could also serve as benchmarking cases to assess the accuracy of
macroscopic models and/or numerical schemes for the modeling/simulation of
rarefied gas flows in complex geometries over a wide range of gas rarefaction.

The limitations of idealizing porous media as bundles of straight cylindrical tubes are that it only results in an increase of the correction factor with the Knudsen number, which contradicts Klinkenberg's experimental observations."
What are the challenges of predicting future sequences in real-world scenarios that involve uncertainty?,e71e4363,"[{'abstract': '  For autonomous agents to successfully operate in the real world, anticipation\nof future events and states of their environment is a key competence. This\nproblem has been formalized as a sequence extrapolation problem, where a number\nof observations are used to predict the sequence into the future. Real-world\nscenarios demand a model of uncertainty of such predictions, as predictions\nbecome increasingly uncertain -- in particular on long time horizons. While\nimpressive results have been shown on point estimates, scenarios that induce\nmulti-modal distributions over future sequences remain challenging. Our work\naddresses these challenges in a Gaussian Latent Variable model for sequence\nprediction. Our core contribution is a ""Best of Many"" sample objective that\nleads to more accurate and more diverse predictions that better capture the\ntrue variations in real-world sequence data. Beyond our analysis of improved\nmodel fit, our models also empirically outperform prior work on three diverse\ntasks ranging from traffic scenes to weather data.\n', 'title': 'Accurate and Diverse Sampling of Sequences based on a ""Best of Many"" Sample Objective'}]","Title: Accurate and Diverse Sampling of Sequences based on a ""Best of Many"" Sample Objective

Abstract: For autonomous agents to successfully operate in the real world, anticipation
of future events and states of their environment is a key competence. This
problem has been formalized as a sequence extrapolation problem, where a number
of observations are used to predict the sequence into the future. Real-world
scenarios demand a model of uncertainty of such predictions, as predictions
become increasingly uncertain -- in particular on long time horizons. While
impressive results have been shown on point estimates, scenarios that induce
multi-modal distributions over future sequences remain challenging."
What unsupervised methods were evaluated for homogenizing electronic health records?,ea693a0b,"[{'abstract': '  Electronic health records (EHR) contain a large variety of information on the\nclinical history of patients such as vital signs, demographics, diagnostic\ncodes and imaging data. The enormous potential for discovery in this rich\ndataset is hampered by its complexity and heterogeneity.\nWe present the first study to assess unsupervised homogenization pipelines\ndesigned for EHR clustering. To identify the optimal pipeline, we tested\naccuracy on simulated data with varying amounts of redundancy, heterogeneity,\nand missingness. We identified two optimal pipelines: 1) Multiple Imputation by\nChained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,\nZ-scoring, and Deep Autoencoders.\n', 'title': 'An Unsupervised Homogenization Pipeline for Clustering Similar Patients using Electronic Health Record Data'}]","Title: An Unsupervised Homogenization Pipeline for Clustering Similar Patients using Electronic Health Record Data

Abstract: Electronic health records (EHR) contain a large variety of information on the clinical history of patients such as vital signs, demographics, diagnostic codes and imaging data. The enormous potential for discovery in this rich dataset is hampered by its complexity and heterogeneity. We present the first study to assess unsupervised homogenization pipelines designed for EHR clustering. To identify the optimal pipeline, we tested accuracy on simulated data with varying amounts of redundancy, heterogeneity, and missingness. We identified two optimal pipelines: 1) Multiple Imputation by Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE, Z-scoring, and Deep Autoencoders."
What chemical system was used to demonstrate the applicability of the vDMRG method to a larger molecule?,48e864c0,"[{'abstract': '  Variational approaches for the calculation of vibrational wave functions and\nenergies are a natural route to obtain highly accurate results with\ncontrollable errors. However, the unfavorable scaling and the resulting high\ncomputational cost of standard variational approaches limit their application\nto small molecules with only few vibrational modes. Here, we demonstrate how\nthe density matrix renormalization group (DMRG) can be exploited to optimize\nvibrational wave functions (vDMRG) expressed as matrix product states. We study\nthe convergence of these calculations with respect to the size of the local\nbasis of each mode, the number of renormalized block states, and the number of\nDMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for\nsmall molecules that were intensively studied in the literature. We then\nproceed to show that the complete fingerprint region of the sarcosyn-glycin\ndipeptide can be calculated with vDMRG.\n', 'title': 'Vibrational Density Matrix Renormalization Group'}]","Title: Vibrational Density Matrix Renormalization Group

Abstract: Variational approaches for the calculation of vibrational wave functions and energies are a natural route to obtain highly accurate results with controllable errors. However, the unfavorable scaling and the resulting high computational cost of standard variational approaches limit their application to small molecules with only few vibrational modes. Here, we demonstrate how the density matrix renormalization group (DMRG) can be exploited to optimize vibrational wave functions (vDMRG) expressed as matrix product states. We study the convergence of these calculations with respect to the size of the local basis of each mode, the number of renormalized block states, and the number of DMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for small molecules that were intensively studied in the literature. We then proceed to show that the complete fingerprint region of the sarcosyn-glycin dipeptide can be calculated with vDMRG."
How is the representation of numbers as the sum of two squares studied in modular arithmetic?,c279748c,"[{'abstract': '  We examine the representation of numbers as the sum of two squares in\n$\\mathbb{Z}_n$ for a general positive integer $n$. Using this information we\nmake some comments about the density of positive integers which can be\nrepresented as the sum of two squares and powers of $2$ in $\\mathbb{N}$.\n', 'title': 'Representing numbers as the sum of squares and powers in the ring $\\mathbb{Z}_n$'}]","Based on the provided context, I found a relevant paper that studies the representation of numbers as the sum of two squares in modular arithmetic.

Title: Representing numbers as the sum of squares and powers in the ring Zn

Abstract: We examine the representation of numbers as the sum of two squares in Zn for a general positive integer n. Using this information we make some comments about the density of positive integers which can be represented as the sum of two squares and powers of 2 in ℕ."
What is meant by a 'white-box' or 'black-box' setup in the context of adversarial attacks?,6be3b6a3,"[{'abstract': '  Deep Learning models are vulnerable to adversarial examples, i.e.\\ images\nobtained via deliberate imperceptible perturbations, such that the model\nmisclassifies them with high confidence. However, class confidence by itself is\nan incomplete picture of uncertainty. We therefore use principled Bayesian\nmethods to capture model uncertainty in prediction for observing adversarial\nmisclassification. We provide an extensive study with different Bayesian neural\nnetworks attacked in both white-box and black-box setups. The behaviour of the\nnetworks for noise, attacks and clean test data is compared. We observe that\nBayesian neural networks are uncertain in their predictions for adversarial\nperturbations, a behaviour similar to the one observed for random Gaussian\nperturbations. Thus, we conclude that Bayesian neural networks can be\nconsidered for detecting adversarial examples.\n', 'title': 'Adversarial Phenomenon in the Eyes of Bayesian Deep Learning'}]","Based on the context, I found the relevant information in the abstract of the paper ""Adversarial Phenomenon in the Eyes of Bayesian Deep Learning"".

Title: Adversarial Phenomenon in the Eyes of Bayesian Deep Learning

Abstract:   Deep Learning models are vulnerable to adversarial examples, i.e.\ images
obtained via deliberate imperceptible perturbations, such that the model
misclassifies them with high confidence. However, class confidence by itself is
an incomplete picture of uncertainty. We therefore use principled Bayesian
methods to capture model uncertainty in prediction for observing adversarial
misclassification. We provide an extensive study with different Bayesian neural
networks attacked in both white-box and black-box setups. The behaviour of the
networks for noise, attacks and clean test data is compared. We observe that
Bayesian neural networks are uncertain in their predictions for adversarial
perturbations, a behaviour similar to the one observed for random Gaussian
perturbations. Thus, we conclude that Bayesian neural networks can be
considered for detecting adversarial examples.

In this context, a 'white-box' setup refers to an attack scenario where the adversary has complete knowledge of the internal structure, parameters, and intermediate confidence values of the target model. On the other hand, a 'black-box' setup refers to an attack scenario where the adversary only has input-output access to the model, without any knowledge of its internal workings."
How does this metric quantify the Shannon entropy of the distance distribution to a specific node?,0283f8f8,"[{'abstract': ""  Open problems abound in the theory of complex networks, which has found\nsuccessful application to diverse fields of science. With the aim of further\nadvancing the understanding of the brain's functional connectivity, we propose\nto evaluate a network metric which we term the geodesic entropy. This entropy,\nin a way that can be made precise, quantifies the Shannon entropy of the\ndistance distribution to a specific node from all other nodes. Measurements of\ngeodesic entropy allow for the characterization of the structural information\nof a network that takes into account the distinct role of each node into the\nnetwork topology. The measurement and characterization of this structural\ninformation has the potential to greatly improve our understanding of sustained\nactivity and other emergent behaviors in networks, such as self-organized\ncriticality sometimes seen in such contexts. We apply these concepts and\nmethods to study the effects of how the psychedelic Ayahuasca affects the\nfunctional connectivity of the human brain. We show that the geodesic entropy\nis able to differentiate the functional networks of the human brain in two\ndifferent states of consciousness in the resting state: (i) the ordinary waking\nstate and (ii) a state altered by ingestion of the Ayahuasca. The entropy of\nthe nodes of brain networks from subjects under the influence of Ayahuasca\ndiverge significantly from those of the ordinary waking state. The functional\nbrain networks from subjects in the altered state have, on average, a larger\ngeodesic entropy compared to the ordinary state. We conclude that geodesic\nentropy is a useful tool for analyzing complex networks and discuss how and why\nit may bring even further valuable insights into the study of the human brain\nand other empirical networks.\n"", 'title': 'Characterizing complex networks using Entropy-degree diagrams: unveiling changes in functional brain connectivity induced by Ayahuasca'}]","Title: Characterizing complex networks using Entropy-degree diagrams: unveiling changes in functional brain connectivity induced by Ayahuasca

Abstract:   Open problems abound in the theory of complex networks, which has found
successful application to diverse fields of science. With the aim of further
advancing the understanding of the brain's functional connectivity, we propose
to evaluate a network metric which we term the geodesic entropy. This entropy,
in a way that can be made precise, quantifies the Shannon entropy of the
distance distribution to a specific node from all other nodes. Measurements of
geodesic entropy allow for the characterization of the structural information
of a network that takes into account the distinct role of each node into the
network topology. The measurement and characterization of this structural
information has the potential to greatly improve our understanding of sustained
activity and other emergent behaviors in networks, such as self-organized
criticality sometimes seen in such contexts. We apply these concepts and
methods to study the effects of how the psychedelic Ayahuasca affects the
functional connectivity of the human brain. We show that the geodesic entropy
is able to differentiate the functional networks of the human brain in two
different states of consciousness in the resting state: (i) the ordinary waking
state and (ii) a state altered by ingestion of the Ayahuasca. The entropy of
the nodes of brain networks from subjects under the influence of Ayahuasca
diverge significantly from those of the ordinary waking state. The functional
brain networks from subjects in the altered state have, on average, a larger
geodesic entropy compared to the ordinary state. We conclude that geodesic
entropy is a useful tool for analyzing complex networks and discuss how and why
it may bring even further valuable insights into the study of the human brain
and other empirical networks."
Is there a method for determining the morphology of ground state wave functions for quantum Hall states in spherical geometry,a82c9dd6,"[{'abstract': '  The topological morphology--order of zeros at the positions of electrons with\nrespect to a specific electron--of Laughlin state at filling fractions $1/m$\n($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the\npositions of other electrons. Although fairly accurate ground state wave\nfunctions for most of the other quantum Hall states in the lowest Landau level\nare quite well-known, it had been an open problem in expressing the ground\nstate wave functions in terms of flux-attachment to particles, {\\em a la}, this\nmorphology of Laughlin state. With a very general consideration of\nflux-particle relations only, in spherical geometry, we here report a novel\nmethod for determining morphologies of these states. Based on these, we\nconstruct almost exact ground state wave-functions for the Coulomb interaction.\nAlthough the form of interaction may change the ground state wave-function, the\nsame morphology constructs the latter irrespective of the nature of the\ninteraction between electrons.\n', 'title': 'Jastrow form of the Ground State Wave Functions for Fractional Quantum Hall States'}]","I think I've found the answer you're looking for!

Title: Jastrow form of the Ground State Wave Functions for Fractional Quantum Hall States

Abstract: The topological morphology--order of zeros at the positions of electrons with respect to a specific electron--of Laughlin state at filling fractions $1/m$ ($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the positions of other electrons. Although fairly accurate ground state wave functions for most of the other quantum Hall states in the lowest Landau level are quite well-known, it had been an open problem in expressing the ground state wave functions in terms of flux-attachment to particles, {\em a la}, this morphology of Laughlin state. With a very general consideration of flux-particle relations only, in spherical geometry, we here report a novel method for determining morphologies of these states. Based on these, we construct almost exact ground state wave-functions for the Coulomb interaction. Although the form of interaction may change the ground state wave-function, the same morphology constructs the latter irrespective of the nature of the interaction between electrons."
How do these new architectures improve upon existing multimodal deep learning techniques?,8679cd2d,"[{'abstract': '  We propose two multimodal deep learning architectures that allow for\ncross-modal dataflow (XFlow) between the feature extractors, thereby extracting\nmore interpretable features and obtaining a better representation than through\nunimodal learning, for the same amount of training data. These models can\nusefully exploit correlations between audio and visual data, which have a\ndifferent dimensionality and are therefore nontrivially exchangeable. Our work\nimproves on existing multimodal deep learning metholodogies in two essential\nways: (1) it presents a novel method for performing cross-modality (before\nfeatures are learned from individual modalities) and (2) extends the previously\nproposed cross-connections, which only transfer information between streams\nthat process compatible data. Both cross-modal architectures outperformed their\nbaselines (by up to 7.5%) when evaluated on the AVletters dataset.\n', 'title': 'XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification'}]","Based on the context, I found an answer to your question.

Title: XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification

Abstract: We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between the feature extractors, thereby extracting more interpretable features and obtaining a better representation than through unimodal learning, for the same amount of training data. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are therefore nontrivially exchangeable. Our work improves on existing multimodal deep learning methodologies in two essential ways: (1) it presents a novel method for performing cross-modality (before features are learned from individual modalities) and (2) extends the previously proposed cross-connections, which only transfer information between streams that process compatible data. Both cross-modal architectures outperformed their baselines (by up to 7.5%) when evaluated on the AVletters dataset.

This research improves upon existing multimodal deep learning techniques by introducing a novel method for performing cross-modality and extending cross-connections, which enables better exploitation of correlations between audio and visual data."
What is the Lefschetz duality theorem for intersection cohomology?,d3822449,"[{'abstract': '  We prove the Lefschetz duality for intersection (co)homology in the framework\nof $\\partial$-pesudomanifolds. We work with general perversities and without\nrestriction on the coefficient ring.\n', 'title': 'Lefschetz duality for intersection (co)homology'}]","I think I can help you with that!

Title: Lefschetz duality for intersection (co)homology

Abstract: We prove the Lefschetz duality for intersection (co)homology in the framework of $\partial$-pesudomanifolds. We work with general perversities and without restriction on the coefficient ring."
What are some examples of  how feature side-information can be used to improve model performance?,83274a30,"[{'abstract': '  Very often features come with their own vectorial descriptions which provide\ndetailed information about their properties. We refer to these vectorial\ndescriptions as feature side-information. In the standard learning scenario,\ninput is represented as a vector of features and the feature side-information\nis most often ignored or used only for feature selection prior to model\nfitting. We believe that feature side-information which carries information\nabout features intrinsic property will help improve model prediction if used in\na proper way during learning process. In this paper, we propose a framework\nthat allows for the incorporation of the feature side-information during the\nlearning of very general model families to improve the prediction performance.\nWe control the structures of the learned models so that they reflect features\nsimilarities as these are defined on the basis of the side-information. We\nperform experiments on a number of benchmark datasets which show significant\npredictive performance gains, over a number of baselines, as a result of the\nexploitation of the side-information.\n', 'title': 'Regularising Non-linear Models Using Feature Side-information'}]","Title: Regularising Non-linear Models Using Feature Side-information

Abstract: Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.

In this paper, the authors propose using feature side-information to improve model performance by incorporating it into the learning process, which leads to significant predictive performance gains."
What recent findings suggest limitations of kernel methods in  ℓp-regularized learning?,173ea41d,"[{'abstract': '  In this paper, we discuss how a suitable family of tensor kernels can be used\nto efficiently solve nonparametric extensions of $\\ell^p$ regularized learning\nmethods. Our main contribution is proposing a fast dual algorithm, and showing\nthat it allows to solve the problem efficiently. Our results contrast recent\nfindings suggesting kernel methods cannot be extended beyond Hilbert setting.\nNumerical experiments confirm the effectiveness of the method.\n', 'title': 'Solving $\\ell^p\\!$-norm regularization with tensor kernels'}]","Title: Solving ℓp-norm regularization with tensor kernels

Abstract: In this paper, we discuss how a suitable family of tensor kernels can be used to efficiently solve nonparametric extensions of ℓp regularized learning methods. Our main contribution is proposing a fast dual algorithm, and showing that it allows to solve the problem efficiently. Our results contrast recent findings suggesting kernel methods cannot be extended beyond Hilbert setting. Numerical experiments confirm the effectiveness of the method."
What new formula related to local class field theory is presented in this paper? ,7d92050f,"[{'abstract': '  Let $K$ be a function field over a finite field $k$ of characteristic $p$ and\nlet $K_{\\infty}/K$ be a geometric extension with Galois group $\\mathbb{Z}_p$.\nLet $K_n$ be the corresponding subextension with Galois group\n$\\mathbb{Z}/p^n\\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple\nexplicit formula $g_n$ in terms of an explicit Witt vector construction of the\n$\\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which\nis quadratic in $p^n$. Furthermore, we determine all $\\mathbb{Z}_p$-towers for\nwhich the genus sequence is stable, in the sense that there are $a,b,c \\in\n\\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus\nstable towers are expected to have strong stable arithmetic properties for\ntheir zeta functions. A key technical contribution of this work is a new\nsimplified formula for the Schmid-Witt symbol coming from local class field\ntheory.\n', 'title': 'Genus growth in $\\mathbb{Z}_p$-towers of function fields'}]","Unfortunately, I couldn't find any new formula related to local class field theory presented in these papers.

None of the papers seem to focus directly on local class field theory or present a new formula related to it. They cover topics such as homotopy categories, number fields, Galois extensions, modular representation theory of finite reductive groups, and dynamical systems induced by the Artin reciprocity map.

If you could provide more context or clarify what you're looking for, I'd be happy to try and help you further!"
What methods exist for measuring the similarity between concepts within an ontology?,808a67c9,"[{'abstract': '  With the use of ontologies in several domains such as semantic web,\ninformation retrieval, artificial intelligence, the concept of similarity\nmeasuring has become a very important domain of research. Therefore, in the\ncurrent paper, we propose our method of similarity measuring which uses the\nDijkstra algorithm to define and compute the shortest path. Then, we use this\none to compute the semantic distance between two concepts defined in the same\nhierarchy of ontology. Afterward, we base on this result to compute the\nsemantic similarity. Finally, we present an experimental comparison between our\nmethod and other methods of similarity measuring.\n', 'title': 'An enhanced method to compute the similarity between concepts of ontology'}]","A researcher looking for methods to measure similarity between concepts within an ontology!

I think I have found a few relevant papers that might be of interest to you.

**Method 1:** Dijkstra Algorithm-based Method

Title: An enhanced method to compute the similarity between concepts of ontology

Abstract: With the use of ontologies in several domains such as semantic web, information retrieval, artificial intelligence, the concept of similarity measuring has become a very important domain of research. Therefore, in the current paper, we propose our method of similarity measuring which uses the Dijkstra algorithm to define and compute the shortest path. Then, we use this one to compute the semantic distance between two concepts defined in the same hierarchy of ontology. Afterward, we base on this result to compute the semantic similarity. Finally, we present an experimental comparison between our method and other methods of similarity measuring.

**Method 2:** Near-metrics for Universal Similarity

Title: A New Family of Near-metrics for Universal Similarity

Abstract: We propose a family of near-metrics based on local graph diffusion to capture similarity for a wide class of data sets. These quasi-metametrics, as their names suggest, dispense with one or two standard axioms of metric spaces, specifically distinguishability and symmetry, so that similarity between data points of arbitrary type and form could be measured broadly and effectively. The proposed near-metric family includes the forward k-step diffusion and its reverse, typically on the graph consisting of data objects and their features. By construction, this family of near-metrics is particularly appropriate for categorical data, continuous data, and vector representations of images and text extracted via deep learning approaches. We conduct extensive experiments to evaluate the performance of this family of similarity measures and compare and contrast with traditional measures of similarity used for each specific application and with the ground truth when available. We show that for structured data including categorical and continuous data, the near-metrics corresponding to normalized forward k-step diffusion (k small) work as one of the best performing similarity measures; for vector representations of text and images including those extracted from deep learning, the near-metrics derived from normalized and reverse k-step graph diffusion (k very small) exhibit outstanding ability to distinguish data points from different classes.

**Method 3:** Distance-based Discriminative Framework

Title: Distance Measure Machines

Abstract: This paper presents a distance-based discriminative framework for learning with probability distributions. Instead of using kernel mean embeddings or generalized radial basis kernels, we introduce embeddings based on dissimilarity of distributions to some reference distributions denoted as templates. Our framework extends the theory of similarity of Balcan et al. (2008) to the population distribution case and we show that, for some learning problems, some dissimilarity on distribution achieves low-error linear decision functions with high probability. Our key result is to prove that the theory also holds for empirical distributions. Algorithmically, the proposed approach consists in computing a mapping based on pairwise dissimilarity where learning a linear decision function is amenable. Our experimental results show that the Wasserstein distance embedding performs better than kernel mean embeddings and computing Wasserstein distance is far more tractable than estimating pairwise Kullback-Leibler divergence of empirical distributions.

I hope these papers are relevant to your research!"
Can a theorem be proven about the coefficients of a stable polynomial based on the stability of a subset of its monomials?,a5d02edd,"[{'abstract': ""  A polynomial $p\\in\\mathbb{R}[z_1,\\dots,z_n]$ is real stable if it has no\nroots in the upper-half complex plane. Gurvits's permanent inequality gives a\nlower bound on the coefficient of the $z_1z_2\\dots z_n$ monomial of a real\nstable polynomial $p$ with nonnegative coefficients. This fundamental\ninequality has been used to attack several counting and optimization problems.\nHere, we study a more general question: Given a stable multilinear polynomial\n$p$ with nonnegative coefficients and a set of monomials $S$, we show that if\nthe polynomial obtained by summing up all monomials in $S$ is real stable, then\nwe can lowerbound the sum of coefficients of monomials of $p$ that are in $S$.\nWe also prove generalizations of this theorem to (real stable) polynomials that\nare not multilinear. We use our theorem to give a new proof of Schrijver's\ninequality on the number of perfect matchings of a regular bipartite graph,\ngeneralize a recent result of Nikolov and Singh, and give deterministic\npolynomial time approximation algorithms for several counting problems.\n"", 'title': 'A Generalization of Permanent Inequalities and Applications in Counting and Optimization'}]","Based on the context, I think I have the answer to your question:

Title: A Generalization of Permanent Inequalities and Applications in Counting and Optimization

Abstract: A polynomial $p\in\mathbb{R}[z_1,\dots,z_n]$ is real stable if it has no roots in the upper-half complex plane. Gurvits's permanent inequality gives a lower bound on the coefficient of the $z_1z_2\dots z_n$ monomial of a real stable polynomial $p$ with nonnegative coefficients. This fundamental inequality has been used to attack several counting and optimization problems. Here, we study a more general question: Given a stable multilinear polynomial $p$ with nonnegative coefficients and a set of monomials $S$, we show that if the polynomial obtained by summing up all monomials in $S$ is real stable, then we can lowerbound the sum of coefficients of monomials of $p$ that are in $S$. We also prove generalizations of this theorem to (real stable) polynomials that are not multilinear. We use our theorem to give a new proof of Schrijver's inequality on the number of perfect matchings of a regular bipartite graph, generalize a recent result of Nikolov and Singh, and give deterministic polynomial time approximation algorithms for several counting problems."
How to find the potential functions on two unspecified edges of a star-shaped graph when those on other edges are known for Sturm-Liouville operators?,0a225f10,"[{'abstract': '  Boundary value problems for Sturm-Liouville operators with potentials from\nthe class $W_2^{-1}$ on a star-shaped graph are considered. We assume that the\npotentials are known on all the edges of the graph except two, and show that\nthe potentials on the remaining edges can be constructed by fractional parts of\ntwo spectra. A uniqueness theorem is proved, and an algorithm for the\nconstructive solution of the partial inverse problem is provided. The main\ningredient of the proofs is the Riesz-basis property of specially constructed\nsystems of functions.\n', 'title': 'A 2-edge partial inverse problem for the Sturm-Liouville operators with singular potentials on a star-shaped graph'}]","I think I can help you with that!

Title: A 2-edge partial inverse problem for the Sturm-Liouville operators with singular potentials on a star-shaped graph

Abstract: Boundary value problems for Sturm-Liouville operators with potentials from the class $W_2^{-1}$ on a star-shaped graph are considered. We assume that the potentials are known on all the edges of the graph except two, and show that the potentials on the remaining edges can be constructed by fractional parts of two spectra. A uniqueness theorem is proved, and an algorithm for the constructive solution of the partial inverse problem is provided. The main ingredient of the proofs is the Riesz-basis property of specially constructed systems of functions."
Why can ISS estimates for systems with boundary disturbances be difficult to express in a strict form?,b8817feb,"[{'abstract': '  This note establishes the input-to-state stability (ISS) property for a\nclamped-free damped string with respect to distributed and boundary\ndisturbances. While efficient methods for establishing ISS properties for\ndistributed parameter systems with respect to distributed disturbances have\nbeen developed during the last decades, establishing ISS properties with\nrespect to boundary disturbances remains challenging. One of the well-known\nmethods for well-posedness analysis of systems with boundary inputs is to use\nan adequate lifting operator, which transfers the boundary disturbance to a\ndistributed one. However, the resulting distributed disturbance involves time\nderivatives of the boundary perturbation. Thus, the subsequent ISS estimate\ndepends on its amplitude, and may not be expressed in the strict form of ISS\nproperties. To solve this problem, we show for a clamped-free damped string\nequation that the projection of the original system trajectories in an adequate\nRiesz basis can be used to establish the desired ISS property.\n', 'title': 'Input-to-State Stability of a Clamped-Free Damped String in the Presence of Distributed and Boundary Disturbances'}]","I think I have the answer to your question!

Title: Input-to-State Stability of a Clamped-Free Damped String in the Presence of Distributed and Boundary Disturbances

Abstract: This note establishes the input-to-state stability (ISS) property for a clamped-free damped string with respect to distributed and boundary disturbances. While efficient methods for establishing ISS properties for distributed parameter systems with respect to distributed disturbances have been developed during the last decades, establishing ISS properties with respect to boundary disturbances remains challenging. One of the well-known methods for well-posedness analysis of systems with boundary inputs is to use an adequate lifting operator, which transfers the boundary disturbance to a distributed one. However, the resulting distributed disturbance involves time derivatives of the boundary perturbation. Thus, the subsequent ISS estimate depends on its amplitude, and may not be expressed in the strict form of ISS properties. To solve this problem, we show for a clamped-free damped string equation that the projection of the original system trajectories in an adequate Riesz basis can be used to establish the desired ISS property.

The reason ISS estimates for systems with boundary disturbances can be difficult to express in a strict form is because the resulting distributed disturbance involves time derivatives of the boundary perturbation, making the subsequent ISS estimate dependent on its amplitude."
Do discrete spectrum transformations have over-recurrent sets?,8a62348f,"[{'abstract': '  We show that every invertible strong mixing transformation on a Lebesgue\nspace has strictly over-recurrent sets. Also, we give an explicit procedure for\nconstructing strong mixing transformations with no under-recurrent sets. This\nanswers both parts of a question of V. Bergelson.\nWe define $\\epsilon$-over-recurrence and show that given $\\epsilon > 0$, any\nergodic measure preserving invertible transformation (including discrete\nspectrum) has $\\epsilon$-over-recurrent sets of arbitrarily small measure.\nDiscrete spectrum transformations and rotations do not have over-recurrent\nsets, but we construct a weak mixing rigid transformation with strictly\nover-recurrent sets.\n', 'title': 'Over Recurrence for Mixing Transformations'}]","Title: Over Recurrence for Mixing Transformations

Abstract: We show that every invertible strong mixing transformation on a Lebesgue space has strictly over-recurrent sets. Also, we give an explicit procedure for constructing strong mixing transformations with no under-recurrent sets. This answers both parts of a question of V. Bergelson. We define ε-over-recurrence and show that given ε > 0, any ergodic measure preserving invertible transformation (including discrete spectrum) has ε-over-recurrent sets of arbitrarily small measure. Discrete spectrum transformations and rotations do not have over-recurrent sets, but we construct a weak mixing rigid transformation with strictly over-recurrent sets.

Answer: No, discrete spectrum transformations do not have over-recurrent sets."
What does this paper say about the density of positive integers representable as the sum of two squares and powers of two?,c279748c,"[{'abstract': '  We examine the representation of numbers as the sum of two squares in\n$\\mathbb{Z}_n$ for a general positive integer $n$. Using this information we\nmake some comments about the density of positive integers which can be\nrepresented as the sum of two squares and powers of $2$ in $\\mathbb{N}$.\n', 'title': 'Representing numbers as the sum of squares and powers in the ring $\\mathbb{Z}_n$'}]","Title: Representing numbers as the sum of squares and powers in the ring $\mathbb{Z}_n$

Abstract:  We examine the representation of numbers as the sum of two squares in
$\mathbb{Z}_n$ for a general positive integer $n$. Using this information we
make some comments about the density of positive integers which can be
represented as the sum of two squares and powers of $2$ in $\mathbb{N}$."
In which cosmological scenarios could a new species  with an energy density redshifting faster than radiation lead to 'relentless' dark matter?,161a67da,"[{'abstract': '  We consider a modification to the standard cosmological history consisting of\nintroducing a new species $\\phi$ whose energy density red-shifts with the scale\nfactor $a$ like $\\rho_\\phi \\propto a^{-(4+n)}$. For $n>0$, such a red-shift is\nfaster than radiation, hence the new species dominates the energy budget of the\nuniverse at early times while it is completely negligible at late times. If\nequality with the radiation energy density is achieved at low enough\ntemperatures, dark matter can be produced as a thermal relic during the new\ncosmological phase. Dark matter freeze-out then occurs at higher temperatures\ncompared to the standard case, implying that reproducing the observed abundance\nrequires significantly larger annihilation rates. Here, we point out a\ncompletely new phenomenon, which we refer to as $\\textit{relentless}$ dark\nmatter: for large enough $n$, unlike the standard case where annihilation ends\nshortly after the departure from thermal equilibrium, dark matter particles\nkeep annihilating long after leaving chemical equilibrium, with a significant\ndepletion of the final relic abundance. Relentless annihilation occurs for $n\n\\geq 2$ and $n \\geq 4$ for s-wave and p-wave annihilation, respectively, and it\nthus occurs in well motivated scenarios such as a quintessence with a kination\nphase. We discuss a few microscopic realizations for the new cosmological\ncomponent and highlight the phenomenological consequences of our calculations\nfor dark matter searches.\n', 'title': 'When the Universe Expands Too Fast: Relentless Dark Matter'}]","Title: When the Universe Expands Too Fast: Relentless Dark Matter

Abstract: We consider a modification to the standard cosmological history consisting of introducing a new species $\phi$ whose energy density red-shifts with the scale factor $a$ like $\rho_\phi \propto a^{-(4+n)}$. For $n>0$, such a red-shift is faster than radiation, hence the new species dominates the energy budget of the universe at early times while it is completely negligible at late times. If equality with the radiation energy density is achieved at low enough temperatures, dark matter can be produced as a thermal relic during the new cosmological phase. Dark matter freeze-out then occurs at higher temperatures compared to the standard case, implying that reproducing the observed abundance requires significantly larger annihilation rates. Here, we point out a completely new phenomenon, which we refer to as $\textit{relentless}$ dark matter: for large enough $n$, unlike the standard case where annihilation ends shortly after the departure from thermal equilibrium, dark matter particles keep annihilating long after leaving chemical equilibrium, with a significant depletion of the final relic abundance. Relentless annihilation occurs for $n \geq 2$ and $n \geq 4$ for s-wave and p-wave annihilation, respectively, and it thus occurs in well motivated scenarios such as a quintessence with a kination phase. We discuss a few microscopic realizations for the new cosmological component and highlight the phenomenological consequences of our calculations for dark matter searches."
Can embedding kernel matrices into graphons help evaluate approximation and generalization errors in Gaussian process regression?,2f5b270f,"[{'abstract': '  In this paper, we study random subsampling of Gaussian process regression,\none of the simplest approximation baselines, from a theoretical perspective.\nAlthough subsampling discards a large part of training data, we show provable\nguarantees on the accuracy of the predictive mean/variance and its\ngeneralization ability. For analysis, we consider embedding kernel matrices\ninto graphons, which encapsulate the difference of the sample size and enables\nus to evaluate the approximation and generalization errors in a unified manner.\nThe experimental results show that the subsampling approximation achieves a\nbetter trade-off regarding accuracy and runtime than the Nyström and random\nFourier expansion methods.\n', 'title': 'On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis'}]","Title: On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis

Abstract:  In this paper, we study random subsampling of Gaussian process regression, one of the simplest approximation baselines, from a theoretical perspective. Although subsampling discards a large part of training data, we show provable guarantees on the accuracy of the predictive mean/variance and its generalization ability. For analysis, we consider embedding kernel matrices into graphons, which encapsulate the difference of the sample size and enables us to evaluate the approximation and generalization errors in a unified manner. The experimental results show that the subsampling approximation achieves a better trade-off regarding accuracy and runtime than the Nyström and random Fourier expansion methods."
Describe the feature of the proposed neural architecture that allows for both bottom-up and top-down information transfer.,b8cbcc5d,"[{'abstract': '  We explore whether useful temporal neural generative models can be learned\nfrom sequential data without back-propagation through time. We investigate the\nviability of a more neurocognitively-grounded approach in the context of\nunsupervised generative modeling of sequences. Specifically, we build on the\nconcept of predictive coding, which has gained influence in cognitive science,\nin a neural framework. To do so we develop a novel architecture, the Temporal\nNeural Coding Network, and its learning algorithm, Discrepancy Reduction. The\nunderlying directed generative model is fully recurrent, meaning that it\nemploys structural feedback connections and temporal feedback connections,\nyielding information propagation cycles that create local learning signals.\nThis facilitates a unified bottom-up and top-down approach for information\ntransfer inside the architecture. Our proposed algorithm shows promise on the\nbouncing balls generative modeling problem. Further experiments could be\nconducted to explore the strengths and weaknesses of our approach.\n', 'title': 'Learning to Adapt by Minimizing Discrepancy'}]","Unfortunately, I couldn't find a direct answer to your question in the provided abstracts. However, I can suggest a possible related answer.

Title: Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration

Abstract: When the brain receives input from multiple sensory systems, it is faced with the question of whether it is appropriate to process the inputs in combination, as if they originated from the same event, or separately, as if they originated from distinct events. Furthermore, it must also have a mechanism through which it can keep sensory inputs calibrated to maintain the accuracy of its internal representations. We have developed a neural network architecture capable of i) approximating optimal multisensory spatial integration, based on Bayesian causal inference, and ii) recalibrating the spatial encoding of sensory systems. The architecture is based on features of the dorsal processing hierarchy, including the spatial tuning properties of unisensory neurons and the convergence of different sensory inputs onto multisensory neurons. Furthermore, we propose that these unisensory and multisensory neurons play dual roles in i) encoding spatial location as separate or integrated estimates and ii) accumulating evidence for the independence or relatedness of multisensory stimuli. We further propose that top-down feedback connections spanning the dorsal pathway play key a role in recalibrating spatial encoding at the level of early unisensory cortices. Our proposed architecture provides possible explanations for a number of human electrophysiological and neuroimaging results and generates testable predictions linking neurophysiology with behaviour.

This abstract mentions ""top-down feedback connections"" which could be related to the aspect of the neural architecture that allows for top-down information transfer. However, it's not a direct answer to your question, as it doesn't explicitly describe a feature of the proposed architecture that enables both bottom-up and top-down information transfer."
What are the different Knowledge Processes promoted by Knowledge Management strategies in Agile Software Development?,d6e9709d,"[{'abstract': '  Knowledge-intensive companies that adopt Agile Software Development (ASD)\nrelay on efficient implementation of Knowledge Management (KM) strategies to\npromotes different Knowledge Processes (KPs) to gain competitive advantage.\nThis study aims to explore how companies that adopt ASD implement KM strategies\nutilizing practices that promote the KPs in the different organizational\nlayers. Through a systematic literature review, we analyzed 32 primary studies,\nselected by automated search and snowballing in the extant literature. To\nanalyze the data, we applied narrative synthesis. Most of the identified KM\npractices implement personalization strategies (81 %), supported by\ncodification (19 %). Our review shows that the primary studies do not report KM\npractices in the strategic layer and two of them in the product portfolio\nlayer; on the other hand, in the project layer, the studies report 33 practices\nthat implement personalization strategy, and seven practices that implement\ncodification. KM strategies in ASD promote mainly the knowledge transfer\nprocess with practices that stimulate social interaction to share tacit\nknowledge in the project layer. As a result of using informal communication, a\nsignificant amount of knowledge can be lost or not properly transferred to\nother individuals and, instead of propagating the knowledge, it remains inside\na few individuals minds.\n', 'title': 'Knowledge Management Strategies and Processes in Agile Software Development: A Systematic Literature Review'}]","Title: Knowledge Management Strategies and Processes in Agile Software Development: A Systematic Literature Review

Abstract: Knowledge-intensive companies that adopt Agile Software Development (ASD) relay on efficient implementation of Knowledge Management (KM) strategies to promotes different Knowledge Processes (KPs) to gain competitive advantage. This study aims to explore how companies that adopt ASD implement KM strategies utilizing practices that promote the KPs in the different organizational layers. Through a systematic literature review, we analyzed 32 primary studies, selected by automated search and snowballing in the extant literature. To analyze the data, we applied narrative synthesis. Most of the identified KM practices implement personalization strategies (81 %), supported by codification (19 %). Our review shows that the primary studies do not report KM practices in the strategic layer and two of them in the product portfolio layer; on the other hand, in the project layer, the studies report 33 practices that implement personalization strategy, and seven practices that implement codification. KM strategies in ASD promote mainly the knowledge transfer process with practices that stimulate social interaction to share tacit knowledge in the project layer. As a result of using informal communication, a significant amount of knowledge can be lost or not properly transferred to other individuals and, instead of propagating the knowledge, it remains inside a few individuals minds.

The Knowledge Processes (KPs) promoted by Knowledge Management strategies in Agile Software Development are:

- Knowledge transfer process"
How does co-channel interference affect the performance of decode-and-forward relay assisted cooperative networks?,4332d126,"[{'abstract': '  The study of relays with the scope of energy-harvesting (EH) looks\ninteresting as a means of enabling sustainable, wireless communication without\nthe need to recharge or replace the battery driving the relays. However,\nreliability of such communication systems becomes an important design challenge\nwhen such relays scavenge energy from the information bearing RF signals\nreceived from the source, using the technique of simultaneous wireless\ninformation and power transfer (SWIPT). To this aim, this work studies\nbidirectional communication in a decode-and-forward (DF) relay assisted\ncooperative wireless network in presence of co-channel interference (CCI). In\norder to quantify the reliability of the bidirectional communication systems, a\nclosed form expression for the outage probability of the system is derived for\nboth power splitting (PS) and time switching (TS) mode of operation of the\nrelay. Simulation results are used to validate the accuracy of our analytical\nresults and illustrate the dependence of the outage probability on various\nsystem parameters, like PS factor, TS factor, and distance of the relay from\nboth the users. Results of performance comparison between PS relaying (PSR) and\nTS relaying (TSR) schemes are also presented. Besides, simulation results are\nalso used to illustrate the spectral-efficiency and the energy-efficiency of\nthe proposed system. The results show that, both in terms of spectral\nefficiency and the energy-efficiency, the two-way communication system in\npresence of moderate CCI power, performs better than the similar system without\nCCI. Additionally, it is also found that PSR is superior to TSR protocol in\nterms of peak energy-efficiency.\n', 'title': 'Outage analysis in two-way communication with RF energy harvesting relay and co-channel interference'}]","Title: Outage analysis in two-way communication with RF energy harvesting relay and co-channel interference

Abstract:   The study of relays with the scope of energy-harvesting (EH) looks
interesting as a means of enabling sustainable, wireless communication without
the need to recharge or replace the battery driving the relays. However,
reliability of such communication systems becomes an important design challenge
when such relays scavenge energy from the information bearing RF signals
received from the source, using the technique of simultaneous wireless
information and power transfer (SWIPT). To this aim, this work studies
bidirectional communication in a decode-and-forward (DF) relay assisted
cooperative wireless network in presence of co-channel interference (CCI). In
order to quantify the reliability of the bidirectional communication systems, a
closed form expression for the outage probability of the system is derived for
both power splitting (PS) and time switching (TS) mode of operation of the
relay. Simulation results are used to validate the accuracy of our analytical
results and illustrate the dependence of the outage probability on various
system parameters, like PS factor, TS factor, and distance of the relay from
both the users. Results of performance comparison between PS relaying (PSR) and
TS relaying (TSR) schemes are also presented. Besides, simulation results are
also used to illustrate the spectral-efficiency and the energy-efficiency of
the proposed system. The results show that, both in terms of spectral
efficiency and the energy-efficiency, the two-way communication system in
presence of moderate CCI power, performs better than the similar system without
CCI. Additionally, it is also found that PSR is superior to TSR protocol in
terms of peak energy-efficiency."
Where can I find an experimental investigation of Mazur and Tate's conjecture number four?,a424b5e2,"[{'abstract': '  Let $E/\\mathbb{Q}$ be an elliptic curve of level $N$ and rank equal to $1$.\nLet $p$ be a prime of ordinary reduction. We experimentally study conjecture\n$4$ of B. Mazur and J. Tate in his article ""Refined Conjectures of the Birch\nand Swinnerton-Dyer Type"". We report the computational evidence.\n', 'title': 'Experimental Evidence on a Refined Conjecture of the BSD type'}]","Title: Experimental Evidence on a Refined Conjecture of the BSD type

Abstract: Let $E/\mathbb{Q}$ be an elliptic curve of level $N$ and rank equal to $1$. Let $p$ be a prime of ordinary reduction. We experimentally study conjecture $4$ of B. Mazur and J. Tate in his article ""Refined Conjectures of the Birch and Swinnerton-Dyer Type"". We report the computational evidence."
What  method is proposed to efficiently plan future states based on the learned model? ,e914b8ed,"[{'abstract': '  We present a representation learning algorithm that learns a low-dimensional\nlatent dynamical system from high-dimensional \\textit{sequential} raw data,\ne.g., video. The framework builds upon recent advances in amortized inference\nmethods that use both an inference network and a refinement procedure to output\nsamples from a variational distribution given an observation sequence, and\ntakes advantage of the duality between control and inference to approximately\nsolve the intractable inference problem using the path integral control\napproach. The learned dynamical model can be used to predict and plan the\nfuture states; we also present the efficient planning method that exploits the\nlearned low-dimensional latent dynamics. Numerical experiments show that the\nproposed path-integral control based variational inference method leads to\ntighter lower bounds in statistical model learning of sequential data. The\nsupplementary video: this https URL\n', 'title': 'Adaptive Path-Integral Autoencoder: Representation Learning and Planning for Dynamical Systems'}]","I think I can help you with that!

Based on the provided abstracts, I found a relevant paper that proposes a method for planning future states based on a learned model. Here's the matching paper:

Title: Internal Model from Observations for Reward Shaping

Abstract: Reinforcement learning methods require careful design involving a reward function to obtain the desired action policy for a given task. In the absence of hand-crafted reward functions, prior work on the topic has proposed several methods for reward estimation by using expert state trajectories and action pairs. However, there are cases where complete or good action information cannot be obtained from expert demonstrations. We propose a novel reinforcement learning method in which the agent learns an internal model of observation on the basis of expert-demonstrated state trajectories to estimate rewards without completely learning the dynamics of the external environment from state-action pairs. The internal model is obtained in the form of a predictive model for the given expert state distribution. During reinforcement learning, the agent predicts the reward as a function of the difference between the actual state and the state predicted by the internal model. We conducted multiple experiments in environments of varying complexity, including the Super Mario Bros and Flappy Bird games. We show our method successfully trains good policies directly from expert game-play videos.

This paper proposes a method for learning an internal model from observations, which can be used to predict future states and plan accordingly. The internal model is learned based on expert-demonstrated state trajectories and is used to estimate rewards without requiring complete knowledge of the external environment."
Can you describe a method for calculating the molar heat capacity at constant pressure ($c_P$)  for a fluid modeled by a force field that includes intramolecular contributions?,a36f168e,"[{'abstract': '  The calculation of caloric properties such as heat capacity, Joule-Thomson\ncoefficients and the speed of sound by classical force-field-based molecular\nsimulation methodology has received scant attention in the literature,\nparticularly for systems composed of complex molecules whose force fields (FFs)\nare characterized by a combination of intramolecular and intermolecular terms\n(referred to herein as ""flexible FFs""). The calculation of a thermodynamic\nproperty for a system whose molecules are described by such a FF involves the\ncalculation of the residual property prior to its addition to the corresponding\nideal-gas (IG) property, the latter of which is separately calculated, either\nusing thermochemical compilations or nowadays accurate quantum mechanical\ncalculations. Although the simulation of a volumetric residual property\nproceeds by simply replacing the intermolecular FF in the rigid molecule case\nby the total (intramolecular plus intermolecular) FF, this is not the case for\na caloric property. We discuss the methodology required in performing such\ncalculations, and focus on the example of the molar heat capacity at constant\npressure, $c_P$, one of the most important caloric properties. We also consider\nthree approximations for the calculation procedure, and illustrate their\nconsequences for the examples of the relatively simple molecule 2-propanol,\n${\\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\\rm HO(CH_2)_2NH_2}$, an\nimportant fluid used in carbon capture.\n', 'title': 'Molecular Simulation of Caloric Properties of Fluids Modelled by Force Fields with Intramolecular Contributions: Application to Heat Capacities'}]","Title: Molecular Simulation of Caloric Properties of Fluids Modelled by Force Fields with Intramolecular Contributions: Application to Heat Capacities

Abstract: The calculation of caloric properties such as heat capacity, Joule-Thomson coefficients and the speed of sound by classical force-field-based molecular simulation methodology has received scant attention in the literature, particularly for systems composed of complex molecules whose force fields (FFs) are characterized by a combination of intramolecular and intermolecular terms (referred to herein as ""flexible FFs""). The calculation of a thermodynamic property for a system whose molecules are described by such a FF involves the calculation of the residual property prior to its addition to the corresponding ideal-gas (IG) property, the latter of which is separately calculated, either using thermochemical compilations or nowadays accurate quantum mechanical calculations. Although the simulation of a volumetric residual property proceeds by simply replacing the intermolecular FF in the rigid molecule case by the total (intramolecular plus intermolecular) FF, this is not the case for a caloric property. We discuss the methodology required in performing such calculations, and focus on the example of the molar heat capacity at constant pressure, $c_P$, one of the most important caloric properties. We also consider three approximations for the calculation procedure, and illustrate their consequences for the examples of the relatively simple molecule 2-propanol, ${\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\rm HO(CH_2)_2NH_2}$, an important fluid used in carbon capture."
How does the tensor of inertia change over time in a viscoelastic shell?,b5fb0ab9,"[{'abstract': '  Several natural satellites of the giant planets have shown evidence of a\nglobal internal ocean, coated by a thin, icy crust. This crust is probably\nviscoelastic, which would alter its rotational response. This response would\ntranslate into several rotational quantities, i.e. the obliquity, and the\nlibrations at different frequencies, for which the crustal elasticity reacts\ndifferently. This study aims at modelling the global response of the\nviscoelastic crust. For that, I derive the time-dependency of the tensor of\ninertia, which I combine with the time evolution of the rotational quantities,\nthanks to an iterative algorithm. This algorithm combines numerical simulations\nof the rotation with a digital filtering of the resulting tensor of inertia.\nThe algorithm works very well in the elastic case, provided the problem is not\nresonant. However, considering tidal dissipation adds different phase lags to\nthe oscillating contributions, which challenge the convergence of the\nalgorithm.\n', 'title': 'Rotation of a synchronous viscoelastic shell'}]","Title: Rotation of a synchronous viscoelastic shell

Abstract:   Several natural satellites of the giant planets have shown evidence of a
global internal ocean, coated by a thin, icy crust. This crust is probably
viscoelastic, which would alter its rotational response. This response would
translate into several rotational quantities, i.e. the obliquity, and the
librations at different frequencies, for which the crustal elasticity reacts
differently. This study aims at modelling the global response of the
viscoelastic crust. For that, I derive the time-dependency of the tensor of
inertia, which I combine with the time evolution of the rotational quantities,
thanks to an iterative algorithm. This algorithm combines numerical simulations
of the rotation with a digital filtering of the resulting tensor of inertia.
The algorithm works very well in the elastic case, provided the problem is not
resonant. However, considering tidal dissipation adds different phase lags to
the oscillating contributions, which challenge the convergence of the
algorithm."
"Besides diagnoses, what other patient information was included in the analysis of factors contributing to readmission prediction? ",1e20a9f2,"[{'abstract': '  Identification of patients at high risk for readmission could help reduce\nmorbidity and mortality as well as healthcare costs. Most of the existing\nstudies on readmission prediction did not compare the contribution of data\ncategories. In this study we analyzed relative contribution of 90,101 variables\nacross 398,884 admission records corresponding to 163,468 patients, including\npatient demographics, historical hospitalization information, discharge\ndisposition, diagnoses, procedures, medications and laboratory test results. We\nestablished an interpretable readmission prediction model based on Logistic\nRegression in scikit-learn, and added the available variables to the model one\nby one in order to analyze the influences of individual data categories on\nreadmission prediction accuracy. Diagnosis related groups (c-statistic\nincrement of 0.0933) and discharge disposition (c-statistic increment of\n0.0269) were the strongest contributors to model accuracy. Additionally, we\nalso identified the top ten contributing variables in every data category.\n', 'title': 'Contribution of Data Categories to Readmission Prediction Accuracy'}]","Title: Contribution of Data Categories to Readmission Prediction Accuracy

Abstract: Identification of patients at high risk for readmission could help reduce morbidty and mortality as well as healthcare costs. Most of the existing studies on readmission prediction did not compare the contribution of data categories. In this study we analyzed relative contribution of 90,101 variables across 398,884 admission records corresponding to 163,468 patients, including patient demographics, historical hospitalization information, discharge disposition, diagnoses, procedures, medications and laboratory test results. We established an interpretable readmission prediction model based on Logistic Regression in scikit-learn, and added the available variables to the model one by one in order to analyze the influences of individual data categories on readmission prediction accuracy. Diagnosis related groups (c-statistic increment of 0.0933) and discharge disposition (c-statistic increment of 0.0269) were the strongest contributors to model accuracy. Additionally, we also identified the top ten contributing variables in every data category.

Besides diagnoses, the other patient information included in the analysis were patient demographics, historical hospitalization information, discharge disposition, procedures, medications, and laboratory test results."
What are the advantages of using LR NURBS compared to uniform refinement for solving engineering problems?,a11c6f18,"[{'abstract': '  A novel adaptive local surface refinement technique based on Locally Refined\nNon-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model\ncomplex geometries exactly and are the rational extension of LR B-splines. The\nlocal representation of the parameter space overcomes the drawback of\nnon-existent local refinement in standard NURBS-based isogeometric analysis.\nFor a convenient embedding into general finite element code, the Bézier\nextraction operator for LR NURBS is formulated. An automatic remeshing\ntechnique is presented that allows adaptive local refinement and coarsening of\nLR NURBS. In this work, LR NURBS are applied to contact computations of 3D\nsolids and membranes. For solids, LR NURBS-enriched finite elements are used to\ndiscretize the contact surfaces with LR NURBS finite elements, while the rest\nof the body is discretized by linear Lagrange finite elements. For membranes,\nthe entire surface is discretized by LR NURBS. Various numerical examples are\nshown, and they demonstrate the benefit of using LR NURBS: Compared to uniform\nrefinement, LR NURBS can achieve high accuracy at lower computational cost.\n', 'title': 'Adaptive local surface refinement based on LR NURBS and its application to contact'}]","Title: Adaptive local surface refinement based on LR NURBS and its application to contact

Abstract: A novel adaptive local surface refinement technique based on Locally Refined Non-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model complex geometries exactly and are the rational extension of LR B-splines. The local representation of the parameter space overcomes the drawback of non-existent local refinement in standard NURBS-based isogeometric analysis. For a convenient embedding into general finite element code, the Bézier extraction operator for LR NURBS is formulated. An automatic remeshing technique is presented that allows adaptive local refinement and coarsening of LR NURBS. In this work, LR NURBS are applied to contact computations of 3D solids and membranes. For solids, LR NURBS-enriched finite elements are used to discretize the contact surfaces with LR NURBS finite elements, while the rest of the body is discretized by linear Lagrange finite elements. For membranes, the entire surface is discretized by LR NURBS. Various numerical examples are shown, and they demonstrate the benefit of using LR NURBS: Compared to uniform refinement, LR NURBS can achieve high accuracy at lower computational cost."
In what geological context was a zero frequency stop-band observed covering a range of 0 to 30Hz?,3ac3e2dd,"[{'abstract': '  The regularity of earthquakes, their destructive power, and the nuisance of\nground vibration in urban environments, all motivate designs of defence\nstructures to lessen the impact of seismic and ground vibration waves on\nbuildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and\nup to a few tens of Hz for vibrations generated by human activities, cause a\nlarge amount of damage, or inconvenience, depending on the geological\nconditions they can travel considerable distances and may match the resonant\nfundamental frequency of buildings. The ultimate aim of any seismic\nmetamaterial, or any other seismic shield, is to protect over this entire range\nof frequencies, the long wavelengths involved, and low frequency, have meant\nthis has been unachievable to date.\nElastic flexural waves, applicable in the mechanical vibrations of thin\nelastic plates, can be designed to have a broad zero-frequency stop-band using\na periodic array of very small clamped circles. Inspired by this experimental\nand theoretical observation, all be it in a situation far removed from seismic\nwaves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)\nand body (pressure P and shear S) wave reflectors at very large wavelengths in\nstructured soils modelled as a fully elastic layer periodically clamped to\nbedrock.\nWe identify zero frequency stop-bands that only exist in the limit of columns\nof concrete clamped at their base to the bedrock. In a realistic configuration\nof a sedimentary basin 15 meters deep we observe a zero frequency stop-band\ncovering a broad frequency range of $0$ to $30$ Hz.\n', 'title': 'Clamped seismic metamaterials: Ultra-low broad frequency stop-bands'}]","Based on the context, I found the answer to your question:

Title: Clamped seismic metamaterials: Ultra-low broad frequency stop-bands

Abstract: The regularity of earthquakes, their destructive power, and the nuisance of
ground vibration in urban environments, all motivate designs of defence
structures to lessen the impact of seismic and ground vibration waves on
buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and
up to a few tens of Hz for vibrations generated by human activities, cause a
large amount of damage, or inconvenience, depending on the geological
conditions they can travel considerable distances and may match the resonant
fundamental frequency of buildings. The ultimate aim of any seismic
metamaterial, or any other seismic shield, is to protect over this entire range
of frequencies, the long wavelengths involved, and low frequency, have meant
this has been unachievable to date.
Elastic flexural waves, applicable in the mechanical vibrations of thin
elastic plates, can be designed to have a broad zero-frequency stop-band using
a periodic array of very small clamped circles. Inspired by this experimental
and theoretical observation, all be it in a situation far removed from seismic
waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)
and body (pressure P and shear S) wave reflectors at very large wavelengths in
structured soils modelled as a fully elastic layer periodically clamped to
bedrock.
We identify zero frequency stop-bands that only exist in the limit of columns
of concrete clamped at their base to the bedrock. In a realistic configuration
of a sedimentary basin 15 meters deep we observe a zero frequency stop-band
covering a broad frequency range of $0$ to $30$ Hz."
What are the limitations of  using averaged kernels in kernel methods for handling temporal information?,583dcadc,"[{'abstract': '  In kernel methods, temporal information on the data is commonly included by\nusing time-delayed embeddings as inputs. Recently, an alternative formulation\nwas proposed by defining a gamma-filter explicitly in a reproducing kernel\nHilbert space, giving rise to a complex model where multiple kernels operate on\ndifferent temporal combinations of the input signal. In the original\nformulation, the kernels are then simply combined to obtain a single kernel\nmatrix (for instance by averaging), which provides computational benefits but\ndiscards important information on the temporal structure of the signal.\nInspired by works on multiple kernel learning, we overcome this drawback by\nconsidering the different kernels separately. We propose an efficient strategy\nto adaptively combine and select these kernels during the training phase. The\nresulting batch and online algorithms automatically learn to process highly\nnonlinear temporal information extracted from the input signal, which is\nimplicitly encoded in the kernel values. We evaluate our proposal on several\nartificial and real tasks, showing that it can outperform classical approaches\nboth in batch and online settings.\n', 'title': 'Recursive Multikernel Filters Exploiting Nonlinear Temporal Structure'}]","Based on the context, I found a relevant paper that discusses the limitations of using averaged kernels in kernel methods for handling temporal information.

Title: Recursive Multikernel Filters Exploiting Nonlinear Temporal Structure

Abstract: In kernel methods, temporal information on the data is commonly included by using time-delayed embeddings as inputs. Recently, an alternative formulation was proposed by defining a gamma-filter explicitly in a reproducing kernel Hilbert space, giving rise to a complex model where multiple kernels operate on different temporal combinations of the input signal. In the original formulation, the kernels are then simply combined to obtain a single kernel matrix (for instance by averaging), which provides computational benefits but discards important information on the temporal structure of the signal. Inspired by works on multiple kernel learning, we overcome this drawback by considering the different kernels separately. We propose an efficient strategy to adaptively combine and select these kernels during the training phase. The resulting batch and online algorithms automatically learn to process highly nonlinear temporal information extracted from the input signal, which is implicitly encoded in the kernel values. We evaluate our proposal on several artificial and real tasks, showing that it can outperform classical approaches both in batch and online settings."
Why does Klinkenberg's correction factor decrease as the Knudsen number increases?,1affc023,"[{'abstract': ""  The apparent gas permeability of the porous medium is an important parameter\nin the prediction of unconventional gas production, which was first\ninvestigated systematically by Klinkenberg in 1941 and found to increase with\nthe reciprocal mean gas pressure (or equivalently, the Knudsen number).\nAlthough the underlying rarefaction effects are well-known, the reason that the\ncorrection factor in Klinkenberg's famous equation decreases when the Knudsen\nnumber increases has not been fully understood. Most of the studies idealize\nthe porous medium as a bundle of straight cylindrical tubes, however, according\nto the gas kinetic theory, this only results in an increase of the correction\nfactor with the Knudsen number, which clearly contradicts Klinkenberg's\nexperimental observations. Here, by solving the Bhatnagar-Gross-Krook equation\nin simplified (but not simple) porous media, we identify, for the first time,\ntwo key factors that can explain Klinkenberg's experimental results: the\ntortuous flow path and the non-unitary tangential momentum accommodation\ncoefficient for the gas-surface interaction. Moreover, we find that\nKlinkenberg's results can only be observed when the ratio between the apparent\nand intrinsic permeabilities is $\\lesssim30$; at large ratios (or Knudsen\nnumbers) the correction factor increases with the Knudsen number. Our numerical\nresults could also serve as benchmarking cases to assess the accuracy of\nmacroscopic models and/or numerical schemes for the modeling/simulation of\nrarefied gas flows in complex geometries over a wide range of gas rarefaction.\n"", 'title': 'On the apparent permeability of porous media in rarefied gas flows'}]","Title: On the apparent permeability of porous media in rarefied gas flows

Abstract: The apparent gas permeability of the porous medium is an important parameter
in the prediction of unconventional gas production, which was first
investigated systematically by Klinkenberg in 1941 and found to increase with
the reciprocal mean gas pressure (or equivalently, the Knudsen number).
Although the underlying rarefaction effects are well-known, the reason that the
correction factor in Klinkenberg's famous equation decreases when the Knudsen
number increases has not been fully understood. Most of the studies idealize
the porous medium as a bundle of straight cylindrical tubes, however, according
to the gas kinetic theory, this only results in an increase of the correction
factor with the Knudsen number, which clearly contradicts Klinkenberg's
experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
in simplified (but not simple) porous media, we identify, for the first time,
two key factors that can explain Klinkenberg's experimental results: the
tortuous flow path and the non-unitary tangential momentum accommodation
coefficient for the gas-surface interaction. Moreover, we find that
Klinkenberg's results can only be observed when the ratio between the apparent
and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
numbers) the correction factor increases with the Knudsen number. Our numerical
results could also serve as benchmarking cases to assess the accuracy of
macroscopic models and/or numerical schemes for the modeling/simulation of
rarefied gas flows in complex geometries over a wide range of gas rarefaction.


According to this paper, Klinkenberg's correction factor does not decrease as the Knudsen number increases. Instead, the correction factor increases with the Knudsen number, which contradicts Klinkenberg's experimental observations. The authors of this paper identify two key factors that can explain Klinkenberg's experimental results: the tortuous flow path and the non-unitary tangential momentum accommodation coefficient for the gas-surface interaction."
What is a potential challenge in calculating the nonclassicality for non-Gaussian mixed states according to this paper? ,a49eaf59,"[{'abstract': '  Nonclassical states of a quantized light are described in terms of\nGlauber-Sudarshan P distribution which is not a genuine classical probability\ndistribution. Despite several attempts, defining a uniform measure of\nnonclassicality (NC) for the single mode quantum states of light is yet an open\ntask. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that\nthe existing well-known measures fail to quantify the NC of single mode states\nthat are generated under multiple NC-inducing operations. Recently, Ivan et.\nal. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of\nnon-Gaussian character of quantum optical states in terms of Wehrl entropy.\nHere, we adopt this concept in the context of single mode NC. In this paper, we\npropose a new quantification of NC for the single mode quantum states of light\nas the difference between the total Wehrl entropy of the state and the maximum\nWehrl entropy arising due to its classical characteristics. This we achieve by\nsubtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any\nclassical state that has same randomness as measured in terms of von-Neumann\nentropy. We obtain analytic expressions of NC for most of the states, in\nparticular, all pure states and Gaussian mixed states. However, the evaluation\nof NC for the non-Gaussian mixed states is subject to extensive numerical\ncomputation that lies beyond the scope of the current work. We show that, along\nwith the states generated under single NC-inducing operations, also for the\nbroader class of states that are generated under multiple NC-inducing\noperations, our quantification enumerates the NC consistently.\n', 'title': 'Wehrl Entropy Based Quantification of Nonclassicality for Single Mode Quantum Optical States'}]","Title: Wehrl Entropy Based Quantification of Nonclassicality for Single Mode Quantum Optical States

Abstract: Nonclassical states of a quantized light are described in terms of Glauber-Sudarshan P distribution which is not a genuine classical probability distribution. Despite several attempts, defining a uniform measure of nonclassicality (NC) for the single mode quantum states of light is yet an open task. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that the existing well-known measures fail to quantify the NC of single mode states that are generated under multiple NC-inducing operations. Recently, Ivan et. al. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of non-Gaussian character of quantum optical states in terms of Wehrl entropy. Here, we adopt this concept in the context of single mode NC. In this paper, we propose a new quantification of NC for the single mode quantum states of light as the difference between the total Wehrl entropy of the state and the maximum Wehrl entropy arising due to its classical characteristics. This we achieve by subtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any classical state that has same randomness as measured in terms of von-Neumann entropy. We obtain analytic expressions of NC for most of the states, in particular, all pure states and Gaussian mixed states. However, the evaluation of NC for the non-Gaussian mixed states is subject to extensive numerical computation that lies beyond the scope of the current work. We show that, along with the states generated under single NC-inducing operations, also for the broader class of states that are generated under multiple NC-inducing operations, our quantification enumerates the NC consistently.

Potential challenge: The challenge in calculating the nonclassicality for non-Gaussian mixed states is the need for extensive numerical computation."
Does this paper  determine inductive formulas for the structure constants in the torus-equivariant cohomology ring? ,11bef1ff,"[{'abstract': '  Let $G:=\\widehat{SL_2}$ denote the affine Kac-Moody group associated to\n$SL_2$ and $\\bar{\\mathcal{X}}$ the associated affine Grassmannian. We determine\nan inductive formula for the Schubert basis structure constants in the\ntorus-equivariant Grothendieck group of $\\bar{\\mathcal{X}}$. In the case of\nordinary (non-equivariant) $K$-theory we find an explicit closed form for the\nstructure constants. We also determine an inductive formula for the structure\nconstants in the torus-equivariant cohomology ring, and use this formula to\nfind closed forms for some of the structure constants.\n', 'title': 'An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$'}]","Title: An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$

Abstract: Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to 
$SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine 
an inductive formula for the Schubert basis structure constants in the 
torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of 
ordinary (non-equivariant) $K$-theory we find an explicit closed form for the 
structure constants. We also determine an inductive formula for the structure 
constants in the torus-equivariant cohomology ring, and use this formula to 
find closed forms for some of the structure constants."
Where is a web application for viewing MMP networks hosted?,52e19c72,"[{'abstract': '  Visualizing a complex network is computationally intensive process and\ndepends heavily on the number of components in the network. One way to solve\nthis problem is not to render the network in real time. PRE-render Content\nUsing Tiles (PRECUT) is a process to convert any complex network into a\npre-rendered network. Tiles are generated from pre-rendered images at different\nzoom levels, and navigating the network simply becomes delivering relevant\ntiles. PRECUT is exemplified by performing large-scale compound-target\nrelationship analyses. Matched molecular pair (MMP) networks were created using\ncompounds and the target class description found in the ChEMBL database. To\nvisualize MMP networks, the MMP network viewer has been implemented in COMBINE\nand as a web application, hosted at this http URL.\n', 'title': 'PRE-render Content Using Tiles (PRECUT). 1. Large-Scale Compound-Target Relationship Analyses'}]","Title: PRE-render Content Using Tiles (PRECUT). 1. Large-Scale Compound-Target Relationship Analyses

Abstract: Visualizing a complex network is computationally intensive process and depends heavily on the number of components in the network. One way to solve this problem is not to render the network in real time. PRE-render Content Using Tiles (PRECUT) is a process to convert any complex network into a pre-rendered network. Tiles are generated from pre-rendered images at different zoom levels, and navigating the network simply becomes delivering relevant tiles. PRECUT is exemplified by performing large-scale compound-target relationship analyses. Matched molecular pair (MMP) networks were created using compounds and the target class description found in the ChEMBL database. To visualize MMP networks, the MMP network viewer has been implemented in COMBINE and as a web application, hosted at this http URL."
What is the significance of this study in the context of topological classifications of interacting bosons and fermions?,dbe4deec,"[{'abstract': '  We explore the topological properties of quantum spin-1/2 chains with two\nIsing symmetries. This class of models does not possess any of the symmetries\nthat are required to protect the Haldane phase. Nevertheless, we show that\nthere are 4 symmetry-protected topological phases, in addition to 6 phases that\nspontaneously break one or both Ising symmetries. By mapping the model to\none-dimensional interacting fermions with particle-hole and time-reversal\nsymmetry, we obtain integrable parent Hamiltonians for the conventional and\ntopological phases of the spin model. We use these Hamiltonians to characterize\nthe physical properties of all 10 phases, identify their local and nonlocal\norder parameters, and understand the effects of weak perturbations that respect\nthe Ising symmetries. Our study provides the first explicit example of a class\nof spin chains with several topologically non-trivial phases, and binds\ntogether the topological classifications of interacting bosons and fermions.\n', 'title': 'The 10 phases of spin chains with two Ising symmetries'}]","Title: The 10 phases of spin chains with two Ising symmetries

Abstract: We explore the topological properties of quantum spin-1/2 chains with two Ising symmetries. This class of models does not possess any of the symmetries that are required to protect the Haldane phase. Nevertheless, we show that there are 4 symmetry-protected topological phases, in addition to 6 phases that spontaneously break one or both Ising symmetries. By mapping the model to one-dimensional interacting fermions with particle-hole and time-reversal symmetry, we obtain integrable parent Hamiltonians for the conventional and topological phases of the spin model. We use these Hamiltonians to characterize the physical properties of all 10 phases, identify their local and nonlocal order parameters, and understand the effects of weak perturbations that respect the Ising symmetries. Our study provides the first explicit example of a class of spin chains with several topologically non-trivial phases, and binds together the topological classifications of interacting bosons and fermions."
How does the Calculus of Constructions (CC)  handle strong dependent pairs (Sigma types)?,2af5263a,"[{'abstract': '  Dependently typed languages such as Coq are used to specify and verify the\nfull functional correctness of source programs. Type-preserving compilation can\nbe used to preserve these specifications and proofs of correctness through\ncompilation into the generated target-language programs. Unfortunately,\ntype-preserving compilation of dependent types is hard. In essence, the problem\nis that dependent type systems are designed around high-level compositional\nabstractions to decide type checking, but compilation interferes with the\ntype-system rules for reasoning about run-time terms.\nWe develop a type-preserving closure-conversion translation from the Calculus\nof Constructions (CC) with strong dependent pairs ($\\Sigma$ types)---a subset\nof the core language of Coq---to a type-safe, dependently typed compiler\nintermediate language named CC-CC. The central challenge in this work is how to\ntranslate the source type-system rules for reasoning about functions into\ntarget type-system rules for reasoning about closures. To justify these rules,\nwe prove soundness of CC-CC by giving a model in CC. In addition to type\npreservation, we prove correctness of separate compilation.\n', 'title': 'Typed Closure Conversion for the Calculus of Constructions'}]","I think I can help you with that!

Title: Typed Closure Conversion for the Calculus of Constructions

Abstract:   Dependently typed languages such as Coq are used to specify and verify the
full functional correctness of source programs. Type-preserving compilation can
be used to preserve these specifications and proofs of correctness through
compilation into the generated target-language programs. Unfortunately,
type-preserving compilation of dependent types is hard. In essence, the problem
is that dependent type systems are designed around high-level compositional
abstractions to decide type checking, but compilation interferes with the
type-system rules for reasoning about run-time terms.
We develop a type-preserving closure-conversion translation from the Calculus
of Constructions (CC) with strong dependent pairs ($\Sigma$ types)---a subset
of the core language of Coq---to a type-safe, dependently typed compiler
intermediate language named CC-CC. The central challenge in this work is how to
translate the source type-system rules for reasoning about functions into
target type-system rules for reasoning about closures. To justify these rules,
we prove soundness of CC-CC by giving a model in CC. In addition to type
preservation, we prove correctness of separate compilation."
What techniques are used to determine the explicit forms of permutation polynomials?,08663858,"[{'abstract': '  Permutation polynomials over finite fields have wide applications in many\nareas of science and engineering. In this paper, we present six new classes of\npermutation trinomials over $\\mathbb{F}_{2^n}$ which have explicit forms by\ndetermining the solutions of some equations.\n', 'title': 'More new classes of permutation trinomials over $\\mathbb{F}_{2^n}$'}]","Title: More new classes of permutation trinomials over $\mathbb{F}_{2^n}$

Abstract: Permutation polynomials over finite fields have wide applications in many areas of science and engineering. In this paper, we present six new classes of permutation trinomials over $\mathbb{F}_{2^n}$ which have explicit forms by determining the solutions of some equations.

From this abstract, it appears that the technique used to determine the explicit forms of permutation polynomials is by determining the solutions of some equations."
"Does this paper demonstrate there are infinitely many weak solutions to these equations and if so, what determines them? ",f590328c,"[{'abstract': ""  The existence of weak solutions to the stationary Navier-Stokes equations in\nthe whole plane $\\mathbb{R}^2$ is proven. This particular geometry was the only\ncase left open since the work of Leray in 1933. The reason is that due to the\nabsence of boundaries the local behavior of the solutions cannot be controlled\nby the enstrophy in two dimensions. We overcome this difficulty by constructing\napproximate weak solutions having a prescribed mean velocity on some given\nbounded set. As a corollary, we obtain infinitely many weak solutions in\n$\\mathbb{R}^2$ parameterized by this mean velocity, which is reminiscent of the\nexpected convergence of the velocity field at large distances to any prescribed\nconstant vector field. This explicit parameterization of the weak solutions\nallows us to prove a weak-strong uniqueness theorem for small data. The\nquestion of the asymptotic behavior of the weak solutions remains however open,\nwhen the uniqueness theorem doesn't apply.\n"", 'title': 'Existence and uniqueness of steady weak solutions to the Navier-Stokes equations in $\\mathbb{R}^2$'}]","I think I can help you with that!

Title: Existence and uniqueness of steady weak solutions to the Navier-Stokes equations in $\mathbb{R}^2$

Abstract: The existence of weak solutions to the stationary Navier-Stokes equations in the whole plane $\mathbb{R}^2$ is proven. This particular geometry was the only case left open since the work of Leray in 1933. The reason is that due to the absence of boundaries the local behavior of the solutions cannot be controlled by the enstrophy in two dimensions. We overcome this difficulty by constructing approximate weak solutions having a prescribed mean velocity on some given bounded set. As a corollary, we obtain infinitely many weak solutions in $\mathbb{R}^2$ parameterized by this mean velocity, which is reminiscent of the expected convergence of the velocity field at large distances to any prescribed constant vector field. This explicit parameterization of the weak solutions allows us to prove a weak-strong uniqueness theorem for small data. The question of the asymptotic behavior of the weak solutions remains however open, when the uniqueness theorem doesn't apply.

This paper shows that there are infinitely many weak solutions to the Navier-Stokes equations in $\mathbb{R}^2$, and they are determined by the prescribed mean velocity on a given bounded set."
What type of celestial body is 'Oumuamua classified as?,3e10061f,"[{'abstract': ""  The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a\nplanetesimal born in another planetary system. This interloper exhibits a\nvariable colour within a range that is broadly consistent with local small\nbodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited\nKuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an\naxial ratio exceeding 5:1. Rotation period estimates are inconsistent and\nvaried, with reported values between 6.9 and 8.3 hours. Here we analyse all\navailable optical photometry reported to date. No single rotation period can\nexplain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be\nin an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or\ntumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135\nand 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the\navailable data are insufficient to uniquely constrain the true frequencies and\nshape. Assuming a body that responds to NPA rotation in a similar manner to\nSolar System asteroids and comets, the timescale to damp 1I/'Oumuamua's\ntumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling\nwithin its parent planetary system, and will remain tumbling well after it has\nleft ours.\n"", 'title': 'The tumbling rotational state of 1I/`Oumuamua'}]","Title: The tumbling rotational state of 1I/`Oumuamua

Abstract: The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a planetesimal born in another planetary system. This interloper exhibits a variable colour within a range that is broadly consistent with local small bodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited Kuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an axial ratio exceeding 5:1. Rotation period estimates are inconsistent and varied, with reported values between 6.9 and 8.3 hours. Here we analyse all available optical photometry reported to date. No single rotation period can explain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be in an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or tumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135 and 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the available data are insufficient to uniquely constrain the true frequencies and shape. Assuming a body that responds to NPA rotation in a similar manner to Solar System asteroids and comets, the timescale to damp 1I/'Oumuamua's tumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling within its parent planetary system, and will remain tumbling well after it has left ours.

Based on this abstract, 'Oumuamua is classified as a planetesimal, and also exhibits characteristics similar to asteroids and comets."
How does the limited coverage of photomultiplier tubes (PMTs)  affect muon reconstruction in the Daya Bay experiment?,e2ce43e5,"[{'abstract': ""  Muon reconstruction in the Daya Bay water pools would serve to verify the\nsimulated muon fluxes and offer the possibility of studying cosmic muons in\ngeneral. This reconstruction is, however, complicated by many optical obstacles\nand the small coverage of photomultiplier tubes (PMTs) as compared to other\nlarge water Cherenkov detectors. The PMTs' timing information is useful only in\nthe case of direct, unreflected Cherenkov light. This requires PMTs to be added\nand removed as an hypothesized muon trajectory is iteratively improved, to\naccount for the changing effects of obstacles and direction of light.\nTherefore, muon reconstruction in the Daya Bay water pools does not lend itself\nto a general fitting procedure employing smoothly varying functions with\ncontinuous derivatives. Here, an algorithm is described which overcomes these\ncomplications. It employs the method of Least Mean Squares to determine an\nhypothesized trajectory from the PMTs' charge-weighted positions. This\ninitially hypothesized trajectory is then iteratively refined using the PMTs'\ntiming information. Reconstructions with simulated data reproduce the simulated\ntrajectory to within about 5 degrees in direction and about 45 cm in position\nat the pool surface, with a bias that tends to pull tracks away from the\nvertical by about 3 degrees.\n"", 'title': 'Muon Reconstruction in the Daya Bay Water Pools'}]","Title: Muon Reconstruction in the Daya Bay Water Pools

Abstract: Muon reconstruction in the Daya Bay water pools would serve to verify the
simulated muon fluxes and offer the possibility of studying cosmic muons in
general. This reconstruction is, however, complicated by many optical obstacles
and the small coverage of photomultiplier tubes (PMTs) as compared to other
large water Cherenkov detectors. The PMTs' timing information is useful only in
the case of direct, unreflected Cherenkov light. This requires PMTs to be added
and removed as an hypothesized muon trajectory is iteratively improved, to
account for the changing effects of obstacles and direction of light.
Therefore, muon reconstruction in the Daya Bay water pools does not lend itself
to a general fitting procedure employing smoothly varying functions with
continuous derivatives. Here, an algorithm is described which overcomes these
complications. It employs the method of Least Mean Squares to determine an
hypothesized trajectory from the PMTs' charge-weighted positions. This
initially hypothesized trajectory is then iteratively refined using the PMTs'
timing information. Reconstructions with simulated data reproduce the simulated
trajectory to within about 5 degrees in direction and about 45 cm in position
at the pool surface, with a bias that tends to pull tracks away from the
vertical by about 3 degrees.

The limited coverage of PMTs complicates the muon reconstruction in the Daya Bay experiment, requiring an iterative process to add and remove PMTs to account for optical obstacles and direction of light."
What are the identifiability conditions for estimating the outcome distribution when exposure data is prone to error?,6fad8f92,"[{'abstract': '  Measurement error in observational datasets can lead to systematic bias in\ninferences based on these datasets. As studies based on observational data are\nincreasingly used to inform decisions with real-world impact, it is critical\nthat we develop a robust set of techniques for analyzing and adjusting for\nthese biases. In this paper we present a method for estimating the distribution\nof an outcome given a binary exposure that is subject to underreporting. Our\nmethod is based on a missing data view of the measurement error problem, where\nthe true exposure is treated as a latent variable that is marginalized out of a\njoint model. We prove three different conditions under which the outcome\ndistribution can still be identified from data containing only error-prone\nobservations of the exposure. We demonstrate this method on synthetic data and\nanalyze its sensitivity to near violations of the identifiability conditions.\nFinally, we use this method to estimate the effects of maternal smoking and\nopioid use during pregnancy on childhood obesity, two import problems from\npublic health. Using the proposed method, we estimate these effects using only\nsubject-reported drug use data and substantially refine the range of estimates\ngenerated by a sensitivity analysis-based approach. Further, the estimates\nproduced by our method are consistent with existing literature on both the\neffects of maternal smoking and the rate at which subjects underreport smoking.\n', 'title': 'Learning Models from Data with Measurement Error: Tackling Underreporting'}]","Title: Learning Models from Data with Measurement Error: Tackling Underreporting

Abstract: Measurement error in observational datasets can lead to systematic bias in inferences based on these datasets. As studies based on observational data are increasingly used to inform decisions with real-world impact, it is critical that we develop a robust set of techniques for analyzing and adjusting for these biases. In this paper we present a method for estimating the distribution of an outcome given a binary exposure that is subject to underreporting. Our method is based on a missing data view of the measurement error problem, where the true exposure is treated as a latent variable that is marginalized out of a joint model. We prove three different conditions under which the outcome distribution can still be identified from data containing only error-prone observations of the exposure."
Are there any web-based tools that use BRAT for entity annotation in the context of named-entity recognition?,689b56e9,"[{'abstract': ""  Named-entity recognition (NER) aims at identifying entities of interest in a\ntext. Artificial neural networks (ANNs) have recently been shown to outperform\nexisting NER systems. However, ANNs remain challenging to use for non-expert\nusers. In this paper, we present NeuroNER, an easy-to-use named-entity\nrecognition tool based on ANNs. Users can annotate entities using a graphical\nweb-based user interface (BRAT): the annotations are then used to train an ANN,\nwhich in turn predict entities' locations and categories in new texts. NeuroNER\nmakes this annotation-training-prediction flow smooth and accessible to anyone.\n"", 'title': 'NeuroNER: an easy-to-use program for named-entity recognition based on neural networks'}]","Title: NeuroNER: an easy-to-use program for named-entity recognition based on neural networks

Abstract: Named-entity recognition (NER) aims at identifying entities of interest in a text. Artificial neural networks (ANNs) have recently been shown to outperform existing NER systems. However, ANNs remain challenging to use for non-expert users. In this paper, we present NeuroNER, an easy-to-use named-entity recognition tool based on ANNs. Users can annotate entities using a graphical web-based user interface (BRAT): the annotations are then used to train an ANN, which in turn predict entities' locations and categories in new texts. NeuroNER makes this annotation-training-prediction flow smooth and accessible to anyone."
Can you describe a deep learning architecture that jointly learns a projection matrix and a reconstruction operator for compressed sensing?,a22b9d6e,"[{'abstract': '  Compressed sensing (CS) is a sampling theory that allows reconstruction of\nsparse (or compressible) signals from an incomplete number of measurements,\nusing of a sensing mechanism implemented by an appropriate projection matrix.\nThe CS theory is based on random Gaussian projection matrices, which satisfy\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\nprojections are more suitable for hardware implementation. In this paper, we\npresent a deep learning approach to obtain very sparse ternary projections for\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\nexperimental results on real images demonstrate the effectiveness of the\nproposed approach compared to state-of-the-art methods, with significant\nadvantage in terms of complexity.\n', 'title': 'Deep Learning Sparse Ternary Projections for Compressed Sensing of Images'}]","Here is the answer to your question:

Title: Deep Learning Sparse Ternary Projections for Compressed Sensing of Images

Abstract: Compressed sensing (CS) is a sampling theory that allows reconstruction of sparse (or compressible) signals from an incomplete number of measurements, using of a sensing mechanism implemented by an appropriate projection matrix. The CS theory is based on random Gaussian projection matrices, which satisfy recovery guarantees with high probability; however, sparse ternary {0, -1, +1} projections are more suitable for hardware implementation. In this paper, we present a deep learning approach to obtain very sparse ternary projections for compressed sensing. Our deep learning architecture jointly learns a pair of a projection matrix and a reconstruction operator in an end-to-end fashion. The experimental results on real images demonstrate the effectiveness of the proposed approach compared to state-of-the-art methods, with significant advantage in terms of complexity."
How does forecasting based on user predictions compare to sentiment analysis and tweet volume baselines?,ec8b782e,"[{'abstract': '  Social media users often make explicit predictions about upcoming events.\nSuch statements vary in the degree of certainty the author expresses toward the\noutcome:""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo DiCaprio may win""\nor ""No way Leonardo wins!"". Can popular beliefs on social media predict who\nwill win? To answer this question, we build a corpus of tweets annotated for\nveridicality on which we train a log-linear classifier that detects positive\nveridicality with high precision. We then forecast uncertain outcomes using the\nwisdom of crowds, by aggregating users\' explicit predictions. Our method for\nforecasting winners is fully automated, relying only on a set of contenders as\ninput. It requires no training data of past outcomes and outperforms sentiment\nand tweet volume baselines on a broad range of contest prediction tasks. We\nfurther demonstrate how our approach can be used to measure the reliability of\nindividual accounts\' predictions and retrospectively identify surprise\noutcomes.\n', 'title': '""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter'}]","Title: ""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter

Abstract: Social media users often make explicit predictions about upcoming events.
Such statements vary in the degree of certainty the author expresses toward the
outcome:""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo DiCaprio may win""
or ""No way Leonardo wins!"". Can popular beliefs on social media predict who
will win? To answer this question, we build a corpus of tweets annotated for
veridicality on which we train a log-linear classifier that detects positive
veridicality with high precision. We then forecast uncertain outcomes using the
wisdom of crowds, by aggregating users' explicit predictions. Our method for
forecasting winners is fully automated, relying only on a set of contenders as
input. It requires no training data of past outcomes and outperforms sentiment
and tweet volume baselines on a broad range of contest prediction tasks. We
further demonstrate how our approach can be used to measure the reliability of
individual accounts' predictions and retrospectively identify surprise
outcomes.

According to this paper, forecasting based on user predictions outperforms sentiment analysis and tweet volume baselines on a broad range of contest prediction tasks."
Three institutions and nine different lab courses were studied to determine the effectiveness of labs. What common factors did these courses share?,de76883b,"[{'abstract': '  Instructional labs are widely seen as a unique, albeit expensive, way to\nteach scientific content. We measured the effectiveness of introductory lab\ncourses at achieving this educational goal across nine different lab courses at\nthree very different institutions. These institutions and courses encompassed a\nbroad range of student populations and instructional styles. The nine courses\nstudied had two key things in common: the labs aimed to reinforce the content\npresented in lectures, and the labs were optional. By comparing the performance\nof students who did and did not take the labs (with careful normalization for\nselection effects), we found universally and precisely no added value to\nlearning from taking the labs as measured by course exam performance. This work\nshould motivate institutions and departments to reexamine the goals and conduct\nof their lab courses, given their resource-intensive nature. We show why these\nresults make sense when looking at the comparative mental processes of students\ninvolved in research and instructional labs, and offer alternative goals and\ninstructional approaches that would make lab courses more educationally\nvaluable.\n', 'title': 'Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content'}]","Title: Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content

Abstract: Instructional labs are widely seen as a unique, albeit expensive, way to teach scientific content. We measured the effectiveness of introductory lab courses at achieving this educational goal across nine different lab courses at three very different institutions. These institutions and courses encompassed a broad range of student populations and instructional styles. The nine courses studied had two key things in common: the labs aimed to reinforce the content presented in lectures, and the labs were optional. By comparing the performance of students who did and did not take the labs (with careful normalization for selection effects), we found universally and precisely no added value to learning from taking the labs as measured by course exam performance. This work should motivate institutions and departments to reexamine the goals and conduct of their lab courses, given their resource-intensive nature. We show why these results make sense when looking at the comparative mental processes of students involved in research and instructional labs, and offer alternative goals and instructional approaches that would make lab courses more educationally valuable."
How does the spin structure of uni-axial chiral magnets compare to that of non-chiral helimagnets?,2c75f6b1,"[{'abstract': '  We investigate the spin structure of a uni-axial chiral magnet near the\ntransition temperatures in low fields perpendicular to the helical axis. We\nfind a fan-type modulation structure where the clockwise and counterclockwise\nwindings appear alternatively along the propagation direction of the modulation\nstructure. This structure is often realized in a Yoshimori-type (non-chiral)\nhelimagnet but it is rarely realized in a chiral helimagnet. To discuss\nunderlying physics of this structure, we reconsider the phase diagram (phase\nboundary and crossover lines) through the free energy and asymptotic behaviors\nof isolated solitons. The fan structure appears slightly below the phase\nboundary of the continuous transition of instability-type. In this region,\nthere are no solutions containing any types of isolated solitons to the mean\nfield equations.\n', 'title': 'Fan-type spin structure in uni-axial chiral magnets'}]","Title: Fan-type spin structure in uni-axial chiral magnets

Abstract: We investigate the spin structure of a uni-axial chiral magnet near the transition temperatures in low fields perpendicular to the helical axis. We find a fan-type modulation structure where the clockwise and counterclockwise windings appear alternatively along the propagation direction of the modulation structure. This structure is often realized in a Yoshimori-type (non-chiral) helimagnet but it is rarely realized in a chiral helimagnet. To discuss underlying physics of this structure, we reconsider the phase diagram (phase boundary and crossover lines) through the free energy and asymptotic behaviors of isolated solitons. The fan structure appears slightly below the phase boundary of the continuous transition of instability-type. In this region, there are no solutions containing any types of isolated solitons to the mean field equations.

According to this research paper, the fan-type spin structure in uni-axial chiral magnets is similar to that of non-chiral helimagnets (Yoshimori-type), but it is less common in chiral helimagnets."
How can an agent estimate the strategies of other agents in a decentralized setting?,51f39a09,"[{'abstract': ""  This paper considers the problem of autonomous multi-agent cooperative target\nsearch in an unknown environment using a decentralized framework under a\nno-communication scenario. The targets are considered as static targets and the\nagents are considered to be homogeneous. The no-communication scenario\ntranslates as the agents do not exchange either the information about the\nenvironment or their actions among themselves. We propose an integrated\ndecision and control theoretic solution for a search problem which generates\nfeasible agent trajectories. In particular, a perception based algorithm is\nproposed which allows an agent to estimate the probable strategies of other\nagents' and to choose a decision based on such estimation. The algorithm shows\nrobustness with respect to the estimation accuracy to a certain degree. The\nperformance of the algorithm is compared with random strategies and numerical\nsimulation shows considerable advantages.\n"", 'title': 'An Integrated Decision and Control Theoretic Solution to Multi-Agent Co-Operative Search Problems'}]","I think I can help you with that!

Title: An Integrated Decision and Control Theoretic Solution to Multi-Agent Co-Operative Search Problems

Abstract: This paper considers the problem of autonomous multi-agent cooperative target search in an unknown environment using a decentralized framework under a no-communication scenario. The targets are considered as static targets and the agents are considered to be homogeneous. The no-communication scenario translates as the agents do not exchange either the information about the environment or their actions among themselves. We propose an integrated decision and control theoretic solution for a search problem which generates feasible agent trajectories. In particular, a perception based algorithm is proposed which allows an agent to estimate the probable strategies of other agents' and to choose a decision based on such estimation. The algorithm shows robustness with respect to the estimation accuracy to a certain degree. The performance of the algorithm is compared with random strategies and numerical simulation shows considerable advantages."
How can the Nyström method be applied to diffusion maps to improve efficiency?,b20583d4,"[{'abstract': '  Diffusion maps are an emerging data-driven technique for non-linear\ndimensionality reduction, which are especially useful for the analysis of\ncoherent structures and nonlinear embeddings of dynamical systems. However, the\ncomputational complexity of the diffusion maps algorithm scales with the number\nof observations. Thus, long time-series data presents a significant challenge\nfor fast and efficient embedding. We propose integrating the Nyström method\nwith diffusion maps in order to ease the computational demand. We achieve a\nspeedup of roughly two to four times when approximating the dominant diffusion\nmap components.\n', 'title': 'Diffusion Maps meet Nyström'}]","I think I have found the answer you're looking for!

Title: Diffusion Maps meet Nyström

Abstract: Diffusion maps are an emerging data-driven technique for non-linear dimensionality reduction, which are especially useful for the analysis of coherent structures and nonlinear embeddings of dynamical systems. However, the computational complexity of the diffusion maps algorithm scales with the number of observations. Thus, long time-series data presents a significant challenge for fast and efficient embedding. We propose integrating the Nyström method with diffusion maps in order to ease the computational demand. We achieve a speedup of roughly two to four times when approximating the dominant diffusion map components."
"Can you describe the algorithm developed for efficiently computing a lower bound on the distance between two networks, and provide examples of its application?",a7ed7722,"[{'abstract': '  We develop the theoretical foundations of a network distance that has\nrecently been applied to various subfields of topological data analysis, namely\npersistent homology and hierarchical clustering. While this network distance\nhas previously appeared in the context of finite networks, we extend the\nsetting to that of compact networks. The main challenge in this new setting is\nthe lack of an easy notion of sampling from compact networks; we solve this\nproblem in the process of obtaining our results. The generality of our setting\nmeans that we automatically establish results for exotic objects such as\ndirected metric spaces and Finsler manifolds. We identify readily computable\nnetwork invariants and establish their quantitative stability under this\nnetwork distance. We also discuss the computational complexity involved in\nprecisely computing this distance, and develop easily-computable lower bounds\nby using the identified invariants. By constructing a wide range of explicit\nexamples, we show that these lower bounds are effective in distinguishing\nbetween networks. Finally, we provide a simple algorithm that computes a lower\nbound on the distance between two networks in polynomial time and illustrate\nour metric and invariant constructions on a database of random networks and a\ndatabase of simulated hippocampal networks.\n', 'title': 'Distances and Isomorphism between Networks and the Stability of Network Invariants'}]","Title: Distances and Isomorphism between Networks and the Stability of Network Invariants

Abstract: We develop the theoretical foundations of a network distance that has recently been applied to various subfields of topological data analysis, namely persistent homology and hierarchical clustering. While this network distance has previously appeared in the context of finite networks, we extend the setting to that of compact networks. The main challenge in this new setting is the lack of an easy notion of sampling from compact networks; we solve this problem in the process of obtaining our results. The generality of our setting means that we automatically establish results for exotic objects such as directed metric spaces and Finsler manifolds. We identify readily computable network invariants and establish their quantitative stability under this network distance. We also discuss the computational complexity involved in precisely computing this distance, and develop easily-computable lower bounds by using the identified invariants. By constructing a wide range of explicit examples, we show that these lower bounds are effective in distinguishing between networks. Finally, we provide a simple algorithm that computes a lower bound on the distance between two networks in polynomial time and illustrate our metric and invariant constructions on a database of random networks and a database of simulated hippocampal networks.

The algorithm developed in this paper is a simple algorithm that computes a lower bound on the distance between two networks in polynomial time. The application of this algorithm is illustrated on a database of random networks and a database of simulated hippocampal networks."
Can passive cross-linkers enhance the efficiency of active motor proteins?,c3033e68,"[{'abstract': '  Current understanding of how contractility emerges in disordered actomyosin\nnetworks of non-muscle cells is still largely based on the intuition derived\nfrom earlier works on muscle contractility. This view, however, largely\noverlooks the free energy gain following passive cross-linker binding, which,\neven in the absence of active fluctuations, provides a thermodynamic drive\ntowards highly overlapping filamentous states. In this work, we shed light on\nthis phenomenon, showing that passive cross-linkers, when considered in the\ncontext of two anti-parallel filaments, generate noticeable contractile forces.\nHowever, as binding free energy of cross-linkers is increased, a sharp onset of\nkinetic arrest follows, greatly diminishing effectiveness of this contractility\nmechanism, allowing the network to contract only with weakly resisting tensions\nat its boundary. We have carried out stochastic simulations elucidating this\nmechanism, followed by a mean-field treatment that predicts how contractile\nforces asymptotically scale at small and large binding energies, respectively.\nFurthermore, when considering an active contractile filament pair, based on\nnon-muscle myosin II, we found that the non-processive nature of these motors\nleads to highly inefficient force generation, due to recoil slippage of the\noverlap during periods when the motor is dissociated. However, we discovered\nthat passive cross-linkers can serve as a structural ratchet during these\nunbound motor time spans, resulting in vast force amplification. Our results\nshed light on the non-equilibrium effects of transiently binding proteins in\nbiological active matter, as observed in the non-muscle actin cytoskeleton,\nshowing that highly efficient contractile force dipoles result from synergy of\npassive cross-linker and active motor dynamics, via a ratcheting mechanism on a\nfunneled energy landscape.\n', 'title': 'Stochastic Ratcheting on a Funneled Energy Landscape is Necessary for Highly Efficient Contractility of Actomyosin Force Dipoles'}]","I think I found the answer you're looking for!

Title: Stochastic Ratcheting on a Funneled Energy Landscape is Necessary for Highly Efficient Contractility of Actomyosin Force Dipoles

Abstract: Current understanding of how contractility emerges in disordered actomyosin networks of non-muscle cells is still largely based on the intuition derived from earlier works on muscle contractility. This view, however, largely overlooks the free energy gain following passive cross-linker binding, which, even in the absence of active fluctuations, provides a thermodynamic drive towards highly overlapping filamentous states. In this work, we shed light on this phenomenon, showing that passive cross-linkers, when considered in the context of two anti-parallel filaments, generate noticeable contractile forces. However, as binding free energy of cross-linkers is increased, a sharp onset of kinetic arrest follows, greatly diminishing effectiveness of this contractility mechanism, allowing the network to contract only with weakly resisting tensions at its boundary. We have carried out stochastic simulations elucidating this mechanism, followed by a mean-field treatment that predicts how contractile forces asymptotically scale at small and large binding energies, respectively. Furthermore, when considering an active contractile filament pair, based on non-muscle myosin II, we found that the non-processive nature of these motors leads to highly inefficient force generation, due to recoil slippage of the overlap during periods when the motor is dissociated. However, we discovered that passive cross-linkers can serve as a structural ratchet during these unbound motor time spans, resulting in vast force amplification. Our results shed light on the non-equilibrium effects of transiently binding proteins in biological active matter, as observed in the non-muscle actin cytoskeleton, showing that highly efficient contractile force dipoles result from synergy of passive cross-linker and active motor dynamics, via a ratcheting mechanism on a funneled energy landscape.

According to this abstract, yes, passive cross-linkers can enhance the efficiency of active motor proteins by serving as a structural ratchet, resulting in vast force amplification."
How does cross-technology interference (CTI) impact wireless communication?,024bba55,"[{'abstract': '  In wireless communication, heterogeneous technologies such as WiFi, ZigBee\nand BlueTooth operate in the same ISM band.With the exponential growth in the\nnumber of wireless devices, the ISM band becomes more and more crowded. These\nheterogeneous devices have to compete with each other to access spectrum\nresources, generating cross-technology interference (CTI). Since CTI may\ndestroy wireless communication, this field is facing an urgent and challenging\nneed to investigate spectrum efficiency under CTI. In this paper, we introduce\na novel framework to address this problem from two aspects. On the one hand,\nfrom the perspective of each communication technology itself, we propose novel\nchannel/link models to capture the channel/link status under CTI. On the other\nhand, we investigate spectrum efficiency from the perspective by taking all\nheterogeneous technologies as a whole and building crosstechnology\ncommunication among them. The capability of direct communication among\nheterogeneous devices brings great opportunities to harmoniously sharing the\nspectrum with collaboration rather than competition.\n', 'title': 'Achieving Spectrum Efficient Communication Under Cross-Technology Interference'}]","Title: Achieving Spectrum Efficient Communication Under Cross-Technology Interference

Abstract: In wireless communication, heterogeneous technologies such as WiFi, ZigBee and BlueTooth operate in the same ISM band.With the exponential growth in the number of wireless devices, the ISM band becomes more and more crowded. These heterogeneous devices have to compete with each other to access spectrum resources, generating cross-technology interference (CTI). Since CTI may destroy wireless communication, this field is facing an urgent and challenging need to investigate spectrum efficiency under CTI. In this paper, we introduce a novel framework to address this problem from two aspects. On the one hand, from the perspective of each communication technology itself, we propose novel channel/link models to capture the channel/link status under CTI. On the other hand, we investigate spectrum efficiency from the perspective by taking all heterogeneous technologies as a whole and building crosstechnology communication among them. The capability of direct communication among heterogeneous devices brings great opportunities to harmoniously sharing the spectrum with collaboration rather than competition."
What algebras map homomorphically into quantized K-theoretic Coulomb branches of 3d N=4 supersymmetric quiver gauge theories? ,55af4c01,"[{'abstract': '  We introduce the shifted quantum affine algebras. They map homomorphically\ninto the quantized $K$-theoretic Coulomb branches of $3d\\ {\\mathcal N}=4$ SUSY\nquiver gauge theories. In type $A$, they are endowed with a coproduct, and they\nact on the equivariant $K$-theory of parabolic Laumon spaces. In type $A_1$,\nthey are closely related to the open relativistic quantum Toda lattice of type\n$A$.\n', 'title': 'Multiplicative slices, relativistic Toda and shifted quantum affine algebras'}]","Based on the context, I think I have found the answer to your question.

Title: Multiplicative slices, relativistic Toda and shifted quantum affine algebras

Abstract: We introduce the shifted quantum affine algebras. They map homomorphically into the quantized $K$-theoretic Coulomb branches of $3d\ {\mathcal N}=4$ SUSY quiver gauge theories. In type $A$, they are endowed with a coproduct, and they act on the equivariant $K$-theory of parabolic Laumon spaces. In type $A_1$, they are closely related to the open relativistic quantum Toda lattice of type $A$."
What are the benefits of high dynamic range (HDR) imaging systems compared to standard dynamic range (SDR) systems?,31c74a75,"[{'abstract': '  In this lecture note, we describe high dynamic range (HDR) imaging systems;\nsuch systems are able to represent luminances of much larger brightness and,\ntypically, also a larger range of colors than conventional standard dynamic\nrange (SDR) imaging systems. The larger luminance range greatly improve the\noverall quality of visual content, making it appears much more realistic and\nappealing to observers. HDR is one of the key technologies of the future\nimaging pipeline, which will change the way the digital visual content is\nrepresented and manipulated today.\n', 'title': 'High Dynamic Range Imaging Technology'}]","I can help you with that!

Title: High Dynamic Range Imaging Technology

Abstract: In this lecture note, we describe high dynamic range (HDR) imaging systems; such systems are able to represent luminances of much larger brightness and, typically, also a larger range of colors than conventional standard dynamic range (SDR) imaging systems. The larger luminance range greatly improve the overall quality of visual content, making it appears much more realistic and appealing to observers. HDR is one of the key technologies of the future imaging pipeline, which will change the way the digital visual content is represented and manipulated today.

According to this abstract, the benefits of HDR imaging systems compared to SDR systems are:

* Representing luminances of much larger brightness
* Representing a larger range of colors
* Improving the overall quality of visual content
* Making visual content appear more realistic and appealing to observers"
What are the implications of D-region electron content variations for satellite applications like global positioning?,b59fce5b,"[{'abstract': '  One of the most important parameters in ionospheric plasma research also\nhaving a wide practical application in wireless satellite telecommunications is\nthe total electron content (TEC) representing the columnal electron number\ndensity. The F region with high electron density provides the biggest\ncontribution to TEC while the relatively weakly ionized plasma of the D region\n(60 km - 90 km above Earths surface) is often considered as a negligible cause\nof satellite signal disturbances. However, sudden intensive ionization\nprocesses like those induced by solar X ray flares can cause relative increases\nof electron density that are significantly larger in the D-region than in\nregions at higher altitudes. Therefore, one cannot exclude a priori the D\nregion from investigations of ionospheric influences on propagation of\nelectromagnetic signals emitted by satellites. We discuss here this problem\nwhich has not been sufficiently treated in literature so far. The obtained\nresults are based on data collected from the D region monitoring by very low\nfrequency radio waves and on vertical TEC calculations from the Global\nNavigation Satellite System (GNSS) signal analyses, and they show noticeable\nvariations in the D region electron content (TECD) during activity of a solar X\nray flare (it rises by a factor of 136 in the considered case) when TECD\ncontribution to TEC can reach several percent and which cannot be neglected in\npractical applications like global positioning procedures by satellites.\n', 'title': 'Behaviour of electron content in the ionospheric D-region during solar X-ray flares'}]","Title: Behaviour of electron content in the ionospheric D-region during solar X-ray flares

Abstract:   One of the most important parameters in ionospheric plasma research also
having a wide practical application in wireless satellite telecommunications is
the total electron content (TEC) representing the columnal electron number
density. The F region with high electron density provides the biggest
contribution to TEC while the relatively weakly ionized plasma of the D region
(60 km - 90 km above Earths surface) is often considered as a negligible cause
of satellite signal disturbances. However, sudden intensive ionization
processes like those induced by solar X ray flares can cause relative increases
of electron density that are significantly larger in the D-region than in
regions at higher altitudes. Therefore, one cannot exclude a priori the D
region from investigations of ionospheric influences on propagation of
electromagnetic signals emitted by satellites. We discuss here this problem
which has not been sufficiently treated in literature so far. The obtained
results are based on data collected from the D region monitoring by very low
frequency radio waves and on vertical TEC calculations from the Global
Navigation Satellite System (GNSS) signal analyses, and they show noticeable
variations in the D region electron content (TECD) during activity of a solar X
ray flare (it rises by a factor of 136 in the considered case) when TECD
contribution to TEC can reach several percent and which cannot be neglected in
practical applications like global positioning procedures by satellites."
What is a novel approach to measuring team faultlines?,db63c88d,"[{'abstract': '  In recent years, the proliferation of online resumes and the need to evaluate\nlarge populations of candidates for on-site and virtual teams have led to a\ngrowing interest in automated team-formation. Given a large pool of candidates,\nthe general problem requires the selection of a team of experts to complete a\ngiven task. Surprisingly, while ongoing research has studied numerous\nvariations with different constraints, it has overlooked a factor with a\nwell-documented impact on team cohesion and performance: team faultlines.\nAddressing this gap is challenging, as the available measures for faultlines in\nexisting teams cannot be efficiently applied to faultline optimization. In this\nwork, we meet this challenge with a new measure that can be efficiently used\nfor both faultline measurement and minimization. We then use the measure to\nsolve the problem of automatically partitioning a large population into\nlow-faultline teams. By introducing faultlines to the team-formation\nliterature, our work creates exciting opportunities for algorithmic work on\nfaultline optimization, as well as on work that combines and studies the\nconnection of faultlines with other influential team characteristics.\n', 'title': 'A Team-Formation Algorithm for Faultline Minimization'}]","Title: A Team-Formation Algorithm for Faultline Minimization

Abstract:  In recent years, the proliferation of online resumes and the need to evaluate
large populations of candidates for on-site and virtual teams have led to a
growing interest in automated team-formation. Given a large pool of candidates,
the general problem requires the selection of a team of experts to complete a
given task. Surprisingly, while ongoing research has studied numerous
variations with different constraints, it has overlooked a factor with a
well-documented impact on team cohesion and performance: team faultlines.
Addressing this gap is challenging, as the available measures for faultlines in
existing teams cannot be efficiently applied to faultline optimization. In this
work, we meet this challenge with a new measure that can be efficiently used
for both faultline measurement and minimization. We then use the measure to
solve the problem of automatically partitioning a large population into
low-faultline teams. By introducing faultlines to the team-formation
literature, our work creates exciting opportunities for algorithmic work on
faultline optimization, as well as on work that combines and studies the
connection of faultlines with other influential team characteristics."
What are the challenges in training RNN transducer models for automatic speech recognition?,46b6be56,"[{'abstract': '  End-to-end approaches have drawn much attention recently for significantly\nsimplifying the construction of an automatic speech recognition (ASR) system.\nRNN transducer (RNN-T) is one of the popular end-to-end methods. Previous\nstudies have shown that RNN-T is difficult to train and a very complex training\nprocess is needed for a reasonable performance. In this paper, we explore RNN-T\nfor a Chinese large vocabulary continuous speech recognition (LVCSR) task and\naim to simplify the training process while maintaining performance. First, a\nnew strategy of learning rate decay is proposed to accelerate the model\nconvergence. Second, we find that adding convolutional layers at the beginning\nof the network and using ordered data can discard the pre-training process of\nthe encoder without loss of performance. Besides, we design experiments to find\na balance among the usage of GPU memory, training circle and model performance.\nFinally, we achieve 16.9% character error rate (CER) on our test set which is\n2% absolute improvement from a strong BLSTM CE system with language model\ntrained on the same text corpus.\n', 'title': 'Exploring RNN-Transducer for Chinese Speech Recognition'}]","Title: Exploring RNN-Transducer for Chinese Speech Recognition

Abstract: End-to-end approaches have drawn much attention recently for significantly simplifying the construction of an automatic speech recognition (ASR) system. RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous studies have shown that RNN-T is difficult to train and a very complex training process is needed for a reasonable performance."
How is the projection of system trajectories in a Riesz basis used to prove ISS for a clamped-free damped string?,b8817feb,"[{'abstract': '  This note establishes the input-to-state stability (ISS) property for a\nclamped-free damped string with respect to distributed and boundary\ndisturbances. While efficient methods for establishing ISS properties for\ndistributed parameter systems with respect to distributed disturbances have\nbeen developed during the last decades, establishing ISS properties with\nrespect to boundary disturbances remains challenging. One of the well-known\nmethods for well-posedness analysis of systems with boundary inputs is to use\nan adequate lifting operator, which transfers the boundary disturbance to a\ndistributed one. However, the resulting distributed disturbance involves time\nderivatives of the boundary perturbation. Thus, the subsequent ISS estimate\ndepends on its amplitude, and may not be expressed in the strict form of ISS\nproperties. To solve this problem, we show for a clamped-free damped string\nequation that the projection of the original system trajectories in an adequate\nRiesz basis can be used to establish the desired ISS property.\n', 'title': 'Input-to-State Stability of a Clamped-Free Damped String in the Presence of Distributed and Boundary Disturbances'}]","Title: Input-to-State Stability of a Clamped-Free Damped String in the Presence of Distributed and Boundary Disturbances

Abstract: This note establishes the input-to-state stability (ISS) property for a clamped-free damped string with respect to distributed and boundary disturbances. While efficient methods for establishing ISS properties for distributed parameter systems with respect to distributed disturbances have been developed during the last decades, establishing ISS properties with respect to boundary disturbances remains challenging. One of the well-known methods for well-posedness analysis of systems with boundary inputs is to use an adequate lifting operator, which transfers the boundary disturbance to a distributed one. However, the resulting distributed disturbance involves time derivatives of the boundary perturbation. Thus, the subsequent ISS estimate depends on its amplitude, and may not be expressed in the strict form of ISS properties. To solve this problem, we show for a clamped-free damped string equation that the projection of the original system trajectories in an adequate Riesz basis can be used to establish the desired ISS property."
What are some examples of image datasets and GAN architectures that benefit from the proposed training method?,253ca9b4,"[{'abstract': '  Generative Adversarial Networks (GANs) excel at creating realistic images\nwith complex models for which maximum likelihood is infeasible. However, the\nconvergence of GAN training has still not been proved. We propose a two\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate\nfor both the discriminator and the generator. Using the theory of stochastic\napproximation, we prove that the TTUR converges under mild assumptions to a\nstationary local Nash equilibrium. The convergence carries over to the popular\nAdam optimization, for which we prove that it follows the dynamics of a heavy\nball with friction and thus prefers flat minima in the objective landscape. For\nthe evaluation of the performance of GANs at image generation, we introduce the\n""Fréchet Inception Distance"" (FID) which captures the similarity of generated\nimages to real ones better than the Inception Score. In experiments, TTUR\nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)\noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN\nBedrooms, and the One Billion Word Benchmark.\n', 'title': 'GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium'}]","Title: GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium

Abstract: Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the ""Fréchet Inception Distance"" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.

In this paper, the following image datasets and GAN architectures benefit from the proposed training method:

* DCGANs on CelebA, CIFAR-10, SVHN, LSUN Bedrooms
* Improved Wasserstein GANs (WGAN-GP) on CelebA, CIFAR-10, SVHN, LSUN Bedrooms"
What type of system is created when two harmonic oscillators representing a levitated nanoparticle are parametrically coupled?,b2067617,"[{'abstract': '  The center-of-mass motion of a single optically levitated nanoparticle\nresembles three uncoupled harmonic oscillators. We show how a suitable\nmodulation of the optical trapping potential can give rise to a coupling\nbetween two of these oscillators, such that their dynamics are governed by a\nclassical equation of motion that resembles the Schrödinger equation for a\ntwo-level system. Based on experimental data, we illustrate the dynamics of\nthis parametrically coupled system both in the frequency and in the time\ndomain. We discuss the limitations and differences of the mechanical analogue\nin comparison to a true quantum mechanical system.\n', 'title': 'A levitated nanoparticle as a classical two-level atom'}]","Title: A levitated nanoparticle as a classical two-level atom

Abstract:   The center-of-mass motion of a single optically levitated nanoparticle 
resembles three uncoupled harmonic oscillators. We show how a suitable modulation 
of the optical trapping potential can give rise to a coupling between two of 
these oscillators, such that their dynamics are governed by a classical equation 
of motion that resembles the Schrödinger equation for a two-level system. Based 
on experimental data, we illustrate the dynamics of this parametrically coupled 
system both in the frequency and in the time domain. We discuss the limitations 
and differences of the mechanical analogue in comparison to a true quantum 
mechanical system."
Why are traditional statistical methods inadequate for analyzing the relationship between biodiversity and ecosystem functioning when nonlinear abundance-ecosystem functioning relationships are considered?,2b516e70,"[{'abstract': '  Resolving the relationship between biodiversity and ecosystem functioning has\nbeen one of the central goals of modern ecology. Early debates about the\nrelationship were finally resolved with the advent of a statistical\npartitioning scheme that decomposed the biodiversity effect into a ""selection""\neffect and a ""complementarity"" effect. We prove that both the biodiversity\neffect and its statistical decomposition into selection and complementarity are\nfundamentally flawed because these methods use a naïve null expectation based\non neutrality, likely leading to an overestimate of the net biodiversity\neffect, and they fail to account for the nonlinear abundance-ecosystem\nfunctioning relationships observed in nature. Furthermore, under such\nnonlinearity no statistical scheme can be devised to partition the biodiversity\neffects. We also present an alternative metric providing a more reasonable\nestimate of biodiversity effect. Our results suggest that all studies conducted\nsince the early 1990s likely overestimated the positive effects of biodiversity\non ecosystem functioning.\n', 'title': 'Not even wrong: The spurious link between biodiversity and ecosystem functioning'}]","Title: Not even wrong: The spurious link between biodiversity and ecosystem functioning

Abstract: Resolving the relationship between biodiversity and ecosystem functioning has been one of the central goals of modern ecology. Early debates about the relationship were finally resolved with the advent of a statistical partitioning scheme that decomposed the biodiversity effect into a ""selection"" effect and a ""complementarity"" effect. We prove that both the biodiversity effect and its statistical decomposition into selection and complementarity are fundamentally flawed because these methods use a naïve null expectation based on neutrality, likely leading to an overestimate of the net biodiversity effect, and they fail to account for the nonlinear abundance-ecosystem functioning relationships observed in nature. Furthermore, under such nonlinearity no statistical scheme can be devised to partition the biodiversity effects. We also present an alternative metric providing a more reasonable estimate of biodiversity effect. Our results suggest that all studies conducted since the early 1990s likely overestimated the positive effects of biodiversity on ecosystem functioning."
What type of codes over Fq2 are used to construct new quantum MDS codes?,b8804b3f,"[{'abstract': '  This paper is devoted to the study of the construction of new quantum MDS\ncodes. Based on constacyclic codes over Fq2 , we derive four new families of\nquantum MDS codes, one of which is an explicit generalization of the\nconstruction given in Theorem 7 in [22]. We also extend the result of Theorem\n3:3 given in [17].\n', 'title': 'New quantum mds constacylıc codes'}]","Based on the context, I found the answer to your question.

Title: New quantum mds constacylıc codes

Abstract: This paper is devoted to the study of the construction of new quantum MDS codes. Based on constacyclic codes over Fq2 , we derive four new families of quantum MDS codes, one of which is an explicit generalization of the construction given in Theorem 7 in [22]. We also extend the result of Theorem 3:3 given in [17].

The type of codes over Fq2 used to construct new quantum MDS codes are constacyclic codes."
Why are existing evaluations of ZIP and ZIA schemes limited?,fb571ac1,"[{'abstract': '  The Internet of Things (IoT) demands authentication systems which can provide\nboth security and usability. Recent research utilizes the rich sensing\ncapabilities of smart devices to build security schemes operating without human\ninteraction, such as zero-interaction pairing (ZIP) and zero-interaction\nauthentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and\nreported promising results. However, those schemes were often evaluated under\nconditions which do not reflect realistic IoT scenarios. In addition, drawing\nany comparison among the existing schemes is impossible due to the lack of a\ncommon public dataset and unavailability of scheme implementations.\nIn this paper, we address these challenges by conducting the first\nlarge-scale comparative study of ZIP and ZIA schemes, carried out under\nrealistic conditions. We collect and release the most comprehensive dataset in\nthe domain to date, containing over 4250 hours of audio recordings and 1\nbillion sensor readings from three different scenarios, and evaluate five\nstate-of-the-art schemes based on these data. Our study reveals that the\neffectiveness of the existing proposals is highly dependent on the scenario\nthey are used in. In particular, we show that these schemes are subject to\nerror rates between 0.6% and 52.8%.\n', 'title': 'Perils of Zero-Interaction Security in the Internet of Things'}]","Title: Perils of Zero-Interaction Security in the Internet of Things

Abstract:   The Internet of Things (IoT) demands authentication systems which can provide
both security and usability. Recent research utilizes the rich sensing
capabilities of smart devices to build security schemes operating without human
interaction, such as zero-interaction pairing (ZIP) and zero-interaction
authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and
reported promising results. However, those schemes were often evaluated under
conditions which do not reflect realistic IoT scenarios. In addition, drawing
any comparison among the existing schemes is impossible due to the lack of a
common public dataset and unavailability of scheme implementations.

The answer is that existing evaluations of ZIP and ZIA schemes are limited because they were often evaluated under conditions which do not reflect realistic IoT scenarios, and also due to the lack of a common public dataset and unavailability of scheme implementations."
What is the name of the specific computational pipeline used in this study that analyzes clustered microRNAs?,1fcbaa77,"[{'abstract': '  MicroRNAs play important roles in many biological processes. Their aberrant\nexpression can have oncogenic or tumor suppressor function directly\nparticipating to carcinogenesis, malignant transformation, invasiveness and\nmetastasis. Indeed, miRNA profiles can distinguish not only between normal and\ncancerous tissue but they can also successfully classify different subtypes of\na particular cancer. Here, we focus on a particular class of transcripts\nencoding polycistronic miRNA genes that yields multiple miRNA components. We\ndescribe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully\nredesigned release of the MMRA computational pipeline (MiRNA Master Regulator\nAnalysis), developed to search for clustered miRNAs potentially driving cancer\nmolecular subtyping. Genomically clustered miRNAs are frequently co-expressed\nto target different components of pro-tumorigenic signalling pathways. By\napplying ClustMMRA to breast cancer patient data, we identified key miRNA\nclusters driving the phenotype of different tumor subgroups. The pipeline was\napplied to two independent breast cancer datasets, providing statistically\nconcordant results between the two analysis. We validated in cell lines the\nmiR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative\nsubtype phenotype through its control of proliferation and EMT.\n', 'title': 'Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer'}]","Title: Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer

Abstract: MicroRNAs play important roles in many biological processes. Their aberrant expression can have oncogenic or tumor suppressor function directly participating to carcinogenesis, malignant transformation, invasiveness and metastasis. Indeed, miRNA profiles can distinguish not only between normal and cancerous tissue but they can also successfully classify different subtypes of a particular cancer. Here, we focus on a particular class of transcripts encoding polycistronic miRNA genes that yields multiple miRNA components. We describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully redesigned release of the MMRA computational pipeline (MiRNA Master Regulator Analysis), developed to search for clustered miRNAs potentially driving cancer molecular subtyping. Genomically clustered miRNAs are frequently co-expressed to target different components of pro-tumorigenic signalling pathways. By applying ClustMMRA to breast cancer patient data, we identified key miRNA clusters driving the phenotype of different tumor subgroups. The pipeline was applied to two independent breast cancer datasets, providing statistically concordant results between the two analysis. We validated in cell lines the miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative subtype phenotype through its control of proliferation and EMT."
How can ensemble data assimilation methods be improved to deal with situations like convection?,f1255665,"[{'abstract': '  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)\nare a key component of probabilistic weather forecasting. They represent the\nuncertainty in the initial conditions by an ensemble which incorporates\ninformation coming from the physical model with the latest observations.\nHigh-resolution numerical weather prediction models ran at operational centers\nare able to resolve non-linear and non-Gaussian physical phenomena such as\nconvection. There is therefore a growing need to develop ensemble assimilation\nalgorithms able to deal with non-Gaussianity while staying computationally\nfeasible. In the present paper we address some of these needs by proposing a\nnew hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully\nformulated in ensemble space and uses a deterministic scheme such that it has\nthe ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a\nlimiting case. A new criterion for choosing the proportion of particle filter\nand ETKF update is also proposed. The new algorithm is implemented in the COSMO\nframework and numerical experiments in a quasi-operational convective-scale\nsetup are conducted. The results show the feasibility of the new algorithm in\npractice and indicate a strong potential for such local hybrid methods, in\nparticular for forecasting non-Gaussian variables such as wind and hourly\nprecipitation.\n', 'title': 'A local ensemble transform Kalman particle filter for convective scale data assimilation'}]","Based on the context, I think I have found a relevant paper that addresses the question.

Title: A local ensemble transform Kalman particle filter for convective scale data assimilation

Abstract: Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF) are a key component of probabilistic weather forecasting. They represent the uncertainty in the initial conditions by an ensemble which incorporates information coming from the physical model with the latest observations. High-resolution numerical weather prediction models ran at operational centers are able to resolve non-linear and non-Gaussian physical phenomena such as convection. There is therefore a growing need to develop ensemble assimilation algorithms able to deal with non-Gaussianity while staying computationally feasible. In the present paper we address some of these needs by proposing a new hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully formulated in ensemble space and uses a deterministic scheme such that it has the ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a limiting case. A new criterion for choosing the proportion of particle filter and ETKF update is also proposed. The new algorithm is implemented in the COSMO framework and numerical experiments in a quasi-operational convective-scale setup are conducted. The results show the feasibility of the new algorithm in practice and indicate a strong potential for such local hybrid methods, in particular for forecasting non-Gaussian variables such as wind and hourly precipitation."
What are the main challenges in accurately matching natural language words and APIs?,6460d63f,"[{'abstract': '  Developers increasingly rely on text matching tools to analyze the relation\nbetween natural language words and APIs. However, semantic gaps, namely textual\nmismatches between words and APIs, negatively affect these tools. Previous\nstudies have transformed words or APIs into low-dimensional vectors for\nmatching; however, inaccurate results were obtained due to the failure of\nmodeling words and APIs simultaneously. To resolve this problem, two main\nchallenges are to be addressed: the acquisition of massive words and APIs for\nmining and the alignment of words and APIs for modeling. Therefore, this study\nproposes Word2API to effectively estimate relatedness of words and APIs.\nWord2API collects millions of commonly used words and APIs from code\nrepositories to address the acquisition challenge. Then, a shuffling strategy\nis used to transform related words and APIs into tuples to address the\nalignment challenge. Using these tuples, Word2API models words and APIs\nsimultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness\nestimation in terms of precision and NDCG. Word2API is also effective on\nsolving typical software tasks, e.g., query expansion and API documents\nlinking. A simple system with Word2API-expanded queries recommends up to 21.4%\nmore related APIs for developers. Meanwhile, Word2API improves comparison\nalgorithms by 7.9%-17.4% in linking questions in Question&Answer communities to\nAPI documents.\n', 'title': 'Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding'}]","Title: Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding

Abstract: Developers increasingly rely on text matching tools to analyze the relation between natural language words and APIs. However, semantic gaps, namely textual mismatches between words and APIs, negatively affect these tools. Previous studies have transformed words or APIs into low-dimensional vectors for matching; however, inaccurate results were obtained due to the failure of modeling words and APIs simultaneously. To resolve this problem, two main challenges are to be addressed: the acquisition of massive words and APIs for mining and the alignment of words and APIs for modeling."
How does the proposed adaptive model predictive control method compare to existing control techniques in handling disturbances?,ed551a94,"[{'abstract': '  Robots and automated systems are increasingly being introduced to unknown and\ndynamic environments where they are required to handle disturbances, unmodeled\ndynamics, and parametric uncertainties. Robust and adaptive control strategies\nare required to achieve high performance in these dynamic environments. In this\npaper, we propose a novel adaptive model predictive controller that combines\nmodel predictive control (MPC) with an underlying $\\mathcal{L}_1$ adaptive\ncontroller to improve trajectory tracking of a system subject to unknown and\nchanging disturbances. The $\\mathcal{L}_1$ adaptive controller forces the\nsystem to behave in a predefined way, as specified by a reference model. A\nhigher-level model predictive controller then uses this reference model to\ncalculate the optimal reference input based on a cost function, while taking\ninto account input and state constraints. We focus on the experimental\nvalidation of the proposed approach and demonstrate its effectiveness in\nexperiments on a quadrotor. We show that the proposed approach has a lower\ntrajectory tracking error compared to non-predictive, adaptive approaches and a\npredictive, non-adaptive approach, even when external wind disturbances are\napplied.\n', 'title': 'Adaptive Model Predictive Control for High-Accuracy Trajectory Tracking in Changing Conditions'}]","Title: Adaptive Model Predictive Control for High-Accuracy Trajectory Tracking in Changing Conditions

Abstract:   Robots and automated systems are increasingly being introduced to unknown and
dynamic environments where they are required to handle disturbances, unmodeled
dynamics, and parametric uncertainties. Robust and adaptive control strategies
are required to achieve high performance in these dynamic environments. In this
paper, we propose a novel adaptive model predictive controller that combines
model predictive control (MPC) with an underlying $\mathcal{L}_1$ adaptive
controller to improve trajectory tracking of a system subject to unknown and
changing disturbances. The $\mathcal{L}_1$ adaptive controller forces the
system to behave in a predefined way, as specified by a reference model. A
higher-level model predictive controller then uses this reference model to
calculate the optimal reference input based on a cost function, while taking
into account input and state constraints. We focus on the experimental
validation of the proposed approach and demonstrate its effectiveness in
experiments on a quadrotor. We show that the proposed approach has a lower
trajectory tracking error compared to non-predictive, adaptive approaches and a
predictive, non-adaptive approach, even when external wind disturbances are
applied."
What is the sample complexity of identity testing when the confidence level  is  arbitrarily small?,7fc98418,"[{'abstract': '  We study the problem of testing identity against a given distribution with a\nfocus on the high confidence regime. More precisely, given samples from an\nunknown distribution $p$ over $n$ elements, an explicitly given distribution\n$q$, and parameters $0< \\epsilon, \\delta < 1$, we wish to distinguish, {\\em\nwith probability at least $1-\\delta$}, whether the distributions are identical\nversus $\\varepsilon$-far in total variation distance. Most prior work focused\non the case that $\\delta = \\Omega(1)$, for which the sample complexity of\nidentity testing is known to be $\\Theta(\\sqrt{n}/\\epsilon^2)$. Given such an\nalgorithm, one can achieve arbitrarily small values of $\\delta$ via black-box\namplification, which multiplies the required number of samples by\n$\\Theta(\\log(1/\\delta))$.\nWe show that black-box amplification is suboptimal for any $\\delta = o(1)$,\nand give a new identity tester that achieves the optimal sample complexity. Our\nnew upper and lower bounds show that the optimal sample complexity of identity\ntesting is \\[\n\\Theta\\left( \\frac{1}{\\epsilon^2}\\left(\\sqrt{n \\log(1/\\delta)} +\n\\log(1/\\delta) \\right)\\right) \\] for any $n, \\varepsilon$, and $\\delta$. For\nthe special case of uniformity testing, where the given distribution is the\nuniform distribution $U_n$ over the domain, our new tester is surprisingly\nsimple: to test whether $p = U_n$ versus $d_{\\mathrm TV}(p, U_n) \\geq\n\\varepsilon$, we simply threshold $d_{\\mathrm TV}(\\widehat{p}, U_n)$, where\n$\\widehat{p}$ is the empirical probability distribution. The fact that this\nsimple ""plug-in"" estimator is sample-optimal is surprising, even in the\nconstant $\\delta$ case. Indeed, it was believed that such a tester would not\nattain sublinear sample complexity even for constant values of $\\varepsilon$\nand $\\delta$.\n', 'title': 'Optimal Identity Testing with High Probability'}]","Title: Optimal Identity Testing with High Probability

Abstract: We study the problem of testing identity against a given distribution with a focus on the high confidence regime. More precisely, given samples from an unknown distribution $p$ over $n$ elements, an explicitly given distribution $q$, and parameters $0< \epsilon, \delta < 1$, we wish to distinguish, {\em with probability at least $1-\delta$}, whether the distributions are identical versus $\varepsilon$-far in total variation distance. Most prior work focused on the case that $\delta = \Omega(1)$, for which the sample complexity of identity testing is known to be $\Theta(\sqrt{n}/\epsilon^2)$. Given such an algorithm, one can achieve arbitrarily small values of $\delta$ via black-box amplification, which multiplies the required number of samples by $\Theta(\log(1/\delta))$.
We show that black-box amplification is suboptimal for any $\delta = o(1)$, and give a new identity tester that achieves the optimal sample complexity. Our new upper and lower bounds show that the optimal sample complexity of identity testing is \[
\Theta\left( \frac{1}{\epsilon^2}\left(\sqrt{n \log(1/\delta)} +
\log(1/\delta) \right)\right) \] for any $n, \varepsilon$, and $\delta$. For the special case of uniformity testing, where the given distribution is the uniform distribution $U_n$ over the domain, our new tester is surprisingly simple: to test whether $p = U_n$ versus $d_{\mathrm TV}(p, U_n) \geq \varepsilon$, we simply threshold $d_{\mathrm TV}(\widehat{p}, U_n)$, where $\widehat{p}$ is the empirical probability distribution. The fact that this simple ""plug-in"" estimator is sample-optimal is surprising, even in the constant $\delta$ case. Indeed, it was believed that such a tester would not attain sublinear sample complexity even for constant values of $\varepsilon$ and $\delta$."
What decentralized optimization algorithm utilizes both gradient and proximal updates?,71d7fbec,"[{'abstract': '  This paper considers the problem of decentralized optimization with a\ncomposite objective containing smooth and non-smooth terms. To solve the\nproblem, a proximal-gradient scheme is studied. Specifically, the smooth and\nnonsmooth terms are dealt with by gradient update and proximal update,\nrespectively. The studied algorithm is closely related to a previous\ndecentralized optimization algorithm, PG-EXTRA [37], but has a few advantages.\nFirst of all, in our new scheme, agents use uncoordinated step-sizes and the\nstable upper bounds on step-sizes are independent from network topology. The\nstep-sizes depend on local objective functions, and they can be as large as\nthat of the gradient descent. Secondly, for the special case without non-smooth\nterms, linear convergence can be achieved under the strong convexity\nassumption. The dependence of the convergence rate on the objective functions\nand the network are separated, and the convergence rate of our new scheme is as\ngood as one of the two convergence rates that match the typical rates for the\ngeneral gradient descent and the consensus averaging. We also provide some\nnumerical experiments to demonstrate the efficacy of the introduced algorithms\nand validate our theoretical discoveries.\n', 'title': 'A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates'}]","I think I can help you with that!

Title: A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates

Abstract: This paper considers the problem of decentralized optimization with a composite objective containing smooth and non-smooth terms. To solve the problem, a proximal-gradient scheme is studied. Specifically, the smooth and nonsmooth terms are dealt with by gradient update and proximal update, respectively. The studied algorithm is closely related to a previous decentralized optimization algorithm, PG-EXTRA [37], but has a few advantages. First of all, in our new scheme, agents use uncoordinated step-sizes and the stable upper bounds on step-sizes are independent from network topology. The step-sizes depend on local objective functions, and they can be as large as that of the gradient descent. Secondly, for the special case without non-smooth terms, linear convergence can be achieved under the strong convexity assumption. The dependence of the convergence rate on the objective functions and the network are separated, and the convergence rate of our new scheme is as good as one of the two convergence rates that match the typical rates for the general gradient descent and the consensus averaging. We also provide some numerical experiments to demonstrate the efficacy of the introduced algorithms and validate our theoretical discoveries."
How does this paper improve upon previously known results regarding the scaling of Green's functions in random graph models?,ad0134d3,"[{'abstract': ""  This paper is the first chapter of three of the author's undergraduate\nthesis. We study the random matrix ensemble of covariance matrices arising from\nrandom $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and\n$N$ white vertices, for $d_b \\gg \\log^4 N$. We simultaneously prove that the\nGreen's functions of these covariance matrices and the adjacency matrices of\nthe underlying graphs agree with the corresponding limiting law (e.g.\nMarchenko-Pastur law for covariance matrices) down to the optimal scale. This\nis an improvement from the previously known mesoscopic results. We obtain\neigenvector delocalization for the covariance matrix ensemble as consequence,\nas well as a weak rigidity estimate.\n"", 'title': 'Local Marchenko-Pastur Law for Random Bipartite Graphs'}]","Title: Local Marchenko-Pastur Law for Random Bipartite Graphs

Abstract: This paper is the first chapter of three of the author's undergraduate thesis. We study the random matrix ensemble of covariance matrices arising from random $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and $N$ white vertices, for $d_b \gg \log^4 N$. We simultaneously prove that the Green's functions of these covariance matrices and the adjacency matrices of the underlying graphs agree with the corresponding limiting law (e.g. Marchenko-Pastur law for covariance matrices) down to the optimal scale. This is an improvement from the previously known mesoscopic results. We obtain eigenvector delocalization for the covariance matrix ensemble as consequence, as well as a weak rigidity estimate."
Which benchmark dataset was used to evaluate the performance of a novel object localization framework based on Bayesian joint topic modeling? ,2292955c,"[{'abstract': '  We address the problem of localisation of objects as bounding boxes in images\nwith weak labels. This weakly supervised object localisation problem has been\ntackled in the past using discriminative models where each object class is\nlocalised independently from other classes. We propose a novel framework based\non Bayesian joint topic modelling. Our framework has three distinctive\nadvantages over previous works: (1) All object classes and image backgrounds\nare modelled jointly together in a single generative model so that ""explaining\naway"" inference can resolve ambiguity and lead to better learning and\nlocalisation. (2) The Bayesian formulation of the model enables easy\nintegration of prior knowledge about object appearance to compensate for\nlimited supervision. (3) Our model can be learned with a mixture of weakly\nlabelled and unlabelled data, allowing the large volume of unlabelled images on\nthe Internet to be exploited for learning. Extensive experiments on the\nchallenging VOC dataset demonstrate that our approach outperforms the\nstate-of-the-art competitors.\n', 'title': 'Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation'}]","I think I found the answer!

Title: Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation

Abstract: We address the problem of localisation of objects as bounding boxes in images with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. We propose a novel framework based on Bayesian joint topic modelling. Our framework has three distinctive advantages over previous works: (1) All object classes and image backgrounds are modelled jointly together in a single generative model so that ""explaining away"" inference can resolve ambiguity and lead to better learning and localisation. (2) The Bayesian formulation of the model enables easy integration of prior knowledge about object appearance to compensate for limited supervision. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Extensive experiments on the challenging VOC dataset demonstrate that our approach outperforms the state-of-the-art competitors.

The benchmark dataset used to evaluate the performance of the novel object localization framework is the VOC dataset."
Do neural networks trained on images mimicking human gaze patterns perform better than those trained on full-resolution images?,cf412edf,"[{'abstract': '  Our eyes sample a disproportionately large amount of information at the\ncentre of gaze with increasingly sparse sampling into the periphery. This\nsampling scheme is widely believed to be a wiring constraint whereby high\nresolution at the centre is achieved by sacrificing spatial acuity in the\nperiphery. Here we propose that this sampling scheme may be optimal for object\nrecognition because the relevant spatial content is dense near an object and\nsparse in the surrounding vicinity. We tested this hypothesis by training deep\nconvolutional neural networks on full-resolution and foveated images. Our main\nfinding is that networks trained on images with foveated sampling show better\nobject classification compared to networks trained on full resolution images.\nImportantly, blurring images according to the human blur function yielded the\nbest performance compared to images with shallower or steeper blurring. Taken\ntogether our results suggest that, peripheral blurring in our eyes may have\nevolved for optimal object recognition, rather than merely to satisfy wiring\nconstraints.\n', 'title': 'Human peripheral blur is optimal for object recognition'}]","Title: Human peripheral blur is optimal for object recognition

Abstract: Our eyes sample a disproportionately large amount of information at the centre of gaze with increasingly sparse sampling into the periphery. This sampling scheme is widely believed to be a wiring constraint whereby high resolution at the centre is achieved by sacrificing spatial acuity in the periphery. Here we propose that this sampling scheme may be optimal for object recognition because the relevant spatial content is dense near an object and sparse in the surrounding vicinity. We tested this hypothesis by training deep convolutional neural networks on full-resolution and foveated images. Our main finding is that networks trained on images with foveated sampling show better object classification compared to networks trained on full resolution images. Importantly, blurring images according to the human blur function yielded the best performance compared to images with shallower or steeper blurring. Taken together our results suggest that, peripheral blurring in our eyes may have evolved for optimal object recognition, rather than merely to satisfy wiring constraints."
Can a simple adsorption model describe the relationship between surface tension and film properties in a study? ,9cc404f5,"[{'abstract': '  The surface tension of flowing soap films is measured with respect to the\nfilm thickness and the concentration of soap solution. We perform this\nmeasurement by measuring the curvature of the nylon wires that bound the soap\nfilm channel and use the measured curvature to parametrize the relation between\nthe surface tension and the tension of the wire. We find the surface tension of\nour soap films increases when the film is relatively thin or made of soap\nsolution of low concentration, otherwise it approaches an asymptotic value 30\nmN/m. A simple adsorption model with only two parameters describes our\nobservations reasonably well. With our measurements, we are also able to\nmeasure Gibbs elasticity for our soap film.\n', 'title': 'Surface tension of flowing soap films'}]","Title: Surface tension of flowing soap films

Abstract: The surface tension of flowing soap films is measured with respect to the film thickness and the concentration of soap solution. We perform this measurement by measuring the curvature of the nylon wires that bound the soap film channel and use the measured curvature to parametrize the relation between the surface tension and the tension of the wire. We find the surface tension of our soap films increases when the film is relatively thin or made of soap solution of low concentration, otherwise it approaches an asymptotic value 30 mN/m. A simple adsorption model with only two parameters describes our observations reasonably well. With our measurements, we are also able to measure Gibbs elasticity for our soap film."
What percentage of Web of Science scholars are estimated to have a Twitter account?,3e2ae4be,"[{'abstract': '  In this paper we present a novel methodology for identifying scholars with a\nTwitter account. By combining bibliometric data from Web of Science and Twitter\nusers identified by Altmetric.com we have obtained the largest set of\nindividual scholars matched with Twitter users made so far. Our methodology\nconsists of a combination of matching algorithms, considering different\nlinguistic elements of both author names and Twitter names; followed by a\nrule-based scoring system that weights the common occurrence of several\nelements related with the names, individual elements and activities of both\nTwitter users and scholars matched. Our results indicate that about 2% of the\noverall population of scholars in the Web of Science is active on Twitter. By\ndomain we find a strong presence of researchers from the Social Sciences and\nthe Humanities. Natural Sciences is the domain with the lowest level of\nscholars on Twitter. Researchers on Twitter also tend to be younger than those\nthat are not on Twitter. As this is a bibliometric-based approach, it is\nimportant to highlight the reliance of the method on the number of publications\nproduced and tweeted by the scholars, thus the share of scholars on Twitter\nranges between 1% and 5% depending on their level of productivity. Further\nresearch is suggested in order to improve and expand the methodology.\n', 'title': 'Scholars on Twitter: who and how many are they?'}]","Title: Scholars on Twitter: who and how many are they?

Abstract:   In this paper we present a novel methodology for identifying scholars with a Twitter account. By combining bibliometric data from Web of Science and Twitter users identified by Altmetric.com we have obtained the largest set of individual scholars matched with Twitter users made so far. Our methodology consists of a combination of matching algorithms, considering different linguistic elements of both author names and Twitter names; followed by a rule-based scoring system that weights the common occurrence of several elements related with the names, individual elements and activities of both Twitter users and scholars matched. Our results indicate that about 2% of the overall population of scholars in the Web of Science is active on Twitter. By domain we find a strong presence of researchers from the Social Sciences and the Humanities. Natural Sciences is the domain with the lowest level of scholars on Twitter. Researchers on Twitter also tend to be younger than those that are not on Twitter. As this is a bibliometric-based approach, it is important to highlight the reliance of the method on the number of publications produced and tweeted by the scholars, thus the share of scholars on Twitter ranges between 1% and 5% depending on their level of productivity. Further research is suggested in order to improve and expand the methodology.

Answer: About 2% of the overall population of scholars in the Web of Science are estimated to have a Twitter account."
Which freely available R-package offers a graphical user interface for analyzing data in academia and educational settings?,7ee7399c,"[{'abstract': '  The numerical availability of statistical inference methods for a modern and\nrobust analysis of longitudinal- and multivariate data in factorial experiments\nis an essential element in research and education. While existing approaches\nthat rely on specific distributional assumptions of the data (multivariate\nnormality and/or characteristic covariance matrices) are implemented in\nstatistical software packages, there is a need for user-friendly software that\ncan be used for the analysis of data that do not fulfill the aforementioned\nassumptions and provide accurate p-value and confidence interval estimates.\nTherefore, newly developed statistical methods for the analysis of repeated\nmeasures designs and multivariate data that neither assume multivariate\nnormality nor specific covariance matrices have been implemented in the freely\navailable R-package MANOVA.RM. The package is equipped with a graphical user\ninterface for plausible applications in academia and other educational purpose.\nSeveral motivating examples illustrate the application of the methods.\n', 'title': 'Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM'}]","Title: Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM

Abstract: The numerical availability of statistical inference methods for a modern and robust analysis of longitudinal- and multivariate data in factorial experiments is an essential element in research and education. While existing approaches that rely on specific distributional assumptions of the data (multivariate normality and/or characteristic covariance matrices) are implemented in statistical software packages, there is a need for user-friendly software that can be used for the analysis of data that do not fulfill the aforementioned assumptions and provide accurate p-value and confidence interval estimates. Therefore, newly developed statistical methods for the analysis of repeated measures designs and multivariate data that neither assume multivariate normality nor specific covariance matrices have been implemented in the freely available R-package MANOVA.RM. The package is equipped with a graphical user interface for plausible applications in academia and other educational purposes. Several motivating examples illustrate the application of the methods."
What are the geographical patterns of both pyriproxyfen use and microcephaly cases in Brazil? ,dbbd26d8,"[{'abstract': ""  The Zika virus has been found in individual cases but has not been confirmed\nas the cause of in the large number of cases of microcephaly in Brazil in\n2015-6. Indeed, disparities between the incidence of Zika and microcephaly\nacross geographic locations has led to questions about the virus's role. Here\nwe consider whether the insecticide pyriproxyfen used in Brazilian drinking\nwater might be the primary cause or a cofactor. Pyriproxifen is a juvenile\nhormone analog which has been shown to correspond in mammals to a number of fat\nsoluble regulatory molecules including retinoic acid, a metabolite of vitamin\nA, with which it has cross-reactivity and whose application during development\nhas been shown to cause microcephaly. Methoprene, another juvenile hormone\nanalog approved as an insecticide in the 1970s has been shown to cause\ndevelopmental disorders in mammals. Isotretinoin is another retinoid causing\nmicrocephaly via activation of the retinoid X receptor in developing fetuses.\nWe review tests of pyriproxyfen by the manufacturer Sumitomo, which actually\nfound some evidence for this effect, including low brain mass and\narhinencephaly in exposed rat pups. Pyriproxyfen use in Brazil is\nunprecedented, never having been applied to a water supply on a large scale.\nClaims that its geographical pattern of use rule it out as a cause have not\nbeen documented or confirmed. On the other hand, the very few microcephaly\ncases reported in Colombia and the wide discrepancies of incidence in different\nstates across Brazil despite large numbers of Zika cases undermine the claim\nthat Zika is the cause. Given this combination of potential molecular\nmechanism, toxicological and epidemiological evidence we strongly recommend\nthat the use of pyriproxyfen in Brazil be suspended until the potential causal\nlink to microcephaly is investigated further.\n"", 'title': 'The Case for Pyriproxyfen as a Potential Cause for Microcephaly; From Biology to Epidemiology'}]","Title: The Case for Pyriproxyfen as a Potential Cause for Microcephaly; From Biology to Epidemiology

Abstract:   The Zika virus has been found in individual cases but has not been confirmed
as the cause of in the large number of cases of microcephaly in Brazil in
2015-6. Indeed, disparities between the incidence of Zika and microcephaly
across geographic locations has led to questions about the virus's role. Here
we consider whether the insecticide pyriproxyfen used in Brazilian drinking
water might be the primary cause or a cofactor. Pyriproxifen is a juvenile
hormone analog which has been shown to correspond in mammals to a number of fat
soluble regulatory molecules including retinoic acid, a metabolite of vitamin
A, with which it has cross-reactivity and whose application during development
has been shown to cause microcephaly. Methoprene, another juvenile hormone
analog approved as an insecticide in the 1970s has been shown to cause
developmental disorders in mammals. Isotretinoin is another retinoid causing
microcephaly via activation of the retinoid X receptor in developing fetuses.
We review tests of pyriproxyfen by the manufacturer Sumitomo, which actually
found some evidence for this effect, including low brain mass and
arhinencephaly in exposed rat pups. Pyriproxyfen use in Brazil is
unprecedented, never having been applied to a water supply on a large scale.
Claims that its geographical pattern of use rule it out as a cause have not
been documented or confirmed. On the other hand, the very few microcephaly
cases reported in Colombia and the wide discrepancies of incidence in different
states across Brazil despite large numbers of Zika cases undermine the claim
that Zika is the cause. Given this combination of potential molecular
mechanism, toxicological and epidemiological evidence we strongly recommend
that the use of pyriproxyfen in Brazil be suspended until the potential causal
link to microcephaly is investigated further."
What are the convergence properties of linear stochastic approximation algorithms with a constant step-size and Polyak-Ruppert averaging?,1d79ba08,"[{'abstract': ""  We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)\nwith a constant step-size and the so called Polyak-Ruppert (PR) averaging of\niterates. LSAs are widely applied in machine learning and reinforcement\nlearning (RL), where the aim is to compute an appropriate $\\theta_{*} \\in\n\\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$\nupdates per iteration. In this paper, we are motivated by the problem (in RL)\nof policy evaluation from experience replay using the \\emph{temporal\ndifference} (TD) class of learning algorithms that are also LSAs. For LSAs with\na constant step-size, and PR averaging, we provide bounds for the mean squared\nerror (MSE) after $t$ iterations. We assume that data is \\iid with finite\nvariance (underlying distribution being $P$) and that the expected dynamics is\nHurwitz. For a given LSA with PR averaging, and data distribution $P$\nsatisfying the said assumptions, we show that there exists a range of constant\nstep-sizes such that its MSE decays as $O(\\frac{1}{t})$.\nWe examine the conditions under which a constant step-size can be chosen\nuniformly for a class of data distributions $\\mathcal{P}$, and show that not\nall data distributions `admit' such a uniform constant step-size. We also\nsuggest a heuristic step-size tuning algorithm to choose a constant step-size\nof a given LSA for a given data distribution $P$. We compare our results with\nrelated work and also discuss the implication of our results in the context of\nTD algorithms that are LSAs.\n"", 'title': 'Linear Stochastic Approximation: Constant Step-Size and Iterate Averaging'}]","Title: Linear Stochastic Approximation: Constant Step-Size and Iterate Averaging

Abstract:   We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)
with a constant step-size and the so called Polyak-Ruppert (PR) averaging of
iterates. LSAs are widely applied in machine learning and reinforcement
learning (RL), where the aim is to compute an appropriate $\theta_{*} \in
\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$
updates per iteration. In this paper, we are motivated by the problem (in RL)
of policy evaluation from experience replay using the \emph{temporal
difference} (TD) class of learning algorithms that are also LSAs. For LSAs with
a constant step-size, and PR averaging, we provide bounds for the mean squared
error (MSE) after $t$ iterations. We assume that data is \iid with finite
variance (underlying distribution being $P$) and that the expected dynamics is
Hurwitz. For a given LSA with PR averaging, and data distribution $P$
satisfying the said assumptions, we show that there exists a range of constant
step-sizes such that its MSE decays as $O(\frac{1}{t})$."
In which tasks did the authors demonstrate the superiority of their proposed training method over traditional cross entropy training methods?,66ccf294,"[{'abstract': '  Beam search is a desirable choice of test-time decoding algorithm for neural\nsequence models because it potentially avoids search errors made by simpler\ngreedy methods. However, typical cross entropy training procedures for these\nmodels do not directly consider the behaviour of the final decoding method. As\na result, for cross-entropy trained models, beam decoding can sometimes yield\nreduced test performance when compared with greedy decoding. In order to train\nmodels that can more effectively make use of beam search, we propose a new\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\nevaluated on the output of beam search. While well-defined, this ""direct loss""\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\napproach, we form a sub-differentiable surrogate objective by introducing a\nnovel continuous approximation of the beam search decoding procedure. In\nexperiments, we show that optimizing this new training objective yields\nsubstantially better results on two sequence tasks (Named Entity Recognition\nand CCG Supertagging) when compared with both cross entropy trained greedy\ndecoding and cross entropy trained beam decoding baselines.\n', 'title': 'A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models'}]","Title: A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models

Abstract: Beam search is a desirable choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when compared with greedy decoding. In order to train models that can more effectively make use of beam search, we propose a new training procedure that focuses on the final loss metric (e.g. Hamming loss) evaluated on the output of beam search. While well-defined, this ""direct loss"" objective is itself discontinuous and thus difficult to optimize. Hence, in our approach, we form a sub-differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure. In experiments, we show that optimizing this new training objective yields substantially better results on two sequence tasks (Named Entity Recognition and CCG Supertagging) when compared with both cross entropy trained greedy decoding and cross entropy trained beam decoding baselines.

The authors demonstrated the superiority of their proposed training method over traditional cross entropy training methods in Named Entity Recognition and CCG Supertagging tasks."
What is the effect of pp-wave modes on electromagnetic waves in a dynamic aether?,7e5f29d0,"[{'abstract': '  In the framework of the Einstein-Maxwell-aether theory we study the\nbirefringence effect, which can occur in the pp-wave symmetric dynamic aether.\nThe dynamic aether is considered to be latently birefringent quasi-medium,\nwhich displays this hidden property if and only if the aether motion is\nnon-uniform, i.e., when the aether flow is characterized by the non-vanishing\nexpansion, shear, vorticity or acceleration. In accordance with the\ndynamo-optical scheme of description of the interaction between electromagnetic\nwaves and the dynamic aether, we shall model the susceptibility tensors by the\nterms linear in the covariant derivative of the aether velocity four-vector.\nWhen the pp-wave modes appear in the dynamic aether, we deal with a\ngravitationally induced degeneracy removal with respect to hidden\nsusceptibility parameters. As a consequence, the phase velocities of\nelectromagnetic waves possessing orthogonal polarizations do not coincide, thus\ndisplaying the birefringence effect. Two electromagnetic field configurations\nare studied in detail: longitudinal and transversal with respect to the aether\npp-wave front. For both cases the solutions are found, which reveal anomalies\nin the electromagnetic response on the action of the pp-wave aether mode.\n', 'title': 'Birefringence induced by pp-wave modes in an electromagnetically active dynamic aether'}]","Title: Birefringence induced by pp-wave modes in an electromagnetically active dynamic aether

Abstract: In the framework of the Einstein-Maxwell-aether theory we study the birefringence effect, which can occur in the pp-wave symmetric dynamic aether. The dynamic aether is considered to be latently birefringent quasi-medium, which displays this hidden property if and only if the aether motion is non-uniform, i.e., when the aether flow is characterized by the non-vanishing expansion, shear, vorticity or acceleration. In accordance with the dynamo-optical scheme of description of the interaction between electromagnetic waves and the dynamic aether, we shall model the susceptibility tensors by the terms linear in the covariant derivative of the aether velocity four-vector. When the pp-wave modes appear in the dynamic aether, we deal with a gravitationally induced degeneracy removal with respect to hidden susceptibility parameters. As a consequence, the phase velocities of electromagnetic waves possessing orthogonal polarizations do not coincide, thus displaying the birefringence effect. Two electromagnetic field configurations are studied in detail: longitudinal and transversal with respect to the aether pp-wave front. For both cases the solutions are found, which reveal anomalies in the electromagnetic response on the action of the pp-wave aether mode."
What is a novel approach to topology optimization using image segmentation?,9c5ddd71,"[{'abstract': '  In this research, we propose a deep learning based approach for speeding up\nthe topology optimization methods. The problem we seek to solve is the layout\nproblem. The main novelty of this work is to state the problem as an image\nsegmentation task. We leverage the power of deep learning methods as the\nefficient pixel-wise image labeling technique to perform the topology\noptimization. We introduce convolutional encoder-decoder architecture and the\noverall approach of solving the above-described problem with high performance.\nThe conducted experiments demonstrate the significant acceleration of the\noptimization process. The proposed approach has excellent generalization\nproperties. We demonstrate the ability of the application of the proposed model\nto other problems. The successful results, as well as the drawbacks of the\ncurrent method, are discussed.\n', 'title': 'Neural networks for topology optimization'}]","Title: Neural networks for topology optimization

Abstract: In this research, we propose a deep learning based approach for speeding up the topology optimization methods. The problem we seek to solve is the layout problem. The main novelty of this work is to state the problem as an image segmentation task. We leverage the power of deep learning methods as the efficient pixel-wise image labeling technique to perform the topology optimization. We introduce convolutional encoder-decoder architecture and the overall approach of solving the above-described problem with high performance. The conducted experiments demonstrate the significant acceleration of the optimization process. The proposed approach has excellent generalization properties. We demonstrate the ability of the application of the proposed model to other problems. The successful results, as well as the drawbacks of the current method, are discussed."
What website hosts the datasets used to create the OpenML benchmark suites?,623f37bf,"[{'abstract': '  We advocate the use of curated, comprehensive benchmark suites of machine\nlearning datasets, backed by standardized OpenML-based interfaces and\ncomplementary software toolkits written in Python, Java and R. Major\ndistinguishing features of OpenML benchmark suites are (a) ease of use through\nstandardized data formats, APIs, and existing client libraries; (b)\nmachine-readable meta-information regarding the contents of the suite; and (c)\nonline sharing of results, enabling large scale comparisons. As a first such\nsuite, we propose the OpenML100, a machine learning benchmark suite of\n100~classification datasets carefully curated from the thousands of datasets\navailable on OpenML.org.\n', 'title': 'OpenML Benchmarking Suites and the OpenML100'}]","The answer can be found in the first abstract:

Title: OpenML Benchmarking Suites and the OpenML100

Abstract: We advocate the use of curated, comprehensive benchmark suites of machine learning datasets, backed by standardized OpenML-based interfaces and complementary software toolkits written in Python, Java and R. Major distinguishing features of OpenML benchmark suites are (a) ease of use through standardized data formats, APIs, and existing client libraries; (b) machine-readable meta-information regarding the contents of the suite; and (c) online sharing of results, enabling large scale comparisons. As a first such suite, we propose the OpenML100, a machine learning benchmark suite of 100~classification datasets carefully curated from the thousands of datasets available on OpenML.org.

The website that hosts the datasets used to create the OpenML benchmark suites is OpenML.org."
What are the potential benefits of finding a superconductor with non-s-wave pairing?,6412fb8a,"[{'abstract': '  The search for a superconductor with non-s-wave pairing is important not only\nfor understanding unconventional mechanisms of superconductivity but also for\nfinding new types of quasiparticles such as Majorana bound states. Materials\nwith both topological band structure and superconductivity are promising\ncandidates as $p+ip$ superconducting states can be generated through pairing\nthe spin-polarized topological surface states. In this work, the electronic and\nphonon properties of the superconductor molybdenum carbide (MoC) are studied\nwith first-principles methods. Our calculations show that nontrivial band\ntopology and superconductivity coexist in both structural phases of MoC,\nnamely, the cubic $\\alpha$ and hexagonal $\\gamma$ phases. The $\\alpha$ phase is\na strong topological insulator and the $\\gamma$ phase is a topological nodal\nline semimetal with drumhead surface states. In addition, hole doping can\nstabilize the crystal structure of the $\\alpha$ phase and elevate the\ntransition temperature in the $\\gamma$ phase. Therefore, MoC in different\nstructural forms can be a practical material platform for studying topological\nsuperconductivity and elusive Majorana fermions.\n', 'title': 'Multiple Topological Electronic Phases in Superconductor MoC'}]","Title: Multiple Topological Electronic Phases in Superconductor MoC

Abstract:   The search for a superconductor with non-s-wave pairing is important not only
for understanding unconventional mechanisms of superconductivity but also for
finding new types of quasiparticles such as Majorana bound states."
How does the combination of motion classification and adaptive zero-velocity detection affect position accuracy during mixed walking and running?,ed0b31a2,"[{'abstract': '  We present a method to improve the accuracy of a foot-mounted,\nzero-velocity-aided inertial navigation system (INS) by varying estimator\nparameters based on a real-time classification of motion type. We train a\nsupport vector machine (SVM) classifier using inertial data recorded by a\nsingle foot-mounted sensor to differentiate between six motion types (walking,\njogging, running, sprinting, crouch-walking, and ladder-climbing) and report\nmean test classification accuracy of over 90% on a dataset with five different\nsubjects. From these motion types, we select two of the most common (walking\nand running), and describe a method to compute optimal zero-velocity detection\nparameters tailored to both a specific user and motion type by maximizing the\ndetector F-score. By combining the motion classifier with a set of optimal\ndetection parameters, we show how we can reduce INS position error during mixed\nwalking and running motion. We evaluate our adaptive system on a total of 5.9\nkm of indoor pedestrian navigation performed by five different subjects moving\nalong a 130 m path with surveyed ground truth markers.\n', 'title': 'Improving Foot-Mounted Inertial Navigation Through Real-Time Motion Classification'}]","Title: Improving Foot-Mounted Inertial Navigation Through Real-Time Motion Classification

Abstract: We present a method to improve the accuracy of a foot-mounted, zero-velocity-aided inertial navigation system (INS) by varying estimator parameters based on a real-time classification of motion type. We train a support vector machine (SVM) classifier using inertial data recorded by a single foot-mounted sensor to differentiate between six motion types (walking, jogging, running, sprinting, crouch-walking, and ladder-climbing) and report mean test classification accuracy of over 90% on a dataset with five different subjects. From these motion types, we select two of the most common (walking and running), and describe a method to compute optimal zero-velocity detection parameters tailored to both a specific user and motion type by maximizing the detector F-score. By combining the motion classifier with a set of optimal detection parameters, we show how we can reduce INS position error during mixed walking and running motion. We evaluate our adaptive system on a total of 5.9 km of indoor pedestrian navigation performed by five different subjects moving along a 130 m path with surveyed ground truth markers."
"According to the paper, what are the two potential outcomes for agents exposed to objective information when considering different biases?",2e41fa92,"[{'abstract': '  We present an introduction to a novel model of an individual and group\nopinion dynamics, taking into account different ways in which different sources\nof information are filtered due to cognitive biases. The agent based model,\nusing Bayesian updating of the individual belief distribution, is based on the\nrecent psychology work by Dan Kahan. Open nature of the model allows to study\nthe effects of both static and time-dependent biases and information processing\nfilters. In particular, the paper compares the effects of two important\npsychological mechanisms: the confirmation bias and the politically motivated\nreasoning. Depending on the effectiveness of the information filtering (agent\nbias), the agents confronted with an objective information source may either\nreach a consensus based on the truth, or remain divided despite the evidence.\nIn general, the model might provide an understanding into the increasingly\npolarized modern societies, especially as it allows mixing of different types\nof filters: psychological, social, and algorithmic.\n', 'title': 'Opinion dynamics model based on cognitive biases'}]","Title: Opinion dynamics model based on cognitive biases

Abstract: We present an introduction to a novel model of an individual and group opinion dynamics, taking into account different ways in which different sources of information are filtered due to cognitive biases. The agent based model, using Bayesian updating of the individual belief distribution, is based on the recent psychology work by Dan Kahan. Open nature of the model allows to study the effects of both static and time-dependent biases and information processing filters. In particular, the paper compares the effects of two important psychological mechanisms: the confirmation bias and the politically motivated reasoning. Depending on the effectiveness of the information filtering (agent bias), the agents confronted with an objective information source may either reach a consensus based on the truth, or remain divided despite the evidence. In general, the model might provide an understanding into the increasingly polarized modern societies, especially as it allows mixing of different types of filters: psychological, social, and algorithmic.

According to the paper, the two potential outcomes for agents exposed to objective information when considering different biases are:

1. Reaching a consensus based on the truth
2. Remaining divided despite the evidence"
What are the functional dependences observed in the best fits to numerical dynamo data? ,412f5254,"[{'abstract': '  Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both\nwith and without a significant mean field are obtained. Exact bounds are\nderived for the total energy which conform with the Kolmogorov phenomenology of\nturbulence. Best fits to numerical data show the same functional dependences as\nthe inequalities obtained from optimum theory.\n', 'title': 'Scaling laws and bounds for the turbulent G.O. Roberts dynamo'}]","Title: Scaling laws and bounds for the turbulent G.O. Roberts dynamo

Abstract: Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both with and without a significant mean field are obtained. Exact bounds are derived for the total energy which conform with the Kolmogorov phenomenology of turbulence. Best fits to numerical data show the same functional dependences as the inequalities obtained from optimum theory."
What are the three main enablers for developing a photodiode-based VLC system for large-scale localization?,1d1a2392,"[{'abstract': '  Indoor localization based on Visible Light Communication (VLC) has been in\nfavor with both the academia and industry for years. In this paper, we present\na prototyping photodiode-based VLC system towards large-scale localization.\nSpecially, we give in-depth analysis of the design constraints and\nconsiderations for large-scale indoor localization research. After that we\nidentify the key enablers for such systems: 1) distributed architecture, 2)\none-way communication, and 3) random multiple access. Accordingly, we propose\nPlugo -- a photodiode-based VLC system conforming to the aforementioned\ncriteria. We present a compact design of the VLC-compatible LED bulbs featuring\nplug-and-go use-cases. The basic framed slotted Additive Links On-line Hawaii\nArea (ALOHA) is exploited to achieve random multiple access over the shared\noptical medium. We show its effectiveness in beacon broadcasting by\nexperiments, and further demonstrate its scalability to large-scale scenarios\nthrough simulations. Finally, preliminary localization experiments are\nconducted using fingerprinting-based methods in a customized testbed, achieving\nan average accuracy of 0.14 m along with a 90-percentile accuracy of 0.33 m.\n', 'title': 'Plugo: a VLC Systematic Perspective of Large-scale Indoor Localization'}]","Title: Plugo: a VLC Systematic Perspective of Large-scale Indoor Localization

Abstract: Indoor localization based on Visible Light Communication (VLC) has been in favor with both the academia and industry for years. In this paper, we present a prototyping photodiode-based VLC system towards large-scale localization. Specially, we give in-depth analysis of the design constraints and considerations for large-scale indoor localization research. After that we identify the key enablers for such systems: 1) distributed architecture, 2) one-way communication, and 3) random multiple access. 

The three main enablers are:
1. Distributed architecture
2. One-way communication
3. Random multiple access"
What year did Brakke introduce the mean curvature flow within the context of geometric measure theory?,99c7c1e7,"[{'abstract': ""  In 1978 Brakke introduced the mean curvature flow in the setting of geometric\nmeasure theory. There exist multiple variants of the original definition. Here\nwe prove that most of them are indeed equal. One central point is to correct\nthe proof of Brakke's §3.5, where he develops an estimate for the evolution\nof the measure of time-dependent test functions.\n"", 'title': 'Equality of the usual definitions of Brakke flow'}]","Title: Equality of the usual definitions of Brakke flow

Abstract: In 1978 Brakke introduced the mean curvature flow in the setting of geometric measure theory. There exist multiple variants of the original definition. Here we prove that most of them are indeed equal. One central point is to correct the proof of Brakke's §3.5, where he develops an estimate for the evolution of the measure of time-dependent test functions.

Answer: Brakke introduced the mean curvature flow in 1978."
Which layer in a deep neural network is most efficient for reliably generating training data patterns and what relationship exists between resolution and relevance in that layer?,c2deb041,"[{'abstract': ""  Deep learning has been successfully applied to various tasks, but its\nunderlying mechanism remains unclear. Neural networks associate similar inputs\nin the visible layer to the same state of hidden variables in deep layers. The\nfraction of inputs that are associated to the same state is a natural measure\nof similarity and is simply related to the cost in bits required to represent\nthese inputs. The degeneracy of states with the same information cost provides\ninstead a natural measure of noise and is simply related the entropy of the\nfrequency of states, that we call relevance. Representations with minimal\nnoise, at a given level of similarity (resolution), are those that maximise the\nrelevance. A signature of such efficient representations is that frequency\ndistributions follow power laws. We show, in extensive numerical experiments,\nthat deep neural networks extract a hierarchy of efficient representations from\ndata, because they i) achieve low levels of noise (i.e. high relevance) and ii)\nexhibit power law distributions. We also find that the layer that is most\nefficient to reliably generate patterns of training data is the one for which\nrelevance and resolution are traded at the same price, which implies that\nfrequency distribution follows Zipf's law.\n"", 'title': 'Resolution and Relevance Trade-offs in Deep Learning'}]","Title: Resolution and Relevance Trade-offs in Deep Learning

Abstract: Deep learning has been successfully applied to various tasks, but its underlying mechanism remains unclear. Neural networks associate similar inputs in the visible layer to the same state of hidden variables in deep layers. The fraction of inputs that are associated to the same state is a natural measure of similarity and is simply related to the cost in bits required to represent these inputs. The degeneracy of states with the same information cost provides instead a natural measure of noise and is simply related the entropy of the frequency of states, that we call relevance. Representations with minimal noise, at a given level of similarity (resolution), are those that maximise the relevance. A signature of such efficient representations is that frequency distributions follow power laws. We show, in extensive numerical experiments, that deep neural networks extract a hierarchy of efficient representations from data, because they i) achieve low levels of noise (i.e. high relevance) and ii) exhibit power law distributions. We also find that the layer that is most efficient to reliably generate patterns of training data is the one for which relevance and resolution are traded at the same price, which implies that frequency distribution follows Zipf's law."
What is stigmergy and how can it be applied to identifying high density areas in a city?,30533fe4,"[{'abstract': '  Positioning data offer a remarkable source of information to analyze crowds\nurban dynamics. However, discovering urban activity patterns from the emergent\nbehavior of crowds involves complex system modeling. An alternative approach is\nto adopt computational techniques belonging to the emergent paradigm, which\nenables self-organization of data and allows adaptive analysis. Specifically,\nour approach is based on stigmergy. By using stigmergy each sample position is\nassociated with a digital pheromone deposit, which progressively evaporates and\naggregates with other deposits according to their spatiotemporal proximity.\nBased on this principle, we exploit positioning data to identify high density\nareas (hotspots) and characterize their activity over time. This\ncharacterization allows the comparison of dynamics occurring in different days,\nproviding a similarity measure exploitable by clustering techniques. Thus, we\ncluster days according to their activity behavior, discovering unexpected urban\nactivity patterns. As a case study, we analyze taxi traces in New York City\nduring 2015.\n', 'title': 'Stigmergy-based modeling to discover urban activity patterns from positioning data'}]","Based on the context, I think I can help you with your question.

Title: Stigmergy-based modeling to discover urban activity patterns from positioning data

Abstract: Positioning data offer a remarkable source of information to analyze crowds
urban dynamics. However, discovering urban activity patterns from the emergent
behavior of crowds involves complex system modeling. An alternative approach is
to adopt computational techniques belonging to the emergent paradigm, which
enables self-organization of data and allows adaptive analysis. Specifically,
our approach is based on stigmergy. By using stigmergy each sample position is
associated with a digital pheromone deposit, which progressively evaporates and
aggregates with other deposits according to their spatiotemporal proximity.
Based on this principle, we exploit positioning data to identify high density
areas (hotspots) and characterize their activity over time. This
characterization allows the comparison of dynamics occurring in different days,
providing a similarity measure exploitable by clustering techniques. Thus, we
cluster days according to their activity behavior, discovering unexpected urban
activity patterns. As a case study, we analyze taxi traces in New York City
during 2015.

In this paper, stigmergy is a computational technique that allows for the self-organization of data and adaptive analysis. It involves associating each sample position with a digital pheromone deposit that progressively evaporates and aggregates with other deposits according to their spatiotemporal proximity. This principle is used to identify high density areas (hotspots) in a city by exploiting positioning data and characterizing their activity over time."
Which book provides the computational background for the functions in FDITOOLS?,c1dadf82,"[{'abstract': '  The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB\nfunctions for the analysis and solution of fault detection and model detection\nproblems. The implemented functions are based on the computational procedures\ndescribed in the Chapters 5, 6 and 7 of the book: ""A. Varga, Solving Fault\nDiagnosis Problems - Linear Synthesis Techniques, Springer, 2017"". This\ndocument is the User\'s Guide for the version V1.0 of FDITOOLS. First, we\npresent the mathematical background for solving several basic exact and\napproximate synthesis problems of fault detection filters and model detection\nfilters. Then, we give in-depth information on the command syntax of the main\nanalysis and synthesis functions. Several examples illustrate the use of the\nmain functions of FDITOOLS.\n', 'title': ""Fault Detection and Isolation Tools (FDITOOLS) User's Guide""}]","Title: A. Varga, Solving Fault Diagnosis Problems - Linear Synthesis Techniques

Abstract: (Not provided in the context, but mentioned as the basis for the computational procedures in FDITOOLS)"
Can a simple empirical estimator be used to achieve optimal sample complexity for identity testing in certain cases? ,7fc98418,"[{'abstract': '  We study the problem of testing identity against a given distribution with a\nfocus on the high confidence regime. More precisely, given samples from an\nunknown distribution $p$ over $n$ elements, an explicitly given distribution\n$q$, and parameters $0< \\epsilon, \\delta < 1$, we wish to distinguish, {\\em\nwith probability at least $1-\\delta$}, whether the distributions are identical\nversus $\\varepsilon$-far in total variation distance. Most prior work focused\non the case that $\\delta = \\Omega(1)$, for which the sample complexity of\nidentity testing is known to be $\\Theta(\\sqrt{n}/\\epsilon^2)$. Given such an\nalgorithm, one can achieve arbitrarily small values of $\\delta$ via black-box\namplification, which multiplies the required number of samples by\n$\\Theta(\\log(1/\\delta))$.\nWe show that black-box amplification is suboptimal for any $\\delta = o(1)$,\nand give a new identity tester that achieves the optimal sample complexity. Our\nnew upper and lower bounds show that the optimal sample complexity of identity\ntesting is \\[\n\\Theta\\left( \\frac{1}{\\epsilon^2}\\left(\\sqrt{n \\log(1/\\delta)} +\n\\log(1/\\delta) \\right)\\right) \\] for any $n, \\varepsilon$, and $\\delta$. For\nthe special case of uniformity testing, where the given distribution is the\nuniform distribution $U_n$ over the domain, our new tester is surprisingly\nsimple: to test whether $p = U_n$ versus $d_{\\mathrm TV}(p, U_n) \\geq\n\\varepsilon$, we simply threshold $d_{\\mathrm TV}(\\widehat{p}, U_n)$, where\n$\\widehat{p}$ is the empirical probability distribution. The fact that this\nsimple ""plug-in"" estimator is sample-optimal is surprising, even in the\nconstant $\\delta$ case. Indeed, it was believed that such a tester would not\nattain sublinear sample complexity even for constant values of $\\varepsilon$\nand $\\delta$.\n', 'title': 'Optimal Identity Testing with High Probability'}]","Title: Optimal Identity Testing with High Probability

Abstract:   We study the problem of testing identity against a given distribution with a
focus on the high confidence regime. More precisely, given samples from an
unknown distribution $p$ over $n$ elements, an explicitly given distribution
$q$, and parameters $0< \epsilon, \delta < 1$, we wish to distinguish, {\em
with probability at least $1-\delta$}, whether the distributions are identical
versus $\varepsilon$-far in total variation distance. Most prior work focused
on the case that $\delta = \Omega(1)$, for which the sample complexity of
identity testing is known to be $\Theta(\sqrt{n}/\epsilon^2)$. Given such an
algorithm, one can achieve arbitrarily small values of $\delta$ via black-box
amplification, which multiplies the required number of samples by
$\Theta(\log(1/\delta))$.
We show that black-box amplification is suboptimal for any $\delta = o(1)$,
and give a new identity tester that achieves the optimal sample complexity. Our
new upper and lower bounds show that the optimal sample complexity of identity
testing is \[
\Theta\left( \frac{1}{\epsilon^2}\left(\sqrt{n \log(1/\delta)} +
\log(1/\delta) \right)\right) \] for any $n, \varepsilon$, and $\delta$. For
the special case of uniformity testing, where the given distribution is the
uniform distribution $U_n$ over the domain, our new tester is surprisingly
simple: to test whether $p = U_n$ versus $d_{\mathrm TV}(p, U_n) \geq
\varepsilon$, we simply threshold $d_{\mathrm TV}(\widehat{p}, U_n)$, where
$\widehat{p}$ is the empirical probability distribution. The fact that this
simple ""plug-in"" estimator is sample-optimal is surprising, even in the
constant $\delta$ case. Indeed, it was believed that such a tester would not
attain sublinear sample complexity even for constant values of $\varepsilon$
and $\delta$."
 How can hybrid automaton models be used to analyze the security of a power system?,467cb5e1,"[{'abstract': '  Dynamic security analysis is an important problem of power systems on\nensuring safe operation and stable power supply even when certain faults occur.\nNo matter such faults are caused by vulnerabilities of system components,\nphysical attacks, or cyber-attacks that are more related to cyber-security,\nthey eventually affect the physical stability of a power system. Examples of\nthe loss of physical stability include the Northeast blackout of 2003 in North\nAmerica and the 2015 system-wide blackout in Ukraine. The nonlinear hybrid\nnature, that is, nonlinear continuous dynamics integrated with discrete\nswitching, and the high degree of freedom property of power system dynamics\nmake it challenging to conduct the dynamic security analysis. In this paper, we\nuse the hybrid automaton model to describe the dynamics of a power system and\nmainly deal with the index-1 differential-algebraic equation models regarding\nthe continuous dynamics in different discrete states. The analysis problem is\nformulated as a reachability problem of the associated hybrid model. A\nsampling-based algorithm is then proposed by integrating modeling and\nrandomized simulation of the hybrid dynamics to search for a feasible execution\nconnecting an initial state of the post-fault system and a target set in the\ndesired operation mode. The proposed method enables the use of existing power\nsystem simulators for the synthesis of discrete switching and control\nstrategies through randomized simulation. The effectiveness and performance of\nthe proposed approach are demonstrated with an application to the dynamic\nsecurity analysis of the New England 39-bus benchmark power system exhibiting\nhybrid dynamics. In addition to evaluating the dynamic security, the proposed\nmethod searches for a feasible strategy to ensure the dynamic security of the\nsystem in face of disruptions.\n', 'title': 'Dynamic Security Analysis of Power Systems by a Sampling-Based Algorithm'}]","Based on the provided context, I found a relevant research paper that discusses the use of hybrid automaton models to analyze the security of a power system.

Title: Dynamic Security Analysis of Power Systems by a Sampling-Based Algorithm

Abstract:   Dynamic security analysis is an important problem of power systems on ensuring safe operation and stable power supply even when certain faults occur. No matter such faults are caused by vulnerabilities of system components, physical attacks, or cyber-attacks that are more related to cyber-security, they eventually affect the physical stability of a power system. Examples of the loss of physical stability include the Northeast blackout of 2003 in North America and the 2015 system-wide blackout in Ukraine. The nonlinear hybrid nature, that is, nonlinear continuous dynamics integrated with discrete switching, and the high degree of freedom property of power system dynamics make it challenging to conduct the dynamic security analysis. In this paper, we use the hybrid automaton model to describe the dynamics of a power system and mainly deal with the index-1 differential-algebraic equation models regarding the continuous dynamics in different discrete states. The analysis problem is formulated as a reachability problem of the associated hybrid model. A sampling-based algorithm is then proposed by integrating modeling and randomized simulation of the hybrid dynamics to search for a feasible execution connecting an initial state of the post-fault system and a target set in the desired operation mode. The proposed method enables the use of existing power system simulators for the synthesis of discrete switching and control strategies through randomized simulation. The effectiveness and performance of the proposed approach are demonstrated with an application to the dynamic security analysis of the New England 39-bus benchmark power system exhibiting hybrid dynamics. In addition to evaluating the dynamic security, the proposed method searches for a feasible strategy to ensure the dynamic security of the system in face of disruptions."
What is a key characteristic of the Constraint Handling Rules programming language?,31e01f2d,"[{'abstract': '  Constraint Handling Rules is an effective concurrent declarative programming\nlanguage and a versatile computational logic formalism. CHR programs consist of\nguarded reactive rules that transform multisets of constraints. One of the main\nfeatures of CHR is its inherent concurrency. Intuitively, rules can be applied\nto parts of a multiset in parallel. In this comprehensive survey, we give an\noverview of concurrent and parallel as well as distributed CHR semantics,\nstandard and more exotic, that have been proposed over the years at various\nlevels of refinement. These semantics range from the abstract to the concrete.\nThey are related by formal soundness results. Their correctness is established\nas correspondence between parallel and sequential computations. We present\ncommon concise sample CHR programs that have been widely used in experiments\nand benchmarks. We review parallel CHR implementations in software and\nhardware. The experimental results obtained show a consistent parallel speedup.\nMost implementations are available online. The CHR formalism can also be used\nto implement and reason with models for concurrency. To this end, the Software\nTransaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus\nhave been faithfully encoded in CHR. Under consideration in Theory and Practice\nof Logic Programming (TPLP).\n', 'title': 'Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey'}]","Title: Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey

Abstract:   Constraint Handling Rules is an effective concurrent declarative programming language and a versatile computational logic formalism. CHR programs consist of guarded reactive rules that transform multisets of constraints. One of the main features of CHR is its inherent concurrency. Intuitively, rules can be applied to parts of a multiset in parallel. In this comprehensive survey, we give an overview of concurrent and parallel as well as distributed CHR semantics, standard and more exotic, that have been proposed over the years at various levels of refinement. These semantics range from the abstract to the concrete. They are related by formal soundness results. Their correctness is established as correspondence between parallel and sequential computations. We present common concise sample CHR programs that have been widely used in experiments and benchmarks. We review parallel CHR implementations in software and hardware. The experimental results obtained show a consistent parallel speedup. Most implementations are available online. The CHR formalism can also be used to implement and reason with models for concurrency. To this end, the Software Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus have been faithfully encoded in CHR. Under consideration in Theory and Practice of Logic Programming (TPLP)."
What type of functions are used to define families of zeta$-kindred elements?,063757ba,"[{'abstract': ""  In this paper we introduce the notion of $\\zeta$-crossbreeding in a set of\n$\\zeta$-factorization formulas and also the notion of complete hybrid formula\nas the final result of that crossbreeding. The last formula is used as a\ncriterion for selection of families of $\\zeta$-kindred elements in class of\nreal continuous functions.\nDedicated to recalling of Gregory Mendel's pea-crossbreeding.\n"", 'title': ""Jacob's ladders, crossbreeding in the set of $ζ$-factorization formulas and selection of families of $ζ$-kindred real continuous functions""}]","Title: Jacob's ladders, crossbreeding in the set of ζ-factorization formulas and selection of families of ζ-kindred real continuous functions

Abstract: In this paper we introduce the notion of ζ-crossbreeding in a set of ζ-factorization formulas and also the notion of complete hybrid formula as the final result of that crossbreeding. The last formula is used as a criterion for selection of families of ζ-kindred elements in class of real continuous functions."
What is the 'semiblind scenario' in network analysis and how can it be addressed?,3b12b1ba,"[{'abstract': ""  Consider a social network where only a few nodes (agents) have meaningful\ninteractions in the sense that the conditional dependency graph over node\nattribute variables (behaviors) is sparse. A company that can only observe the\ninteractions between its own customers will generally not be able to accurately\nestimate its customers' dependency subgraph: it is blinded to any external\ninteractions of its customers and this blindness creates false edges in its\nsubgraph. In this paper we address the semiblind scenario where the company has\naccess to a noisy summary of the complementary subgraph connecting external\nagents, e.g., provided by a consolidator. The proposed framework applies to\nother applications as well, including field estimation from a network of awake\nand sleeping sensors and privacy-constrained information sharing over social\nsubnetworks. We propose a penalized likelihood approach in the context of a\ngraph signal obeying a Gaussian graphical models (GGM). We use a convex-concave\niterative optimization algorithm to maximize the penalized likelihood.\n"", 'title': 'Semiblind subgraph reconstruction in Gaussian graphical models'}]","Title: Semiblind subgraph reconstruction in Gaussian graphical models

Abstract: Consider a social network where only a few nodes (agents) have meaningful interactions in the sense that the conditional dependency graph over node attribute variables (behaviors) is sparse. A company that can only observe the interactions between its own customers will generally not be able to accurately estimate its customers' dependency subgraph: it is blinded to any external interactions of its customers and this blindness creates false edges in its subgraph. In this paper we address the semiblind scenario where the company has access to a noisy summary of the complementary subgraph connecting external agents, e.g., provided by a consolidator. The proposed framework applies to other applications as well, including field estimation from a network of awake and sleeping sensors and privacy-constrained information sharing over social subnetworks. We propose a penalized likelihood approach in the context of a graph signal obeying a Gaussian graphical models (GGM). We use a convex-concave iterative optimization algorithm to maximize the penalized likelihood."
What are some nonstandard methods used in the study of vector measures and transfunctions? ,e394fb8d,"[{'abstract': '  We are interested in extending operators defined on positive measures, called\nhere transfunctions, to signed measures and vector measures. Our methods use a\nsomewhat nonstandard approach to measures and vector measures. The necessary\nbackground, including proofs of some auxiliary results, is included.\n', 'title': 'On vector measures and extensions of transfunctions'}]","After reviewing the provided abstracts, I found that the paper ""On vector measures and extensions of transfunctions"" mentions using a ""somewhat nonstandard approach to measures and vector measures.""

Here is the relevant paper:

Title: On vector measures and extensions of transfunctions

Abstract: We are interested in extending operators defined on positive measures, called here transfunctions, to signed measures and vector measures. Our methods use a somewhat nonstandard approach to measures and vector measures. The necessary background, including proofs of some auxiliary results, is included."
What is the relationship between the  mu-ordinary locus and Ekedahl-Kottwitz-Oort-Rapoport strata in Shimura varieties?,61dbff19,"[{'abstract': '  In this paper, we study the $\\mu$-ordinary locus of a Shimura variety with\nparahoric level structure. Under the axioms in \\cite{HR}, we show that\n$\\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport\nstrata introduced in \\cite{HR} and we give criteria on the density of the\n$\\mu$-ordinary locus.\n', 'title': 'On the $μ$-ordinary locus of a Shimura variety'}]","Here is the answer to your question:

Title: On the $μ$-ordinary locus of a Shimura variety

Abstract: In this paper, we study the $\mu$-ordinary locus of a Shimura variety with parahoric level structure. Under the axioms in \cite{HR}, we show that $\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport strata introduced in \cite{HR} and we give criteria on the density of the $\mu$-ordinary locus.

So, according to the abstract, the μ-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport strata in Shimura varieties with parahoric level structure, under certain axioms."
What are the advantages of using continuation value functions in decision making?,753fa835,"[{'abstract': '  Building on insights of Jovanovic (1982) and subsequent authors, we develop a\ncomprehensive theory of optimal timing of decisions based around continuation\nvalue functions and operators that act on them. Optimality results are provided\nunder general settings, with bounded or unbounded reward functions. This\napproach has several intrinsic advantages that we exploit in developing the\ntheory. One is that continuation value functions are smoother than value\nfunctions, allowing for sharper analysis of optimal policies and more efficient\ncomputation. Another is that, for a range of problems, the continuation value\nfunction exists in a lower dimensional space than the value function,\nmitigating the curse of dimensionality. In one typical experiment, this reduces\nthe computation time from over a week to less than three minutes.\n', 'title': 'Optimal Timing of Decisions: A General Theory Based on Continuation Values'}]","I think I can help you find the answer!

Title: Optimal Timing of Decisions: A General Theory Based on Continuation Values

Abstract: Building on insights of Jovanovic (1982) and subsequent authors, we develop a comprehensive theory of optimal timing of decisions based around continuation value functions and operators that act on them. Optimality results are provided under general settings, with bounded or unbounded reward functions. This approach has several intrinsic advantages that we exploit in developing the theory. One is that continuation value functions are smoother than value functions, allowing for sharper analysis of optimal policies and more efficient computation. Another is that, for a range of problems, the continuation value function exists in a lower dimensional space than the value function, mitigating the curse of dimensionality. In one typical experiment, this reduces the computation time from over a week to less than three minutes.

The advantages of using continuation value functions in decision making are:

1. Smoother functions, allowing for sharper analysis of optimal policies and more efficient computation.
2. Existence in a lower dimensional space, mitigating the curse of dimensionality, which can significantly reduce computation time."
What are some methods for visualizing and analyzing the output density of a generative model?,e8ab2bbb,"[{'abstract': '  It can be difficult to tell whether a trained generative model has learned to\ngenerate novel examples or has simply memorized a specific set of outputs. In\npublished work, it is common to attempt to address this visually, for example\nby displaying a generated example and its nearest neighbor(s) in the training\nset (in, for example, the L2 metric). As any generative model induces a\nprobability density on its output domain, we propose studying this density\ndirectly. We first study the geometry of the latent representation and\ngenerator, relate this to the output density, and then develop techniques to\ncompute and inspect the output density. As an application, we demonstrate that\n""memorization"" tends to a density made of delta functions concentrated on the\nmemorized examples. We note that without first understanding the geometry, the\nmeasurement would be essentially impossible to make.\n', 'title': 'Latent Geometry and Memorization in Generative Models'}]","I think I can help you with that!

Title: Latent Geometry and Memorization in Generative Models

Abstract: It can be difficult to tell whether a trained generative model has learned to generate novel examples or has simply memorized a specific set of outputs. In published work, it is common to attempt to address this visually, for example by displaying a generated example and its nearest neighbor(s) in the training set (in, for example, the L2 metric). As any generative model induces a probability density on its output domain, we propose studying this density directly. We first study the geometry of the latent representation and generator, relate this to the output density, and then develop techniques to compute and inspect the output density. As an application, we demonstrate that ""memorization"" tends to a density made of delta functions concentrated on the memorized examples. We note that without first understanding the geometry, the measurement would be essentially impossible to make.

This paper proposes studying the output density of a generative model directly, and developing techniques to compute and inspect it. They relate the geometry of the latent representation and generator to the output density, which could be a method for visualizing and analyzing the output density."
What are sufficient second-order conditions for bang-bang controls in optimal control problems?,b2198912,"[{'abstract': '  We consider bilinear optimal control problems, whose objective functionals do\nnot depend on the controls. Hence, bang-bang solutions will appear. We\ninvestigate sufficient second-order conditions for bang-bang controls, which\nguarantee local quadratic growth of the objective functional in $L^1$. In\naddition, we prove that for controls that are not bang-bang, no such growth can\nbe expected. Finally, we study the finite-element discretization, and prove\nerror estimates of bang-bang controls in $L^1$-norms.\n', 'title': 'Second-Order Analysis and Numerical Approximation for Bang-Bang Bilinear Control Problems'}]","I think I can help you with that!

Title: Second-Order Analysis and Numerical Approximation for Bang-Bang Bilinear Control Problems

Abstract: We consider bilinear optimal control problems, whose objective functionals do not depend on the controls. Hence, bang-bang solutions will appear. We investigate sufficient second-order conditions for bang-bang controls, which guarantee local quadratic growth of the objective functional in $L^1$. In addition, we prove that for controls that are not bang-bang, no such growth can be expected. Finally, we study the finite-element discretization, and prove error estimates of bang-bang controls in $L^1$-norms."
What types of self-motion are observed in camphoric acid loaded agarose tablets at an air-water interface?,1656ac4b,"[{'abstract': '  We report experiments on an agarose gel tablet loaded with camphoric acid\n(c-boat) set into self-motion by interfacial tension gradients at the air-water\ninterface. We observe three distinct modes of c-boat motion: harmonic mode\nwhere the c-boat speed oscillates sinusoidally in time, a steady mode where the\nc-boat maintains constant speed, and a relaxation oscillation mode where the\nc-boat maintains near-zero speed between sudden jumps in speed and position at\nregular time intervals. Whereas all three modes have been separately reported\nbefore in different systems, we show they belong to a common description.\nThrough control of the air-water surface tension with Sodium Dodecyl Sulfate\n(SDS), we experimentally deduce the three self-propulsive modes result from\nsurface tension difference between Camphoric Acid (CA) and the ambient\nsurroundings.\n', 'title': 'Dynamics of a Camphoric Acid boat at the air-water interface'}]","Title: Dynamics of a Camphoric Acid boat at the air-water interface

Abstract: We report experiments on an agarose gel tablet loaded with camphoric acid (c-boat) set into self-motion by interfacial tension gradients at the air-water interface. We observe three distinct modes of c-boat motion: harmonic mode where the c-boat speed oscillates sinusoidally in time, a steady mode where the c-boat maintains constant speed, and a relaxation oscillation mode where the c-boat maintains near-zero speed between sudden jumps in speed and position at regular time intervals. Whereas all three modes have been separately reported before in different systems, we show they belong to a common description. Through control of the air-water surface tension with Sodium Dodecyl Sulfate (SDS), we experimentally deduce the three self-propulsive modes result from surface tension difference between Camphoric Acid (CA) and the ambient surroundings."
"According to the study, what are the benefits of using Between-Class learning?",77de4118,"[{'abstract': ""  Deep learning methods have achieved high performance in sound recognition\ntasks. Deciding how to feed the training data is important for further\nperformance improvement. We propose a novel learning method for deep sound\nrecognition: Between-Class learning (BC learning). Our strategy is to learn a\ndiscriminative feature space by recognizing the between-class sounds as\nbetween-class sounds. We generate between-class sounds by mixing two sounds\nbelonging to different classes with a random ratio. We then input the mixed\nsound to the model and train the model to output the mixing ratio. The\nadvantages of BC learning are not limited only to the increase in variation of\nthe training data; BC learning leads to an enlargement of Fisher's criterion in\nthe feature space and a regularization of the positional relationship among the\nfeature distributions of the classes. The experimental results show that BC\nlearning improves the performance on various sound recognition networks,\ndatasets, and data augmentation schemes, in which BC learning proves to be\nalways beneficial. Furthermore, we construct a new deep sound recognition\nnetwork (EnvNet-v2) and train it with BC learning. As a result, we achieved a\nperformance surpasses the human level.\n"", 'title': 'Learning from Between-class Examples for Deep Sound Recognition'}]","Title: Learning from Between-class Examples for Deep Sound Recognition

Abstract: Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher's criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.

Benefits of using Between-Class learning:

* Increase in variation of the training data
* Enlargement of Fisher's criterion in the feature space
* Regularization of the positional relationship among the feature distributions of the classes
* Improved performance on various sound recognition networks, datasets, and data augmentation schemes
* Proves to be always beneficial"
What is the BrnoCompSpeed dataset used for?,eb3265f2,"[{'abstract': '  In this paper, we focus on fully automatic traffic surveillance camera\ncalibration, which we use for speed measurement of passing vehicles. We improve\nover a recent state-of-the-art camera calibration method for traffic\nsurveillance based on two detected vanishing points. More importantly, we\npropose a novel automatic scene scale inference method. The method is based on\nmatching bounding boxes of rendered 3D models of vehicles with detected\nbounding boxes in the image. The proposed method can be used from arbitrary\nviewpoints, since it has no constraints on camera placement. We evaluate our\nmethod on the recent comprehensive dataset for speed measurement BrnoCompSpeed.\nExperiments show that our automatic camera calibration method by detection of\ntwo vanishing points reduces error by 50% (mean distance ratio error reduced\nfrom 0.18 to 0.09) compared to the previous state-of-the-art method. We also\nshow that our scene scale inference method is more precise, outperforming both\nstate-of-the-art automatic calibration method for speed measurement (error\nreduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error\nreduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results\nof the proposed automatic camera calibration method on video sequences obtained\nfrom real surveillance cameras in various places, and under different lighting\nconditions (night, dawn, day).\n', 'title': 'Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement'}]","Title: Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement

Abstract:   In this paper, we focus on fully automatic traffic surveillance camera
calibration, which we use for speed measurement of passing vehicles. We improve
over a recent state-of-the-art camera calibration method for traffic
surveillance based on two detected vanishing points. More importantly, we
propose a novel automatic scene scale inference method. The method is based on
matching bounding boxes of rendered 3D models of vehicles with detected
bounding boxes in the image. The proposed method can be used from arbitrary
viewpoints, since it has no constraints on camera placement. We evaluate our
method on the recent comprehensive dataset for speed measurement BrnoCompSpeed.
Experiments show that our automatic camera calibration method by detection of
two vanishing points reduces error by 50% (mean distance ratio error reduced
from 0.18 to 0.09) compared to the previous state-of-the-art method. We also
show that our scene scale inference method is more precise, outperforming both
state-of-the-art automatic calibration method for speed measurement (error
reduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error
reduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results
of the proposed automatic camera calibration method on video sequences obtained
from real surveillance cameras in various places, and under different lighting
conditions (night, dawn, day)."
What are the computational benefits of quantizing deep neural networks?,110011ee,"[{'abstract': '  In recent years Deep Neural Networks (DNNs) have been rapidly developed in\nvarious applications, together with increasingly complex architectures. The\nperformance gain of these DNNs generally comes with high computational costs\nand large memory consumption, which may not be affordable for mobile platforms.\nDeep model quantization can be used for reducing the computation and memory\ncosts of DNNs, and deploying complex DNNs on mobile equipment. In this work, we\npropose an optimization framework for deep model quantization. First, we\npropose a measurement to estimate the effect of parameter quantization errors\nin individual layers on the overall model prediction accuracy. Then, we propose\nan optimization process based on this measurement for finding optimal\nquantization bit-width for each layer. This is the first work that\ntheoretically analyse the relationship between parameter quantization errors of\nindividual layers and model accuracy. Our new quantization algorithm\noutperforms previous quantization optimization methods, and achieves 20-40%\nhigher compression rate compared to equal bit-width quantization at the same\nmodel prediction accuracy.\n', 'title': 'Adaptive Quantization for Deep Neural Network'}]","Title: Adaptive Quantization for Deep Neural Network

Abstract: In recent years Deep Neural Networks (DNNs) have been rapidly developed in various applications, together with increasingly complex architectures. The performance gain of these DNNs generally comes with high computational costs and large memory consumption, which may not be affordable for mobile platforms. Deep model quantization can be used for reducing the computation and memory costs of DNNs, and deploying complex DNNs on mobile equipment. In this work, we propose an optimization framework for deep model quantization. First, we propose a measurement to estimate the effect of parameter quantization errors in individual layers on the overall model prediction accuracy. Then, we propose an optimization process based on this measurement for finding optimal quantization bit-width for each layer. This is the first work that theoretically analyses the relationship between parameter quantization errors of individual layers and model accuracy. Our new quantization algorithm outperforms previous quantization optimization methods, and achieves 20-40% higher compression rate compared to equal bit-width quantization at the same model prediction accuracy."
What impact does using a Leja interpolation framework have on the computation of the matrix exponential in numerical methods for the discrete Schrödinger equation? ,1e0977bf,"[{'abstract': '  In the present paper we consider numerical methods to solve the discrete\nSchrödinger equation with a time dependent Hamiltonian (motivated by problems\nencountered in the study of spin systems). We will consider both short-range\ninteractions, which lead to evolution equations involving sparse matrices, and\nlong-range interactions, which lead to dense matrices. Both of these settings\nshow very different computational characteristics. We use Magnus integrators\nfor time integration and employ a framework based on Leja interpolation to\ncompute the resulting action of the matrix exponential. We consider both\ntraditional Magnus integrators (which are extensively used for these types of\nproblems in the literature) as well as the recently developed commutator-free\nMagnus integrators and implement them on modern CPU and GPU (graphics\nprocessing unit) based systems.\nWe find that GPUs can yield a significant speed-up (up to a factor of $10$ in\nthe dense case) for these types of problems. In the sparse case GPUs are only\nadvantageous for large problem sizes and the achieved speed-ups are more\nmodest. In most cases the commutator-free variant is superior but especially on\nthe GPU this advantage is rather small. In fact, none of the advantage of\ncommutator-free methods on GPUs (and on multi-core CPUs) is due to the\nelimination of commutators. This has important consequences for the design of\nmore efficient numerical methods.\n', 'title': 'Magnus integrators on multicore CPUs and GPUs'}]","Title: Magnus integrators on multicore CPUs and GPUs

Abstract: In the present paper we consider numerical methods to solve the discrete Schrödinger equation with a time dependent Hamiltonian (motivated by problems encountered in the study of spin systems). We will consider both short-range interactions, which lead to evolution equations involving sparse matrices, and long-range interactions, which lead to dense matrices. Both of these settings show very different computational characteristics. We use Magnus integrators for time integration and employ a framework based on Leja interpolation to compute the resulting action of the matrix exponential. We consider both traditional Magnus integrators (which are extensively used for these types of problems in the literature) as well as the recently developed commutator-free Magnus integrators and implement them on modern CPU and GPU (graphics processing unit) based systems.

According to this paper, employing a Leja interpolation framework with Magnus integrators helps to compute the resulting action of the matrix exponential in numerical methods for the discrete Schrödinger equation."
"In the paper 'On a question of Buchweitz about ranks of syzygies of modules of finite length', what is the relationship between the syzygy rank of a module and the syzygy rank of the residue field?",8311b9fd,"[{'abstract': '  Let R be a local ring of dimension d. Buchweitz asks if the rank of the d-th\nsyzygy of a module of finite lengh is greater than or equal to the rank of the\nd-th syzygy of the residue field, unless the module has finite projective\ndimension. Assuming that R is Gorenstein, we prove that if the question is\naffrmative, then R is a hypersurface. If moreover R has dimension two, then we\nshow that the converse also holds true.\n', 'title': 'On a question of Buchweitz about ranks of syzygies of modules of finite length'}]","Title: On a question of Buchweitz about ranks of syzygies of modules of finite length

Abstract:   Let R be a local ring of dimension d. Buchweitz asks if the rank of the d-th
syzygy of a module of finite lengh is greater than or equal to the rank of the
d-th syzygy of the residue field, unless the module has finite projective
dimension. Assuming that R is Gorenstein, we prove that if the question is
affrmative, then R is a hypersurface. If moreover R has dimension two, then we
show that the converse also holds true.

The relationship between the syzygy rank of a module and the syzygy rank of the residue field is that the rank of the d-th syzygy of a module of finite length is greater than or equal to the rank of the d-th syzygy of the residue field, unless the module has finite projective dimension."
"In the context of Nanoparticle Tracking Analysis (NTA), how does the Mean Square Displacement (MSD) of particles relate to their size?",1673853e,"[{'abstract': '  The interest in the extracellular vesicles (EVs) is rapidly growing as they\nbecame reliable biomarkers for many diseases. For this reason, fast and\naccurate techniques of EVs size characterization are the matter of utmost\nimportance. One increasingly popular technique is the Nanoparticle Tracking\nAnalysis (NTA), in which the diameters of EVs are calculated from their\ndiffusion constants. The crucial assumption here is that the diffusion in NTA\nfollows the Stokes-Einstein relation, i.e. that the Mean Square Displacement\n(MSD) of a particle grows linearly in time (MSD $\\propto t$). However, we show\nthat NTA violates this assumption in both artificial and biological samples,\ni.e. a large population of particles show a strongly sub-diffusive behaviour\n(MSD $\\propto t^\\alpha$, $0<\\alpha<1$). To support this observation we present\na range of experimental results for both polystyrene beads and EVs. This is\nalso related to another problem: for the same samples there exists a huge\ndiscrepancy (by the factor of 2-4) between the sizes measured with NTA and with\nthe direct imaging methods, such as AFM. This can be remedied by e.g. the\nFinite Track Length Adjustment (FTLA) method in NTA, but its applicability is\nlimited in the biological and poly-disperse samples. On the other hand, the\nmodels of sub-diffusion rarely provide the direct relation between the size of\na particle and the generalized diffusion constant. However, we solve this last\nproblem by introducing the logarithmic model of sub-diffusion, aimed at\nretrieving the size data. In result, we propose a novel protocol of NTA data\nanalysis. The accuracy of our method is on par with FTLA for small\n($\\simeq$200nm) particles. We apply our method to study the EVs samples and\ncorroborate the results with AFM.\n', 'title': 'The effects of subdiffusion on the NTA size measurements of extracellular vesicles in biological samples'}]","Title: The effects of subdiffusion on the NTA size measurements of extracellular vesicles in biological samples

Abstract: The interest in the extracellular vesicles (EVs) is rapidly growing as they became reliable biomarkers for many diseases. For this reason, fast and accurate techniques of EVs size characterization are the matter of utmost importance. One increasingly popular technique is the Nanoparticle Tracking Analysis (NTA), in which the diameters of EVs are calculated from their diffusion constants. The crucial assumption here is that the diffusion in NTA follows the Stokes-Einstein relation, i.e. that the Mean Square Displacement (MSD) of a particle grows linearly in time (MSD ∝ t). However, we show that NTA violates this assumption in both artificial and biological samples, i.e. a large population of particles show a strongly sub-diffusive behaviour (MSD ∝ t^α, 0<α<1)."
Describe methods used to approximate the convolution of a program with a Gaussian kernel beyond the approximation of Dorn et al. ,ad0f6f36,"[{'abstract': '  This paper introduces a general method to approximate the convolution of an\narbitrary program with a Gaussian kernel. This process has the effect of\nsmoothing out a program. Our compiler framework models intermediate values in\nthe program as random variables, by using mean and variance statistics. Our\napproach breaks the input program into parts and relates the statistics of the\ndifferent parts, under the smoothing process. We give several approximations\nthat can be used for the different parts of the program. These include the\napproximation of Dorn et al., a novel adaptive Gaussian approximation, Monte\nCarlo sampling, and compactly supported kernels. Our adaptive Gaussian\napproximation is accurate up to the second order in the standard deviation of\nthe smoothing kernel, and mathematically smooth. We show how to construct a\ncompiler that applies chosen approximations to given parts of the input\nprogram. Because each expression can have multiple approximation choices, we\nuse a genetic search to automatically select the best approximations. We apply\nthis framework to the problem of automatically bandlimiting procedural shader\nprograms. We evaluate our method on a variety of complex shaders, including\nshaders with parallax mapping, animation, and spatially varying statistics. The\nresulting smoothed shader programs outperform previous approaches both\nnumerically, and aesthetically, due to the smoothing properties of our\napproximations.\n', 'title': 'Approximate Program Smoothing Using Mean-Variance Statistics, with Application to Procedural Shader Bandlimiting'}]","The researcher is looking for methods to approximate the convolution of a program with a Gaussian kernel beyond the approximation of Dorn et al.

Title: Approximate Program Smoothing Using Mean-Variance Statistics, with Application to Procedural Shader Bandlimiting

Abstract: This paper introduces a general method to approximate the convolution of an arbitrary program with a Gaussian kernel. This process has the effect of smoothing out a program. Our compiler framework models intermediate values in the program as random variables, by using mean and variance statistics. Our approach breaks the input program into parts and relates the statistics of the different parts, under the smoothing process. We give several approximations that can be used for the different parts of the program. These include the approximation of Dorn et al., a novel adaptive Gaussian approximation, Monte Carlo sampling, and compactly supported kernels. Our adaptive Gaussian approximation is accurate up to the second order in the standard deviation of the smoothing kernel, and mathematically smooth. We show how to construct a compiler that applies chosen approximations to given parts of the input program. Because each expression can have multiple approximation choices, we use a genetic search to automatically select the best approximations. We apply this framework to the problem of automatically bandlimiting procedural shader programs. We evaluate our method on a variety of complex shaders, including shaders with parallax mapping, animation, and spatially varying statistics. The resulting smoothed shader programs outperform previous approaches both numerically, and aesthetically, due to the smoothing properties of our approximations."
What instrument was designed to measure precipitable water vapor (PWV) at the Fred Lawrence Whipple Observatory?,e09ea9bd,"[{'abstract': ""  Ground-based astronomical observations may be limited by telluric water vapor\nabsorption, which is highly variable in time and significantly complicates both\nspectroscopy and photometry in the near-infrared (NIR). To achieve the\nsensitivity required to detect Earth-sized exoplanets in the NIR, simultaneous\nmonitoring of precipitable water vapor (PWV) becomes necessary to mitigate the\nimpact of variable telluric lines on radial velocity measurements and transit\nlight curves. To address this issue, we present the Camera for the Automatic\nMonitoring of Atmospheric Lines (CAMAL), a stand-alone, inexpensive six-inch\naperture telescope dedicated to measuring PWV at the Fred Lawrence Whipple\nObservatory on Mount Hopkins. CAMAL utilizes three narrowband NIR filters to\ntrace the amount of atmospheric water vapor affecting simultaneous observations\nwith the MINiature Exoplanet Radial Velocity Array (MINERVA) and MINERVA-Red\ntelescopes. Here we present the current design of CAMAL, discuss our data\nanalysis methods, and show results from 11 nights of PWV measurements taken\nwith CAMAL. For seven nights of data, we have independent PWV measurements\nextracted from high-resolution stellar spectra taken with the Tillinghast\nReflector Echelle Spectrometer (TRES) also located on Mount Hopkins. We use the\nTRES spectra to calibrate the CAMAL absolute PWV scale. Comparisons between\nCAMAL and TRES PWV estimates show excellent agreement, matching to within 1 mm\nover a 10 mm range in PWV. Analysis of CAMAL's photometric precision propagates\nto PWV measurements precise to better than 0.5 mm in dry (PWV < 4 mm)\nconditions. We also find that CAMAL-derived PWVs are highly correlated with\nthose from a GPS-based water vapor monitor located approximately 90 km away at\nKitt Peak National Observatory, with a root mean square PWV difference of 0.8\nmm.\n"", 'title': 'Monitoring Telluric Absorption with CAMAL'}]","Title: Monitoring Telluric Absorption with CAMAL

Abstract: Ground-based astronomical observations may be limited by telluric water vapor absorption, which is highly variable in time and significantly complicates both spectroscopy and photometry in the near-infrared (NIR). To achieve the sensitivity required to detect Earth-sized exoplanets in the NIR, simultaneous monitoring of precipitable water vapor (PWV) becomes necessary to mitigate the impact of variable telluric lines on radial velocity measurements and transit light curves. To address this issue, we present the Camera for the Automatic Monitoring of Atmospheric Lines (CAMAL), a stand-alone, inexpensive six-inch aperture telescope dedicated to measuring PWV at the Fred Lawrence Whipple Observatory on Mount Hopkins. CAMAL utilizes three narrowband NIR filters to trace the amount of atmospheric water vapor affecting simultaneous observations with the MINiature Exoplanet Radial Velocity Array (MINERVA) and MINERVA-Red telescopes. Here we present the current design of CAMAL, discuss our data analysis methods, and show results from 11 nights of PWV measurements taken with CAMAL. For seven nights of data, we have independent PWV measurements extracted from high-resolution stellar spectra taken with the Tillinghast Reflector Echelle Spectrometer (TRES) also located on Mount Hopkins. We use the TRES spectra to calibrate the CAMAL absolute PWV scale. Comparisons between CAMAL and TRES PWV estimates show excellent agreement, matching to within 1 mm over a 10 mm range in PWV. Analysis of CAMAL's photometric precision propagates to PWV measurements precise to better than 0.5 mm in dry (PWV < 4 mm) conditions. We also find that CAMAL-derived PWVs are highly correlated with those from a GPS-based water vapor monitor located approximately 90 km away at Kitt Peak National Observatory, with a root mean square PWV difference of 0.8 mm."
What statistical method was employed to determine the correlation of magnetic responses between different locations?,85077dd8,"[{'abstract': '  We characterize the response of the quiet time (no substorms or storms)\nlarge-scale ionospheric transient equivalent currents to north-south and\nsouth-north IMF turnings by using a dynamical network of ground-based\nmagnetometers. Canonical correlation between all pairs of SuperMAG magnetometer\nstations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\\circ}$)\nis used to establish the extent of near-simultaneous magnetic response between\nregions of magnetic local time-MLAT. Parameters and maps that describe\nspatial-temporal correlation are used to characterize the system and its\nresponse to the turnings aggregated over several hundred events. We find that\nregions that experience large increases in correlation post turning coincide\nwith typical locations of a two-cell convection system and are influenced by\nthe interplanetary magnetic field $\\mathit{B}_{y}$. The time between the\nturnings reaching the magnetopause and a network response is found to be\n$\\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the\nnightside.\n', 'title': 'Characterizing the ionospheric current pattern response to southward and northward IMF turnings with dynamical SuperMAG correlation networks'}]","Based on the context, I found the answer to the question.

Title: Characterizing the ionospheric current pattern response to southward and northward IMF turnings with dynamical SuperMAG correlation networks

Abstract: We characterize the response of the quiet time (no substorms or storms) large-scale ionospheric transient equivalent currents to north-south and south-north IMF turnings by using a dynamical network of ground-based magnetometers. Canonical correlation between all pairs of SuperMAG magnetometer stations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\circ}$) is used to establish the extent of near-simultaneous magnetic response between regions of magnetic local time-MLAT. Parameters and maps that describe spatial-temporal correlation are used to characterize the system and its response to the turnings aggregated over several hundred events. We find that regions that experience large increases in correlation post turning coincide with typical locations of a two-cell convection system and are influenced by the interplanetary magnetic field $B_{y}$. The time between the turnings reaching the magnetopause and a network response is found to be $\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the nightside.

The statistical method employed to determine the correlation of magnetic responses between different locations is Canonical Correlation."
What are the challenges of deploying components into a system?,ed0d10f4,"[{'abstract': '  Component-based design is a different way of constructing systems which\noffers numerous benefits, in particular, decreasing the complexity of system\ndesign. However, deploying components into a system is a challenging and\nerror-prone task. Model checking is one of the reliable methods that\nautomatically and systematically analyse the correctness of a given system. Its\nbrute-force check of the state space significantly expands the level of\nconfidence in the system. Nevertheless, model checking is limited by a critical\nproblem so-called State Space Explosion (SSE). To benefit from model checking,\nappropriate methods to reduce SSE, is required. In two last decades, a great\nnumber of methods to mitigate the state space explosion have been proposed\nwhich have many similarities, dissimilarities, and unclear concepts in some\ncases. This research, firstly, aims at present a review and brief discussion of\nthe methods of handling SSE problem and classify them based on their\nsimilarities, principle and characteristics. Second, it investigates the\nmethods for handling SSE problem in verifying Component-based system (CBS) and\nprovides insight into CBS verification limitations that have not been addressed\nyet. The analysis in this research has revealed the patterns, specific\nfeatures, and gaps in the state-of-the-art methods. In addition, we identified\nand discussed suitable methods to soften SSE problem in CBS and underlined the\nkey challenges for future research efforts.\n', 'title': 'Handling state space explosion in verification of component-based systems: A review'}]","Title: Handling state space explosion in verification of component-based systems: A review

Abstract: Component-based design is a different way of constructing systems which offers numerous benefits, in particular, decreasing the complexity of system design. However, deploying components into a system is a challenging and error-prone task. Model checking is one of the reliable methods that automatically and systematically analyse the correctness of a given system. Its brute-force check of the state space significantly expands the level of confidence in the system. Nevertheless, model checking is limited by a critical problem so-called State Space Explosion (SSE). To benefit from model checking, appropriate methods to reduce SSE, is required. In two last decades, a great number of methods to mitigate the state space explosion have been proposed which have many similarities, dissimilarities, and unclear concepts in some cases. This research, firstly, aims at present a review and brief discussion of the methods of handling SSE problem and classify them based on their similarities, principle and characteristics. Second, it investigates the methods for handling SSE problem in verifying Component-based system (CBS) and provides insight into CBS verification limitations that have not been addressed yet. The analysis in this research has revealed the patterns, specific features, and gaps in the state-of-the-art methods. In addition, we identified and discussed suitable methods to soften SSE problem in CBS and underlined the key challenges for future research efforts."
What is a new fault tolerant consensus algorithm designed for?,30bcb009,"[{'abstract': '  Recently a new fault tolerant and simple mechanism was designed for solving\ncommit consensus problem. It is based on replicated validation of messages sent\nbetween transaction participants and a special dispatcher validator manager\nnode. This paper presents a correctness, safety proofs and performance analysis\nof this algorithm.\n', 'title': 'Fault Tolerant Consensus Agreement Algorithm'}]","Title: Fault Tolerant Consensus Agreement Algorithm

Abstract: Recently a new fault tolerant and simple mechanism was designed for solving commit consensus problem. It is based on replicated validation of messages sent between transaction participants and a special dispatcher validator manager node. This paper presents a correctness, safety proofs and performance analysis of this algorithm."
How can causal inference be performed using a mini-max optimal score?,663a6d9c,"[{'abstract': '  The algorithmic Markov condition states that the most likely causal direction\nbetween two random variables X and Y can be identified as that direction with\nthe lowest Kolmogorov complexity. Due to the halting problem, however, this\nnotion is not computable.\nWe hence propose to do causal inference by stochastic complexity. That is, we\npropose to approximate Kolmogorov complexity via the Minimum Description Length\n(MDL) principle, using a score that is mini-max optimal with regard to the\nmodel class under consideration. This means that even in an adversarial\nsetting, such as when the true distribution is not in this class, we still\nobtain the optimal encoding for the data relative to the class.\nWe instantiate this framework, which we call CISC, for pairs of univariate\ndiscrete variables, using the class of multinomial distributions. Experiments\nshow that CISC is highly accurate on synthetic, benchmark, as well as\nreal-world data, outperforming the state of the art by a margin, and scales\nextremely well with regard to sample and domain sizes.\n', 'title': 'Causal Inference by Stochastic Complexity'}]","Title: Causal Inference by Stochastic Complexity

Abstract: The algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identified as that direction with the lowest Kolmogorov complexity. Due to the halting problem, however, this notion is not computable. We hence propose to do causal inference by stochastic complexity. That is, we propose to approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle, using a score that is mini-max optimal with regard to the model class under consideration. This means that even in an adversarial setting, such as when the true distribution is not in this class, we still obtain the optimal encoding for the data relative to the class. We instantiate this framework, which we call CISC, for pairs of univariate discrete variables, using the class of multinomial distributions. Experiments show that CISC is highly accurate on synthetic, benchmark, as well as real-world data, outperforming the state of the art by a margin, and scales extremely well with regard to sample and domain sizes."
How do Dirichlet problems on finite intervals relate to the behavior of solutions on the entire real line?,d7cd1108,"[{'abstract': '  We obtain bounded for all $t$ solutions of ordinary differential equations as\nlimits of the solutions of the corresponding Dirichlet problems on $(-L,L)$,\nwith $L \\rightarrow \\infty$. We derive a priori estimates for the Dirichlet\nproblems, allowing passage to the limit, via a diagonal sequence. This approach\ncarries over to the PDE case.\n', 'title': 'Bounded solutions for a class of Hamiltonian systems'}]","Based on the context, I think I can help you find the research paper you're looking for.

The answer to your question can be found in the paper:

Title: Bounded solutions for a class of Hamiltonian systems

Abstract: We obtain bounded for all $t$ solutions of ordinary differential equations as limits of the solutions of the corresponding Dirichlet problems on $(-L,L)$, with $L \rightarrow \infty$. We derive a priori estimates for the Dirichlet problems, allowing passage to the limit, via a diagonal sequence. This approach carries over to the PDE case.

This paper discusses how Dirichlet problems on finite intervals ($(-L,L)$) relate to the behavior of solutions on the entire real line, which is exactly what you were looking for!"
What are the main problems of the theory of complex networks when examining flow characteristics?,7c7b7766,"[{'abstract': '  Subject of research is complex networks and network systems. The network\nsystem is defined as a complex network in which flows are moved. Classification\nof flows in the network is carried out on the basis of ordering and continuity.\nIt is shown that complex networks with different types of flows generate\nvarious network systems. Flow analogues of the basic concepts of the theory of\ncomplex networks are introduced and the main problems of this theory in terms\nof flow characteristics are formulated. Local and global flow characteristics\nof networks bring closer the theory of complex networks to the systems theory\nand systems analysis. Concept of flow core of network system is introduced and\ndefined how it simplifies the process of its investigation. Concepts of kernel\nand flow core of multiplex are determined. Features of operation of multiplex\ntype systems are analyzed.\n', 'title': 'Flow Characteristics and Cores of Complex Network and Multiplex Type Systems'}]","Title: Flow Characteristics and Cores of Complex Network and Multiplex Type Systems

Abstract: Subject of research is complex networks and network systems. The network system is defined as a complex network in which flows are moved. Classification of flows in the network is carried out on the basis of ordering and continuity. It is shown that complex networks with different types of flows generate various network systems. Flow analogues of the basic concepts of the theory of complex networks are introduced and the main problems of this theory in terms of flow characteristics are formulated. Local and global flow characteristics of networks bring closer the theory of complex networks to the systems theory and systems analysis. Concept of flow core of network system is introduced and defined how it simplifies the process of its investigation. Concepts of kernel and flow core of multiplex are determined. Features of operation of multiplex type systems are analyzed.

[Note: The main problems of the theory of complex networks in terms of flow characteristics are not explicitly stated in the abstract. However, it is mentioned that the flow analogues of the basic concepts of the theory of complex networks are introduced and the main problems of this theory in terms of flow characteristics are formulated, implying that the paper discusses these problems, but does not provide a clear answer in the abstract.]"
In what Bousfield localization does every complex in the homotopy category of complexes of projective left R-modules vanish?,c7d699fd,"[{'abstract': ""  Let $R$ be an associative ring with unit and denote by $K({\\rm R\n\\mbox{-}Proj})$ the homotopy category of complexes of projective left\n$R$-modules. Neeman proved the theorem that $K({\\rm R \\mbox{-}Proj})$ is\n$\\aleph_1$-compactly generated, with the category $K^+ ({\\rm R \\mbox{-}proj})$\nof left bounded complexes of finitely generated projective $R$-modules\nproviding an essentially small class of such generators. Another proof of\nNeeman's theorem is explained, using recent ideas of Christensen and Holm, and\nEmmanouil. The strategy of the proof is to show that every complex in $K({\\rm R\n\\mbox{-}Proj})$ vanishes in the Bousfield localization $K({\\rm R\n\\mbox{-}Flat})/\\langle K^+ ({\\rm R \\mbox{-}proj}) \\rangle.$\n"", 'title': ""Neeman's characterization of K(R-Proj) via Bousfield localization""}]","I think I've found the answer!

Title: Neeman's characterization of K(R-Proj) via Bousfield localization

Abstract: Let $R$ be an associative ring with unit and denote by $K({\rm R \mbox{-}Proj})$ the homotopy category of complexes of projective left $R$-modules. Neeman proved the theorem that $K({\rm R \mbox{-}Proj})$ is $\aleph_1$-compactly generated, with the category $K^+ ({\rm R \mbox{-}proj})$ of left bounded complexes of finitely generated projective $R$-modules providing an essentially small class of such generators. Another proof of Neeman's theorem is explained, using recent ideas of Christensen and Holm, and Emmanouil. The strategy of the proof is to show that every complex in $K({\rm R \mbox{-}Proj})$ vanishes in the Bousfield localization $K({\rm R \mbox{-}Flat})/\langle K^+ ({\rm R \mbox{-}proj}) \rangle.$"
What transformations on a Lebesgue space  have strictly over-recurrent sets?,8a62348f,"[{'abstract': '  We show that every invertible strong mixing transformation on a Lebesgue\nspace has strictly over-recurrent sets. Also, we give an explicit procedure for\nconstructing strong mixing transformations with no under-recurrent sets. This\nanswers both parts of a question of V. Bergelson.\nWe define $\\epsilon$-over-recurrence and show that given $\\epsilon > 0$, any\nergodic measure preserving invertible transformation (including discrete\nspectrum) has $\\epsilon$-over-recurrent sets of arbitrarily small measure.\nDiscrete spectrum transformations and rotations do not have over-recurrent\nsets, but we construct a weak mixing rigid transformation with strictly\nover-recurrent sets.\n', 'title': 'Over Recurrence for Mixing Transformations'}]","Title: Over Recurrence for Mixing Transformations

Abstract: We show that every invertible strong mixing transformation on a Lebesgue space has strictly over-recurrent sets. Also, we give an explicit procedure for constructing strong mixing transformations with no under-recurrent sets. This answers both parts of a question of V. Bergelson. We define ε-over-recurrence and show that given ε > 0, any ergodic measure preserving invertible transformation (including discrete spectrum) has ε-over-recurrent sets of arbitrarily small measure. Discrete spectrum transformations and rotations do not have over-recurrent sets, but we construct a weak mixing rigid transformation with strictly over-recurrent sets."
Which authors extended previous work on immersions of graphs and 2-dimensional CW-complexes?,a8da760c,"[{'abstract': '  An immersion $f : {\\mathcal D} \\rightarrow \\mathcal C$ between cell complexes\nis a local homeomorphism onto its image that commutes with the characteristic\nmaps of the cell complexes. We study immersions between finite-dimensional\nconnected $\\Delta$-complexes by replacing the fundamental group of the base\nspace by an appropriate inverse monoid. We show how conjugacy classes of the\nclosed inverse submonoids of this inverse monoid may be used to classify\nconnected immersions into the complex. This extends earlier results of Margolis\nand Meakin for immersions between graphs and of Meakin and Szakács on\nimmersions into $2$-dimensional $CW$-complexes.\n', 'title': 'Inverse monoids and immersions of cell complexes'}]","Based on the context, I found the answer to your question:

Title: Inverse monoids and immersions of cell complexes

Abstract: An immersion $f : {\mathcal D} \rightarrow \mathcal C$ between cell complexes
is a local homeomorphism onto its image that commutes with the characteristic
maps of the cell complexes. We study immersions between finite-dimensional
connected $\Delta$-complexes by replacing the fundamental group of the base
space by an appropriate inverse monoid. We show how conjugacy classes of the
closed inverse submonoids of this inverse monoid may be used to classify
connected immersions into the complex. This extends earlier results of Margolis
and Meakin for immersions between graphs and of Meakin and Szakács on
immersions into $2$-dimensional $CW$-complexes.

The authors who extended previous work on immersions of graphs and 2-dimensional CW-complexes are Margolis, Meakin, and Szakács."
What are the conditions for a set function to be considered submodular?,01628af0,"[{'abstract': '  A set function $f$ on a finite set $V$ is submodular if $f(X) + f(Y) \\geq f(X\n\\cup Y) + f(X \\cap Y)$ for any pair $X, Y \\subseteq V$. The symmetric\ndifference transformation (SD-transformation) of $f$ by a canonical set $S\n\\subseteq V$ is a set function $g$ given by $g(X) = f(X \\vartriangle S)$ for $X\n\\subseteq V$,where $X \\vartriangle S = (X \\setminus S) \\cup (S \\setminus X)$\ndenotes the symmetric difference between $X$ and $S$. Submodularity and\nSD-transformations are regarded as the counterparts of convexity and affine\ntransformations in a discrete space, respectively. However, submodularity is\nnot preserved under SD-transformations, in contrast to the fact that convexity\nis invariant under affine transformations. This paper presents a\ncharacterization of SD-stransformations preserving submodularity. Then, we are\nconcerned with the problem of discovering a canonical set $S$, given the\nSD-transformation $g$ of a submodular function $f$ by $S$, provided that $g(X)$\nis given by a function value oracle. A submodular function $f$ on $V$ is said\nto be strict if $f(X) + f(Y) > f(X \\cup Y) + f(X \\cap Y)$ holds whenever both\n$X \\setminus Y$ and $Y \\setminus X$ are nonempty. We show that the problem is\nsolved by using ${\\rm O}(|V|)$ oracle calls when $f$ is strictly submodular,\nalthough it requires exponentially many oracle calls in general.\n', 'title': 'Finding Submodularity Hidden in Symmetric Difference'}]","Title: Finding Submodularity Hidden in Symmetric Difference

Abstract: A set function $f$ on a finite set $V$ is submodular if $f(X) + f(Y) \geq f(X \cup Y) + f(X \cap Y)$ for any pair $X, Y \subseteq V$."
What is a duality theorem used for in graph minor theory?,77d735a2,"[{'abstract': '  We prove a general width duality theorem for combinatorial structures with\nwell-defined notions of cohesion and separation. These might be graphs and\nmatroids, but can be much more general or quite different. The theorem asserts\na duality between the existence of high cohesiveness somewhere local and a\nglobal overall tree structure.\nWe describe cohesive substructures in a unified way in the format of tangles:\nas orientations of low-order separations satisfying certain consistency axioms.\nThese axioms can be expressed without reference to the underlying structure,\nsuch as a graph or matroid, but just in terms of the poset of the separations\nthemselves. This makes it possible to identify tangles, and apply our\ntangle-tree duality theorem, in very diverse settings.\nOur result implies all the classical duality theorems for width parameters in\ngraph minor theory, such as path-width, tree-width, branch-width or rank-width.\nIt yields new, tangle-type, duality theorems for tree-width and path-width. It\nimplies the existence of width parameters dual to cohesive substructures such\nas $k$-blocks, edge-tangles, or given subsets of tangles, for which no width\nduality theorems were previously known.\nAbstract separation systems can be found also in structures quite unlike\ngraphs and matroids. For example, our theorem can be applied to image analysis\nby capturing the regions of an image as tangles of separations defined as\nnatural partitions of its set of pixels. It can be applied in big data contexts\nby capturing clusters as tangles. It can be applied in the social sciences,\ne.g. by capturing as tangles the few typical mindsets of individuals found by a\nsurvey. It could also be applied in pure mathematics, e.g. to separations of\ncompact manifolds.\n', 'title': 'Tangle-tree duality in abstract separation systems'}]","Title: Tangle-tree duality in abstract separation systems

Abstract: We prove a general width duality theorem for combinatorial structures with well-defined notions of cohesion and separation. These might be graphs and matroids, but can be much more general or quite different. The theorem asserts a duality between the existence of high cohesiveness somewhere local and a global overall tree structure.

This duality theorem is used to imply all the classical duality theorems for width parameters in graph minor theory, such as path-width, tree-width, branch-width or rank-width."
How can boundary conditions for multiphase flows ensure energy stability and reduction consistency?,3c553cbc,"[{'abstract': ""  We present a set of effective outflow/open boundary conditions and an\nassociated algorithm for simulating the dynamics of multiphase flows consisting\nof $N$ ($N\\geqslant 2$) immiscible incompressible fluids in domains involving\noutflows or open boundaries. These boundary conditions are devised based on the\nproperties of energy stability and reduction consistency. The energy stability\nproperty ensures that the contributions of these boundary conditions to the\nenergy balance will not cause the total energy of the N-phase system to\nincrease over time. Therefore, these open/outflow boundary conditions are very\neffective in overcoming the backflow instability in multiphase systems. The\nreduction consistency property ensures that if some fluid components are absent\nfrom the N-phase system then these N-phase boundary conditions will reduce to\nthose corresponding boundary conditions for the equivalent smaller system. Our\nnumerical algorithm for the proposed boundary conditions together with the\nN-phase governing equations involves only the solution of a set of de-coupled\nindividual Helmholtz-type equations within each time step, and the resultant\nlinear algebraic systems after discretization involve only constant and\ntime-independent coefficient matrices which can be pre-computed. Therefore, the\nalgorithm is computationally very efficient and attractive. We present\nextensive numerical experiments for flow problems involving multiple fluid\ncomponents and inflow/outflow boundaries to test the proposed method. In\nparticular, we compare in detail the simulation results of a three-phase\ncapillary wave problem with Prosperetti's exact physical solution and\ndemonstrate that the method developed herein produces physically accurate\nresults.\n"", 'title': 'Multiphase Flows of N Immiscible Incompressible Fluids: An Outflow/Open Boundary Condition and Algorithm'}]","Title: Multiphase Flows of N Immiscible Incompressible Fluids: An Outflow/Open Boundary Condition and Algorithm

Abstract: We present a set of effective outflow/open boundary conditions and an associated algorithm for simulating the dynamics of multiphase flows consisting of $N$ ($N\geqslant 2$) immiscible incompressible fluids in domains involving outflows or open boundaries. These boundary conditions are devised based on the properties of energy stability and reduction consistency. The energy stability property ensures that the contributions of these boundary conditions to the energy balance will not cause the total energy of the N-phase system to increase over time. Therefore, these open/outflow boundary conditions are very effective in overcoming the backflow instability in multiphase systems. The reduction consistency property ensures that if some fluid components are absent from the N-phase system then these N-phase boundary conditions will reduce to those corresponding boundary conditions for the equivalent smaller system."
How does the method handle random inputs from an anisotropic collision kernel?,e5504332,"[{'abstract': ""  In this paper, we study the generalized polynomial chaos (gPC) based\nstochastic Galerkin method for the linear semiconductor Boltzmann equation\nunder diffusive scaling and with random inputs from an anisotropic collision\nkernel and the random initial condition. While the numerical scheme and the\nproof of uniform-in-Knudsen-number regularity of the distribution function in\nthe random space has been introduced in [Jin-Liu-16'], the main goal of this\npaper is to first obtain a sharper estimate on the regularity of the\nsolution-an exponential decay towards its local equilibrium, which then lead to\nthe uniform spectral convergence of the stochastic Galerkin method for the\nproblem under study.\n"", 'title': 'Uniform Spectral Convergence of the Stochastic Galerkin Method for the Linear Semiconductor Boltzmann Equation with Random Inputs and Diffusive Scalings'}]","I couldn't find any information in the provided abstracts that directly addresses how the method handles random inputs from an anisotropic collision kernel. None of the abstracts mention anisotropic collision kernels or how they are handled. 

If you can provide more context or clarify what you are looking for, I'd be happy to try and help you further."
What are the limitations of regression-based methods for human pose estimation?,e722db9d,"[{'abstract': '  Regression based methods are not performing as well as detection based\nmethods for human pose estimation. A central problem is that the structural\ninformation in the pose is not well exploited in the previous regression\nmethods. In this work, we propose a structure-aware regression approach. It\nadopts a reparameterized pose representation using bones instead of joints. It\nexploits the joint connection structure to define a compositional loss function\nthat encodes the long range interactions in the pose. It is simple, effective,\nand general for both 2D and 3D pose estimation in a unified setting.\nComprehensive evaluation validates the effectiveness of our approach. It\nsignificantly advances the state-of-the-art on Human3.6M and is competitive\nwith state-of-the-art results on MPII.\n', 'title': 'Compositional Human Pose Regression'}]","Title: Compositional Human Pose Regression

Abstract: Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M and is competitive with state-of-the-art results on MPII.

The limitation mentioned is that the structural information in the pose is not well exploited in previous regression methods."
"Can magnetocapillary self-assemblies be used to create microrobots, and what tasks can they perform?",3f4f05dc,"[{'abstract': '  This paper presents an overview and discussion of magnetocapillary\nself-assemblies. New results are presented, in particular concerning the\npossible development of future applications. These self-organizing structures\npossess the notable ability to move along an interface when powered by an\noscillatory, uniform magnetic field. The system is constructed as follows. Soft\nmagnetic particles are placed on a liquid interface, and submitted to a\nmagnetic induction field. An attractive force due to the curvature of the\ninterface around the particles competes with an interaction between magnetic\ndipoles. Ordered structures can spontaneously emerge from these conditions.\nFurthermore, time-dependent magnetic fields can produce a wide range of dynamic\nbehaviours, including non-time-reversible deformation sequences that produce\ntranslational motion at low Reynolds number. In other words, due to a\nspontaneous breaking of time-reversal symmetry, the assembly can turn into a\nsurface microswimmer. Trajectories have been shown to be precisely\ncontrollable. As a consequence, this system offers a way to produce microrobots\nable to perform different tasks. This is illustrated in this paper by the\ncapture, transport and release of a floating cargo, and the controlled mixing\nof fluids at low Reynolds number.\n', 'title': 'Magnetocapillary self-assemblies: locomotion and micromanipulation along a liquid interface'}]","Title: Magnetocapillary self-assemblies: locomotion and micromanipulation along a liquid interface

Abstract:   This paper presents an overview and discussion of magnetocapillary
self-assemblies. New results are presented, in particular concerning the
possible development of future applications. These self-organizing structures
possess the notable ability to move along an interface when powered by an
oscillatory, uniform magnetic field. The system is constructed as follows. Soft
magnetic particles are placed on a liquid interface, and submitted to a
magnetic induction field. An attractive force due to the curvature of the
interface around the particles competes with an interaction between magnetic
dipoles. Ordered structures can spontaneously emerge from these conditions.
Furthermore, time-dependent magnetic fields can produce a wide range of dynamic
behaviours, including non-time-reversible deformation sequences that produce
translational motion at low Reynolds number. In other words, due to a
spontaneous breaking of time-reversal symmetry, the assembly can turn into a
surface microswimmer. Trajectories have been shown to be precisely
controllable. As a consequence, this system offers a way to produce microrobots
able to perform different tasks. This is illustrated in this paper by the
capture, transport and release of a floating cargo, and the controlled mixing
of fluids at low Reynolds number.

According to this abstract, yes, magnetocapillary self-assemblies can be used to create microrobots, and they can perform tasks such as capture, transport, and release of a floating cargo, and controlled mixing of fluids at low Reynolds number."
What is the maximum change in galactic electron intensity observed by Voyager 1 in the heliosheath?,7976e680,"[{'abstract': '  We have derived background corrected intensities of 3-50 MeV galactic\nelectrons observed by Voyager 1 as it passes through the heliosheath from 95 to\n122 AU. The overall intensity change of the background corrected data from the\ninner to the outer boundary of the heliosheath is a maximum of a factor ~100 at\n15 MeV. At lower energies this fractional change becomes less and the corrected\nelectron spectra in the heliosheath becomes progressively steeper, reaching\nvalues ~ -2.5 for the spectral index just outside of the termination shock. At\nhigher energies the spectra of electrons has an exponent changing from the\nnegative LIS spectral index of -1.3 to values approaching zero in the\nheliosheath as a result of the solar modulation of the galactic electron\ncomponent. The large modulation effects observed below ~100 MV are possible\nevidence for enhanced diffusion as part of the modulation process for electrons\nin the heliosheath.\n', 'title': 'A Galactic Cosmic Ray Electron Intensity Increase of a factor of up to 100 At Energies between 3 and 50 MeV in the Heliosheath between the Termination Shock and the Heliopause Due to Solar Modulation As Measured by Voyager 1'}]","Title: A Galactic Cosmic Ray Electron Intensity Increase of a factor of up to 100 At Energies between 3 and 50 MeV in the Heliosheath between the Termination Shock and the Heliopause Due to Solar Modulation As Measured by Voyager 1

Abstract: We have derived background corrected intensities of 3-50 MeV galactic electrons observed by Voyager 1 as it passes through the heliosheath from 95 to 122 AU. The overall intensity change of the background corrected data from the inner to the outer boundary of the heliosheath is a maximum of a factor ~100 at 15 MeV.

The answer is a factor of 100."
Are there methods to  define  varieties of automata that  incorporate  ordering  or distinguished elements?,cc7c5e89,"[{'abstract': '  The classical Eilenberg correspondence, based on the concept of the syntactic\nmonoid, relates varieties of regular languages with pseudovarieties of finite\nmonoids. Various modifications of this correspondence appeared, with more\ngeneral classes of regular languages on one hand and classes of more complex\nalgebraic structures on the other hand. For example, classes of languages need\nnot be closed under complementation or all preimages under homomorphisms, while\nmonoids can be equipped with a compatible order or they can have a\ndistinguished set of generators. Such generalized varieties and pseudovarieties\nalso have natural counterparts formed by classes of finite (ordered) automata.\nIn this paper the previous approaches are combined. The notion of positive\n$\\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final\nstates are specified) is introduced and their correspondence with positive\n$\\mathcal C$-varieties of languages is proved.\n', 'title': 'On Varieties of Ordered Automata'}]","Yes, I can help you with that.

Title: On Varieties of Ordered Automata

Abstract:  The classical Eilenberg correspondence, based on the concept of the syntactic
monoid, relates varieties of regular languages with pseudovarieties of finite
monoids. Various modifications of this correspondence appeared, with more
general classes of regular languages on one hand and classes of more complex
algebraic structures on the other hand. For example, classes of languages need
not be closed under complementation or all preimages under homomorphisms, while
monoids can be equipped with a compatible order or they can have a
distinguished set of generators. Such generalized varieties and pseudovarieties
also have natural counterparts formed by classes of finite (ordered) automata.
In this paper the previous approaches are combined. The notion of positive
$\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final
states are specified) is introduced and their correspondence with positive
$\mathcal C$-varieties of languages is proved."
How does the performance of a gradient based meta-learner with an automatically found architecture compare to the first gradient-based meta-learner called MAML?,e5acd0c3,"[{'abstract': '  Fully automating machine learning pipelines is one of the key challenges of\ncurrent artificial intelligence research, since practical machine learning\noften requires costly and time-consuming human-powered processes such as model\ndesign, algorithm development, and hyperparameter tuning. In this paper, we\nverify that automated architecture search synergizes with the effect of\ngradient-based meta learning. We adopt the progressive neural architecture\nsearch \\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal\narchitectures for meta-learners. The gradient based meta-learner whose\narchitecture was automatically found achieved state-of-the-art results on the\n5-shot 5-way Mini-ImageNet classification problem with $74.65\\%$ accuracy,\nwhich is $11.54\\%$ improvement over the result obtained by the first\ngradient-based meta-learner called MAML\n\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is\nthe first successful neural architecture search implementation in the context\nof meta learning.\n', 'title': 'Auto-Meta: Automated Gradient Based Meta Learner Search'}]","Title: Auto-Meta: Automated Gradient Based Meta Learner Search

Abstract: Fully automating machine learning pipelines is one of the key challenges of current artificial intelligence research, since practical machine learning often requires costly and time-consuming human-powered processes such as model design, algorithm development, and hyperparameter tuning. In this paper, we verify that automated architecture search synergizes with the effect of gradient-based meta learning. We adopt the progressive neural architecture search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal architectures for meta-learners. The gradient based meta-learner whose architecture was automatically found achieved state-of-the-art results on the 5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy, which is $11.54\%$ improvement over the result obtained by the first gradient-based meta-learner called MAML \cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is the first successful neural architecture search implementation in the context of meta learning."
What are the benefits of using range-based estimators for stock volatility compared to close-to-close estimators?,51ecad88,"[{'abstract': '  We investigate the predictability of several range-based stock volatility\nestimators, and compare them to the standard close-to-close estimator which is\nmost commonly acknowledged as the volatility. The patterns of volatility\nchanges are analyzed using LSTM recurrent neural networks, which are a state of\nthe art method of sequence learning. We implement the analysis on all current\nconstituents of the Dow Jones Industrial Average index, and report averaged\nevaluation results. We find that changes in the values of range-based\nestimators are more predictable than that of the estimator using daily closing\nvalues only.\n', 'title': 'Exploring the predictability of range-based volatility estimators using RNNs'}]","Based on the context, I found the relevant research paper:

Title: Exploring the predictability of range-based volatility estimators using RNNs

Abstract: We investigate the predictability of several range-based stock volatility estimators, and compare them to the standard close-to-close estimator which is most commonly acknowledged as the volatility. The patterns of volatility changes are analyzed using LSTM recurrent neural networks, which are a state of the art method of sequence learning. We implement the analysis on all current constituents of the Dow Jones Industrial Average index, and report averaged evaluation results. We find that changes in the values of range-based estimators are more predictable than that of the estimator using daily closing values only.

According to this paper, the benefit of using range-based estimators for stock volatility is that changes in their values are more predictable than those of close-to-close estimators."
What is a new myopic strategy for sequential design of experiment (DOE) problems,85b24256,"[{'abstract': '  We design a new myopic strategy for a wide class of sequential design of\nexperiment (DOE) problems, where the goal is to collect data in order to to\nfulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling\n(MPS), is inspired by the classical posterior (Thompson) sampling algorithm for\nmulti-armed bandits and leverages the flexibility of probabilistic programming\nand approximate Bayesian inference to address a broad set of problems.\nEmpirically, this general-purpose strategy is competitive with more specialised\nmethods in a wide array of DOE tasks, and more importantly, enables addressing\ncomplex DOE goals where no existing method seems applicable. On the theoretical\nside, we leverage ideas from adaptive submodularity and reinforcement learning\nto derive conditions under which MPS achieves sublinear regret against natural\nbenchmark policies.\n', 'title': 'Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming'}]","Title: Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming

Abstract: We design a new myopic strategy for a wide class of sequential design of experiment (DOE) problems, where the goal is to collect data in order to to fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling (MPS), is inspired by the classical posterior (Thompson) sampling algorithm for multi-armed bandits and leverages the flexibility of probabilistic programming and approximate Bayesian inference to address a broad set of problems. Empirically, this general-purpose strategy is competitive with more specialised methods in a wide array of DOE tasks, and more importantly, enables addressing complex DOE goals where no existing method seems applicable. On the theoretical side, we leverage ideas from adaptive submodularity and reinforcement learning to derive conditions under which MPS achieves sublinear regret against natural benchmark policies."
Guide to understanding isogeny based cryptography using elliptic curves,3027d5ca,"[{'abstract': ""  These lectures notes were written for a summer school on Mathematics for\npost-quantum cryptography in Thiès, Senegal. They try to provide a guide for\nMasters' students to get through the vast literature on elliptic curves,\nwithout getting lost on their way to learning isogeny based cryptography. They\nare by no means a reference text on the theory of elliptic curves, nor on\ncryptography; students are encouraged to complement these notes with some of\nthe books recommended in the bibliography.\nThe presentation is divided in three parts, roughly corresponding to the\nthree lectures given. In an effort to keep the reader interested, each part\nalternates between the fundamental theory of elliptic curves, and applications\nin cryptography. We often prefer to have the main ideas flow smoothly, rather\nthan having a rigorous presentation as one would have in a more classical book.\nThe reader will excuse us for the inaccuracies and the omissions.\n"", 'title': 'Mathematics of Isogeny Based Cryptography'}]","Based on the context, I think I can help you find what you're looking for.

Title: Mathematics of Isogeny Based Cryptography

Abstract: These lectures notes were written for a summer school on Mathematics for post-quantum cryptography in Thiès, Senegal. They try to provide a guide for Masters' students to get through the vast literature on elliptic curves, without getting lost on their way to learning isogeny based cryptography. They are by no means a reference text on the theory of elliptic curves, nor on cryptography; students are encouraged to complement these notes with some of the books recommended in the bibliography. The presentation is divided in three parts, roughly corresponding to the three lectures given. In an effort to keep the reader interested, each part alternates between the fundamental theory of elliptic curves, and applications in cryptography. We often prefer to have the main ideas flow smoothly, rather than having a rigorous presentation as one would have in a more classical book. The reader will excuse us for the inaccuracies and the omissions."
What paper by Marsman and Ising is focused on sampling methods for Bayesian inference?,cd7f5e3b,"[{'abstract': '  This paper discusses a Metropolis-Hastings algorithm developed by\n\\citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is\nproven that the algorithm becomes more efficient with more data and meets the\ngrowing demands of large scale educational measurement.\n', 'title': 'An Asymptotically Efficient Metropolis-Hastings Sampler for Bayesian Inference in Large-Scale Educational Measuremen'}]","Based on the context, I believe the paper you are looking for is:

Title: An Asymptotically Efficient Metropolis-Hastings Sampler for Bayesian Inference in Large-Scale Educational Measurement

Abstract: This paper discusses a Metropolis-Hastings algorithm developed by \citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is proven that the algorithm becomes more efficient with more data and meets the growing demands of large scale educational measurement."
What techniques can be used to enable the co-location of computation and data for near-data processing?,5fbab273,"[{'abstract': ""  Recent studies have demonstrated that near-data processing (NDP) is an\neffective technique for improving performance and energy efficiency of\ndata-intensive workloads. However, leveraging NDP in realistic systems with\nmultiple memory modules introduces a new challenge. In today's systems, where\nno computation occurs in memory modules, the physical address space is\ninterleaved at a fine granularity among all memory modules to help improve the\nutilization of processor-memory interfaces by distributing the memory traffic.\nHowever, this is at odds with efficient use of NDP, which requires careful\nplacement of data in memory modules such that near-data computations and their\nexclusively used data can be localized in individual memory modules, while\ndistributing shared data among memory modules to reduce hotspots. In order to\naddress this new challenge, we propose a set of techniques that (1) enable\ncollections of OS pages to either be fine-grain interleaved among memory\nmodules (as is done today) or to be placed contiguously on individual memory\nmodules (as is desirable for NDP private data), and (2) decide whether to\nlocalize or distribute each memory object based on its anticipated access\npattern and steer computations to the memory where the data they access is\nlocated. Our evaluations across a wide range of workloads show that the\nproposed mechanism improves performance by 31% and reduces 38% remote data\naccesses over a baseline system that cannot exploit computate-data affinity\ncharacteristics.\n"", 'title': 'CODA: Enabling Co-location of Computation and Data for Near-Data Processing'}]","Title: CODA: Enabling Co-location of Computation and Data for Near-Data Processing

Abstract: Recent studies have demonstrated that near-data processing (NDP) is an effective technique for improving performance and energy efficiency of data-intensive workloads. However, leveraging NDP in realistic systems with multiple memory modules introduces a new challenge. In today's systems, where no computation occurs in memory modules, the physical address space is interleaved at a fine granularity among all memory modules to help improve the utilization of processor-memory interfaces by distributing the memory traffic. However, this is at odds with efficient use of NDP, which requires careful placement of data in memory modules such that near-data computations and their exclusively used data can be localized in individual memory modules, while distributing shared data among memory modules to reduce hotspots. In order to address this new challenge, we propose a set of techniques that (1) enable collections of OS pages to either be fine-grain interleaved among memory modules (as is done today) or to be placed contiguously on individual memory modules (as is desirable for NDP private data), and (2) decide whether to localize or distribute each memory object based on its anticipated access pattern and steer computations to the memory where the data they access is located. Our evaluations across a wide range of workloads show that the proposed mechanism improves performance by 31% and reduces 38% remote data accesses over a baseline system that cannot exploit compute-data affinity characteristics."
What method is proposed to marginalize stochastic inputs when inferring missing labels in a model with a discriminative component?,78357dd1,"[{'abstract': ""  Neural network based generative models with discriminative components are a\npowerful approach for semi-supervised learning. However, these techniques a)\ncannot account for model uncertainty in the estimation of the model's\ndiscriminative component and b) lack flexibility to capture complex stochastic\npatterns in the label generation process. To avoid these problems, we first\npropose to use a discriminative component with stochastic inputs for increased\nnoise flexibility. We show how an efficient Gibbs sampling procedure can\nmarginalize the stochastic inputs when inferring missing labels in this model.\nFollowing this, we extend the discriminative component to be fully Bayesian and\nproduce estimates of uncertainty in its parameter values. This opens the door\nfor semi-supervised Bayesian active learning.\n"", 'title': 'Bayesian Semisupervised Learning with Deep Generative Models'}]","Title: Bayesian Semisupervised Learning with Deep Generative Models

Abstract: Neural network based generative models with discriminative components are a powerful approach for semi-supervised learning. However, these techniques a) cannot account for model uncertainty in the estimation of the model's discriminative component and b) lack flexibility to capture complex stochastic patterns in the label generation process. To avoid these problems, we first propose to use a discriminative component with stochastic inputs for increased noise flexibility. We show how an efficient Gibbs sampling procedure can marginalize the stochastic inputs when inferring missing labels in this model. Following this, we extend the discriminative component to be fully Bayesian and produce estimates of uncertainty in its parameter values. This opens the door for semi-supervised Bayesian active learning."
How do scaling laws of drops and bubbles in Hele-Shaw cells differ from those in three-dimensional environments?,0e6a7d1a,"[{'abstract': '  In this review article, we discuss recent studies on drops and bubbles in\nHele-Shaw cells, focusing on how scaling laws exhibit crossovers from the\nthree-dimensional counterparts and focusing on topics in which viscosity plays\nan important role. By virtue of progresses in analytical theory and high-speed\nimaging, dynamics of drops and bubbles have actively been studied with the aid\nof scaling arguments. However, compared with three dimensional problems,\nstudies on the corresponding problems in Hele-Shaw cells are still limited.\nThis review demonstrates that the effect of confinement in the Hele-Shaw cell\nintroduces new physics allowing different scaling regimes to appear. For this\npurpose, we discuss various examples that are potentially important for\nindustrial applications handling drops and bubbles in confined spaces by\nshowing agreement between experiments and scaling theories. As a result, this\nreview provides a collection of problems in hydrodynamics that may be\nanalytically solved or that may be worth studying numerically in the near\nfuture.\n', 'title': 'Viscous dynamics of drops and bubbles in Hele-Shaw cells: drainage, drag friction, coalescence, and bursting'}]","Title: Viscous dynamics of drops and bubbles in Hele-Shaw cells: drainage, drag friction, coalescence, and bursting

Abstract: In this review article, we discuss recent studies on drops and bubbles in Hele-Shaw cells, focusing on how scaling laws exhibit crossovers from the three-dimensional counterparts and focusing on topics in which viscosity plays an important role. By virtue of progresses in analytical theory and high-speed imaging, dynamics of drops and bubbles have actively been studied with the aid of scaling arguments. However, compared with three dimensional problems, studies on the corresponding problems in Hele-Shaw cells are still limited. This review demonstrates that the effect of confinement in the Hele-Shaw cell introduces new physics allowing different scaling regimes to appear. For this purpose, we discuss various examples that are potentially important for industrial applications handling drops and bubbles in confined spaces by showing agreement between experiments and scaling theories. As a result, this review provides a collection of problems in hydrodynamics that may be analytically solved or that may be worth studying numerically in the near future.

The abstract states that the effect of confinement in the Hele-Shaw cell introduces new physics, allowing different scaling regimes to appear, which implies that the scaling laws of drops and bubbles in Hele-Shaw cells differ from those in three-dimensional environments."
How is the quality of the generated random vectors evaluated in the context of Gaussian tree synthesis?,7a47fc9e,"[{'abstract': '  A new synthesis scheme is proposed to effectively generate a random vector\nwith prescribed joint density that induces a (latent) Gaussian tree structure.\nThe quality of synthesis is measured by total variation distance between the\nsynthesized and desired statistics. The proposed layered and successive\nencoding scheme relies on the learned structure of tree to use minimal number\nof common random variables to synthesize the desired density. We characterize\nthe achievable rate region for the rate tuples of multi-layer latent Gaussian\ntree, through which the number of bits needed to simulate such Gaussian joint\ndensity are determined. The random sources used in our algorithm are the latent\nvariables at the top layer of tree, the additive independent Gaussian noises,\nand the Bernoulli sign inputs that capture the ambiguity of correlation signs\nbetween the variables.\n', 'title': 'Tree Structured Synthesis of Gaussian Trees'}]","Title: Tree Structured Synthesis of Gaussian Trees

Abstract: A new synthesis scheme is proposed to effectively generate a random vector with prescribed joint density that induces a (latent) Gaussian tree structure. The quality of synthesis is measured by total variation distance between the synthesized and desired statistics. The proposed layered and successive encoding scheme relies on the learned structure of tree to use minimal number of common random variables to synthesize the desired density. We characterize the achievable rate region for the rate tuples of multi-layer latent Gaussian tree, through which the number of bits needed to simulate such Gaussian joint density are determined. The random sources used in our algorithm are the latent variables at the top layer of tree, the additive independent Gaussian noises, and the Bernoulli sign inputs that capture the ambiguity of correlation signs between the variables."
Can you describe an all-flux quantum pump and its operating mechanism?,8058a65d,"[{'abstract': ""  Quantum charge pumping phenomenon connects band topology through the dynamics\nof a one-dimensional quantum system. In terms of a microscopic model, the\nSu-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful\nstarting point for many considerations of topological physics. Here we present\na generalized Creutz scheme as a distinct two-band quantum pump model. By\nnoting that it undergoes two kinds of topological band transitions accompanying\nwith a Zak-phase-difference of $\\pi$ and $2\\pi$, respectively, various charge\npumping schemes are studied by applying an elaborate Peierl's phase\nsubstitution. Translating into real space, the transportation of quantized\ncharges is a result of cooperative quantum interference effect. In particular,\nan all-flux quantum pump emerges which operates with time-varying fluxes only\nand transports two charge units. This puts cold atoms with artificial gauge\nfields as an unique system where this kind of phenomena can be realized.\n"", 'title': 'Quantum Charge Pumps with Topological Phases in Creutz Ladder'}]","Based on the provided context, I can help you with your question.

Title: Quantum Charge Pumps with Topological Phases in Creutz Ladder

Abstract: Quantum charge pumping phenomenon connects band topology through the dynamics
of a one-dimensional quantum system. In terms of a microscopic model, the
Su-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful
starting point for many considerations of topological physics. Here we present
a generalized Creutz scheme as a distinct two-band quantum pump model. By
noting that it undergoes two kinds of topological band transitions accompanying
with a Zak-phase-difference of $\pi$ and $2\pi$, respectively, various charge
pumping schemes are studied by applying an elaborate Peierl's phase
substitution. Translating into real space, the transportation of quantized
charges is a result of cooperative quantum interference effect. In particular,
an all-flux quantum pump emerges which operates with time-varying fluxes only
and transports two charge units. This puts cold atoms with artificial gauge
fields as an unique system where this kind of phenomena can be realized.

The all-flux quantum pump operates with time-varying fluxes only and transports two charge units. It works by applying an elaborate Peierl's phase substitution, which enables the transportation of quantized charges through cooperative quantum interference effects."
How much compression can be achieved using a method based on low-rank matrix approximation and token frequency distribution?,f4874485,"[{'abstract': '  Model compression is essential for serving large deep neural nets on devices\nwith limited resources or applications that require real-time responses. As a\ncase study, a state-of-the-art neural language model usually consists of one or\nmore recurrent layers sandwiched between an embedding layer used for\nrepresenting input tokens and a softmax layer for generating output tokens. For\nproblems with a very large vocabulary size, the embedding and the softmax\nmatrices can account for more than half of the model size. For instance, the\nbigLSTM model achieves state-of- the-art performance on the One-Billion-Word\n(OBW) dataset with around 800k vocabulary, and its word embedding and softmax\nmatrices use more than 6GBytes space, and are responsible for over 90% of the\nmodel parameters. In this paper, we propose GroupReduce, a novel compression\nmethod for neural language models, based on vocabulary-partition (block) based\nlow-rank matrix approximation and the inherent frequency distribution of tokens\n(the power-law distribution of words). The experimental results show our method\ncan significantly outperform traditional compression methods such as low-rank\napproximation and pruning. On the OBW dataset, our method achieved 6.6 times\ncompression rate for the embedding and softmax matrices, and when combined with\nquantization, our method can achieve 26 times compression rate, which\ntranslates to a factor of 12.8 times compression for the entire model with very\nlittle degradation in perplexity.\n', 'title': 'GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking'}]","Based on the context, I found a relevant research paper that answers your question.

Title: GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking

Abstract: Model compression is essential for serving large deep neural nets on devices with limited resources or applications that require real-time responses. As a case study, a state-of-the-art neural language model usually consists of one or more recurrent layers sandwiched between an embedding layer used for representing input tokens and a softmax layer for generating output tokens. For problems with a very large vocabulary size, the embedding and the softmax matrices can account for more than half of the model size. For instance, the bigLSTM model achieves state-of- the-art performance on the One-Billion-Word (OBW) dataset with around 800k vocabulary, and its word embedding and softmax matrices use more than 6GBytes space, and are responsible for over 90% of the model parameters. In this paper, we propose GroupReduce, a novel compression method for neural language models, based on vocabulary-partition (block) based low-rank matrix approximation and the inherent frequency distribution of tokens (the power-law distribution of words). The experimental results show our method can significantly outperform traditional compression methods such as low-rank approximation and pruning. On the OBW dataset, our method achieved 6.6 times compression rate for the embedding and softmax matrices, and when combined with quantization, our method can achieve 26 times compression rate, which translates to a factor of 12.8 times compression for the entire model with very little degradation in perplexity."
What are some examples of manifolds for which the spectrum of the curl operator has been computed?,04d11ed9,"[{'abstract': '  We study the spectral properties of curl, a linear differential operator of\nfirst order acting on differential forms of appropriate degree on an\nodd-dimensional closed oriented Riemannian manifold. In three dimensions its\neigenvalues are the electromagnetic oscillation frequencies in vacuum without\nexternal sources. In general, the spectrum consists of the eigenvalue 0 with\ninfinite multiplicity and further real discrete eigenvalues of finite\nmultiplicity. We compute the Weyl asymptotics and study the zeta-function. We\ngive a sharp lower eigenvalue bound for positively curved manifolds and analyze\nthe equality case. Finally, we compute the spectrum for flat tori, round\nspheres and 3-dimensional spherical space forms.\n', 'title': 'The curl operator on odd-dimensional manifolds'}]","Title: The curl operator on odd-dimensional manifolds

Abstract: We study the spectral properties of curl, a linear differential operator of first order acting on differential forms of appropriate degree on an odd-dimensional closed oriented Riemannian manifold. In three dimensions its eigenvalues are the electromagnetic oscillation frequencies in vacuum without external sources. In general, the spectrum consists of the eigenvalue 0 with infinite multiplicity and further real discrete eigenvalues of finite multiplicity. We compute the Weyl asymptotics and study the zeta-function. We give a sharp lower eigenvalue bound for positively curved manifolds and analyze the equality case. Finally, we compute the spectrum for flat tori, round spheres and 3-dimensional spherical space forms.

Examples of manifolds for which the spectrum of the curl operator has been computed include:

* Flat tori
* Round spheres
* 3-dimensional spherical space forms"
What are some network analysis techniques used to determine network functionality?,f916d301,"[{'abstract': ""  Several social, medical, engineering and biological challenges rely on\ndiscovering the functionality of networks from their structure and node\nmetadata, when it is available. For example, in chemoinformatics one might want\nto detect whether a molecule is toxic based on structure and atomic types, or\ndiscover the research field of a scientific collaboration network. Existing\ntechniques rely on counting or measuring structural patterns that are known to\nshow large variations from network to network, such as the number of triangles,\nor the assortativity of node metadata. We introduce the concept of multi-hop\nassortativity, that captures the similarity of the nodes situated at the\nextremities of a randomly selected path of a given length. We show that\nmulti-hop assortativity unifies various existing concepts and offers a\nversatile family of 'fingerprints' to characterize networks. These fingerprints\nallow in turn to recover the functionalities of a network, with the help of the\nmachine learning toolbox. Our method is evaluated empirically on established\nsocial and chemoinformatic network benchmarks. Results reveal that our\nassortativity based features are competitive providing highly accurate results\noften outperforming state of the art methods for the network classification\ntask.\n"", 'title': 'Multi-hop assortativities for networks classification'}]","I think I have found what you're looking for!

Title: Multi-hop assortativities for networks classification

Abstract: Several social, medical, engineering and biological challenges rely on discovering the functionality of networks from their structure and node metadata, when it is available. For example, in chemoinformatics one might want to detect whether a molecule is toxic based on structure and atomic types, or discover the research field of a scientific collaboration network. Existing techniques rely on counting or measuring structural patterns that are known to show large variations from network to network, such as the number of triangles, or the assortativity of node metadata. We introduce the concept of multi-hop assortativity, that captures the similarity of the nodes situated at the extremities of a randomly selected path of a given length. We show that multi-hop assortativity unifies various existing concepts and offers a versatile family of 'fingerprints' to characterize networks. These fingerprints allow in turn to recover the functionalities of a network, with the help of the machine learning toolbox. Our method is evaluated empirically on established social and chemoinformatic network benchmarks. Results reveal that our assortativity based features are competitive providing highly accurate results often outperforming state of the art methods for the network classification task."
"In the context of two-candidate elections, what is a drawback of voting systems that are alternatives to majority rule?",c01d2b5f,"[{'abstract': '  In a single winner election with several candidates and ranked choice or\nrating scale ballots, a Condorcet winner is one who wins all their two way\nraces by majority rule or MR. A voting system has Condorcet consistency or CC\nif it names any Condorcet winner the winner. Many voting systems lack CC, but a\nthree step line of reasoning is used here to show why it is necessary. In step\n1 we show that we can dismiss all the electoral criteria which conflict with\nCC. In step 2 we point out that CC follows almost automatically if we can agree\nthat MR is the only acceptable system for elections with two candidates. In\nstep 3 we make that argument for MR. This argument itself has three parts.\nFirst, in races with two candidates, the only well known alternatives to MR can\nsometimes name as winner a candidate who is preferred over their opponent by\nonly one voter, with all others preferring the opponent. That is unacceptable.\nSecond, those same systems are also extremely susceptible to strategic\ninsincere voting. Third, in simulation studies using spatial models with two\ncandidates, the best known alternative to MR picks the best or most centrist\ncandidate significantly less often than MR does.\n', 'title': 'Why Condorcet Consistency is Essential'}]","Title: Why Condorcet Consistency is Essential

Abstract: In a single winner election with several candidates and ranked choice or rating scale ballots, a Condorcet winner is one who wins all their two way races by majority rule or MR. A voting system has Condorcet consistency or CC if it names any Condorcet winner the winner. Many voting systems lack CC, but a three step line of reasoning is used here to show why it is necessary. In step 1 we show that we can dismiss all the electoral criteria which conflict with CC. In step 2 we point out that CC follows almost automatically if we can agree that MR is the only acceptable system for elections with two candidates. In step 3 we make that argument for MR. This argument itself has three parts. First, in races with two candidates, the only well known alternatives to MR can sometimes name as winner a candidate who is preferred over their opponent by only one voter, with all others preferring the opponent. That is unacceptable. Second, those same systems are also extremely susceptible to strategic insincere voting. Third, in simulation studies using spatial models with two candidates, the best known alternative to MR picks the best or most centrist candidate significantly less often than MR does."
What is the relationship between sampling weights and the exponentiated pseudo likelihood?,dc837bc0,"[{'abstract': '  Bayesian estimation is increasingly popular for performing model based\ninference to support policymaking. These data are often collected from surveys\nunder informative sampling designs where subject inclusion probabilities are\ndesigned to be correlated with the response variable of interest. Sampling\nweights constructed from marginal inclusion probabilities are typically used to\nform an exponentiated pseudo likelihood that adjusts the population likelihood\nfor estimation on the sample due to ease-of-estimation. We propose an\nalternative adjustment based on a Bayes rule construction that simultaneously\nperforms weight smoothing and estimates the population model parameters in a\nfully Bayesian construction. We formulate conditions on known marginal and\npairwise inclusion probabilities that define a class of sampling designs where\n$L_{1}$ consistency of the joint posterior is guaranteed. We compare\nperformances between the two approaches on synthetic data, which reveals that\nour fully Bayesian approach better estimates posterior uncertainty without a\nrequirement to calibrate the normalization of the sampling weights. We\ndemonstrate our method on an application concerning the National Health and\nNutrition Examination Survey exploring the relationship between caffeine\nconsumption and systolic blood pressure.\n', 'title': 'Fully Bayesian Estimation Under Informative Sampling'}]","Title: Fully Bayesian Estimation Under Informative Sampling

Abstract:   Bayesian estimation is increasingly popular for performing model based
inference to support policymaking. These data are often collected from surveys
under informative sampling designs where subject inclusion probabilities are
designed to be correlated with the response variable of interest. Sampling
weights constructed from marginal inclusion probabilities are typically used to
form an exponentiated pseudo likelihood that adjusts the population likelihood
for estimation on the sample due to ease-of-estimation. We propose an
alternative adjustment based on a Bayes rule construction that simultaneously
performs weight smoothing and estimates the population model parameters in a
fully Bayesian construction. We formulate conditions on known marginal and
pairwise inclusion probabilities that define a class of sampling designs where
$L_{1}$ consistency of the joint posterior is guaranteed. We compare
performances between the two approaches on synthetic data, which reveals that
our fully Bayesian approach better estimates posterior uncertainty without a
requirement to calibrate the normalization of the sampling weights. We
demonstrate our method on an application concerning the National Health and
Nutrition Examination Survey exploring the relationship between caffeine
consumption and systolic blood pressure.

The answer is that sampling weights constructed from marginal inclusion probabilities are typically used to form an exponentiated pseudo likelihood that adjusts the population likelihood for estimation on the sample."
What are the limitations of using high-level music representations for realistic music generation?,35f285fc,"[{'abstract': '  Realistic music generation is a challenging task. When building generative\nmodels of music that are learnt from data, typically high-level representations\nsuch as scores or MIDI are used that abstract away the idiosyncrasies of a\nparticular performance. But these nuances are very important for our perception\nof musicality and realism, so in this work we embark on modelling music in the\nraw audio domain. It has been shown that autoregressive models excel at\ngenerating raw audio waveforms of speech, but when applied to music, we find\nthem biased towards capturing local signal structure at the expense of\nmodelling long-range correlations. This is problematic because music exhibits\nstructure at many different timescales. In this work, we explore autoregressive\ndiscrete autoencoders (ADAs) as a means to enable autoregressive models to\ncapture long-range correlations in waveforms. We find that they allow us to\nunconditionally generate piano music directly in the raw audio domain, which\nshows stylistic consistency across tens of seconds.\n', 'title': 'The challenge of realistic music generation: modelling raw audio at scale'}]","I think I can help you with that!

Title: The challenge of realistic music generation: modelling raw audio at scale

Abstract: Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.

The limitation mentioned is that high-level representations (such as scores or MIDI) abstract away the idiosyncrasies of a particular performance, which are important for realism and musicality."
What is the name of the superconductor studied in a research paper using first-principles methods to investigate its electronic and phonon properties?,6412fb8a,"[{'abstract': '  The search for a superconductor with non-s-wave pairing is important not only\nfor understanding unconventional mechanisms of superconductivity but also for\nfinding new types of quasiparticles such as Majorana bound states. Materials\nwith both topological band structure and superconductivity are promising\ncandidates as $p+ip$ superconducting states can be generated through pairing\nthe spin-polarized topological surface states. In this work, the electronic and\nphonon properties of the superconductor molybdenum carbide (MoC) are studied\nwith first-principles methods. Our calculations show that nontrivial band\ntopology and superconductivity coexist in both structural phases of MoC,\nnamely, the cubic $\\alpha$ and hexagonal $\\gamma$ phases. The $\\alpha$ phase is\na strong topological insulator and the $\\gamma$ phase is a topological nodal\nline semimetal with drumhead surface states. In addition, hole doping can\nstabilize the crystal structure of the $\\alpha$ phase and elevate the\ntransition temperature in the $\\gamma$ phase. Therefore, MoC in different\nstructural forms can be a practical material platform for studying topological\nsuperconductivity and elusive Majorana fermions.\n', 'title': 'Multiple Topological Electronic Phases in Superconductor MoC'}]","Title: Multiple Topological Electronic Phases in Superconductor MoC

Abstract: The search for a superconductor with non-s-wave pairing is important not only
for understanding unconventional mechanisms of superconductivity but also for
finding new types of quasiparticles such as Majorana bound states. Materials
with both topological band structure and superconductivity are promising
candidates as $p+ip$ superconducting states can be generated through pairing
the spin-polarized topological surface states. In this work, the electronic and
phonon properties of the superconductor molybdenum carbide (MoC) are studied
with first-principles methods. Our calculations show that nontrivial band
topology and superconductivity coexist in both structural phases of MoC,
namely, the cubic $\alpha$ and hexagonal $\gamma$ phases. The $\alpha$ phase is
a strong topological insulator and the $\gamma$ phase is a topological nodal
line semimetal with drumhead surface states. In addition, hole doping can
stabilize the crystal structure of the $\alpha$ phase and elevate the
transition temperature in the $\gamma$ phase. Therefore, MoC in different
structural forms can be a practical material platform for studying topological
superconductivity and elusive Majorana fermions."
What is the relationship between local spatial stability theory and the global analysis of jet turbulence?,43fc6488,"[{'abstract': '  Informed by LES data and resolvent analysis of the mean flow, we examine the\nstructure of turbulence in jets in the subsonic, transonic, and supersonic\nregimes. Spectral (frequency-space) proper orthogonal decomposition is used to\nextract energy spectra and decompose the flow into energy-ranked coherent\nstructures. The educed structures are generally well predicted by the resolvent\nanalysis. Over a range of low frequencies and the first few azimuthal mode\nnumbers, these jets exhibit a low-rank response characterized by\nKelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer\nup to the end of the potential core and that are excited by forcing in the\nvery-near-nozzle shear layer. These modes too the have been experimentally\nobserved before and predicted by quasi-parallel stability theory and other\napproximations--they comprise a considerable portion of the total turbulent\nenergy. At still lower frequencies, particularly for the axisymmetric mode, and\nagain at high frequencies for all azimuthal wavenumbers, the response is not\nlow rank, but consists of a family of similarly amplified modes. These modes,\nwhich are primarily active downstream of the potential core, are associated\nwith the Orr mechanism. They occur also as sub-dominant modes in the range of\nfrequencies dominated by the KH response. Our global analysis helps tie\ntogether previous observations based on local spatial stability theory, and\nexplains why quasi-parallel predictions were successful at some frequencies and\nazimuthal wavenumbers, but failed at others.\n', 'title': 'Spectral analysis of jet turbulence'}]","Title: Spectral analysis of jet turbulence

Abstract: Informed by LES data and resolvent analysis of the mean flow, we examine the structure of turbulence in jets in the subsonic, transonic, and supersonic regimes. Spectral (frequency-space) proper orthogonal decomposition is used to extract energy spectra and decompose the flow into energy-ranked coherent structures. The educed structures are generally well predicted by the resolvent analysis. Over a range of low frequencies and the first few azimuthal mode numbers, these jets exhibit a low-rank response characterized by Kelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer up to the end of the potential core and that are excited by forcing in the very-near-nozzle shear layer. These modes too the have been experimentally observed before and predicted by quasi-parallel stability theory and other approximations--they comprise a considerable portion of the total turbulent energy. At still lower frequencies, particularly for the axisymmetric mode, and again at high frequencies for all azimuthal wavenumbers, the response is not low rank, but consists of a family of similarly amplified modes. These modes, which are primarily active downstream of the potential core, are associated with the Orr mechanism. They occur also as sub-dominant modes in the range of frequencies dominated by the KH response. Our global analysis helps tie together previous observations based on local spatial stability theory, and explains why quasi-parallel predictions were successful at some frequencies and azimuthal wavenumbers, but failed at others.

This paper suggests that local spatial stability theory has successfully predicted some characteristics of jet turbulence in certain regimes, but has also failed in other regimes. The global analysis in this paper provides a more comprehensive understanding of jet turbulence, explaining why local spatial stability theory was successful in some cases and not in others."
How does the energy consumption of robotic functions like computation and communication compare to that of maneuvering?,784156fe,"[{'abstract': '  Mobile robots are increasingly being used to assist with active pursuit and\nlaw enforcement. One major limitation for such missions is the resource\n(battery) allocated to the robot. Factors like nature and agility of evader,\nterrain over which pursuit is being carried out, plausible traversal velocity\nand the amount of necessary data to be collected all influence how long the\nrobot can last in the field and how far it can travel. In this paper, we\ndevelop an analytical model that analyzes the energy utilization for a variety\nof components mounted on a robot to estimate the maximum operational range\nachievable by the robot operating on a single battery discharge. We categorize\nthe major consumers of energy as: 1.) ancillary robotic functions such as\ncomputation, communication, sensing etc., and 2.) maneuvering which involves\npropulsion, steering etc. Both these consumers draw power from the common power\nsource but the achievable range is largely affected by the proportion of power\navailable for maneuvering. For this case study, we performed experiments with\nreal robots on planar and graded surfaces and evaluated the estimation error\nfor each case.\n', 'title': 'Estimating Achievable Range of Ground Robots Operating on Single Battery Discharge for Operational Efficacy Amelioration'}]","I think I can help you with that!

Title: Estimating Achievable Range of Ground Robots Operating on Single Battery Discharge for Operational Efficacy Amelioration

Abstract:  Mobile robots are increasingly being used to assist with active pursuit and 
law enforcement. One major limitation for such missions is the resource 
(battery) allocated to the robot. Factors like nature and agility of evader, 
terrain over which pursuit is being carried out, plausible traversal velocity 
and the amount of necessary data to be collected all influence how long the 
robot can last in the field and how far it can travel. In this paper, we 
develop an analytical model that analyzes the energy utilization for a variety 
of components mounted on a robot to estimate the maximum operational range 
achievable by the robot operating on a single battery discharge. We categorize 
the major consumers of energy as: 1.) ancillary robotic functions such as 
computation, communication, sensing etc., and 2.) maneuvering which involves 
propulsion, steering etc. Both these consumers draw power from the common power 
source but the achievable range is largely affected by the proportion of power 
available for maneuvering. For this case study, we performed experiments with 
real robots on planar and graded surfaces and evaluated the estimation error 
for each case.

According to the abstract, the paper categorizes energy consumers into two types: ancillary robotic functions (computation, communication, sensing, etc.) and maneuvering (propulsion, steering, etc.). Although it doesn't provide a direct comparison of energy consumption between these two categories, it highlights that the proportion of power available for maneuvering significantly affects the achievable range of the robot."
"According to a study on crack propagation in a two-dimensional viscoelastic model, what insight can be gained regarding the development of more refined models?",37e0557b,"[{'abstract': '  We investigate crack propagation in a simple two-dimensional visco-elastic\nmodel and find a scaling regime in the relation between the propagation\nvelocity and energy release rate or fracture energy, together with lower and\nupper bounds of the scaling regime. On the basis of our result, the existence\nof the lower and upper bounds is expected to be universal or model-independent:\nthe present simple simulation model provides generic insight into the physics\nof crack propagation, and the model will be a first step towards the\ndevelopment of a more refined coarse-grained model. Relatively abrupt changes\nof velocity are predicted near the lower and upper bounds for the scaling\nregime and the positions of the bounds could be good markers for the\ndevelopment of tough polymers, for which we provide simple views that could be\nuseful as guiding principles for toughening polymer-based materials.\n', 'title': 'Stationary crack propagation in a two-dimensional visco-elastic network model'}]","Title: Stationary crack propagation in a two-dimensional visco-elastic network model

Abstract: We investigate crack propagation in a simple two-dimensional visco-elastic model and find a scaling regime in the relation between the propagation velocity and energy release rate or fracture energy, together with lower and upper bounds of the scaling regime. On the basis of our result, the existence of the lower and upper bounds is expected to be universal or model-independent: the present simple simulation model provides generic insight into the physics of crack propagation, and the model will be a first step towards the development of a more refined coarse-grained model. Relatively abrupt changes of velocity are predicted near the lower and upper bounds for the scaling regime and the positions of the bounds could be good markers for the development of tough polymers, for which we provide simple views that could be useful as guiding principles for toughening polymer-based materials."
What is a weakness of classical principal component analysis (PCA) that Robust PCA (RPCA) addresses?,c2ae51e3,"[{'abstract': '  Classical principal component analysis (PCA) is not robust to the presence of\nsparse outliers in the data. The use of the $\\ell_1$ norm in the Robust PCA\n(RPCA) method successfully eliminates the weakness of PCA in separating the\nsparse outliers. In this paper, by sticking a simple weight to the Frobenius\nnorm, we propose a weighted low rank (WLR) method to avoid the often\ncomputationally expensive algorithms relying on the $\\ell_1$ norm. As a proof\nof concept, a background estimation model has been presented and compared with\ntwo $\\ell_1$ norm minimization algorithms. We illustrate that as long as a\nsimple weight matrix is inferred from the data, one can use the weighted\nFrobenius norm and achieve the same or better performance.\n', 'title': 'Weighted Low Rank Approximation for Background Estimation Problems'}]","Title: Weighted Low Rank Approximation for Background Estimation Problems

Abstract: Classical principal component analysis (PCA) is not robust to the presence of sparse outliers in the data. The use of the $\ell_1$ norm in the Robust PCA (RPCA) method successfully eliminates the weakness of PCA in separating the sparse outliers. In this paper, by sticking a simple weight to the Frobenius norm, we propose a weighted low rank (WLR) method to avoid the often computationally expensive algorithms relying on the $\ell_1$ norm. As a proof of concept, a background estimation model has been presented and compared with two $\ell_1$ norm minimization algorithms. We illustrate that as long as a simple weight matrix is inferred from the data, one can use the weighted Frobenius norm and achieve the same or better performance."
Are there any combinatorial formulas linking  omega class intersections to other classes?,6796d365,"[{'abstract': '  We provide a graph formula which describes an arbitrary monomial in {\\omega}\nclasses (also referred to as stable {\\psi} classes) in terms of a simple family\nof dual graphs (pinwheel graphs) with edges decorated by rational functions in\n{\\psi} classes. We deduce some numerical consequences and in particular a\ncombinatorial formula expressing top intersections of \\k{appa} classes on Mg in\nterms of top intersections of {\\psi} classes.\n', 'title': 'Intersections of $ω$ classes in $\\overline{\\mathcal{M}}_{g,n}$'}]","A great question!

After a quick search, I think I have found a paper that might be relevant to your question:

Title: Intersections of $ω$ classes in $\overline{\mathcal{M}}_{g,n}$

Abstract: We provide a graph formula which describes an arbitrary monomial in ω classes (also referred to as stable ψ classes) in terms of a simple family of dual graphs (pinwheel graphs) with edges decorated by rational functions in ψ classes. We deduce some numerical consequences and in particular a combinatorial formula expressing top intersections of κ classes on Mg in terms of top intersections of ψ classes.

This paper seems to provide a combinatorial formula linking ω class intersections to ψ classes, which might be what you're looking for!"
What is the primary advantage of using the PULSE method for feature discovery in music modeling?,6c996f7a,"[{'abstract': '  Predictive models for music are studied by researchers of algorithmic\ncomposition, the cognitive sciences and machine learning. They serve as base\nmodels for composition, can simulate human prediction and provide a\nmultidisciplinary application domain for learning algorithms. A particularly\nwell established and constantly advanced subtask is the prediction of\nmonophonic melodies. As melodies typically involve non-Markovian dependencies\ntheir prediction requires a capable learning algorithm. In this thesis, I apply\nthe recent feature discovery and learning method PULSE to the realm of symbolic\nmusic modeling. PULSE is comprised of a feature generating operation and\nL1-regularized optimization. These are used to iteratively expand and cull the\nfeature set, effectively exploring feature spaces that are too large for common\nfeature selection approaches. I design a general Python framework for PULSE,\npropose task-optimized feature generating operations and various\nmusic-theoretically motivated features that are evaluated on a standard corpus\nof monophonic folk and chorale melodies. The proposed method significantly\noutperforms comparable state-of-the-art models. I further discuss the free\nparameters of the learning algorithm and analyze the feature composition of the\nlearned models. The models learned by PULSE afford an easy inspection and are\nmusicologically interpreted for the first time.\n', 'title': 'Learning a Predictive Model for Music Using PULSE'}]","Title: Learning a Predictive Model for Music Using PULSE

Abstract: Predictive models for music are studied by researchers of algorithmic composition, the cognitive sciences and machine learning. They serve as base models for composition, can simulate human prediction and provide a multidisciplinary application domain for learning algorithms. A particularly well established and constantly advanced subtask is the prediction of monophonic melodies. As melodies typically involve non-Markovian dependencies their prediction requires a capable learning algorithm. In this thesis, I apply the recent feature discovery and learning method PULSE to the realm of symbolic music modeling. PULSE is comprised of a feature generating operation and L1-regularized optimization. These are used to iteratively expand and cull the feature set, effectively exploring feature spaces that are too large for common feature selection approaches. I design a general Python framework for PULSE, propose task-optimized feature generating operations and various music-theoretically motivated features that are evaluated on a standard corpus of monophonic folk and chorale melodies. The proposed method significantly outperforms comparable state-of-the-art models. I further discuss the free parameters of the learning algorithm and analyze the feature composition of the learned models. The models learned by PULSE afford an easy inspection and are musicologically interpreted for the first time.

The primary advantage of using the PULSE method for feature discovery in music modeling is that it can effectively explore feature spaces that are too large for common feature selection approaches, allowing for the discovery of music-theoretically motivated features that significantly outperform comparable state-of-the-art models."
"What key factor is essential for constructing effective controls in this scenario, and how are theoretical findings demonstrated numerically?",a6741407,"[{'abstract': '  Using a projection-based decoupling of the Fokker-Planck equation, control\nstrategies that allow to speed up the convergence to the stationary\ndistribution are investigated. By means of an operator theoretic framework for\na bilinear control system, two different feedback control laws are proposed.\nProjected Riccati and Lyapunov equations are derived and properties of the\nassociated solutions are given. The well-posedness of the closed loop systems\nis shown and local and global stabilization results, respectively, are\nobtained. An essential tool in the construction of the controls is the choice\nof appropriate control shape functions. Results for a two dimensional double\nwell potential illustrate the theoretical findings in a numerical setup.\n', 'title': 'Control Strategies for the Fokker-Planck Equation'}]","Title: Control Strategies for the Fokker-Planck Equation

Abstract:   Using a projection-based decoupling of the Fokker-Planck equation, control
strategies that allow to speed up the convergence to the stationary
distribution are investigated. By means of an operator theoretic framework for
a bilinear control system, two different feedback control laws are proposed.
Projected Riccati and Lyapunov equations are derived and properties of the
associated solutions are given. The well-posedness of the closed loop systems
is shown and local and global stabilization results, respectively, are
obtained. An essential tool in the construction of the controls is the choice
of appropriate control shape functions. Results for a two dimensional double
well potential illustrate the theoretical findings in a numerical setup.

The key factor essential for constructing effective controls in this scenario is the choice of appropriate control shape functions. Theoretical findings are demonstrated numerically through results for a two dimensional double well potential."
What are some limitations of Temporal Ensembling for training large datasets?,9b29cb1e,"[{'abstract': '  The recently proposed Temporal Ensembling has achieved state-of-the-art\nresults in several semi-supervised learning benchmarks. It maintains an\nexponential moving average of label predictions on each training example, and\npenalizes predictions that are inconsistent with this target. However, because\nthe targets change only once per epoch, Temporal Ensembling becomes unwieldy\nwhen learning large datasets. To overcome this problem, we propose Mean\nTeacher, a method that averages model weights instead of label predictions. As\nan additional benefit, Mean Teacher improves test accuracy and enables training\nwith fewer labels than Temporal Ensembling. Without changing the network\narchitecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250\nlabels, outperforming Temporal Ensembling trained with 1000 labels. We also\nshow that a good network architecture is crucial to performance. Combining Mean\nTeacher and Residual Networks, we improve the state of the art on CIFAR-10 with\n4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels\nfrom 35.24% to 9.11%.\n', 'title': 'Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results'}]","I'm happy to help you with your question! However, I must point out that none of the provided abstracts mention ""Temporal Ensembling"" or its limitations. It's possible that you may be thinking of a different concept or technique.

If you meant to ask about the limitations of a specific training method or technique mentioned in one of the abstracts, I'd be happy to help you with that. Alternatively, if you could provide more context or clarify your question, I'll do my best to assist you."
What are some examples of where the block bootstrap for means has been validated?,ac24635f,"[{'abstract': '  The block bootstrap approximates sampling distributions from dependent data\nby resampling data blocks. A fundamental problem is establishing its\nconsistency for the distribution of a sample mean, as a prototypical statistic.\nWe use a structural relationship with subsampling to characterize the bootstrap\nin a new and general manner. While subsampling and block bootstrap differ, the\nblock bootstrap distribution of a sample mean equals that of a $k$-fold\nself-convolution of a subsampling distribution. Motivated by this, we provide\nsimple necessary and sufficient conditions for a convolved subsampling\nestimator to produce a normal limit that matches the target of bootstrap\nestimation. These conditions may be linked to consistency properties of an\noriginal subsampling distribution, which are often obtainable under minimal\nassumptions. Through several examples, the results are shown to validate the\nblock bootstrap for means under significantly weakened assumptions in many\nexisting (and some new) dependence settings, which also addresses a standing\nconjecture of Politis, Romano and Wolf(1999). Beyond sample means, the\nconvolved subsampling estimator may not match the block bootstrap, but instead\nprovides a hybrid-resampling estimator of interest in its own right. For\ngeneral statistics with normal limits, results also establish the consistency\nof convolved subsampling under minimal dependence conditions, including\nnon-stationarity.\n', 'title': 'Convolved subsampling estimation with applications to block bootstrap'}]","Based on the context, I found the relevant information in the first abstract. Here is the answer:

Title: Convolved subsampling estimation with applications to block bootstrap

Abstract: The block bootstrap approximates sampling distributions from dependent data by resampling data blocks. A fundamental problem is establishing its consistency for the distribution of a sample mean, as a prototypical statistic. We use a structural relationship with subsampling to characterize the bootstrap in a new and general manner. While subsampling and block bootstrap differ, the block bootstrap distribution of a sample mean equals that of a $k$-fold self-convolution of a subsampling distribution. Motivated by this, we provide simple necessary and sufficient conditions for a convolved subsampling estimator to produce a normal limit that matches the target of bootstrap estimation. These conditions may be linked to consistency properties of an original subsampling distribution, which are often obtainable under minimal assumptions. Through several examples, the results are shown to validate the block bootstrap for means under significantly weakened assumptions in many existing (and some new) dependence settings, which also addresses a standing conjecture of Politis, Romano and Wolf(1999).

The examples mentioned in the abstract are ""many existing (and some new) dependence settings"", but unfortunately, no specific examples are mentioned."
What type of magnetic structure is observed in uni-axial chiral magnets near their transition temperatures in low magnetic fields?,2c75f6b1,"[{'abstract': '  We investigate the spin structure of a uni-axial chiral magnet near the\ntransition temperatures in low fields perpendicular to the helical axis. We\nfind a fan-type modulation structure where the clockwise and counterclockwise\nwindings appear alternatively along the propagation direction of the modulation\nstructure. This structure is often realized in a Yoshimori-type (non-chiral)\nhelimagnet but it is rarely realized in a chiral helimagnet. To discuss\nunderlying physics of this structure, we reconsider the phase diagram (phase\nboundary and crossover lines) through the free energy and asymptotic behaviors\nof isolated solitons. The fan structure appears slightly below the phase\nboundary of the continuous transition of instability-type. In this region,\nthere are no solutions containing any types of isolated solitons to the mean\nfield equations.\n', 'title': 'Fan-type spin structure in uni-axial chiral magnets'}]","Title: Fan-type spin structure in uni-axial chiral magnets

Abstract: We investigate the spin structure of a uni-axial chiral magnet near the transition temperatures in low fields perpendicular to the helical axis. We find a fan-type modulation structure where the clockwise and counterclockwise windings appear alternatively along the propagation direction of the modulation structure. This structure is often realized in a Yoshimori-type (non-chiral) helimagnet but it is rarely realized in a chiral helimagnet. To discuss underlying physics of this structure, we reconsider the phase diagram (phase boundary and crossover lines) through the free energy and asymptotic behaviors of isolated solitons. The fan structure appears slightly below the phase boundary of the continuous transition of instability-type. In this region, there are no solutions containing any types of isolated solitons to the mean field equations."
How does a hybrid approach that combines images and videos improve video source identification?,9177be94,"[{'abstract': '  Multimedia Forensics allows to determine whether videos or images have been\ncaptured with the same device, and thus, eventually, by the same person.\nCurrently, the most promising technology to achieve this task, exploits the\nunique traces left by the camera sensor into the visual content. Anyway, image\nand video source identification are still treated separately from one another.\nThis approach is limited and anachronistic if we consider that most of the\nvisual media are today acquired using smartphones, that capture both images and\nvideos. In this paper we overcome this limitation by exploring a new approach\nthat allows to synergistically exploit images and videos to study the device\nfrom which they both come. Indeed, we prove it is possible to identify the\nsource of a digital video by exploiting a reference sensor pattern noise\ngenerated from still images taken by the same device of the query video. The\nproposed method provides comparable or even better performance, when compared\nto the current video identification strategies, where a reference pattern is\nestimated from video frames. We also show how this strategy can be effective\neven in case of in-camera digitally stabilized videos, where a non-stabilized\nreference is not available, by solving some state-of-the-art limitations. We\nexplore a possible direct application of this result, that is social media\nprofile linking, i.e. discovering relationships between two or more social\nmedia profiles by comparing the visual contents - images or videos - shared\ntherein.\n', 'title': 'A Hybrid Approach to Video Source Identification'}]","Title: A Hybrid Approach to Video Source Identification

Abstract: Multimedia Forensics allows to determine whether videos or images have been
captured with the same device, and thus, eventually, by the same person.
Currently, the most promising technology to achieve this task, exploits the
unique traces left by the camera sensor into the visual content. Anyway, image
and video source identification are still treated separately from one another.
This approach is limited and anachronistic if we consider that most of the
visual media are today acquired using smartphones, that capture both images and
videos. In this paper we overcome this limitation by exploring a new approach
that allows to synergistically exploit images and videos to study the device
from which they both come. Indeed, we prove it is possible to identify the
source of a digital video by exploiting a reference sensor pattern noise
generated from still images taken by the same device of the query video. The
proposed method provides comparable or even better performance, when compared
to the current video identification strategies, where a reference pattern is
estimated from video frames. We also show how this strategy can be effective
even in case of in-camera digitally stabilized videos, where a non-stabilized
reference is not available, by solving some state-of-the-art limitations. We
explore a possible direct application of this result, that is social media
profile linking, i.e. discovering relationships between two or more social
media profiles by comparing the visual contents - images or videos - shared
therein."
What conditions on inclusion probabilities ensure consistency of the joint posterior in Bayesian estimation under informative sampling?,dc837bc0,"[{'abstract': '  Bayesian estimation is increasingly popular for performing model based\ninference to support policymaking. These data are often collected from surveys\nunder informative sampling designs where subject inclusion probabilities are\ndesigned to be correlated with the response variable of interest. Sampling\nweights constructed from marginal inclusion probabilities are typically used to\nform an exponentiated pseudo likelihood that adjusts the population likelihood\nfor estimation on the sample due to ease-of-estimation. We propose an\nalternative adjustment based on a Bayes rule construction that simultaneously\nperforms weight smoothing and estimates the population model parameters in a\nfully Bayesian construction. We formulate conditions on known marginal and\npairwise inclusion probabilities that define a class of sampling designs where\n$L_{1}$ consistency of the joint posterior is guaranteed. We compare\nperformances between the two approaches on synthetic data, which reveals that\nour fully Bayesian approach better estimates posterior uncertainty without a\nrequirement to calibrate the normalization of the sampling weights. We\ndemonstrate our method on an application concerning the National Health and\nNutrition Examination Survey exploring the relationship between caffeine\nconsumption and systolic blood pressure.\n', 'title': 'Fully Bayesian Estimation Under Informative Sampling'}]","Title: Fully Bayesian Estimation Under Informative Sampling

Abstract: Bayesian estimation is increasingly popular for performing model based inference to support policymaking. These data are often collected from surveys under informative sampling designs where subject inclusion probabilities are designed to be correlated with the response variable of interest. Sampling weights constructed from marginal inclusion probabilities are typically used to form an exponentiated pseudo likelihood that adjusts the population likelihood for estimation on the sample due to ease-of-estimation. We propose an alternative adjustment based on a Bayes rule construction that simultaneously performs weight smoothing and estimates the population model parameters in a fully Bayesian construction. We formulate conditions on known marginal and pairwise inclusion probabilities that define a class of sampling designs where $L_{1}$ consistency of the joint posterior is guaranteed. We compare performances between the two approaches on synthetic data, which reveals that our fully Bayesian approach better estimates posterior uncertainty without a requirement to calibrate the normalization of the sampling weights. We demonstrate our method on an application concerning the National Health and Nutrition Examination Survey exploring the relationship between caffeine consumption and systolic blood pressure."
What is the degree condition  for the bipartite graph considered in this paper?,ad0134d3,"[{'abstract': ""  This paper is the first chapter of three of the author's undergraduate\nthesis. We study the random matrix ensemble of covariance matrices arising from\nrandom $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and\n$N$ white vertices, for $d_b \\gg \\log^4 N$. We simultaneously prove that the\nGreen's functions of these covariance matrices and the adjacency matrices of\nthe underlying graphs agree with the corresponding limiting law (e.g.\nMarchenko-Pastur law for covariance matrices) down to the optimal scale. This\nis an improvement from the previously known mesoscopic results. We obtain\neigenvector delocalization for the covariance matrix ensemble as consequence,\nas well as a weak rigidity estimate.\n"", 'title': 'Local Marchenko-Pastur Law for Random Bipartite Graphs'}]","Title: Local Marchenko-Pastur Law for Random Bipartite Graphs

Abstract: This paper is the first chapter of three of the author's undergraduate thesis. We study the random matrix ensemble of covariance matrices arising from random $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and $N$ white vertices, for $d_b \gg \log^4 N$. We simultaneously prove that the Green's functions of these covariance matrices and the adjacency matrices of the underlying graphs agree with the corresponding limiting law (e.g. Marchenko-Pastur law for covariance matrices) down to the optimal scale. This is an improvement from the previously known mesoscopic results. We obtain eigenvector delocalization for the covariance matrix ensemble as consequence, as well as a weak rigidity estimate.

The degree condition for the bipartite graph considered in this paper is $d_b \gg \log^4 N$."
What are the challenges of controlling a 2U cubesat's attitude?,6a470a56,"[{'abstract': ""  This paper describes the development of a magnetic attitude control subsystem\nfor a 2U cubesat. Due to the presence of gravity gradient torques, the\nsatellite dynamics are open-loop unstable near the desired pointing\nconfiguration. Nevertheless the linearized time-varying system is completely\ncontrollable, under easily verifiable conditions, and the system's disturbance\nrejection capabilities can be enhanced by adding air drag panels exemplifying a\nbeneficial interplay between hardware design and control. In the paper,\nconditions for the complete controllability for the case of a magnetically\ncontrolled satellite with passive air drag panels are developed, and simulation\ncase studies with the LQR and MPC control designs applied in combination with a\nnonlinear time-varying input transformation are presented to demonstrate the\nability of the closed-loop system to satisfy mission objectives despite\ndisturbance torques.\n"", 'title': 'Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques'}]","Title: Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques

Abstract: This paper describes the development of a magnetic attitude control subsystem for a 2U cubesat. Due to the presence of gravity gradient torques, the satellite dynamics are open-loop unstable near the desired pointing configuration. Nevertheless the linearized time-varying system is completely controllable, under easily verifiable conditions, and the system's disturbance rejection capabilities can be enhanced by adding air drag panels exemplifying a beneficial interplay between hardware design and control. In the paper, conditions for the complete controllability for the case of a magnetically controlled satellite with passive air drag panels are developed, and simulation case studies with the LQR and MPC control designs applied in combination with a nonlinear time-varying input transformation are presented to demonstrate the ability of the closed-loop system to satisfy mission objectives despite disturbance torques.

The challenges of controlling a 2U cubesat's attitude mentioned in this paper are:

* The presence of gravity gradient torques, making the satellite dynamics open-loop unstable near the desired pointing configuration."
What specific technique is used in this  meta-algorithm to optimize network components?,4d057a8f,"[{'abstract': '  The life of the modern world essentially depends on the work of the large\nartificial homogeneous networks, such as wired and wireless communication\nsystems, networks of roads and pipelines. The support of their effective\ncontinuous functioning requires automatic screening and permanent optimization\nwith processing of the huge amount of data by high-performance distributed\nsystems. We propose new meta-algorithm of large homogeneous network analysis,\nits decomposition into alternative sets of loosely connected subnets, and\nparallel optimization of the most independent elements. This algorithm is based\non a network-specific correlation function, Simulated Annealing technique, and\nis adapted to work in the computer cluster. On the example of large wireless\nnetwork, we show that proposed algorithm essentially increases speed of\nparallel optimization. The elaborated general approach can be used for analysis\nand optimization of the wide range of networks, including such specific types\nas artificial neural networks or organized in networks physiological systems of\nliving organisms.\n', 'title': 'Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks'}]","Title: Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks

Abstract:  The life of the modern world essentially depends on the work of the large
artificial homogeneous networks, such as wired and wireless communication
systems, networks of roads and pipelines. The support of their effective
continuous functioning requires automatic screening and permanent optimization
with processing of the huge amount of data by high-performance distributed
systems. We propose new meta-algorithm of large homogeneous network analysis,
its decomposition into alternative sets of loosely connected subnets, and
parallel optimization of the most independent elements. This algorithm is based
on a network-specific correlation function, Simulated Annealing technique, and
is adapted to work in the computer cluster. On the example of large wireless
network, we show that proposed algorithm essentially increases speed of
parallel optimization. The elaborated general approach can be used for analysis
and optimization of the wide range of networks, including such specific types
as artificial neural networks or organized in networks physiological systems of
living organisms.

The specific technique used in this meta-algorithm to optimize network components is the Simulated Annealing technique."
How does this framework combine discriminative and generative models?,21d71280,"[{'abstract': '  We describe a novel weakly supervised deep learning framework that combines\nboth the discriminative and generative models to learn meaningful\nrepresentation in the multiple instance learning (MIL) setting. MIL is a weakly\nsupervised learning problem where labels are associated with groups of\ninstances (referred as bags) instead of individual instances. To address the\nessential challenge in MIL problems raised from the uncertainty of positive\ninstances label, we use a discriminative model regularized by variational\nautoencoders (VAEs) to maximize the differences between latent representations\nof all instances and negative instances. As a result, the hidden layer of the\nvariational autoencoder learns meaningful representation. This representation\ncan effectively be used for MIL problems as illustrated by better performance\non the standard benchmark datasets comparing to the state-of-the-art\napproaches. More importantly, unlike most related studies, the proposed\nframework can be easily scaled to large dataset problems, as illustrated by the\naudio event detection and segmentation task. Visualization also confirms the\neffectiveness of the latent representation in discriminating positive and\nnegative classes.\n', 'title': 'Deep Multiple Instance Feature Learning via Variational Autoencoder'}]","Based on the provided abstracts, I found that the framework that combines discriminative and generative models is:

Title: Bayesian Semisupervised Learning with Deep Generative Models

Abstract: Neural network based generative models with discriminative components are a powerful approach for semi-supervised learning. However, these techniques a) cannot account for model uncertainty in the estimation of the model's discriminative component and b) lack flexibility to capture complex stochastic patterns in the label generation process. To avoid these problems, we first propose to use a discriminative component with stochastic inputs for increased noise flexibility. We show how an efficient Gibbs sampling procedure can marginalize the stochastic inputs when inferring missing labels in this model. Following this, we extend the discriminative component to be fully Bayesian and produce estimates of uncertainty in its parameter values. This opens the door for semi-supervised Bayesian active learning."
What are reductions in the context of comparing system models for bisimulations?,0d579552,"[{'abstract': '  Reductions for transition systems have been recently introduced as a uniform\nand principled method for comparing the expressiveness of system models with\nrespect to a range of properties, especially bisimulations. In this paper we\nstudy the expressiveness (w.r.t. bisimulations) of models for quantitative\ncomputations such as weighted labelled transition systems (WLTSs), uniform\nlabelled transition systems (ULTraSs), and state-to-function transition systems\n(FuTSs). We prove that there is a trade-off between labels and weights: at one\nextreme lays the class of (unlabelled) weighted transition systems where\ninformation is presented using weights only; at the other lays the class of\nlabelled transition systems (LTSs) where information is shifted on labels.\nThese categories of systems cannot be further reduced in any significant way\nand subsume all the aforementioned models.\n', 'title': 'On the trade-off between labels and weights in quantitative bisimulation'}]","Title: On the trade-off between labels and weights in quantitative bisimulation

Abstract: Reductions for transition systems have been recently introduced as a uniform and principled method for comparing the expressiveness of system models with respect to a range of properties, especially bisimulations. In this paper we study the expressiveness (w.r.t. bisimulations) of models for quantitative computations such as weighted labelled transition systems (WLTSs), uniform labelled transition systems (ULTraSs), and state-to-function transition systems (FuTSs). We prove that there is a trade-off between labels and weights: at one extreme lays the class of (unlabelled) weighted transition systems where information is presented using weights only; at the other lays the class of labelled transition systems (LTSs) where information is shifted on labels. These categories of systems cannot be further reduced in any significant way and subsume all the aforementioned models."
The authors mention three approximations for calculating caloric properties. What are some examples of these approximations and how do they affect the results?,a36f168e,"[{'abstract': '  The calculation of caloric properties such as heat capacity, Joule-Thomson\ncoefficients and the speed of sound by classical force-field-based molecular\nsimulation methodology has received scant attention in the literature,\nparticularly for systems composed of complex molecules whose force fields (FFs)\nare characterized by a combination of intramolecular and intermolecular terms\n(referred to herein as ""flexible FFs""). The calculation of a thermodynamic\nproperty for a system whose molecules are described by such a FF involves the\ncalculation of the residual property prior to its addition to the corresponding\nideal-gas (IG) property, the latter of which is separately calculated, either\nusing thermochemical compilations or nowadays accurate quantum mechanical\ncalculations. Although the simulation of a volumetric residual property\nproceeds by simply replacing the intermolecular FF in the rigid molecule case\nby the total (intramolecular plus intermolecular) FF, this is not the case for\na caloric property. We discuss the methodology required in performing such\ncalculations, and focus on the example of the molar heat capacity at constant\npressure, $c_P$, one of the most important caloric properties. We also consider\nthree approximations for the calculation procedure, and illustrate their\nconsequences for the examples of the relatively simple molecule 2-propanol,\n${\\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\\rm HO(CH_2)_2NH_2}$, an\nimportant fluid used in carbon capture.\n', 'title': 'Molecular Simulation of Caloric Properties of Fluids Modelled by Force Fields with Intramolecular Contributions: Application to Heat Capacities'}]","Title: Molecular Simulation of Caloric Properties of Fluids Modelled by Force Fields with Intramolecular Contributions: Application to Heat Capacities

Abstract:   The calculation of caloric properties such as heat capacity, Joule-Thomson
coefficients and the speed of sound by classical force-field-based molecular
simulation methodology has received scant attention in the literature,
particularly for systems composed of complex molecules whose force fields (FFs)
are characterized by a combination of intramolecular and intermolecular terms
(referred to herein as ""flexible FFs""). The calculation of a thermodynamic
property for a system whose molecules are described by such a FF involves the
calculation of the residual property prior to its addition to the corresponding
ideal-gas (IG) property, the latter of which is separately calculated, either
using thermochemical compilations or nowadays accurate quantum mechanical
calculations. Although the simulation of a volumetric residual property
proceeds by simply replacing the intermolecular FF in the rigid molecule case
by the total (intramolecular plus intermolecular) FF, this is not the case for
a caloric property. We discuss the methodology required in performing such
calculations, and focus on the example of the molar heat capacity at constant
pressure, $c_P$, one of the most important caloric properties. We also consider
three approximations for the calculation procedure, and illustrate their
consequences for the examples of the relatively simple molecule 2-propanol,
${\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\rm HO(CH_2)_2NH_2}$, an
important fluid used in carbon capture.

The three approximations for calculating caloric properties are not explicitly stated in the abstract, but it mentions that the authors consider them and illustrate their consequences for the examples of 2-propanol and monoethanolamine. However, it does not provide further details about the approximations themselves."
What is the  resilience of the proposed susceptibility propagation approach across different network structures?,cfdce7e2,"[{'abstract': '  A susceptibility propagation that is constructed by combining a belief\npropagation and a linear response method is used for approximate computation\nfor Markov random fields. Herein, we formulate a new, improved susceptibility\npropagation by using the concept of a diagonal matching method that is based on\nmean-field approaches to inverse Ising problems. The proposed susceptibility\npropagation is robust for various network structures, and it is reduced to the\nordinary susceptibility propagation and to the adaptive\nThouless-Anderson-Palmer equation in special cases.\n', 'title': 'Susceptibility Propagation by Using Diagonal Consistency'}]","The answer to your question can be found in the first abstract. 

Title: Susceptibility Propagation by Using Diagonal Consistency

Abstract: A susceptibility propagation that is constructed by combining a belief propagation and a linear response method is used for approximate computation for Markov random fields. Herein, we formulate a new, improved susceptibility propagation by using the concept of a diagonal matching method that is based on mean-field approaches to inverse Ising problems. The proposed susceptibility propagation is robust for various network structures, and it is reduced to the ordinary susceptibility propagation and to the adaptive Thouless-Anderson-Palmer equation in special cases.

According to this abstract, the proposed susceptibility propagation approach is ""robust for various network structures"", which implies that it is resilient across different network structures."
What is a novel deep learning framework for multiple instance learning?,21d71280,"[{'abstract': '  We describe a novel weakly supervised deep learning framework that combines\nboth the discriminative and generative models to learn meaningful\nrepresentation in the multiple instance learning (MIL) setting. MIL is a weakly\nsupervised learning problem where labels are associated with groups of\ninstances (referred as bags) instead of individual instances. To address the\nessential challenge in MIL problems raised from the uncertainty of positive\ninstances label, we use a discriminative model regularized by variational\nautoencoders (VAEs) to maximize the differences between latent representations\nof all instances and negative instances. As a result, the hidden layer of the\nvariational autoencoder learns meaningful representation. This representation\ncan effectively be used for MIL problems as illustrated by better performance\non the standard benchmark datasets comparing to the state-of-the-art\napproaches. More importantly, unlike most related studies, the proposed\nframework can be easily scaled to large dataset problems, as illustrated by the\naudio event detection and segmentation task. Visualization also confirms the\neffectiveness of the latent representation in discriminating positive and\nnegative classes.\n', 'title': 'Deep Multiple Instance Feature Learning via Variational Autoencoder'}]","Title: Deep Multiple Instance Feature Learning via Variational Autoencoder

Abstract: We describe a novel weakly supervised deep learning framework that combines both the discriminative and generative models to learn meaningful representation in the multiple instance learning (MIL) setting. MIL is a weakly supervised learning problem where labels are associated with groups of instances (referred as bags) instead of individual instances. To address the essential challenge in MIL problems raised from the uncertainty of positive instances label, we use a discriminative model regularized by variational autoencoders (VAEs) to maximize the differences between latent representations of all instances and negative instances. As a result, the hidden layer of the variational autoencoder learns meaningful representation. This representation can effectively be used for MIL problems as illustrated by better performance on the standard benchmark datasets comparing to the state-of-the-art approaches. More importantly, unlike most related studies, the proposed framework can be easily scaled to large dataset problems, as illustrated by the audio event detection and segmentation task. Visualization also confirms the effectiveness of the latent representation in discriminating positive and negative classes."
How do magnetic field and temperature affect the dynamics of EMP formation in these quantum wells?,955d5312,"[{'abstract': '  We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se\ndiluted-magnetic-semiconductor quantum wells using time-resolved\nphotoluminescence (PL). The magnetic field and temperature dependencies of this\ndynamics allow us to separate the non-magnetic and magnetic contributions to\nthe exciton localization. We deduce the EMP energy of 14 meV, which is in\nagreement with time-integrated measurements based on selective excitation and\nthe magnetic field dependence of the PL circular polarization degree. The\npolaron formation time of 500 ps is significantly longer than the corresponding\nvalues reported earlier. We propose that this behavior is related to strong\nself-localization of the EMP, accompanied with a squeezing of the heavy-hole\nenvelope wavefunction. This conclusion is also supported by the decrease of the\nexciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and\ntemperature.\n', 'title': 'Dynamics of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells: the effect of self-localization'}]","Title: Dynamics of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells: the effect of self-localization

Abstract:   We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se
diluted-magnetic-semiconductor quantum wells using time-resolved
photoluminescence (PL). The magnetic field and temperature dependencies of this
dynamics allow us to separate the non-magnetic and magnetic contributions to
the exciton localization. We deduce the EMP energy of 14 meV, which is in
agreement with time-integrated measurements based on selective excitation and
the magnetic field dependence of the PL circular polarization degree. The
polaron formation time of 500 ps is significantly longer than the corresponding
values reported earlier. We propose that this behavior is related to strong
self-localization of the EMP, accompanied with a squeezing of the heavy-hole
envelope wavefunction. This conclusion is also supported by the decrease of the
exciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and
temperature."
What are some techniques used to enhance the accuracy of single-trial classification of P300 responses?,5bdb0297,"[{'abstract': ""  The P300 event-related potential (ERP), evoked in scalp-recorded\nelectroencephalography (EEG) by external stimuli, has proven to be a reliable\nresponse for controlling a BCI. The P300 component of an event related\npotential is thus widely used in brain-computer interfaces to translate the\nsubjects' intent by mere thoughts into commands to control artificial devices.\nThe main challenge in the classification of P300 trials in\nelectroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of\nthe P300 response. To overcome the low SNR of individual trials, it is common\npractice to average together many consecutive trials, which effectively\ndiminishes the random noise. Unfortunately, when more repeated trials are\nrequired for applications such as the P300 speller, the communication rate is\ngreatly reduced. This has resulted in a need for better methods to improve\nsingle-trial classification accuracy of P300 response. In this work, we use\nPrincipal Component Analysis (PCA) as a preprocessing method and use Linear\nDiscriminant Analysis (LDA)and neural networks for classification. The results\nshow that a combination of PCA with these methods provided as high as 13\\%\naccuracy gain for single-trial classification while using only 3 to 4 principal\ncomponents.\n"", 'title': 'Single-trial P300 Classification using PCA with LDA, QDA and Neural Networks'}]","Title: Single-trial P300 Classification using PCA with LDA, QDA and Neural Networks

Abstract: The P300 event-related potential (ERP), evoked in scalp-recorded electroencephalography (EEG) by external stimuli, has proven to be a reliable response for controlling a BCI. The P300 component of an event related potential is thus widely used in brain-computer interfaces to translate the subjects' intent by mere thoughts into commands to control artificial devices. The main challenge in the classification of P300 trials in electroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of the P300 response. To overcome the low SNR of individual trials, it is common practice to average together many consecutive trials, which effectively diminishes the random noise. Unfortunately, when more repeated trials are required for applications such as the P300 speller, the communication rate is greatly reduced. This has resulted in a need for better methods to improve single-trial classification accuracy of P300 response. In this work, we use Principal Component Analysis (PCA) as a preprocessing method and use Linear Discriminant Analysis (LDA) and neural networks for classification. The results show that a combination of PCA with these methods provided as high as 13% accuracy gain for single-trial classification while using only 3 to 4 principal components."
How do different weight initialization methods affect the performance of a neural network on the MNIST task?,7b97d455,"[{'abstract': '  We provide an overview of several non-linear activation functions in a neural\nnetwork architecture that have proven successful in many machine learning\napplications. We conduct an empirical analysis on the effectiveness of using\nthese function on the MNIST classification task, with the aim of clarifying\nwhich functions produce the best results overall. Based on this first set of\nresults, we examine the effects of building deeper architectures with an\nincreasing number of hidden layers. We also survey the impact of using, on the\nsame task, different initialisation schemes for the weights of our neural\nnetwork. Using these sets of experiments as a base, we conclude by providing a\noptimal neural network architecture that yields impressive results in accuracy\non the MNIST classification task.\n', 'title': 'Empirical analysis of non-linear activation functions for Deep Neural Networks in classification tasks'}]","I couldn't find a direct answer to how different weight initialization methods affect the performance of a neural network on the MNIST task. However, I found a paper that is related to weight initialization:

Title: Invariance of Weight Distributions in Rectified MLPs

Abstract: An interesting approach to analyzing neural networks that has received renewed attention is to examine the equivalent kernel of the neural network. This is based on the fact that a fully connected feedforward network with one hidden layer, a certain weight distribution, an activation function, and an infinite number of neurons can be viewed as a mapping into a Hilbert space. We derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for all rotationally-invariant weight distributions, generalizing a previous result that required Gaussian weight distributions. Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresponding to layers with weight distributions having $0$ mean and finite absolute third moment are asymptotically universal, and are well approximated by the kernel corresponding to layers with spherical Gaussian weights. In deep networks, as depth increases the equivalent kernel approaches a pathological fixed point, which can be used to argue why training randomly initialized networks can be difficult. Our results also have implications for weight initialization.

This paper discusses the invariance of weight distributions in rectified MLPs and has implications for weight initialization, but it does not specifically discuss the effect of different weight initialization methods on the performance of a neural network on the MNIST task."
What are the rotational quantities that are influenced by a viscoelastic crust?,b5fb0ab9,"[{'abstract': '  Several natural satellites of the giant planets have shown evidence of a\nglobal internal ocean, coated by a thin, icy crust. This crust is probably\nviscoelastic, which would alter its rotational response. This response would\ntranslate into several rotational quantities, i.e. the obliquity, and the\nlibrations at different frequencies, for which the crustal elasticity reacts\ndifferently. This study aims at modelling the global response of the\nviscoelastic crust. For that, I derive the time-dependency of the tensor of\ninertia, which I combine with the time evolution of the rotational quantities,\nthanks to an iterative algorithm. This algorithm combines numerical simulations\nof the rotation with a digital filtering of the resulting tensor of inertia.\nThe algorithm works very well in the elastic case, provided the problem is not\nresonant. However, considering tidal dissipation adds different phase lags to\nthe oscillating contributions, which challenge the convergence of the\nalgorithm.\n', 'title': 'Rotation of a synchronous viscoelastic shell'}]","Title: Rotation of a synchronous viscoelastic shell

Abstract: Several natural satellites of the giant planets have shown evidence of a
global internal ocean, coated by a thin, icy crust. This crust is probably
viscoelastic, which would alter its rotational response. This response would
translate into several rotational quantities, i.e. the obliquity, and the
librations at different frequencies, for which the crustal elasticity reacts
differently. This study aims at modelling the global response of the
viscoelastic crust. For that, I derive the time-dependency of the tensor of
inertia, which I combine with the time evolution of the rotational quantities,
thanks to an iterative algorithm. This algorithm combines numerical simulations
of the rotation with a digital filtering of the resulting tensor of inertia.
The algorithm works very well in the elastic case, provided the problem is not
resonant. However, considering tidal dissipation adds different phase lags to
the oscillating contributions, which challenge the convergence of the
algorithm.

The rotational quantities that are influenced by a viscoelastic crust are the obliquity and librations at different frequencies."
What is meant by the term 'isomorphism'  when applied to classical games?,2cda7fce,"[{'abstract': ""  Our purpose is to focus attention on a new criterion for quantum schemes by\nbringing together the notions of quantum game and game isomorphism. A quantum\ngame scheme is required to generate the classical game as a special case. Now,\ngiven a quantum game scheme and two isomorphic classical games, we additionally\nrequire the resulting quantum games to be isomorphic as well. We show how this\nisomorphism condition influences the players' strategy sets. We are concerned\nwith the Marinatto-Weber type quantum game scheme and the strong isomorphism\nbetween games in strategic form.\n"", 'title': 'Strong isomorphism in Marinatto-Weber type quantum games'}]","Based on the context, I think I can help you with your question.

From the abstract of ""Strong isomorphism in Marinatto-Weber type quantum games"", it seems that the term 'isomorphism' refers to a condition where two classical games are considered equivalent in some sense.

Title: Strong isomorphism in Marinatto-Weber type quantum games

Abstract: Our purpose is to focus attention on a new criterion for quantum schemes by bringing together the notions of quantum game and game isomorphism. A quantum game scheme is required to generate the classical game as a special case. Now, given a quantum game scheme and two isomorphic classical games, we additionally require the resulting quantum games to be isomorphic as well. We show how this isomorphism condition influences the players' strategy sets. We are concerned with the Marinatto-Weber type quantum game scheme and the strong isomorphism between games in strategic form.

Please let me know if this answers your question!"
What are some possible explanations for the observed bluing during transits of WD1145+017?,9bc0b2c8,"[{'abstract': ""  The first transiting planetesimal orbiting a white dwarf was recently\ndetected in K2 data of WD1145+017 and has been followed up intensively. The\nmultiple, long, and variable transits suggest the transiting objects are dust\nclouds, probably produced by a disintegrating asteroid. In addition, the system\ncontains circumstellar gas, evident by broad absorption lines, mostly in the\nu'-band, and a dust disc, indicated by an infrared excess. Here we present the\nfirst detection of a change in colour of WD1145+017 during transits, using\nsimultaneous multi-band fast-photometry ULTRACAM measurements over the\nu'g'r'i'-bands. The observations reveal what appears to be 'bluing' during\ntransits; transits are deeper in the redder bands, with a u'-r' colour\ndifference of up to ~-0.05 mag. We explore various possible explanations for\nthe bluing. 'Spectral' photometry obtained by integrating over bandpasses in\nthe spectroscopic data in- and out-of-transit, compared to the photometric\ndata, shows that the observed colour difference is most likely the result of\nreduced circumstellar absorption in the spectrum during transits. This\nindicates that the transiting objects and the gas share the same line-of-sight,\nand that the gas covers the white dwarf only partially, as would be expected if\nthe gas, the transiting debris, and the dust emitting the infrared excess, are\npart of the same general disc structure (although possibly at different radii).\nIn addition, we present the results of a week-long monitoring campaign of the\nsystem.\n"", 'title': ""Once in a blue moon: detection of 'bluing' during debris transits in the white dwarf WD1145+017""}]","The researcher is looking for possible explanations for the observed ""bluing"" during transits of WD1145+017.

According to the abstract of the paper ""Once in a blue moon: detection of 'bluing' during debris transits in the white dwarf WD1145+017"", one possible explanation for the observed bluing is the reduced circumstellar absorption in the spectrum during transits.

Here is the relevant abstract:

Title: Once in a blue moon: detection of 'bluing' during debris transits in the white dwarf WD1145+017

Abstract: The first transiting planetesimal orbiting a white dwarf was recently
detected in K2 data of WD1145+017 and has been followed up intensively. The
multiple, long, and variable transits suggest the transiting objects are dust
clouds, probably produced by a disintegrating asteroid. In addition, the system
contains circumstellar gas, evident by broad absorption lines, mostly in the
u'-band, and a dust disc, indicated by an infrared excess. Here we present the
first detection of a change in colour of WD1145+017 during transits, using
simultaneous multi-band fast-photometry ULTRACAM measurements over the
u'g'r'i'-bands. The observations reveal what appears to be 'bluing' during
transits; transits are deeper in the redder bands, with a u'-r' colour
difference of up to ~-0.05 mag. We explore various possible explanations for
the bluing. 'Spectral' photometry obtained by integrating over bandpasses in
the spectroscopic data in- and out-of-transit, compared to the photometric
data, shows that the observed colour difference is most likely the result of
reduced circumstellar absorption in the spectrum during transits. This
indicates that the transiting objects and the gas share the same line-of-sight,
and that the gas covers the white dwarf only partially, as would be expected if
the gas, the transiting debris, and the dust emitting the infrared excess, are
part of the same general disc structure (although possibly at different radii).
In addition, we present the results of a week-long monitoring campaign of the
system."
What is the role of transition probabilities in determining the capacity of a communication channel?,9925f427,"[{'abstract': '  Muroga [M52] showed how to express the Shannon channel capacity of a discrete\nchannel with noise [S49] as an explicit function of the transition\nprobabilities. His method accommodates channels with any finite number of input\nsymbols, any finite number of output symbols and any transition probability\nmatrix. Silverman [S55] carried out Muroga\'s method in the special case of a\nbinary channel (and went on to analyse ""cascades"" of several such binary\nchannels).\nThis article is a note on the resulting formula for the capacity C(a, c) of a\nsingle binary channel. We aim to clarify some of the arguments and correct a\nsmall error. In service of this aim, we first formulate several of Shannon\'s\ndefinitions and proofs in terms of discrete measure-theoretic probability\ntheory. We provide an alternate proof to Silverman\'s, of the feasibility of the\noptimal input distribution for a binary channel. For convenience, we also\nexpress C(a, c) in a single expression explicitly dependent on a and c only,\nwhich Silverman stopped short of doing.\n', 'title': 'On the Computation of the Shannon Capacity of a Discrete Channel with Noise'}]","Based on the context, I found a relevant paper that might answer your question.

Title: On the Computation of the Shannon Capacity of a Discrete Channel with Noise

Abstract:   Muroga [M52] showed how to express the Shannon channel capacity of a discrete
channel with noise [S49] as an explicit function of the transition
probabilities. His method accommodates channels with any finite number of input
symbols, any finite number of output symbols and any transition probability
matrix. Silverman [S55] carried out Muroga's method in the special case of a
binary channel (and went on to analyse ""cascades"" of several such binary
channels).
This article is a note on the resulting formula for the capacity C(a, c) of a
single binary channel. We aim to clarify some of the arguments and correct a
small error. In service of this aim, we first formulate several of Shannon's
definitions and proofs in terms of discrete measure-theoretic probability
theory. We provide an alternate proof to Silverman's, of the feasibility of the
optimal input distribution for a binary channel. For convenience, we also
express C(a, c) in a single expression explicitly dependent on a and c only,
which Silverman stopped short of doing.

This paper suggests that transition probabilities play a crucial role in determining the capacity of a communication channel, as they can be used to express the Shannon channel capacity as an explicit function."
How does random matrix theory help analyze the performance of random feature maps in machine learning?,7fbd7879,"[{'abstract': '  Random feature maps are ubiquitous in modern statistical machine learning,\nwhere they generalize random projections by means of powerful, yet often\ndifficult to analyze nonlinear operators. In this paper, we leverage the\n""concentration"" phenomenon induced by random matrix theory to perform a\nspectral analysis on the Gram matrix of these random feature maps, here for\nGaussian mixture models of simultaneously large dimension and size. Our results\nare instrumental to a deeper understanding on the interplay of the nonlinearity\nand the statistics of the data, thereby allowing for a better tuning of random\nfeature-based techniques.\n', 'title': 'On the Spectrum of Random Features Maps of High Dimensional Data'}]","Title: On the Spectrum of Random Features Maps of High Dimensional Data

Abstract: Random feature maps are ubiquitous in modern statistical machine learning,
where they generalize random projections by means of powerful, yet often
difficult to analyze nonlinear operators. In this paper, we leverage the
""concentration"" phenomenon induced by random matrix theory to perform a
spectral analysis on the Gram matrix of these random feature maps, here for
Gaussian mixture models of simultaneously large dimension and size. Our results
are instrumental to a deeper understanding on the interplay of the nonlinearity
and the statistics of the data, thereby allowing for a better tuning of random
feature-based techniques."
"In the context of multi-block transmission, what are the upper bounds on performance achievable by energy harvesting receivers and the conditions required to meet these bounds? ",0bae3ed7,"[{'abstract': '  The difficulty of modeling energy consumption in communication systems leads\nto challenges in energy harvesting (EH) systems, in which nodes scavenge energy\nfrom their environment. An EH receiver must harvest enough energy for\ndemodulating and decoding. The energy required depends upon factors, like code\nrate and signal-to-noise ratio, which can be adjusted dynamically. We consider\na receiver which harvests energy from ambient sources and the transmitter,\nmeaning the received signal is used for both EH and information decoding.\nAssuming a generalized function for energy consumption, we maximize the total\nnumber of information bits decoded, under both average and peak power\nconstraints at the transmitter, by carefully optimizing the power used for EH,\npower used for information transmission, fraction of time for EH, and code\nrate. For transmission over a single block, we find there exist problem\nparameters for which either maximizing power for information transmission or\nmaximizing power for EH is optimal. In the general case, the optimal solution\nis a tradeoff of the two. For transmission over multiple blocks, we give an\nupper bound on performance and give sufficient and necessary conditions to\nachieve this bound. Finally, we give some numerical results to illustrate our\nresults and analysis.\n', 'title': 'Performance of Energy Harvesting Receivers with Power Optimization'}]","Title: Performance of Energy Harvesting Receivers with Power Optimization

Abstract: The difficulty of modeling energy consumption in communication systems leads to challenges in energy harvesting (EH) systems, in which nodes scavenge energy from their environment. An EH receiver must harvest enough energy for demodulating and decoding. The energy required depends upon factors, like code rate and signal-to-noise ratio, which can be adjusted dynamically. We consider a receiver which harvests energy from ambient sources and the transmitter, meaning the received signal is used for both EH and information decoding. Assuming a generalized function for energy consumption, we maximize the total number of information bits decoded, under both average and peak power constraints at the transmitter, by carefully optimizing the power used for EH, power used for information transmission, fraction of time for EH, and code rate. For transmission over a single block, we find there exist problem parameters for which either maximizing power for information transmission or maximizing power for EH is optimal. In the general case, the optimal solution is a tradeoff of the two. For transmission over multiple blocks, we give an upper bound on performance and give sufficient and necessary conditions to achieve this bound. Finally, we give some numerical results to illustrate our results and analysis."
What is the relationship between the stability of the universal bundle and polarizations on the product space X times the moduli stack?,516a8550,"[{'abstract': '  Let X be an irreducible smooth projective curve, of genus at least two, over\nan algebraically closed field k. Let $\\mathcal{M}^d_G$ denote the moduli stack\nof principal G-bundles over X of fixed topological type $d \\in \\pi_1(G)$, where\nG is any almost simple affine algebraic group over k. We prove that the\nuniversal bundle over $X \\times \\mathcal{M}^d_G$ is stable with respect to any\npolarization on $X \\times \\mathcal{M}^d_G$. A similar result is proved for the\nPoincaré adjoint bundle over $X \\times M_G^{d, rs}$, where $M_G^{d, rs}$ is\nthe coarse moduli space of regularly stable principal G-bundles over X of fixed\ntopological type d.\n', 'title': 'Stability of the Poincaré bundle'}]","Title: Stability of the Poincaré bundle

Abstract: Let X be an irreducible smooth projective curve, of genus at least two, over an algebraically closed field k. Let $\mathcal{M}^d_G$ denote the moduli stack of principal G-bundles over X of fixed topological type $d \in \pi_1(G)$, where G is any almost simple affine algebraic group over k. We prove that the universal bundle over $X \times \mathcal{M}^d_G$ is stable with respect to any polarization on $X \times \mathcal{M}^d_G$. A similar result is proved for the Poincaré adjoint bundle over $X \times M_G^{d, rs}$, where $M_G^{d, rs}$ is the coarse moduli space of regularly stable principal G-bundles over X of fixed topological type d."
What is an alternative to the rich club in complex networks that may be more important for global integration?,d5f3b750,"[{'abstract': '  A complex system can be represented and analyzed as a network, where nodes\nrepresent the units of the network and edges represent connections between\nthose units. For example, a brain network represents neurons as nodes and axons\nbetween neurons as edges. In many networks, some nodes have a\ndisproportionately high number of edges. These nodes also have many edges\nbetween each other, and are referred to as the rich club. In many different\nnetworks, the nodes of this club are assumed to support global network\nintegration. However, another set of nodes potentially exhibits a connectivity\nstructure that is more advantageous to global network integration. Here, in a\nmyriad of different biological and man-made networks, we discover the diverse\nclub--a set of nodes that have edges diversely distributed across the network.\nThe diverse club exhibits, to a greater extent than the rich club, properties\nconsistent with an integrative network function--these nodes are more highly\ninterconnected and their edges are more critical for efficient global\nintegration. Moreover, we present a generative evolutionary network model that\nproduces networks with a diverse club but not a rich club, thus demonstrating\nthat these two clubs potentially evolved via distinct selection pressures.\nGiven the variety of different networks that we analyzed--the c. elegans, the\nmacaque brain, the human brain, the United States power grid, and global air\ntraffic--the diverse club appears to be ubiquitous in complex networks. These\nresults warrant the distinction and analysis of two critical clubs of nodes in\nall complex systems.\n', 'title': 'The Diverse Club: The Integrative Core of Complex Networks'}]","I think I can help you with that!

Title: The Diverse Club: The Integrative Core of Complex Networks

Abstract: A complex system can be represented and analyzed as a network, where nodes represent the units of the network and edges represent connections between those units. For example, a brain network represents neurons as nodes and axons between neurons as edges. In many networks, some nodes have a disproportionately high number of edges. These nodes also have many edges between each other, and are referred to as the rich club. In many different networks, the nodes of this club are assumed to support global network integration. However, another set of nodes potentially exhibits a connectivity structure that is more advantageous to global network integration. Here, in a myriad of different biological and man-made networks, we discover the diverse club--a set of nodes that have edges diversely distributed across the network. The diverse club exhibits, to a greater extent than the rich club, properties consistent with an integrative network function--these nodes are more highly interconnected and their edges are more critical for efficient global integration. Moreover, we present a generative evolutionary network model that produces networks with a diverse club but not a rich club, thus demonstrating that these two clubs potentially evolved via distinct selection pressures. Given the variety of different networks that we analyzed--the c. elegans, the macaque brain, the human brain, the United States power grid, and global air traffic--the diverse club appears to be ubiquitous in complex networks. These results warrant the distinction and analysis of two critical clubs of nodes in all complex systems."
Does the structure of a vertex operator algebra of central charge 24 depend on its weight one Lie algebra?,ae3e04e8,"[{'abstract': '  In this article, we study orbifold constructions associated with the Leech\nlattice vertex operator algebra. As an application, we prove that the structure\nof a strongly regular holomorphic vertex operator algebra of central charge\n$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra\nhas the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,\n$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse\norbifold construction. Our result also provides alternative constructions of\nthese vertex operator algebras (except for the case $A_{6,7}$) from the Leech\nlattice vertex operator algebra.\n', 'title': 'On orbifold constructions associated with the Leech lattice vertex operator algebra'}]","Title: On orbifold constructions associated with the Leech lattice vertex operator algebra

Abstract:   In this article, we study orbifold constructions associated with the Leech 
lattice vertex operator algebra. As an application, we prove that the structure 
of a strongly regular holomorphic vertex operator algebra of central charge 
$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra 
has the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$, 
$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse 
orbifold construction. Our result also provides alternative constructions of 
these vertex operator algebras (except for the case $A_{6,7}$) from the Leech 
lattice vertex operator algebra."
What are the characteristics of citation age distributions as they relate to the passage of time?,5991e3a3,"[{'abstract': '  A central question in science of science concerns how time affects citations.\nDespite the long-standing interests and its broad impact, we lack systematic\nanswers to this simple yet fundamental question. By reviewing and classifying\nprior studies for the past 50 years, we find a significant lack of consensus in\nthe literature, primarily due to the coexistence of retrospective and\nprospective approaches to measuring citation age distributions. These two\napproaches have been pursued in parallel, lacking any known connections between\nthe two. Here we developed a new theoretical framework that not only allows us\nto connect the two approaches through precise mathematical relationships, it\nalso helps us reconcile the interplay between temporal decay of citations and\nthe growth of science, helping us uncover new functional forms characterizing\ncitation age distributions. We find retrospective distribution follows a\nlognormal distribution with exponential cutoff, while prospective distribution\nis governed by the interplay between a lognormal distribution and the growth in\nthe number of references. Most interestingly, the two approaches can be\nconnected once rescaled by the growth of publications and citations. We further\nvalidate our framework using both large-scale citation datasets and analytical\nmodels capturing citation dynamics. Together this paper presents a\ncomprehensive analysis of the time dimension of science, representing a new\nempirical and theoretical basis for all future studies in this area.\n', 'title': 'The Time Dimension of Science: Connecting the Past to the Future'}]","Title: The Time Dimension of Science: Connecting the Past to the Future

Abstract: A central question in science of science concerns how time affects citations. Despite the long-standing interests and its broad impact, we lack systematic answers to this simple yet fundamental question. By reviewing and classifying prior studies for the past 50 years, we find a significant lack of consensus in the literature, primarily due to the coexistence of retrospective and prospective approaches to measuring citation age distributions. These two approaches have been pursued in parallel, lacking any known connections between the two. Here we developed a new theoretical framework that not only allows us to connect the two approaches through precise mathematical relationships, it also helps us reconcile the interplay between temporal decay of citations and the growth of science, helping us uncover new functional forms characterizing citation age distributions. We find retrospective distribution follows a lognormal distribution with exponential cutoff, while prospective distribution is governed by the interplay between a lognormal distribution and the growth in the number of references. Most interestingly, the two approaches can be connected once rescaled by the growth of publications and citations. We further validate our framework using both large-scale citation datasets and analytical models capturing citation dynamics. Together this paper presents a comprehensive analysis of the time dimension of science, representing a new empirical and theoretical basis for all future studies in this area."
"How does imperfect channel state information, pilot contamination, and channel spatial correlation affect the SINR in these systems?",294bbcc4,"[{'abstract': '  This work provides a comprehensive scaling law based performance analysis for\nmulti-cell multi-user massive multiple-input-multiple-output (MIMO) downlink\nsystems. Imperfect channel state information (CSI), pilot contamination, and\nchannel spatial correlation are all considered. First, a sum- rate lower bound\nis derived by exploiting the asymptotically deterministic property of the\nreceived signal power, while keeping the random nature of other components in\nthe signal-to-interference-plus-noise-ratio (SINR) intact. Via a general\nscaling model on important network parameters, including the number of users,\nthe channel training energy and the data transmission power, with respect to\nthe number of base station antennas, the asymptotic scaling law of the\neffective SINR is obtained, which reveals quantitatively the tradeoff of the\nnetwork parameters. More importantly, pilot contamination and pilot\ncontamination elimination (PCE) are considered in the analytical framework. In\naddition, the applicability of the derived asymptotic scaling law in practical\nsystems with large but finite antenna numbers are discussed. Finally,\nsufficient conditions on the parameter scalings for the SINR to be\nasymptotically deterministic in the sense of mean square convergence are\nprovided, which covers existing results on such analysis as special cases and\nshows the effect of PCE explicitly.\n', 'title': 'Performance Scaling Law for Multi-Cell Multi-User Massive MIMO'}]","I think I can help you with that!

Title: Performance Scaling Law for Multi-Cell Multi-User Massive MIMO

Abstract: This work provides a comprehensive scaling law based performance analysis for multi-cell multi-user massive multiple-input-multiple-output (MIMO) downlink systems. Imperfect channel state information (CSI), pilot contamination, and channel spatial correlation are all considered. First, a sum- rate lower bound is derived by exploiting the asymptotically deterministic property of the received signal power, while keeping the random nature of other components in the signal-to-interference-plus-noise-ratio (SINR) intact. Via a general scaling model on important network parameters, including the number of users, the channel training energy and the data transmission power, with respect to the number of base station antennas, the asymptotic scaling law of the effective SINR is obtained, which reveals quantitatively the tradeoff of the network parameters. More importantly, pilot contamination and pilot contamination elimination (PCE) are considered in the analytical framework. In addition, the applicability of the derived asymptotic scaling law in practical systems with large but finite antenna numbers are discussed. Finally, sufficient conditions on the parameter scalings for the SINR to be asymptotically deterministic in the sense of mean square convergence are provided, which covers existing results on such analysis as special cases and shows the effect of PCE explicitly.

This research paper analyzes the impact of imperfect channel state information, pilot contamination, and channel spatial correlation on the SINR in massive MIMO systems."
How can regression quantiles be used to assess probabilistic risk?,43db1701,"[{'abstract': '  The processes of the averaged regression quantiles and of their modifications\nprovide useful tools in the regression models when the covariates are not fully\nunder our control. As an application we mention the probabilistic risk\nassessment in the situation when the return depends on some exogenous\nvariables. The processes enable to evaluate the expected $\\alpha$-shortfall\n($0\\leq\\alpha\\leq 1$) and other measures of the risk, recently generally\naccepted in the financial literature, but also help to measure the risk in\nenvironment analysis and elsewhere.\n', 'title': 'Empirical regression quantile process with possible application to risk analysis'}]","Based on the provided context, I found a relevant article that answers the question:

Title: Empirical regression quantile process with possible application to risk analysis

Abstract: The processes of the averaged regression quantiles and of their modifications provide useful tools in the regression models when the covariates are not fully under our control. As an application we mention the probabilistic risk assessment in the situation when the return depends on some exogenous variables. The processes enable to evaluate the expected α-shortfall (0≤α≤1) and other measures of the risk, recently generally accepted in the financial literature, but also help to measure the risk in environmental analysis and elsewhere."
How does the functional renormalization group (fRG) approach describe the particle-hole susceptibility?,a28c7967,"[{'abstract': '  We study the Fermi-edge singularity, describing the response of a degenerate\nelectron system to optical excitation, in the framework of the functional\nrenormalization group (fRG). Results for the (interband) particle-hole\nsusceptibility from various implementations of fRG (one- and two-\nparticle-irreducible, multi-channel Hubbard-Stratonovich, flowing\nsusceptibility) are compared to the summation of all leading logarithmic (log)\ndiagrams, achieved by a (first-order) solution of the parquet equations. For\nthe (zero-dimensional) special case of the X-ray-edge singularity, we show that\nthe leading log formula can be analytically reproduced in a consistent way from\na truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic\nstructure, we show that this derivation relies on fortuitous partial\ncancellations special to the form of and accuracy applied to the X-ray-edge\nsingularity and does not generalize.\n', 'title': 'Fermi-edge singularity and the functional renormalization group'}]","Title: Fermi-edge singularity and the functional renormalization group

Abstract: We study the Fermi-edge singularity, describing the response of a degenerate electron system to optical excitation, in the framework of the functional renormalization group (fRG). Results for the (interband) particle-hole susceptibility from various implementations of fRG (one- and two-particle-irreducible, multi-channel Hubbard-Stratonovich, flowing susceptibility) are compared to the summation of all leading logarithmic (log) diagrams, achieved by a (first-order) solution of the parquet equations. For the (zero-dimensional) special case of the X-ray-edge singularity, we show that the leading log formula can be analytically reproduced in a consistent way from a truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic structure, we show that this derivation relies on fortuitous partial cancellations special to the form of and accuracy applied to the X-ray-edge singularity and does not generalize."
"How can computer science concepts like linked lists, hash functions, and sharding be applied to understanding memory?",6378a917,"[{'abstract': '  We relate the concepts used in decentralized ledger technology to studies of\nepisodic memory in the mammalian brain. Specifically, we introduce the standard\nconcepts of linked list, hash functions, and sharding, from computer science.\nWe argue that these concepts may be more relevant to studies of the neural\nmechanisms of memory than has been previously appreciated. In turn, we also\nhighlight that certain phenomena studied in the brain, namely metacognition,\nreality monitoring, and how perceptual conscious experiences come about, may\ninspire development in blockchain technology too, specifically regarding\nprobabilistic consensus protocols.\n', 'title': 'Blockchain and human episodic memory'}]","Title: Blockchain and human episodic memory

Abstract: We relate the concepts used in decentralized ledger technology to studies of
episodic memory in the mammalian brain. Specifically, we introduce the standard
concepts of linked list, hash functions, and sharding, from computer science.
We argue that these concepts may be more relevant to studies of the neural
mechanisms of memory than has been previously appreciated. In turn, we also
highlight that certain phenomena studied in the brain, namely metacognition,
reality monitoring, and how perceptual conscious experiences come about, may
inspire development in blockchain technology too, specifically regarding
probabilistic consensus protocols."
What is the benefit of using a domain-specific language (DSL) for writing Molecular Dynamics code?,874fde0a,"[{'abstract': '  Developers of Molecular Dynamics (MD) codes face significant challenges when\nadapting existing simulation packages to new hardware. In a continuously\ndiversifying hardware landscape it becomes increasingly difficult for\nscientists to be experts both in their own domain (physics/chemistry/biology)\nand specialists in the low level parallelisation and optimisation of their\ncodes. To address this challenge, we describe a ""Separation of Concerns""\napproach for the development of parallel and optimised MD codes: the science\nspecialist writes code at a high abstraction level in a domain specific\nlanguage (DSL), which is then translated into efficient computer code by a\nscientific programmer. In a related context, an abstraction for the solution of\npartial differential equations with grid based methods has recently been\nimplemented in the (Py)OP2 library. Inspired by this approach, we develop a\nPython code generation system for molecular dynamics simulations on different\nparallel architectures, including massively parallel distributed memory systems\nand GPUs. We demonstrate the efficiency of the auto-generated code by studying\nits performance and scalability on different hardware and compare it to other\nstate-of-the-art simulation packages. With growing data volumes the extraction\nof physically meaningful information from the simulation becomes increasingly\nchallenging and requires equally efficient implementations. A particular\nadvantage of our approach is the easy expression of such analysis algorithms.\nWe consider two popular methods for deducing the crystalline structure of a\nmaterial from the local environment of each atom, show how they can be\nexpressed in our abstraction and implement them in the code generation\nframework.\n', 'title': 'A Domain Specific Language for Performance Portable Molecular Dynamics Algorithms'}]","Title: A Domain Specific Language for Performance Portable Molecular Dynamics Algorithms

Abstract: Developers of Molecular Dynamics (MD) codes face significant challenges when adapting existing simulation packages to new hardware. In a continuously diversifying hardware landscape it becomes increasingly difficult for scientists to be experts both in their own domain (physics/chemistry/biology) and specialists in the low level parallelisation and optimisation of their codes. To address this challenge, we describe a ""Separation of Concerns"" approach for the development of parallel and optimised MD codes: the science specialist writes code at a high abstraction level in a domain specific language (DSL), which is then translated into efficient computer code by a scientific programmer. In a related context, an abstraction for the solution of partial differential equations with grid based methods has recently been implemented in the (Py)OP2 library. Inspired by this approach, we develop a Python code generation system for molecular dynamics simulations on different parallel architectures, including massively parallel distributed memory systems and GPUs. We demonstrate the efficiency of the auto-generated code by studying its performance and scalability on different hardware and compare it to other state-of-the-art simulation packages. With growing data volumes the extraction of physically meaningful information from the simulation becomes increasingly challenging and requires equally efficient implementations. A particular advantage of our approach is the easy expression of such analysis algorithms. We consider two popular methods for deducing the crystalline structure of a material from the local environment of each atom, show how they can be expressed in our abstraction and implement them in the code generation framework.

The benefit of using a DSL for writing Molecular Dynamics code is that it allows scientists to focus on their domain-specific knowledge without having to be experts in low-level parallelization and optimization, thereby making it easier to adapt existing simulation packages to new hardware."
How does the formation time of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells compare to previously reported values?,955d5312,"[{'abstract': '  We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se\ndiluted-magnetic-semiconductor quantum wells using time-resolved\nphotoluminescence (PL). The magnetic field and temperature dependencies of this\ndynamics allow us to separate the non-magnetic and magnetic contributions to\nthe exciton localization. We deduce the EMP energy of 14 meV, which is in\nagreement with time-integrated measurements based on selective excitation and\nthe magnetic field dependence of the PL circular polarization degree. The\npolaron formation time of 500 ps is significantly longer than the corresponding\nvalues reported earlier. We propose that this behavior is related to strong\nself-localization of the EMP, accompanied with a squeezing of the heavy-hole\nenvelope wavefunction. This conclusion is also supported by the decrease of the\nexciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and\ntemperature.\n', 'title': 'Dynamics of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells: the effect of self-localization'}]","Title: Dynamics of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells: the effect of self-localization

Abstract: We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se diluted-magnetic-semiconductor quantum wells using time-resolved photoluminescence (PL). The magnetic field and temperature dependencies of this dynamics allow us to separate the non-magnetic and magnetic contributions to the exciton localization. We deduce the EMP energy of 14 meV, which is in agreement with time-integrated measurements based on selective excitation and the magnetic field dependence of the PL circular polarization degree. The polaron formation time of 500 ps is significantly longer than the corresponding values reported earlier. We propose that this behavior is related to strong self-localization of the EMP, accompanied with a squeezing of the heavy-hole envelope wavefunction. This conclusion is also supported by the decrease of the exciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and temperature.

The formation time of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells is 500 ps, which is significantly longer than previously reported values."
What type of approximation is used to address the intractability of the conditioned hazard/propensity function for Markov jump processes?,a87f2385,"[{'abstract': '  We consider the task of generating draws from a Markov jump process (MJP)\nbetween two time points at which the process is known. Resulting draws are\ntypically termed bridges and the generation of such bridges plays a key role in\nsimulation-based inference algorithms for MJPs. The problem is challenging due\nto the intractability of the conditioned process, necessitating the use of\ncomputationally intensive methods such as weighted resampling or Markov chain\nMonte Carlo. An efficient implementation of such schemes requires an\napproximation of the intractable conditioned hazard/propensity function that is\nboth cheap and accurate. In this paper, we review some existing approaches to\nthis problem before outlining our novel contribution. Essentially, we leverage\nthe tractability of a Gaussian approximation of the MJP and suggest a\ncomputationally efficient implementation of the resulting conditioned hazard\napproximation. We compare and contrast our approach with existing methods using\nthree examples.\n', 'title': 'Efficient sampling of conditioned Markov jump processes'}]","Title: Efficient sampling of conditioned Markov jump processes

Abstract: We consider the task of generating draws from a Markov jump process (MJP)
between two time points at which the process is known. Resulting draws are
typically termed bridges and the generation of such bridges plays a key role in
simulation-based inference algorithms for MJPs. The problem is challenging due
to the intractability of the conditioned process, necessitating the use of
computationally intensive methods such as weighted resampling or Markov chain
Monte Carlo. An efficient implementation of such schemes requires an
approximation of the intractable conditioned hazard/propensity function that is
both cheap and accurate. In this paper, we review some existing approaches to
this problem before outlining our novel contribution. Essentially, we leverage
the tractability of a Gaussian approximation of the MJP and suggest a
computationally efficient implementation of the resulting conditioned hazard
approximation. We compare and contrast our approach with existing methods using
three examples.

The approximation used is a Gaussian approximation of the Markov jump process (MJP)."
What is the benefit of using Wasserstein GAN compared to standard GANs?,2b0a59ca,"[{'abstract': '  Generative Adversarial Networks (GANs) produce systematically better quality\nsamples when class label information is provided., i.e. in the conditional GAN\nsetup. This is still observed for the recently proposed Wasserstein GAN\nformulation which stabilized adversarial training and allows considering high\ncapacity network architectures such as ResNet. In this work we show how to\nboost conditional GAN by augmenting available class labels. The new classes\ncome from clustering in the representation space learned by the same GAN model.\nThe proposed strategy is also feasible when no class information is available,\ni.e. in the unsupervised setup. Our generated samples reach state-of-the-art\nInception scores for CIFAR-10 and STL-10 datasets in both supervised and\nunsupervised setup.\n', 'title': 'Class-Splitting Generative Adversarial Networks'}]","Based on the provided context, I found a relevant paper that mentions the benefit of using Wasserstein GAN compared to standard GANs.

Title: Class-Splitting Generative Adversarial Networks

Abstract: Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided, i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet."
How can a near-optimal algorithm for finding the envelope of a signal be implemented?,50e16b41,"[{'abstract': ""  Lowpass envelope approximation of smooth continuous-variable signals are\nintroduced in this work. Envelope approximations are necessary when a given\nsignal has to be approximated always to a larger value (such as in TV white\nspace protection regions). In this work, a near-optimal approximate algorithm\nfor finding a signal's envelope, while minimizing a mean-squared cost function,\nis detailed. The sparse (lowpass) signal approximation is obtained in the\nlinear Fourier series basis. This approximate algorithm works by discretizing\nthe envelope property from an infinite number of points to a large (but finite)\nnumber of points. It is shown that this approximate algorithm is near-optimal\nand can be solved by using efficient convex optimization programs available in\nthe literature. Simulation results are provided towards the end to gain more\ninsights into the analytical results presented.\n"", 'title': 'Optimal Envelope Approximation in Fourier Basis with Applications in TV White Space'}]","Title: Optimal Envelope Approximation in Fourier Basis with Applications in TV White Space

Abstract: Lowpass envelope approximation of smooth continuous-variable signals are introduced in this work. Envelope approximations are necessary when a given signal has to be approximated always to a larger value (such as in TV white space protection regions). In this work, a near-optimal approximate algorithm for finding a signal's envelope, while minimizing a mean-squared cost function, is detailed. The sparse (lowpass) signal approximation is obtained in the linear Fourier series basis. This approximate algorithm works by discretizing the envelope property from an infinite number of points to a large (but finite) number of points. It is shown that this approximate algorithm is near-optimal and can be solved by using efficient convex optimization programs available in the literature. Simulation results are provided towards the end to gain more insights into the analytical results presented."
What is unique about the  discovery of these planets relative to other exoplanet discoveries by the K2 mission? ,613aae29,"[{'abstract': '  We report the discovery of three small transiting planets orbiting GJ 9827, a\nbright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\\pm0.11$\n$R_{\\rm \\oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$\n$R_{\\rm \\oplus}$ super Earth on a 3.6 day period, and a $2.07\\pm0.14$ $R_{\\rm\n\\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting\nGJ 9827 span the transition between predominantly rocky and gaseous planets,\nand GJ 9827 b and c fall in or close to the known gap in the radius\ndistribution of small planets between these populations. At a distance of 30\nparsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making\nthese planets well-suited for atmospheric studies with the upcoming James Webb\nSpace Telescope. The GJ 9827 system provides a valuable opportunity to\ncharacterize interior structure and atmospheric properties of coeval planets\nspanning the rocky to gaseous transition.\n', 'title': 'A System of Three Super Earths Transiting the Late K-Dwarf GJ 9827 at Thirty Parsecs'}]","Title: A System of Three Super Earths Transiting the Late K-Dwarf GJ 9827 at Thirty Parsecs

Abstract: We report the discovery of three small transiting planets orbiting GJ 9827, a bright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\pm0.11$ $R_{\rm \oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$ $R_{\rm \oplus}$ super Earth on a 3.6 day period, and a $2.07\pm0.14$ $R_{\rm \oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting GJ 9827 span the transition between predominantly rocky and gaseous planets, and GJ 9827 b and c fall in or close to the known gap in the radius distribution of small planets between these populations. At a distance of 30 parsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making these planets well-suited for atmospheric studies with the upcoming James Webb Space Telescope. The GJ 9827 system provides a valuable opportunity to characterize interior structure and atmospheric properties of coeval planets spanning the rocky to gaseous transition.

What is unique about the discovery of these planets relative to other exoplanet discoveries by the K2 mission is that GJ 9827 is the closest exoplanet host discovered by K2 to date, at a distance of 30 parsecs."
What is the potential use of deep neural networks for detecting heart disease?,dcab86b6,"[{'abstract': '  Heart disease is the leading cause of death, and experts estimate that\napproximately half of all heart attacks and strokes occur in people who have\nnot been flagged as ""at risk."" Thus, there is an urgent need to improve the\naccuracy of heart disease diagnosis. To this end, we investigate the potential\nof using data analysis, and in particular the design and use of deep neural\nnetworks (DNNs) for detecting heart disease based on routine clinical data. Our\nmain contribution is the design, evaluation, and optimization of DNN\narchitectures of increasing depth for heart disease diagnosis. This work led to\nthe discovery of a novel five layer DNN architecture - named Heart Evaluation\nfor Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields\nbest prediction accuracy. HEARO-5\'s design employs regularization optimization\nand automatically deals with missing data and/or data outliers. To evaluate and\ntune the architectures we use k-way cross-validation as well as Matthews\ncorrelation coefficient (MCC) to measure the quality of our classifications.\nThe study is performed on the publicly available Cleveland dataset of medical\ninformation, and we are making our developments open source, to further\nfacilitate openness and research on the use of DNNs in medicine. The HEARO-5\narchitecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms\ncurrently published research in the area.\n', 'title': 'On Deep Neural Networks for Detecting Heart Disease'}]","Based on the provided context, I found a relevant research paper that addresses the potential use of deep neural networks for detecting heart disease.

Title: On Deep Neural Networks for Detecting Heart Disease

Abstract: Heart disease is the leading cause of death, and experts estimate that approximately half of all heart attacks and strokes occur in people who have not been flagged as ""at risk."" Thus, there is an urgent need to improve the accuracy of heart disease diagnosis. To this end, we investigate the potential of using data analysis, and in particular the design and use of deep neural networks (DNNs) for detecting heart disease based on routine clinical data. Our main contribution is the design, evaluation, and optimization of DNN architectures of increasing depth for heart disease diagnosis. This work led to the discovery of a novel five-layer DNN architecture - named Heart Evaluation for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields best prediction accuracy. HEARO-5's design employs regularization optimization and automatically deals with missing data and/or data outliers. To evaluate and tune the architectures we use k-way cross-validation as well as Matthews correlation coefficient (MCC) to measure the quality of our classifications. The study is performed on the publicly available Cleveland dataset of medical information, and we are making our developments open source, to further facilitate openness and research on the use of DNNs in medicine. The HEARO-5 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms currently published research in the area."
What type of confidence bands can be derived for isotonic regression?,eeb728e2,"[{'abstract': '  We consider the problem of isotonic regression, where the underlying signal\n$x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the\ncone $\\{ x\\in\\mathbb{R}^n : x_1 \\leq \\dots \\leq x_n\\}$. We study the isotonic\nprojection operator (projection to this cone), and find a necessary and\nsufficient condition characterizing all norms with respect to which this\nprojection is contractive. This enables a simple and non-asymptotic analysis of\nthe convergence properties of isotonic regression, yielding uniform confidence\nbands that adapt to the local Lipschitz properties of the signal.\n', 'title': 'Contraction and uniform convergence of isotonic regression'}]","Title: Contraction and uniform convergence of isotonic regression

Abstract: We consider the problem of isotonic regression, where the underlying signal $x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the cone $\{ x\in\mathbb{R}^n : x_1 \leq \dots \leq x_n\}$. We study the isotonic projection operator (projection to this cone), and find a necessary and sufficient condition characterizing all norms with respect to which this projection is contractive. This enables a simple and non-asymptotic analysis of the convergence properties of isotonic regression, yielding uniform confidence bands that adapt to the local Lipschitz properties of the signal."
What is the article 'Representing numbers as the sum of squares and powers in the ring $\mathbb{Z}_n$' about?,c279748c,"[{'abstract': '  We examine the representation of numbers as the sum of two squares in\n$\\mathbb{Z}_n$ for a general positive integer $n$. Using this information we\nmake some comments about the density of positive integers which can be\nrepresented as the sum of two squares and powers of $2$ in $\\mathbb{N}$.\n', 'title': 'Representing numbers as the sum of squares and powers in the ring $\\mathbb{Z}_n$'}]","Title: Representing numbers as the sum of squares and powers in the ring $\mathbb{Z}_n$

Abstract: We examine the representation of numbers as the sum of two squares in $\mathbb{Z}_n$ for a general positive integer $n$. Using this information we make some comments about the density of positive integers which can be represented as the sum of two squares and powers of $2$ in $\mathbb{N}$."
How does the limiting UV magnitude of a survey influence the observed distribution of Lyman alpha emission in Lyman break galaxies?,459e19ea,"[{'abstract': '  We present an exhaustive census of Lyman alpha (Ly$\\alpha$) emission in the\ngeneral galaxy population at $3<z<4.6$. We use the Michigan/Magellan Fiber\nSystem (M2FS) spectrograph to study a stellar mass (M$_*$) selected sample of\n625 galaxies homogeneously distributed in the range\n$7.6<\\log{\\mbox{M$_*$/M$_{\\odot}$}}<10.6$. Our sample is selected from the\n3D-HST/CANDELS survey, which provides the complementary data to estimate\nLy$\\alpha$ equivalent widths ($W_{Ly\\alpha}$) and escape fractions ($f_{esc}$)\nfor our galaxies. We find both quantities to anti-correlate with M$_*$,\nstar-formation rate (SFR), UV luminosity, and UV slope ($\\beta$). We then model\nthe $W_{Ly\\alpha}$ distribution as a function of M$_{UV}$ and $\\beta$ using a\nBayesian approach. Based on our model and matching the properties of typical\nLyman break galaxy (LBG) selections, we conclude that the $W_{Ly\\alpha}$\ndistribution in such samples is heavily dependent on the limiting M$_{UV}$ of\nthe survey. Regarding narrowband surveys, we find their $W_{Ly\\alpha}$\nselections to bias samples toward low M$_*$, while their line-flux limitations\npreferentially leave out low-SFR galaxies. We can also use our model to predict\nthe fraction of Ly$\\alpha$-emitting LBGs at $4\\leqslant z\\leqslant 7$. We show\nthat reported drops in the Ly$\\alpha$ fraction at $z\\geqslant6$, usually\nattributed to the rapidly increasing neutral gas fraction of the universe, can\nalso be explained by survey M$_{UV}$ incompleteness. This result does not\ndismiss reionization occurring at $z\\sim7$, but highlights that current data is\nnot inconsistent with this process taking place at $z>7$.\n', 'title': 'A Comprehensive Study of Ly$α$ Emission in the High-redshift Galaxy Population'}]","Title: A Comprehensive Study of Lyα Emission in the High-redshift Galaxy Population

Abstract: We present an exhaustive census of Lyman alpha (Lyα) emission in the general galaxy population at $3<z<4.6$. We use the Michigan/Magellan Fiber System (M2FS) spectrograph to study a stellar mass (M$_*$) selected sample of 625 galaxies homogeneously distributed in the range $7.6<\log{\mbox{M$_*$/M$_{\odot}$}}<10.6$. Our sample is selected from the 3D-HST/CANDELS survey, which provides the complementary data to estimate Lyα equivalent widths ($W_{Ly\alpha}$) and escape fractions ($f_{esc}$) for our galaxies. We find both quantities to anti-correlate with M$_*$, star-formation rate (SFR), UV luminosity, and UV slope ($\beta$). We then model the $W_{Ly\alpha}$ distribution as a function of M$_{UV}$ and $\beta$ using a Bayesian approach. Based on our model and matching the properties of typical Lyman break galaxy (LBG) selections, we conclude that the $W_{Ly\alpha}$ distribution in such samples is heavily dependent on the limiting M$_{UV}$ of the survey."
"According to a study, how does the performance of a deep learning model for lung nodule malignancy prediction compare to that of experienced radiologists?",56eee4d2,"[{'abstract': '  Computed tomography (CT) examinations are commonly used to predict lung\nnodule malignancy in patients, which are shown to improve noninvasive early\ndiagnosis of lung cancer. It remains challenging for computational approaches\nto achieve performance comparable to experienced radiologists. Here we present\nNoduleX, a systematic approach to predict lung nodule malignancy from CT data,\nbased on deep learning convolutional neural networks (CNN). For training and\nvalidation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.\nAll nodules were identified and classified by four experienced thoracic\nradiologists who participated in the LIDC project. NoduleX achieves high\naccuracy for nodule malignancy classification, with an AUC of ~0.99. This is\ncommensurate with the analysis of the dataset by experienced radiologists. Our\napproach, NoduleX, provides an effective framework for highly accurate nodule\nmalignancy prediction with the model trained on a large patient population. Our\nresults are replicable with software available at\nthis http URL.\n', 'title': 'Highly accurate model for prediction of lung nodule malignancy with CT scans'}]","Title: Highly accurate model for prediction of lung nodule malignancy with CT scans

Abstract: Computed tomography (CT) examinations are commonly used to predict lung nodule malignancy in patients, which are shown to improve noninvasive early diagnosis of lung cancer. It remains challenging for computational approaches to achieve performance comparable to experienced radiologists. Here we present NoduleX, a systematic approach to predict lung nodule malignancy from CT data, based on deep learning convolutional neural networks (CNN). For training and validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort. All nodules were identified and classified by four experienced thoracic radiologists who participated in the LIDC project. NoduleX achieves high accuracy for nodule malignancy classification, with an AUC of ~0.99. This is commensurate with the analysis of the dataset by experienced radiologists. Our approach, NoduleX, provides an effective framework for highly accurate nodule malignancy prediction with the model trained on a large patient population. Our results are replicable with software available at this http URL.

According to this study, the performance of the deep learning model (NoduleX) is commensurate with that of experienced radiologists, with an AUC of ~0.99."
"What type of Jordan curve, composed of edges from an n-dimensional cube, can be used to create a Plateau minimal disk?",fb6f9a05,"[{'abstract': '  We construct embedded minimal surfaces which are $n$-periodic in\n$\\mathbb{R}^n$. They are new for codimension $n-2\\ge 2$. We start with a Jordan\ncurve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk\nwhich Schwarz reflection extends to a complete minimal surface. Studying the\ngroup of Schwarz reflections, we can characterize those Jordan curves for which\nthe complete surface is embedded. For example, for $n=4$ exactly five such\nJordan curves generate embedded surfaces. Our results apply to surface classes\nother than minimal as well, for instance polygonal surfaces.\n', 'title': 'Construction of embedded periodic surfaces in $\\mathbb{R}^n$'}]","Title: Construction of embedded periodic surfaces in $\mathbb{R}^n$

Abstract:   We construct embedded minimal surfaces which are $n$-periodic in
$\mathbb{R}^n$. They are new for codimension $n-2\ge 2$. We start with a Jordan
curve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk
which Schwarz reflection extends to a complete minimal surface. Studying the
group of Schwarz reflections, we can characterize those Jordan curves for which
the complete surface is embedded. For example, for $n=4$ exactly five such
Jordan curves generate embedded surfaces. Our results apply to surface classes
other than minimal as well, for instance polygonal surfaces."
How are complex networks with different types of flows categorized?,7c7b7766,"[{'abstract': '  Subject of research is complex networks and network systems. The network\nsystem is defined as a complex network in which flows are moved. Classification\nof flows in the network is carried out on the basis of ordering and continuity.\nIt is shown that complex networks with different types of flows generate\nvarious network systems. Flow analogues of the basic concepts of the theory of\ncomplex networks are introduced and the main problems of this theory in terms\nof flow characteristics are formulated. Local and global flow characteristics\nof networks bring closer the theory of complex networks to the systems theory\nand systems analysis. Concept of flow core of network system is introduced and\ndefined how it simplifies the process of its investigation. Concepts of kernel\nand flow core of multiplex are determined. Features of operation of multiplex\ntype systems are analyzed.\n', 'title': 'Flow Characteristics and Cores of Complex Network and Multiplex Type Systems'}]","I think I can help you with that!

Title: Flow Characteristics and Cores of Complex Network and Multiplex Type Systems

Abstract: Subject of research is complex networks and network systems. The network
system is defined as a complex network in which flows are moved. Classification
of flows in the network is carried out on the basis of ordering and continuity.
It is shown that complex networks with different types of flows generate
various network systems. Flow analogues of the basic concepts of the theory of
complex networks are introduced and the main problems of this theory in terms
of flow characteristics are formulated. Local and global flow characteristics
of networks bring closer the theory of complex networks to the systems theory
and systems analysis. Concept of flow core of network system is introduced and
defined how it simplifies the process of its investigation. Concepts of kernel
and flow core of multiplex are determined. Features of operation of multiplex
type systems are analyzed.

The abstract suggests that complex networks with different types of flows are categorized based on ordering and continuity of flows."
How does the Hamiltonian influence thermometry in coupled bosonic modes?,ebaaaff9,"[{'abstract': '  We study the ultimate bounds on the estimation of temperature for an\ninteracting quantum system. We consider two coupled bosonic modes that are\nassumed to be thermal and using quantum estimation theory establish the role\nthe Hamiltonian parameters play in thermometry. We show that in the case of a\nconserved particle number the interaction between the modes leads to a decrease\nin the overall sensitivity to temperature, while interestingly, if particle\nexchange is allowed with the thermal bath the converse is true. We explain this\ndichotomy by examining the energy spectra. Finally, we devise experimentally\nimplementable thermometry schemes that rely only on locally accessible\ninformation from the total system, showing that almost Heisenberg limited\nprecision can still be achieved, and we address the (im)possibility for\nmultiparameter estimation in the system.\n', 'title': 'Global and local thermometry schemes in coupled quantum systems'}]","Title: Global and local thermometry schemes in coupled quantum systems

Abstract:   We study the ultimate bounds on the estimation of temperature for an interacting quantum system. We consider two coupled bosonic modes that are assumed to be thermal and using quantum estimation theory establish the role the Hamiltonian parameters play in thermometry. We show that in the case of a conserved particle number the interaction between the modes leads to a decrease in the overall sensitivity to temperature, while interestingly, if particle exchange is allowed with the thermal bath the converse is true. We explain this dichotomy by examining the energy spectra. Finally, we devise experimentally implementable thermometry schemes that rely only on locally accessible information from the total system, showing that almost Heisenberg limited precision can still be achieved, and we address the (im)possibility for multiparameter estimation in the system."
"Besides Bayesian statistics, what other area of mathematics does the paper discuss?",2b9ac1f7,"[{'abstract': ""  In 1933 Kolmogorov constructed a general theory that defines the modern\nconcept of conditional expectation. In 1955 Renyi fomulated a new axiomatic\ntheory for probability motivated by the need to include unbounded measures. We\nintroduce a general concept of conditional expectation in Renyi spaces. In this\ntheory improper priors are allowed, and the resulting posterior can also be\nimproper.\nIn 1965 Lindley published his classic text on Bayesian statistics using the\ntheory of Renyi, but retracted this idea in 1973 due to the appearance of\nmarginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes\nare investigated, and the seemingly conflicting results are explained. The\ntheory of Renyi can hence be used as an axiomatic basis for statistics that\nallows use of unbounded priors.\nKeywords: Haldane's prior; Poisson intensity; Marginalization paradox;\nMeasure theory; conditional probability space; axioms for statistics;\nconditioning on a sigma field; improper prior\n"", 'title': 'Improper posteriors are not improper'}]","Title: On the economics of knowledge creation and sharing

Abstract:   This work bridges the technical concepts underlying distributed computing and
blockchain technologies with their profound socioeconomic and sociopolitical
implications, particularly on academic research and the healthcare industry.
Several examples from academia, industry, and healthcare are explored
throughout this paper. The limiting factor in contemporary life sciences
research is often funding: for example, to purchase expensive laboratory
equipment and materials, to hire skilled researchers and technicians, and to
acquire and disseminate data through established academic channels. In the case
of the U.S. healthcare system, hospitals generate massive amounts of data, only
a small minority of which is utilized to inform current and future medical
practice. Similarly, corporations too expend large amounts of money to collect,
secure and transmit data from one centralized source to another. In all three
scenarios, data moves under the traditional paradigm of centralization, in
which data is hosted and curated by individuals and organizations and of
benefit to only a small subset of people.

This paper discusses the area of mathematics related to distributed computing and blockchain technologies."
Can Trimmed Hard Thresholding be applied to sparse Gaussian graphical model (precision matrix) estimation?,6a050080,"[{'abstract': '  We study the problem of sparsity constrained $M$-estimation with arbitrary\ncorruptions to both {\\em explanatory and response} variables in the\nhigh-dimensional regime, where the number of variables $d$ is larger than the\nsample size $n$. Our main contribution is a highly efficient gradient-based\noptimization algorithm that we call Trimmed Hard Thresholding -- a robust\nvariant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient\ncomputations. Our algorithm can deal with a wide class of sparsity constrained\n$M$-estimation problems, and we can tolerate a nearly dimension independent\nfraction of arbitrarily corrupted samples. More specifically, when the\ncorrupted fraction satisfies $\\epsilon \\lesssim {1} /\\left({\\sqrt{k} \\log\n(nd)}\\right)$, where $k$ is the sparsity of the parameter, we obtain accurate\nestimation and model selection guarantees with optimal sample complexity.\nFurthermore, we extend our algorithm to sparse Gaussian graphical model\n(precision matrix) estimation via a neighborhood selection approach. We\ndemonstrate the effectiveness of robust estimation in sparse linear, logistic\nregression, and sparse precision matrix estimation on synthetic and real-world\nUS equities data.\n', 'title': 'High Dimensional Robust Estimation of Sparse Models via Trimmed Hard Thresholding'}]","Title: High Dimensional Robust Estimation of Sparse Models via Trimmed Hard Thresholding

Abstract: We study the problem of sparsity constrained $M$-estimation with arbitrary corruptions to both {\em explanatory and response} variables in the high-dimensional regime, where the number of variables $d$ is larger than the sample size $n$. Our main contribution is a highly efficient gradient-based optimization algorithm that we call Trimmed Hard Thresholding -- a robust variant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient computations. Our algorithm can deal with a wide class of sparsity constrained $M$-estimation problems, and we can tolerate a nearly dimension independent fraction of arbitrarily corrupted samples. More specifically, when the corrupted fraction satisfies $\epsilon \lesssim {1} /\left({\sqrt{k} \log (nd)}\right)$, where $k$ is the sparsity of the parameter, we obtain accurate estimation and model selection guarantees with optimal sample complexity. Furthermore, we extend our algorithm to sparse Gaussian graphical model (precision matrix) estimation via a neighborhood selection approach. We demonstrate the effectiveness of robust estimation in sparse linear, logistic regression, and sparse precision matrix estimation on synthetic and real-world US equities data."
Can the reverse orbifold construction be used to determine the structure of a specific type of vertex operator algebra?,ae3e04e8,"[{'abstract': '  In this article, we study orbifold constructions associated with the Leech\nlattice vertex operator algebra. As an application, we prove that the structure\nof a strongly regular holomorphic vertex operator algebra of central charge\n$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra\nhas the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,\n$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse\norbifold construction. Our result also provides alternative constructions of\nthese vertex operator algebras (except for the case $A_{6,7}$) from the Leech\nlattice vertex operator algebra.\n', 'title': 'On orbifold constructions associated with the Leech lattice vertex operator algebra'}]","Title: On orbifold constructions associated with the Leech lattice vertex operator algebra

Abstract: In this article, we study orbifold constructions associated with the Leech lattice vertex operator algebra. As an application, we prove that the structure of a strongly regular holomorphic vertex operator algebra of central charge $24$ is uniquely determined by its weight one Lie algebra if the Lie algebra has the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$, $A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse orbifold construction. Our result also provides alternative constructions of these vertex operator algebras (except for the case $A_{6,7}$) from the Leech lattice vertex operator algebra.

Yes, the reverse orbifold construction can be used to determine the structure of a specific type of vertex operator algebra, namely strongly regular holomorphic vertex operator algebras of central charge 24 with certain types of weight one Lie algebras."
What is the role of anabelian geometry in understanding the relationship between conjugacy of dynamical systems and field isomorphism?,066236d4,"[{'abstract': '  We study a dynamical system induced by the Artin reciprocity map for a global\nfield. We translate the conjugacy of such dynamical systems into various\narithmetical properties that are equivalent to field isomorphism, relating it\nto anabelian geometry.\n', 'title': 'Reconstructing global fields from dynamics in the abelianized Galois group'}]","Title: Reconstructing global fields from dynamics in the abelianized Galois group

Abstract: We study a dynamical system induced by the Artin reciprocity map for a global field. We translate the conjugacy of such dynamical systems into various arithmetical properties that are equivalent to field isomorphism, relating it to anabelian geometry."
What challenges arise when considering many-body localization in higher dimensions?,c66f8c4d,"[{'abstract': '  Rare regions with weak disorder (Griffiths regions) have the potential to\nspoil localization. We describe a non-perturbative construction of local\nintegrals of motion (LIOMs) for a weakly interacting spin chain in one\ndimension, under a physically reasonable assumption on the statistics of\neigenvalues. We discuss ideas about the situation in higher dimensions, where\none can no longer ensure that interactions involving the Griffiths regions are\nmuch smaller than the typical energy-level spacing for such regions. We argue\nthat ergodicity is restored in dimension d > 1, although equilibration should\nbe extremely slow, similar to the dynamics of glasses.\n', 'title': 'Many-Body Localization: Stability and Instability'}]","The challenge that arises when considering many-body localization in higher dimensions is that one can no longer ensure that interactions involving the Griffiths regions are much smaller than the typical energy-level spacing for such regions.

Title: Many-Body Localization: Stability and Instability

Abstract: Rare regions with weak disorder (Griffiths regions) have the potential to spoil localization. We describe a non-perturbative construction of local integrals of motion (LIOMs) for a weakly interacting spin chain in one dimension, under a physically reasonable assumption on the statistics of eigenvalues. We discuss ideas about the situation in higher dimensions, where one can no longer ensure that interactions involving the Griffiths regions are much smaller than the typical energy-level spacing for such regions. We argue that ergodicity is restored in dimension d > 1, although equilibration should be extremely slow, similar to the dynamics of glasses."
What is a new proof of a result of Frantzikinakis and Host for multilinear averages of multiplicative functions?,45331fe2,"[{'abstract': ""  Let $f_1,\\ldots,f_k : \\mathbb{N} \\rightarrow \\mathbb{C}$ be multiplicative\nfunctions taking values in the closed unit disc. Using an analytic approach in\nthe spirit of Halász' mean value theorem, we compute multidimensional\naverages of the shape $$x^{-l} \\sum_{\\mathbf{n} \\in [x]^l} \\prod_{1 \\leq j \\leq\nk} f_j(L_j(\\mathbf{n}))$$ as $x \\rightarrow \\infty$, where $[x] := [1,x]$ and\n$L_1,\\ldots, L_k$ are affine linear forms that satisfy some natural conditions.\nOur approach gives a new proof of a result of Frantzikinakis and Host that is\ndistinct from theirs, with \\emph{explicit} main and error terms. \\\\ As an\napplication of our formulae, we establish a \\emph{local-to-global} principle\nfor Gowers norms of multiplicative functions. We also compute the asymptotic\ndensities of the sets of integers $n$ such that a given multiplicative function\n$f: \\mathbb{N} \\rightarrow \\{-1, 1\\}$ yields a fixed sign pattern of length 3\nor 4 on almost all 3- and 4-term arithmetic progressions, respectively, with\nfirst term $n$.\n"", 'title': 'Effective Asymptotic Formulae for Multilinear Averages of Multiplicative Functions'}]","I think I have found the answer!

Title: Effective Asymptotic Formulae for Multilinear Averages of Multiplicative Functions

Abstract: Let $f_1,\ldots,f_k : \mathbb{N} \rightarrow \mathbb{C}$ be multiplicative functions taking values in the closed unit disc. Using an analytic approach in the spirit of Halász' mean value theorem, we compute multidimensional averages of the shape $$x^{-l} \sum_{\mathbf{n} \in [x]^l} \prod_{1 \leq j \leq k} f_j(L_j(\mathbf{n}))$$ as $x \rightarrow \infty$, where $[x] := [1,x]$ and $L_1,\ldots, L_k$ are affine linear forms that satisfy some natural conditions. Our approach gives a new proof of a result of Frantzikinakis and Host that is distinct from theirs, with \emph{explicit} main and error terms. As an application of our formulae, we establish a \emph{local-to-global} principle for Gowers norms of multiplicative functions. We also compute the asymptotic densities of the sets of integers $n$ such that a given multiplicative function $f: \mathbb{N} \rightarrow \{-1, 1\}$ yields a fixed sign pattern of length 3 or 4 on almost all 3- and 4-term arithmetic progressions, respectively, with first term $n$."
What is the time complexity of a method for learning sparse polymatrix games from observations?,0cc15a21,"[{'abstract': '  We consider the problem of learning sparse polymatrix games from observations\nof strategic interactions. We show that a polynomial time method based on\n$\\ell_{1,2}$-group regularized logistic regression recovers a game, whose Nash\nequilibria are the $\\epsilon$-Nash equilibria of the game from which the data\nwas generated (true game), in $\\mathcal{O}(m^4 d^4 \\log (pd))$ samples of\nstrategy profiles --- where $m$ is the maximum number of pure strategies of a\nplayer, $p$ is the number of players, and $d$ is the maximum degree of the game\ngraph. Under slightly more stringent separability conditions on the payoff\nmatrices of the true game, we show that our method learns a game with the exact\nsame Nash equilibria as the true game. We also show that $\\Omega(d \\log (pm))$\nsamples are necessary for any method to consistently recover a game, with the\nsame Nash-equilibria as the true game, from observations of strategic\ninteractions. We verify our theoretical results through simulation experiments.\n', 'title': 'Learning Sparse Polymatrix Games in Polynomial Time and Sample Complexity'}]","Title: Learning Sparse Polymatrix Games in Polynomial Time and Sample Complexity

Abstract: We consider the problem of learning sparse polymatrix games from observations of strategic interactions. We show that a polynomial time method based on $\ell_{1,2}$-group regularized logistic regression recovers a game, whose Nash equilibria are the $\epsilon$-Nash equilibria of the game from which the data was generated (true game), in $\mathcal{O}(m^4 d^4 \log (pd))$ samples of strategy profiles --- where $m$ is the maximum number of pure strategies of a player, $p$ is the number of players, and $d$ is the maximum degree of the game graph. Under slightly more stringent separability conditions on the payoff matrices of the true game, we show that our method learns a game with the exact same Nash equilibria as the true game. We also show that $\Omega(d \log (pm))$ samples are necessary for any method to consistently recover a game, with the same Nash-equilibria as the true game, from observations of strategic interactions. We verify our theoretical results through simulation experiments."
What question about odd-dimensional ellipsoids does this paper partially answer? ,64afa0d9,"[{'abstract': '  Let $D$ be a bounded domain $D$ in $\\mathbb R^n $ with infinitely smooth\nboundary and $n$ is odd. We prove that if the volume cut off from the domain by\na hyperplane is an algebraic function of the hyperplane, free of real singular\npoints, then the domain is an ellipsoid. This partially answers a question of\nV.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically\nintegrable domains?\n', 'title': 'On algebraically integrable domains in Euclidean spaces'}]","Title: On algebraically integrable domains in Euclidean spaces

Abstract: Let $D$ be a bounded domain $D$ in $\mathbb R^n $ with infinitely smooth boundary and $n$ is odd. We prove that if the volume cut off from the domain by a hyperplane is an algebraic function of the hyperplane, free of real singular points, then the domain is an ellipsoid. This partially answers a question of V.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically integrable domains?"
What is a new measure of nonclassicality for single mode quantum states of light proposed in the context of Wehrl entropy?,a49eaf59,"[{'abstract': '  Nonclassical states of a quantized light are described in terms of\nGlauber-Sudarshan P distribution which is not a genuine classical probability\ndistribution. Despite several attempts, defining a uniform measure of\nnonclassicality (NC) for the single mode quantum states of light is yet an open\ntask. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that\nthe existing well-known measures fail to quantify the NC of single mode states\nthat are generated under multiple NC-inducing operations. Recently, Ivan et.\nal. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of\nnon-Gaussian character of quantum optical states in terms of Wehrl entropy.\nHere, we adopt this concept in the context of single mode NC. In this paper, we\npropose a new quantification of NC for the single mode quantum states of light\nas the difference between the total Wehrl entropy of the state and the maximum\nWehrl entropy arising due to its classical characteristics. This we achieve by\nsubtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any\nclassical state that has same randomness as measured in terms of von-Neumann\nentropy. We obtain analytic expressions of NC for most of the states, in\nparticular, all pure states and Gaussian mixed states. However, the evaluation\nof NC for the non-Gaussian mixed states is subject to extensive numerical\ncomputation that lies beyond the scope of the current work. We show that, along\nwith the states generated under single NC-inducing operations, also for the\nbroader class of states that are generated under multiple NC-inducing\noperations, our quantification enumerates the NC consistently.\n', 'title': 'Wehrl Entropy Based Quantification of Nonclassicality for Single Mode Quantum Optical States'}]","Title: Wehrl Entropy Based Quantification of Nonclassicality for Single Mode Quantum Optical States

Abstract: Nonclassical states of a quantized light are described in terms of 
Glauber-Sudarshan P distribution which is not a genuine classical probability 
distribution. Despite several attempts, defining a uniform measure of 
nonclassicality (NC) for the single mode quantum states of light is yet an open 
task. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that 
the existing well-known measures fail to quantify the NC of single mode states 
that are generated under multiple NC-inducing operations. Recently, Ivan et. 
al. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of 
non-Gaussian character of quantum optical states in terms of Wehrl entropy. 
Here, we adopt this concept in the context of single mode NC. In this paper, we 
propose a new quantification of NC for the single mode quantum states of light 
as the difference between the total Wehrl entropy of the state and the maximum 
Wehrl entropy arising due to its classical characteristics. This we achieve by 
subtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any 
classical state that has same randomness as measured in terms of von-Neumann 
entropy. We obtain analytic expressions of NC for most of the states, in 
particular, all pure states and Gaussian mixed states. However, the evaluation 
of NC for the non-Gaussian mixed states is subject to extensive numerical 
computation that lies beyond the scope of the current work. We show that, along 
with the states generated under single NC-inducing operations, also for the 
broader class of states that are generated under multiple NC-inducing 
operations, our quantification enumerates the NC consistently."
Are there any papers that simplify the technical aspects of Karmarkar's Algorithm?,dcb42f5f,"[{'abstract': ""  An extremely simple, description of Karmarkar's algorithm with very few\ntechnical terms is given.\n"", 'title': ""A simple introduction to Karmarkar's Algorithm for Linear Programming""}]","Yes, I can help you with that!

Title: A simple introduction to Karmarkar's Algorithm for Linear Programming

Abstract: An extremely simple, description of Karmarkar's algorithm with very few technical terms is given.

This paper seems to be a good match for your request, as it promises to provide a simple explanation of Karmarkar's Algorithm with minimal technical jargon."
"In what ways does the set of holomorphic functions in the unit disk, where zeros lie on one ray and 1-points lie on another, behave?",fd28ed35,"[{'abstract': '  Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let\n${\\mathcal F}$ be the family of all functions holomorphic in the unit disk\n${\\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on\n$L_1$. It is shown that ${\\mathcal F}$ is normal in ${\\mathbb\nD}\\backslash\\{0\\}$. The case where $L_0$ is the positive real axis and $L_1$ is\nthe negative real axis is studied in more detail.\n', 'title': 'Radially distributed values and normal families'}]","Based on the context, I think I can help you with your question.

Title: Radially distributed values and normal families

Abstract: Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let ${\mathcal F}$ be the family of all functions holomorphic in the unit disk ${\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on $L_1$. It is shown that ${\mathcal F}$ is normal in ${\mathbb D}\backslash\{0\}$. The case where $L_0$ is the positive real axis and $L_1$ is the negative real axis is studied in more detail.

This research paper seems to be discussing the behavior of the set of holomorphic functions in the unit disk, where zeros lie on one ray and 1-points lie on another, and it is shown that this family of functions is normal in the unit disk excluding the origin."
How does this hierarchical method compare to other methods in terms of learning speed and accuracy?,a0eea4fb,"[{'abstract': '  We propose a new splitting criterion for a meta-learning approach to\nmulticlass classifier design that adaptively merges the classes into a\ntree-structured hierarchy of increasingly difficult binary classification\nproblems. The classification tree is constructed from empirical estimates of\nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that\nrank the binary subproblems in terms of difficulty of classification. The\nproposed empirical estimates of the Bayes error rate are computed from the\nminimal spanning tree (MST) of the samples from each pair of classes. Moreover,\na meta-learning technique is presented for quantifying the one-vs-rest Bayes\nerror rate for each individual class from a single MST on the entire dataset.\nExtensive simulations on benchmark datasets show that the proposed hierarchical\nmethod can often be learned much faster than competing methods, while achieving\ncompetitive accuracy.\n', 'title': 'Fast Meta-Learning for Adaptive Hierarchical Classifier Design'}]","Title: Fast Meta-Learning for Adaptive Hierarchical Classifier Design

Abstract: We propose a new splitting criterion for a meta-learning approach to multiclass classifier design that adaptively merges the classes into a tree-structured hierarchy of increasingly difficult binary classification problems. The classification tree is constructed from empirical estimates of the Henze-Penrose bounds on the pairwise Bayes misclassification rates that rank the binary subproblems in terms of difficulty of classification. The proposed empirical estimates of the Bayes error rate are computed from the minimal spanning tree (MST) of the samples from each pair of classes. Moreover, a meta-learning technique is presented for quantifying the one-vs-rest Bayes error rate for each individual class from a single MST on the entire dataset. Extensive simulations on benchmark datasets show that the proposed hierarchical method can often be learned much faster than competing methods, while achieving competitive accuracy.

According to the abstract, the proposed hierarchical method can ""often be learned much faster than competing methods, while achieving competitive accuracy"". This suggests that the method has an advantage in terms of learning speed, while still maintaining competitive accuracy. However, it does not provide a direct comparison to other specific methods in terms of learning speed and accuracy."
What is the impact of using a recurrent neural network for analyzing Dow Jones Industrial Average index constituent volatility? ,51ecad88,"[{'abstract': '  We investigate the predictability of several range-based stock volatility\nestimators, and compare them to the standard close-to-close estimator which is\nmost commonly acknowledged as the volatility. The patterns of volatility\nchanges are analyzed using LSTM recurrent neural networks, which are a state of\nthe art method of sequence learning. We implement the analysis on all current\nconstituents of the Dow Jones Industrial Average index, and report averaged\nevaluation results. We find that changes in the values of range-based\nestimators are more predictable than that of the estimator using daily closing\nvalues only.\n', 'title': 'Exploring the predictability of range-based volatility estimators using RNNs'}]","Title: Exploring the predictability of range-based volatility estimators using RNNs

Abstract: We investigate the predictability of several range-based stock volatility estimators, and compare them to the standard close-to-close estimator which is most commonly acknowledged as the volatility. The patterns of volatility changes are analyzed using LSTM recurrent neural networks, which are a state of the art method of sequence learning. We implement the analysis on all current constituents of the Dow Jones Industrial Average index, and report averaged evaluation results. We find that changes in the values of range-based estimators are more predictable than that of the estimator using daily closing values only."
What algorithms are used to manage spectrum access in a multi-user environment where the number of users is unknown?,9e452d96,"[{'abstract': '  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms\nfor uncoordinated spectrum access. The number of users is assumed to be unknown\nto each user. A stochastic setting is first considered, where the rewards on a\nchannel are the same for each user. In contrast to prior work, it is assumed\nthat the number of users can possibly exceed the number of channels, and that\nrewards can be non-zero even under collisions. The proposed algorithm consists\nof an estimation phase and an allocation phase. It is shown that if every user\nadopts the algorithm, the system wide regret is constant with time with high\nprobability. The regret guarantees hold for any number of users and channels,\nin particular, even when the number of users is less than the number of\nchannels. Next, an adversarial multi-user MAB framework is considered, where\nthe rewards on the channels are user-dependent. It is assumed that the number\nof users is less than the number of channels, and that the users receive zero\nreward on collision. The proposed algorithm combines the Exp3.P algorithm\ndeveloped in prior work for single user adversarial bandits with a collision\nresolution mechanism to achieve sub-linear regret. It is shown that if every\nuser employs the proposed algorithm, the system wide regret is of the order\n$O(T^\\frac{3}{4})$ over a horizon of time $T$. The algorithms in both\nstochastic and adversarial scenarios are extended to the dynamic case where the\nnumber of users in the system evolves over time and are shown to lead to\nsub-linear regret.\n', 'title': 'Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access'}]","I think I have the answer!

Title: Deep Multi-User Reinforcement Learning for Distributed Dynamic Spectrum Access

Abstract: We consider the problem of dynamic spectrum access for network utility maximization in multichannel wireless networks. The shared bandwidth is divided into K orthogonal channels. In the beginning of each time slot, each user selects a channel and transmits a packet with a certain transmission probability. After each time slot, each user that has transmitted a packet receives a local observation indicating whether its packet was successfully delivered or not (i.e., ACK signal). The objective is a multi-user strategy for accessing the spectrum that maximizes a certain network utility in a distributed manner without online coordination or message exchanges between users. Obtaining an optimal solution for the spectrum access problem is computationally expensive in general due to the large state space and partial observability of the states. To tackle this problem, we develop a novel distributed dynamic spectrum access algorithm based on deep multi-user reinforcement leaning. Specifically, at each time slot, each user maps its current state to spectrum access actions based on a trained deep-Q network used to maximize the objective function. Game theoretic analysis of the system dynamics is developed for establishing design principles for the implementation of the algorithm. Experimental results demonstrate strong performance of the algorithm.

Additionally, I also found another relevant paper:

Title: Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access

Abstract: A multi-user multi-armed bandit (MAB) framework is used to develop algorithms for uncoordinated spectrum access. The number of users is assumed to be unknown to each user. A stochastic setting is first considered, where the rewards on a channel are the same for each user. In contrast to prior work, it is assumed that the number of users can possibly exceed the number of channels, and that rewards can be non-zero even under collisions. The proposed algorithm consists of an estimation phase and an allocation phase. It is shown that if every user adopts the algorithm, the system wide regret is constant with time with high probability. The regret guarantees hold for any number of users and channels, in particular, even when the number of users is less than the number of channels. Next, an adversarial multi-user MAB framework is considered, where the rewards on the channels are user-dependent. It is assumed that the number of users is less than the number of channels, and that the users receive zero reward on collision. The proposed algorithm combines the Exp3.P algorithm developed in prior work for single user adversarial bandits with a collision resolution mechanism to achieve sub-linear regret. It is shown that if every user employs the proposed algorithm, the system wide regret is of the order $O(T^\frac{3}{4})$ over a horizon of time $T$. The algorithms in both stochastic and adversarial scenarios are extended to the dynamic case where the number of users in the system evolves over time and are shown to lead to sub-linear regret."
What is a novel technique described in a paper to visualize the Chern number?,1212ea3c,"[{'abstract': '  Topology has appeared in different physical contexts. The most prominent\napplication is topologically protected edge transport in condensed matter\nphysics. The Chern number, the topological invariant of gapped Bloch\nHamiltonians, is an important quantity in this field. Another example of\ntopology, in polarization physics, are polarization singularities, called L\nlines and C points. By establishing a connection between these two theories, we\ndevelop a novel technique to visualize and potentially measure the Chern\nnumber: it can be expressed either as the winding of the polarization azimuth\nalong L lines in reciprocal space, or in terms of the handedness and the index\nof the C points. For mechanical systems, this is directly connected to the\nvisible motion patterns.\n', 'title': 'L lines, C points and Chern numbers: understanding band structure topology using polarization fields'}]","Title: L lines, C points and Chern numbers: understanding band structure topology using polarization fields

Abstract: Topology has appeared in different physical contexts. The most prominent application is topologically protected edge transport in condensed matter physics. The Chern number, the topological invariant of gapped Bloch Hamiltonians, is an important quantity in this field. Another example of topology, in polarization physics, are polarization singularities, called L lines and C points. By establishing a connection between these two theories, we develop a novel technique to visualize and potentially measure the Chern number: it can be expressed either as the winding of the polarization azimuth along L lines in reciprocal space, or in terms of the handedness and the index of the C points. For mechanical systems, this is directly connected to the visible motion patterns."
What are the challenges of implementing near-data processing in systems with multiple memory modules?,5fbab273,"[{'abstract': ""  Recent studies have demonstrated that near-data processing (NDP) is an\neffective technique for improving performance and energy efficiency of\ndata-intensive workloads. However, leveraging NDP in realistic systems with\nmultiple memory modules introduces a new challenge. In today's systems, where\nno computation occurs in memory modules, the physical address space is\ninterleaved at a fine granularity among all memory modules to help improve the\nutilization of processor-memory interfaces by distributing the memory traffic.\nHowever, this is at odds with efficient use of NDP, which requires careful\nplacement of data in memory modules such that near-data computations and their\nexclusively used data can be localized in individual memory modules, while\ndistributing shared data among memory modules to reduce hotspots. In order to\naddress this new challenge, we propose a set of techniques that (1) enable\ncollections of OS pages to either be fine-grain interleaved among memory\nmodules (as is done today) or to be placed contiguously on individual memory\nmodules (as is desirable for NDP private data), and (2) decide whether to\nlocalize or distribute each memory object based on its anticipated access\npattern and steer computations to the memory where the data they access is\nlocated. Our evaluations across a wide range of workloads show that the\nproposed mechanism improves performance by 31% and reduces 38% remote data\naccesses over a baseline system that cannot exploit computate-data affinity\ncharacteristics.\n"", 'title': 'CODA: Enabling Co-location of Computation and Data for Near-Data Processing'}]","Title: CODA: Enabling Co-location of Computation and Data for Near-Data Processing

Abstract: Recent studies have demonstrated that near-data processing (NDP) is an effective technique for improving performance and energy efficiency of data-intensive workloads. However, leveraging NDP in realistic systems with multiple memory modules introduces a new challenge. In today's systems, where no computation occurs in memory modules, the physical address space is interleaved at a fine granularity among all memory modules to help improve the utilization of processor-memory interfaces by distributing the memory traffic. However, this is at odds with efficient use of NDP, which requires careful placement of data in memory modules such that near-data computations and their exclusively used data can be localized in individual memory modules, while distributing shared data among memory modules to reduce hotspots."
What is a novel image segmentation pipeline that combines Fully Convolutional Networks and Fully Convolutional Residual Networks?,27a1f020,"[{'abstract': '  In this paper, we introduce a simple, yet powerful pipeline for medical image\nsegmentation that combines Fully Convolutional Networks (FCNs) with Fully\nConvolutional Residual Networks (FC-ResNets). We propose and examine a design\nthat takes particular advantage of recent advances in the understanding of both\nConvolutional Neural Networks as well as ResNets. Our approach focuses upon the\nimportance of a trainable pre-processing when using FC-ResNets and we show that\na low-capacity FCN model can serve as a pre-processor to normalize medical\ninput data. In our image segmentation pipeline, we use FCNs to obtain\nnormalized images, which are then iteratively refined by means of a FC-ResNet\nto generate a segmentation prediction. As in other fully convolutional\napproaches, our pipeline can be used off-the-shelf on different image\nmodalities. We show that using this pipeline, we exhibit state-of-the-art\nperformance on the challenging Electron Microscopy benchmark, when compared to\nother 2D methods. We improve segmentation results on CT images of liver\nlesions, when contrasting with standard FCN methods. Moreover, when applying\nour 2D pipeline on a challenging 3D MRI prostate segmentation challenge we\nreach results that are competitive even when compared to 3D methods. The\nobtained results illustrate the strong potential and versatility of the\npipeline by achieving highly accurate results on multi-modality images from\ndifferent anatomical regions and organs.\n', 'title': 'Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation'}]","I think I can help you with that!

The answer is:

Title: Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation

Abstract:   In this paper, we introduce a simple, yet powerful pipeline for medical image
segmentation that combines Fully Convolutional Networks (FCNs) with Fully
Convolutional Residual Networks (FC-ResNets). We propose and examine a design
that takes particular advantage of recent advances in the understanding of both
Convolutional Neural Networks as well as ResNets. Our approach focuses upon the
importance of a trainable pre-processing when using FC-ResNets and we show that
a low-capacity FCN model can serve as a pre-processor to normalize medical
input data. In our image segmentation pipeline, we use FCNs to obtain
normalized images, which are then iteratively refined by means of a FC-ResNet
to generate a segmentation prediction. As in other fully convolutional
approaches, our pipeline can be used off-the-shelf on different image
modalities. We show that using this pipeline, we exhibit state-of-the-art
performance on the challenging Electron Microscopy benchmark, when compared to
other 2D methods. We improve segmentation results on CT images of liver
lesions, when contrasting with standard FCN methods. Moreover, when applying
our 2D pipeline on a challenging 3D MRI prostate segmentation challenge we
reach results that are competitive even when compared to 3D methods. The
obtained results illustrate the strong potential and versatility of the
pipeline by achieving highly accurate results on multi-modality images from
different anatomical regions and organs."
How is the surface gradient of pressure phase used in the inverse Helmholtz solver?,423ccaae,"[{'abstract': ""  Assigning homogeneous boundary conditions, such as acoustic impedance, to the\nthermoviscous wave equations (TWE) derived by transforming the linearized\nNavier-Stokes equations (LNSE) to the frequency domain yields a so-called\nHelmholtz solver, whose output is a discrete set of complex eigenfunction and\neigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --\nreverses such procedure by returning the value of acoustic impedance at one or\nmore unknown impedance boundaries (IBs) of a given domain via spatial\nintegration of the TWE for a given real-valued frequency with assigned\nconditions on other boundaries. The iHS procedure is applied to a second-order\nspatial discretization of the TWEs derived on an unstructured grid with\nstaggered grid arrangement. The momentum equation only is extended to the\ncenter of each IB face where pressure and velocity components are co-located\nand treated as unknowns. One closure condition considered for the iHS is the\nassignment of the surface gradient of pressure phase over the IBs,\ncorresponding to assigning the shape of the acoustic waveform at the IB. The\niHS procedure is carried out independently for each frequency in order to\nreturn the complete broadband complex impedance distribution at the IBs in any\ndesired frequency range. The iHS approach is first validated against Rott's\ntheory for both inviscid and viscous, rectangular and circular ducts. The\nimpedance of a geometrically complex toy cavity is then reconstructed and\nverified against companion full compressible unstructured Navier-Stokes\nsimulations resolving the cavity geometry and one-dimensional impedance test\ntube calculations based on time-domain impedance boundary conditions (TDIBC).\nThe iHS methodology is also shown to capture thermoacoustic effects, with\nreconstructed impedance values quantitatively in agreement with thermoacoustic\ngrowth rates.\n"", 'title': 'Acoustic Impedance Calculation via Numerical Solution of the Inverse Helmholtz Problem'}]","Title: Acoustic Impedance Calculation via Numerical Solution of the Inverse Helmholtz Problem

Abstract:   Assigning homogeneous boundary conditions, such as acoustic impedance, to the thermoviscous wave equations (TWE) derived by transforming the linearized Navier-Stokes equations (LNSE) to the frequency domain yields a so-called Helmholtz solver, whose output is a discrete set of complex eigenfunction and eigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) -- reverses such procedure by returning the value of acoustic impedance at one or more unknown impedance boundaries (IBs) of a given domain via spatial integration of the TWE for a given real-valued frequency with assigned conditions on other boundaries. The iHS procedure is applied to a second-order spatial discretization of the TWEs derived on an unstructured grid with staggered grid arrangement. The momentum equation only is extended to the center of each IB face where pressure and velocity components are co-located and treated as unknowns. One closure condition considered for the iHS is the assignment of the surface gradient of pressure phase over the IBs, corresponding to assigning the shape of the acoustic waveform at the IB. The iHS procedure is carried out independently for each frequency in order to return the complete broadband complex impedance distribution at the IBs in any desired frequency range. The iHS approach is first validated against Rott's theory for both inviscid and viscous, rectangular and circular ducts. The impedance of a geometrically complex toy cavity is then reconstructed and verified against companion full compressible unstructured Navier-Stokes simulations resolving the cavity geometry and one-dimensional impedance test tube calculations based on time-domain impedance boundary conditions (TDIBC). The iHS methodology is also shown to capture thermoacoustic effects, with reconstructed impedance values quantitatively in agreement with thermoacoustic growth rates.

The surface gradient of pressure phase is used as a closure condition in the inverse Helmholtz solver, corresponding to assigning the shape of the acoustic waveform at the impedance boundary."
What conferences was the VPT 2017 workshop affiliated with?,fb9c9cbf,"[{'abstract': '  This volume contains the proceedings of the Fifth International Workshop on\nVerification and Program Transformation (VPT 2017). The workshop took place in\nUppsala, Sweden, on April 29th, 2017, affiliated with the European Joint\nConferences on Theory and Practice of Software (ETAPS). The aim of the VPT\nworkshop series is to provide a forum where people from the areas of program\ntransformation and program verification can fruitfully exchange ideas and gain\na deeper understanding of the interactions between those two fields. Seven\npapers were presented at the workshop. Additionally, three invited talks were\ngiven by Javier Esparza (Technische Universität München, Germany), Manuel\nHermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov\n(Linux Verification Center, ISPRAS, Moscow, Russia).\n', 'title': 'Proceedings Fifth International Workshop on Verification and Program Transformation'}]","Title: Proceedings Fifth International Workshop on Verification and Program Transformation

Abstract: This volume contains the proceedings of the Fifth International Workshop on Verification and Program Transformation (VPT 2017). The workshop took place in Uppsala, Sweden, on April 29th, 2017, affiliated with the European Joint Conferences on Theory and Practice of Software (ETAPS). The aim of the VPT workshop series is to provide a forum where people from the areas of program transformation and program verification can fruitfully exchange ideas and gain a deeper understanding of the interactions between those two fields. Seven papers were presented at the workshop. Additionally, three invited talks were given by Javier Esparza (Technische Universität München, Germany), Manuel Hermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov (Linux Verification Center, ISPRAS, Moscow, Russia).

Answer: The VPT 2017 workshop was affiliated with the European Joint Conferences on Theory and Practice of Software (ETAPS)."
"What is the target uptime for the FNAL magnetron H- ion source, and how has the upgrade impacted its lifetime?",5ae3c478,"[{'abstract': '  This paper will detail changes in the operational paradigm of the Fermi\nNational Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to\nupgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions\nfor High Energy Physics (HEP) experiments were extracted at ~18 keV vertically\ndownward into a 90 degree bending magnet and accelerated through a\nCockcroft-Walton accelerating column to 750 keV. Following the upgrade in the\nfall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35\nkeV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This\nchange in extraction energy as well as the orientation of the ion source\nrequired not only a redesign of the ion source, but an updated understanding of\nits operation at these new values. Discussed in detail are the changes to the\nion source timing, arc discharge current, hydrogen gas pressure, and cesium\ndelivery system that were needed to maintain consistent operation at >99%\nuptime for HEP, with an increased ion source lifetime of over 9 months.\n', 'title': 'Recent Operation of the FNAL Magnetron $H^{-}$ Ion Source'}]","Title: Recent Operation of the FNAL Magnetron $H^{-}$ Ion Source

Abstract: This paper will detail changes in the operational paradigm of the Fermi National Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to upgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions for High Energy Physics (HEP) experiments were extracted at ~18 keV vertically downward into a 90 degree bending magnet and accelerated through a Cockcroft-Walton accelerating column to 750 keV. Following the upgrade in the fall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35 keV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This change in extraction energy as well as the orientation of the ion source required not only a redesign of the ion source, but an updated understanding of its operation at these new values. Discussed in detail are the changes to the ion source timing, arc discharge current, hydrogen gas pressure, and cesium delivery system that were needed to maintain consistent operation at >99% uptime for HEP, with an increased ion source lifetime of over 9 months.

The target uptime for the FNAL magnetron H- ion source is >99%, and the upgrade has increased its lifetime to over 9 months."
"Besides self-similar random recursive sets, what other types of random homogeneous and V-variable sets are discussed in relation to overlap conditions? ",a96d0ccc,"[{'abstract': '  The class of stochastically self-similar sets contains many famous examples\nof random sets, e.g. Mandelbrot percolation and general fractal percolation.\nUnder the assumption of the uniform open set condition and some mild\nassumptions on the iterated function systems used, we show that the\nquasi-Assouad dimension of self-similar random recursive sets is almost surely\nequal to the almost sure Hausdorff dimension of the set. We further comment on\nrandom homogeneous and $V$-variable sets and the removal of overlap conditions.\n', 'title': 'The quasi-Assouad dimension for stochastically self-similar sets'}]","Unfortunately, I couldn't find any specific information about other types of random homogeneous and V-variable sets discussed in relation to overlap conditions besides self-similar random recursive sets.

The only mention of these sets is in the abstract of the paper ""The quasi-Assouad dimension for stochastically self-similar sets"", which states: ""We further comment on random homogeneous and V-variable sets and the removal of overlap conditions."" However, it doesn't provide any further information about these sets.

If you're interested in learning more about this topic, I can try to help you search for related papers or resources. Just let me know!"
How does the eccentricity of a planet's orbit affect the outcome of Roche-lobe overflow?,9a807874,"[{'abstract': '  Many giant exoplanets are found near their Roche limit and in mildly\neccentric orbits. In this study we examine the fate of such planets through\nRoche-lobe overflow as a function of the physical properties of the binary\ncomponents, including the eccentricity and the asynchronicity of the rotating\nplanet. We use a direct three-body integrator to compute the trajectories of\nthe lost mass in the ballistic limit and investigate the possible outcomes. We\nfind three different outcomes for the mass transferred through the Lagrangian\npoint $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the\nstellar surface, (iii) disk formation around the star. We explore the parameter\nspace of the three different regimes and find that at low eccentricities,\n$e\\lesssim 0.2$, mass overflow leads to disk formation for most systems, while\nfor higher eccentricities or retrograde orbits self-accretion is the only\npossible outcome. We conclude that the assumption often made in previous work\nthat when a planet overflows its Roche lobe it is quickly disrupted and\naccreted by the star is not always valid.\n', 'title': 'Roche-lobe overflow in eccentric planet-star systems'}]","Title: Roche-lobe overflow in eccentric planet-star systems

Abstract: Many giant exoplanets are found near their Roche limit and in mildly eccentric orbits. In this study we examine the fate of such planets through Roche-lobe overflow as a function of the physical properties of the binary components, including the eccentricity and the asynchronicity of the rotating planet. We use a direct three-body integrator to compute the trajectories of the lost mass in the ballistic limit and investigate the possible outcomes. We find three different outcomes for the mass transferred through the Lagrangian point $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the stellar surface, (iii) disk formation around the star. We explore the parameter space of the three different regimes and find that at low eccentricities, $e\lesssim 0.2$, mass overflow leads to disk formation for most systems, while for higher eccentricities or retrograde orbits self-accretion is the only possible outcome. We conclude that the assumption often made in previous work that when a planet overflows its Roche lobe it is quickly disrupted and accreted by the star is not always valid.

According to this paper, the eccentricity of a planet's orbit affects the outcome of Roche-lobe overflow. Specifically, at low eccentricities (e <= 0.2), mass overflow leads to disk formation around the star, while at higher eccentricities, self-accretion by the planet is the only possible outcome."
What kind of datasets are used to evaluate the efficacy of the proposed algorithm?,421a2f14,"[{'abstract': '  One of the popular approaches for low-rank tensor completion is to use the\nlatent trace norm regularization. However, most existing works in this\ndirection learn a sparse combination of tensors. In this work, we fill this gap\nby proposing a variant of the latent trace norm that helps in learning a\nnon-sparse combination of tensors. We develop a dual framework for solving the\nlow-rank tensor completion problem. We first show a novel characterization of\nthe dual solution space with an interesting factorization of the optimal\nsolution. Overall, the optimal solution is shown to lie on a Cartesian product\nof Riemannian manifolds. Furthermore, we exploit the versatile Riemannian\noptimization framework for proposing computationally efficient trust region\nalgorithm. The experiments illustrate the efficacy of the proposed algorithm on\nseveral real-world datasets across applications.\n', 'title': 'A dual framework for low-rank tensor completion'}]","Based on the context, I found that the answer to the question is:

Title: Experimental Design of a Prescribed Burn Instrumentation

Abstract:   Observational data collected during experiments, such as the planned Fire and Smoke Model Evaluation Experiment (FASMEE), are critical for progressing and transitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM into operational use. Historical meteorological data, representing typical weather conditions for the anticipated burn locations and times, have been processed to initialize and run a set of simulations representing the planned experimental burns. Based on an analysis of these numerical simulations, this paper provides recommendations on the experimental setup that include the ignition procedures, size and duration of the burns, and optimal sensor placement. New techniques are developed to initialize coupled fire-atmosphere simulations with weather conditions typical of the planned burn locations and time of the year. Analysis of variation and sensitivity analysis of simulation design to model parameters by repeated Latin Hypercube Sampling are used to assess the locations of the sensors. The simulations provide the locations of the measurements that maximize the expected variation of the sensor outputs with the model parameters.

The datasets used to evaluate the efficacy of the proposed algorithm are numerical simulations representing planned experimental burns, and historical meteorological data."
What is image repurposing and how do malicious actors use it?,f0ca9d23,"[{'abstract': ""  Nefarious actors on social media and other platforms often spread rumors and\nfalsehoods through images whose metadata (e.g., captions) have been modified to\nprovide visual substantiation of the rumor/falsehood. This type of modification\nis referred to as image repurposing, in which often an unmanipulated image is\npublished along with incorrect or manipulated metadata to serve the actor's\nulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)\ndataset, a substantially challenging dataset over that which has been\npreviously available to support research into image repurposing detection. The\nnew dataset includes location, person, and organization manipulations on\nreal-world data sourced from Flickr. We also present a novel, end-to-end, deep\nmultimodal learning model for assessing the integrity of an image by combining\ninformation extracted from the image with related information from a knowledge\nbase. The proposed method is compared against state-of-the-art techniques on\nexisting datasets as well as MEIR, where it outperforms existing methods across\nthe board, with AUC improvement up to 0.23.\n"", 'title': 'Deep Multimodal Image-Repurposing Detection'}]","I think I can help you with that!

Title: Deep Multimodal Image-Repurposing Detection

Abstract: Nefarious actors on social media and other platforms often spread rumors and falsehoods through images whose metadata (e.g., captions) have been modified to provide visual substantiation of the rumor/falsehood. This type of modification is referred to as image repurposing, in which often an unmanipulated image is published along with incorrect or manipulated metadata to serve the actor's ulterior motives.

So, according to this abstract, image repurposing is when malicious actors modify the metadata (e.g. captions) of an image to spread false information, often leaving the original image unchanged but providing misleading context."
What are some foundational results about classifying spaces of topological categories?,c8a268a2,"[{'abstract': ""  This is an exposition of homotopical results on the geometric realization of\nsemi-simplicial spaces. We then use these to derive basic foundational results\nabout classifying spaces of topological categories, possibly without units. The\ntopics considered include: fibrancy conditions on topological categories; the\neffect on classifying spaces of freely adjoining units; approximate notions of\nunits; Quillen's Theorems A and B for non-unital topological categories; the\neffect on classifying spaces of changing the topology on the space of objects;\nthe Group-Completion Theorem.\n"", 'title': 'Semi-simplicial spaces'}]","I think I can help you with that!

Title: Semi-simplicial spaces

Abstract: This is an exposition of homotopical results on the geometric realization of semi-simplicial spaces. We then use these to derive basic foundational results about classifying spaces of topological categories, possibly without units. The topics considered include: fibrancy conditions on topological categories; the effect on classifying spaces of freely adjoining units; approximate notions of units; Quillen's Theorems A and B for non-unital topological categories; the effect on classifying spaces of changing the topology on the space of objects; the Group-Completion Theorem.

Let me know if this is what you were looking for!"
How does analyzing secondary eclipse spectra differ from studying primary eclipses in exoplanet research?,1f456bc1,"[{'abstract': '  We extend a data-based model-free multifractal method of exoplanet detection\nto probe exoplanetary atmospheres. Whereas the transmission spectrum is studied\nduring the primary eclipse, we analyze the emission spectrum during the\nsecondary eclipse, thereby probing the atmospheric limb. In addition to the\nspectral structure of exoplanet atmospheres, the approach provides information\nto study phenomena such as atmospheric flows, tidal-locking behavior, and the\ndayside-nightside redistribution of energy. The approach is demonstrated using\nSpitzer data for exoplanet HD189733b. The central advantage of the method is\nthe lack of model assumptions in the detection and observational schemes.\n', 'title': 'Exoplanet Atmosphere Retrieval using Multifractal Analysis of Secondary Eclipse Spectra'}]","Here's the answer:

Title: Exoplanet Atmosphere Retrieval using Multifractal Analysis of Secondary Eclipse Spectra

Abstract:   We extend a data-based model-free multifractal method of exoplanet detection
to probe exoplanetary atmospheres. Whereas the transmission spectrum is studied
during the primary eclipse, we analyze the emission spectrum during the
secondary eclipse, thereby probing the atmospheric limb. In addition to the
spectral structure of exoplanet atmospheres, the approach provides information
to study phenomena such as atmospheric flows, tidal-locking behavior, and the
dayside-nightside redistribution of energy. The approach is demonstrated using
Spitzer data for exoplanet HD189733b. The central advantage of the method is
the lack of model assumptions in the detection and observational schemes.

Analyzing secondary eclipse spectra differs from studying primary eclipses in that it allows researchers to study the emission spectrum of the exoplanet's atmosphere during the secondary eclipse, which provides information about the atmospheric limb, whereas primary eclipses study the transmission spectrum. This difference in approach enables the study of phenomena such as atmospheric flows, tidal-locking behavior, and the dayside-nightside redistribution of energy."
Are there papers discussing the Baire product problem and its implications for the strong Choquet game?,3dd138cd,"[{'abstract': '  New results on the Baire product problem are presented. It is shown that an\narbitrary product of almost locally ccc Baire spaces is Baire; moreover, the\nproduct of a Baire space and a 1st countable space which is $\\beta$-unfavorable\nin the strong Choquet game is Baire.\n', 'title': 'More on products of Baire spaces'}]","I think I can help you with that!

Title: More on products of Baire spaces

Abstract: New results on the Baire product problem are presented. It is shown that an arbitrary product of almost locally ccc Baire spaces is Baire; moreover, the product of a Baire space and a 1st countable space which is β-unfavorable in the strong Choquet game is Baire."
What are the potential applications of game design principles in developing new algorithms?,23c0f86a,"[{'abstract': '  Examining games from a fresh perspective we present the idea of game-inspired\nand game-based algorithms, dubbed ""gamorithms"".\n', 'title': 'Gamorithm'}]","I think I can help you with that!

Title: Gamorithm

Abstract: Examining games from a fresh perspective we present the idea of game-inspired and game-based algorithms, dubbed ""gamorithms""."
What types of attacks are considered in the threat model for adversarial examples against reinforcement learning policies?,d129d5d7,"[{'abstract': '  Machine learning classifiers are known to be vulnerable to inputs maliciously\nconstructed by adversaries to force misclassification. Such adversarial\nexamples have been extensively studied in the context of computer vision\napplications. In this work, we show adversarial attacks are also effective when\ntargeting neural network policies in reinforcement learning. Specifically, we\nshow existing adversarial example crafting techniques can be used to\nsignificantly degrade test-time performance of trained policies. Our threat\nmodel considers adversaries capable of introducing small perturbations to the\nraw input of the policy. We characterize the degree of vulnerability across\ntasks and training algorithms, for a subclass of adversarial-example attacks in\nwhite-box and black-box settings. Regardless of the learned task or training\nalgorithm, we observe a significant drop in performance, even with small\nadversarial perturbations that do not interfere with human perception. Videos\nare available at this http URL.\n', 'title': 'Adversarial Attacks on Neural Network Policies'}]","Title: Adversarial Attacks on Neural Network Policies

Abstract: Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception."
Does this paper discuss the notion of representations for BiHom-Lie colour algebras?,283bbf47,"[{'abstract': '  BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with\ntwo commuting multiplicative linear maps. The main purpose of this paper is to\ndefine representations and a cohomology of BiHom-Lie colour algebras and to\nstudy some key constructions and properties.\nMoreover, we discuss $\\alpha^{k}\\beta^l$-generalized derivations,\n$\\alpha^{k}\\beta^l$-quasi-derivations and $\\alpha^{k}\\beta^l$-quasi-centroid.\nWe provide some properties and their relationships with BiHom-Jordan colour\nalgebra.\n', 'title': 'BiHom-Lie colour algebras structures'}]","Title: BiHom-Lie Colour algebras structures

Abstract: BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with two commuting multiplicative linear maps. The main purpose of this paper is to define representations and a cohomology of BiHom-Lie colour algebras and to study some key constructions and properties. Moreover, we discuss $\alpha^{k}\beta^l$-generalized derivations, $\alpha^{k}\beta^l$-quasi-derivations and $\alpha^{k}\beta^l$-quasi-centroid. We provide some properties and their relationships with BiHom-Jordan colour algebra."
Can the majorization-minimization paradigm be applied to SVM risk minimization problems with different types of losses and penalties?,f563d740,"[{'abstract': '  Support vector machines (SVMs) are an important tool in modern data analysis.\nTraditionally, support vector machines have been fitted via quadratic\nprogramming, either using purpose-built or off-the-shelf algorithms. We present\nan alternative approach to SVM fitting via the majorization--minimization (MM)\nparadigm. Algorithms that are derived via MM algorithm constructions can be\nshown to monotonically decrease their objectives at each iteration, as well as\nbe globally convergent to stationary points. We demonstrate the construction of\niteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,\nfor SVM risk minimization problems involving the hinge, least-square,\nsquared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net\npenalizations. Successful implementations of our algorithms are presented via\nsome numerical examples.\n', 'title': 'Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines: A Majorization--Minimization Algorithm Approach'}]","Title: Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines: A Majorization--Minimization Algorithm Approach

Abstract:  Support vector machines (SVMs) are an important tool in modern data analysis.
Traditionally, support vector machines have been fitted via quadratic
programming, either using purpose-built or off-the-shelf algorithms. We present
an alternative approach to SVM fitting via the majorization--minimization (MM)
paradigm. Algorithms that are derived via MM algorithm constructions can be
shown to monotonically decrease their objectives at each iteration, as well as
be globally convergent to stationary points. We demonstrate the construction of
iteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,
for SVM risk minimization problems involving the hinge, least-square,
squared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net
penalizations. Successful implementations of our algorithms are presented via
some numerical examples."
What geometrical condition must be met for a triangle to be isosceles if it has two equal cevians?,bf5e1493,"[{'abstract': ""  In 1840 Jacob Steiner on Christian Rudolf's request proved that a triangle\nwith two equal bisectors is isosceles. But what about changing the bisectors to\ncevians? Cevian is any line segment in a triangle with one endpoint on a vertex\nof the triangle and other endpoint on the opposite side. Not for any pairs of\nequal cevians the triangle is isosceles. Theorem. If for a triangle ABC there\nare equal cevians issuing from A and B, which intersect on the bisector or on\nthe median of the angle C, then AC=BC (so the triangle ABC is isosceles).\nProposition. Let ABC be an isosceles triangle. Define circle C to be the circle\nsymmetric relative to AB to the circumscribed circle of the triangle ABC. Then\nthe locus of intersection points of pairs of equal cevians is the union of the\nbase AB, the triangle's axis of symmetry, and the circle C.\n"", 'title': 'When a triangle is isosceles?'}]","Title: When a triangle is isosceles?

Abstract:   In 1840 Jacob Steiner on Christian Rudolf's request proved that a triangle
with two equal bisectors is isosceles. But what about changing the bisectors to
cevians? Cevian is any line segment in a triangle with one endpoint on a vertex
of the triangle and other endpoint on the opposite side. Not for any pairs of
equal cevians the triangle is isosceles. Theorem. If for a triangle ABC there
are equal cevians issuing from A and B, which intersect on the bisector or on
the median of the angle C, then AC=BC (so the triangle ABC is isosceles).
Proposition. Let ABC be an isosceles triangle. Define circle C to be the circle
symmetric relative to AB to the circumscribed circle of the triangle ABC. Then
the locus of intersection points of pairs of equal cevians is the union of the
base AB, the triangle's axis of symmetry, and the circle C."
This dataset includes manipulations on what type of data sourced from Flickr?,f0ca9d23,"[{'abstract': ""  Nefarious actors on social media and other platforms often spread rumors and\nfalsehoods through images whose metadata (e.g., captions) have been modified to\nprovide visual substantiation of the rumor/falsehood. This type of modification\nis referred to as image repurposing, in which often an unmanipulated image is\npublished along with incorrect or manipulated metadata to serve the actor's\nulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)\ndataset, a substantially challenging dataset over that which has been\npreviously available to support research into image repurposing detection. The\nnew dataset includes location, person, and organization manipulations on\nreal-world data sourced from Flickr. We also present a novel, end-to-end, deep\nmultimodal learning model for assessing the integrity of an image by combining\ninformation extracted from the image with related information from a knowledge\nbase. The proposed method is compared against state-of-the-art techniques on\nexisting datasets as well as MEIR, where it outperforms existing methods across\nthe board, with AUC improvement up to 0.23.\n"", 'title': 'Deep Multimodal Image-Repurposing Detection'}]","I think I can help you with that!

Title: Deep Multimodal Image-Repurposing Detection

Abstract: Nefarious actors on social media and other platforms often spread rumors and falsehoods through images whose metadata (e.g., captions) have been modified to provide visual substantiation of the rumor/falsehood. This type of modification is referred to as image repurposing, in which often an unmanipulated image is published along with incorrect or manipulated metadata to serve the actor's ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR) dataset, a substantially challenging dataset over that which has been previously available to support research into image repurposing detection. The new dataset includes location, person, and organization manipulations on real-world data sourced from Flickr. We also present a novel, end-to-end, deep multimodal learning model for assessing the integrity of an image by combining information extracted from the image with related information from a knowledge base. The proposed method is compared against state-of-the-art techniques on existing datasets as well as MEIR, where it outperforms existing methods across the board, with AUC improvement up to 0.23.

The answer is: real-world data."
Does the U3Si2-FeCrAl system exhibit less reactivity variation over its service lifetime compared to the current fuel system?,7764154e,"[{'abstract': '  Neutronic performance is investigated for a potential accident tolerant fuel\n(ATF),which consists of U$_3$Si$_2$ fuel and FeCrAl cladding. In comparison\nwith current UO$_2$-Zr system, FeCrAl has a better oxidation resistance but a\nlarger thermal neutron absorption cross section. U$_3$Si$_2$ has a higher\nthermal conductivity and a higher uranium density, which can compensate the\nreactivity suppressed by FeCrAl. Based on neutronic investigations, a possible\nU$_3$Si$_2$-FeCrAl fuel-cladding systemis taken into consideration. Fundamental\nproperties of the suggested fuel-cladding combination are investigated in a\nfuel assembly.These properties include moderator and fuel temperature\ncoefficients, control rods worth, radial power distribution (in a fuel rod),\nand different void reactivity coefficients. The present work proves that the\nnew combination has less reactivity variation during its service lifetime.\nAlthough, compared with the current system, it has a little larger deviation on\npower distribution and a little less negative temperature coefficient and void\nreactivity coefficient and its control rods worth is less important, variations\nof these parameters are less important during the service lifetime of fuel.\nHence, U$_3$Si$_2$-FeCrAl system is a potential ATF candidate from a neutronic\nview.\n', 'title': 'Neutronic Analysis on Potential Accident Tolerant Fuel-Cladding Combination U$_3$Si$_2$-FeCrAl'}]","Here is the answer:

Title: Neutronic Analysis on Potential Accident Tolerant Fuel-Cladding Combination U$_3$Si$_2$-FeCrAl

Abstract: Neutronic performance is investigated for a potential accident tolerant fuel (ATF), which consists of U$_3$Si$_2$ fuel and FeCrAl cladding. In comparison with current UO$_2$-Zr system, FeCrAl has a better oxidation resistance but a larger thermal neutron absorption cross section. U$_3$Si$_2$ has a higher thermal conductivity and a higher uranium density, which can compensate the reactivity suppressed by FeCrAl. Based on neutronic investigations, a possible U$_3$Si$_2$-FeCrAl fuel-cladding system is taken into consideration. Fundamental properties of the suggested fuel-cladding combination are investigated in a fuel assembly. These properties include moderator and fuel temperature coefficients, control rods worth, radial power distribution (in a fuel rod), and different void reactivity coefficients. The present work proves that the new combination has less reactivity variation during its service lifetime."
What recent research explores the Baire property of topological spaces in product settings?,3dd138cd,"[{'abstract': '  New results on the Baire product problem are presented. It is shown that an\narbitrary product of almost locally ccc Baire spaces is Baire; moreover, the\nproduct of a Baire space and a 1st countable space which is $\\beta$-unfavorable\nin the strong Choquet game is Baire.\n', 'title': 'More on products of Baire spaces'}]","Based on the provided context, the research paper that explores the Baire property of topological spaces in product settings is:

Title: More on products of Baire spaces

Abstract: New results on the Baire product problem are presented. It is shown that an arbitrary product of almost locally ccc Baire spaces is Baire; moreover, the product of a Baire space and a 1st countable space which is $\beta$-unfavorable in the strong Choquet game is Baire."
"Which statistical problems, such as detection or clustering, are explored in the context of sparse mixture models and phase retrieval?",02e195cb,"[{'abstract': '  We study the fundamental tradeoffs between statistical accuracy and\ncomputational tractability in the analysis of high dimensional heterogeneous\ndata. As examples, we study sparse Gaussian mixture model, mixture of sparse\nlinear regressions, and sparse phase retrieval model. For these models, we\nexploit an oracle-based computational model to establish conjecture-free\ncomputationally feasible minimax lower bounds, which quantify the minimum\nsignal strength required for the existence of any algorithm that is both\ncomputationally tractable and statistically accurate. Our analysis shows that\nthere exist significant gaps between computationally feasible minimax risks and\nclassical ones. These gaps quantify the statistical price we must pay to\nachieve computational tractability in the presence of data heterogeneity. Our\nresults cover the problems of detection, estimation, support recovery, and\nclustering, and moreover, resolve several conjectures of Azizyan et al. (2013,\n2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our\nresults reveal a new but counter-intuitive phenomenon in heterogeneous data\nanalysis that more data might lead to less computation complexity.\n', 'title': 'Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models and Phase Retrieval'}]","The answer to the question can be found in the first paper:

Title: Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models and Phase Retrieval

Abstract: We study the fundamental tradeoffs between statistical accuracy and computational tractability in the analysis of high dimensional heterogeneous data. As examples, we study sparse Gaussian mixture model, mixture of sparse linear regressions, and sparse phase retrieval model. For these models, we exploit an oracle-based computational model to establish conjecture-free computationally feasible minimax lower bounds, which quantify the minimum signal strength required for the existence of any algorithm that is both computationally tractable and statistically accurate. Our analysis shows that there exist significant gaps between computationally feasible minimax risks and classical ones. These gaps quantify the statistical price we must pay to achieve computational tractability in the presence of data heterogeneity. Our results cover the problems of **detection, estimation, support recovery, and clustering**, and moreover, resolve several conjectures of Azizyan et al. (2013, 2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our results reveal a new but counter-intuitive phenomenon in heterogeneous data analysis that more data might lead to less computation complexity."
Can camphoric acid movement in a gel tablet be described by using three distinct modes?,1656ac4b,"[{'abstract': '  We report experiments on an agarose gel tablet loaded with camphoric acid\n(c-boat) set into self-motion by interfacial tension gradients at the air-water\ninterface. We observe three distinct modes of c-boat motion: harmonic mode\nwhere the c-boat speed oscillates sinusoidally in time, a steady mode where the\nc-boat maintains constant speed, and a relaxation oscillation mode where the\nc-boat maintains near-zero speed between sudden jumps in speed and position at\nregular time intervals. Whereas all three modes have been separately reported\nbefore in different systems, we show they belong to a common description.\nThrough control of the air-water surface tension with Sodium Dodecyl Sulfate\n(SDS), we experimentally deduce the three self-propulsive modes result from\nsurface tension difference between Camphoric Acid (CA) and the ambient\nsurroundings.\n', 'title': 'Dynamics of a Camphoric Acid boat at the air-water interface'}]","Title: Dynamics of a Camphoric Acid boat at the air-water interface

Abstract: We report experiments on an agarose gel tablet loaded with camphoric acid (c-boat) set into self-motion by interfacial tension gradients at the air-water interface. We observe three distinct modes of c-boat motion: harmonic mode where the c-boat speed oscillates sinusoidally in time, a steady mode where the c-boat maintains constant speed, and a relaxation oscillation mode where the c-boat maintains near-zero speed between sudden jumps in speed and position at regular time intervals. Whereas all three modes have been separately reported before in different systems, we show they belong to a common description. Through control of the air-water surface tension with Sodium Dodecyl Sulfate (SDS), we experimentally deduce the three self-propulsive modes result from surface tension difference between Camphoric Acid (CA) and the ambient surroundings."
What are the main challenges in conducting dynamic security analysis of power systems?,467cb5e1,"[{'abstract': '  Dynamic security analysis is an important problem of power systems on\nensuring safe operation and stable power supply even when certain faults occur.\nNo matter such faults are caused by vulnerabilities of system components,\nphysical attacks, or cyber-attacks that are more related to cyber-security,\nthey eventually affect the physical stability of a power system. Examples of\nthe loss of physical stability include the Northeast blackout of 2003 in North\nAmerica and the 2015 system-wide blackout in Ukraine. The nonlinear hybrid\nnature, that is, nonlinear continuous dynamics integrated with discrete\nswitching, and the high degree of freedom property of power system dynamics\nmake it challenging to conduct the dynamic security analysis. In this paper, we\nuse the hybrid automaton model to describe the dynamics of a power system and\nmainly deal with the index-1 differential-algebraic equation models regarding\nthe continuous dynamics in different discrete states. The analysis problem is\nformulated as a reachability problem of the associated hybrid model. A\nsampling-based algorithm is then proposed by integrating modeling and\nrandomized simulation of the hybrid dynamics to search for a feasible execution\nconnecting an initial state of the post-fault system and a target set in the\ndesired operation mode. The proposed method enables the use of existing power\nsystem simulators for the synthesis of discrete switching and control\nstrategies through randomized simulation. The effectiveness and performance of\nthe proposed approach are demonstrated with an application to the dynamic\nsecurity analysis of the New England 39-bus benchmark power system exhibiting\nhybrid dynamics. In addition to evaluating the dynamic security, the proposed\nmethod searches for a feasible strategy to ensure the dynamic security of the\nsystem in face of disruptions.\n', 'title': 'Dynamic Security Analysis of Power Systems by a Sampling-Based Algorithm'}]","The main challenges in conducting dynamic security analysis of power systems are:

Title: Dynamic Security Analysis of Power Systems by a Sampling-Based Algorithm

Abstract: ...The nonlinear hybrid nature, that is, nonlinear continuous dynamics integrated with discrete switching, and the high degree of freedom property of power system dynamics make it challenging to conduct the dynamic security analysis..."
Which video datasets were used to evaluate the performance of the proposed method and how did it compare to existing methods?,9329dc97,"[{'abstract': '  Deep learning has been demonstrated to achieve excellent results for image\nclassification and object detection. However, the impact of deep learning on\nvideo analysis (e.g. action detection and recognition) has been limited due to\ncomplexity of video data and lack of annotations. Previous convolutional neural\nnetworks (CNN) based video action detection approaches usually consist of two\nmajor steps: frame-level action proposal detection and association of proposals\nacross frames. Also, these methods employ two-stream CNN framework to handle\nspatial and temporal feature separately. In this paper, we propose an\nend-to-end deep network called Tube Convolutional Neural Network (T-CNN) for\naction detection in videos. The proposed architecture is a unified network that\nis able to recognize and localize action based on 3D convolution features. A\nvideo is first divided into equal length clips and for each clip a set of tube\nproposals are generated next based on 3D Convolutional Network (ConvNet)\nfeatures. Finally, the tube proposals of different clips are linked together\nemploying network flow and spatio-temporal action detection is performed using\nthese linked video proposals. Extensive experiments on several video datasets\ndemonstrate the superior performance of T-CNN for classifying and localizing\nactions in both trimmed and untrimmed videos compared to state-of-the-arts.\n', 'title': 'Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos'}]","Based on the provided context, I found a relevant answer in the following paper:

Title: Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement

Abstract: In this paper, we focus on fully automatic traffic surveillance camera calibration, which we use for speed measurement of passing vehicles. We improve over a recent state-of-the-art camera calibration method for traffic surveillance based on two detected vanishing points. More importantly, we propose a novel automatic scene scale inference method. The method is based on matching bounding boxes of rendered 3D models of vehicles with detected bounding boxes in the image. The proposed method can be used from arbitrary viewpoints, since it has no constraints on camera placement. We evaluate our method on the recent comprehensive dataset for speed measurement BrnoCompSpeed. Experiments show that our automatic camera calibration method by detection of two vanishing points reduces error by 50% (mean distance ratio error reduced from 0.18 to 0.09) compared to the previous state-of-the-art method. We also show that our scene scale inference method is more precise, outperforming both state-of-the-art automatic calibration method for speed measurement (error reduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error reduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results of the proposed automatic camera calibration method on video sequences obtained from real surveillance cameras in various places, and under different lighting conditions (night, dawn, day).

The video dataset used to evaluate the performance of the proposed method is BrnoCompSpeed."
What are the challenges of aligning narrow beams in millimeter wave backhaul systems with fixed nodes?,400eec78,"[{'abstract': '  Wireless backhaul communication has been recently realized with large\nantennas operating in the millimeter wave (mmWave) frequency band and\nimplementing highly directional beamforming. In this paper, we focus on the\nalignment problem of narrow beams between fixed position network nodes in\nmmWave backhaul systems that are subject to small displacements due to wind\nflow or ground vibration. We consider nodes equipped with antenna arrays that\nare capable of performing only analog processing and communicate through\nwireless channels including a line-of-sight component. Aiming at minimizing the\ntime needed to achieve beam alignment, we present an efficient method that\ncapitalizes on the exchange of position information between the nodes to design\ntheir beamforming and combining vectors. Some numerical results on the outage\nprobability with the proposed beam alignment method offer useful preliminary\ninsights on the impact of some system and operation parameters.\n', 'title': 'Position Aided Beam Alignment for Millimeter Wave Backhaul Systems with Large Phased Arrays'}]","I think I can help you with that!

Title: Position Aided Beam Alignment for Millimeter Wave Backhaul Systems with Large Phased Arrays

Abstract:  Wireless backhaul communication has been recently realized with large
antennas operating in the millimeter wave (mmWave) frequency band and
implementing highly directional beamforming. In this paper, we focus on the
alignment problem of narrow beams between fixed position network nodes in
mmWave backhaul systems that are subject to small displacements due to wind
flow or ground vibration. We consider nodes equipped with antenna arrays that
are capable of performing only analog processing and communicate through
wireless channels including a line-of-sight component. Aiming at minimizing the
time needed to achieve beam alignment, we present an efficient method that
capitalizes on the exchange of position information between the nodes to design
their beamforming and combining vectors. Some numerical results on the outage
probability with the proposed beam alignment method offer useful preliminary
insights on the impact of some system and operation parameters.

The challenge of aligning narrow beams in millimeter wave backhaul systems with fixed nodes appears to be the small displacements due to wind flow or ground vibration, which affects the alignment of the beams."
What are the advantages and disadvantages of AStar+ and DFS+ for GED computation?,6d4e833e,"[{'abstract': '  Graph edit distance (GED) is an important similarity measure adopted in a\nsimilarity-based analysis between two graphs, and computing GED is a primitive\noperator in graph database analysis. Partially due to the NP-hardness, the\nexisting techniques for computing GED are only able to process very small\ngraphs with less than 30 vertices. Motivated by this, in this paper we\nsystematically study the problems of both GED computation, and GED verification\n(i.e., verify whether the GED between two graphs is no larger than a user-given\nthreshold). Firstly, we develop a unified framework that can be instantiated\ninto either a best-first search approach AStar+ or a depth-first search\napproach DFS+. Secondly, we design anchor-aware lower bound estimation\ntechniques to compute tighter lower bounds for intermediate search states,\nwhich significantly reduce the search spaces of both AStar+ and DFS+. We also\npropose efficient techniques to compute the lower bounds. Thirdly, based on our\nunified framework, we contrast AStar+ with DFS+ regarding their time and space\ncomplexities, and recommend that AStar+ is better than DFS+ by having a much\nsmaller search space. Extensive empirical studies validate that AStar+ performs\nbetter than DFS+, and show that our AStar+-BMa approach outperforms the\nstate-of-the-art technique by more than four orders of magnitude.\n', 'title': 'Efficient Graph Edit Distance Computation and Verification via Anchor-aware Lower Bound Estimation'}]","Title: Efficient Graph Edit Distance Computation and Verification via Anchor-aware Lower Bound Estimation

Abstract: Graph edit distance (GED) is an important similarity measure adopted in a similarity-based analysis between two graphs, and computing GED is a primitive operator in graph database analysis. Partially due to the NP-hardness, the existing techniques for computing GED are only able to process very small graphs with less than 30 vertices. Motivated by this, in this paper we systematically study the problems of both GED computation, and GED verification (i.e., verify whether the GED between two graphs is no larger than a user-given threshold). Firstly, we develop a unified framework that can be instantiated into either a best-first search approach AStar+ or a depth-first search approach DFS+. Secondly, we design anchor-aware lower bound estimation techniques to compute tighter lower bounds for intermediate search states, which significantly reduce the search spaces of both AStar+ and DFS+. We also propose efficient techniques to compute the lower bounds. Thirdly, based on our unified framework, we contrast AStar+ with DFS+ regarding their time and space complexities, and recommend that AStar+ is better than DFS+ by having a much smaller search space. Extensive empirical studies validate that AStar+ performs better than DFS+, and show that our AStar+-BMa approach outperforms the state-of-the-art technique by more than four orders of magnitude.

Advantages of AStar+: 
- Has a much smaller search space compared to DFS+
- Performs better than DFS+ in extensive empirical studies
- Outperforms the state-of-the-art technique by more than four orders of magnitude with AStar+-BMa approach

Disadvantages of AStar+: 
- None mentioned in the abstract

Advantages of DFS+: 
- Can be used as an alternative approach for GED computation and verification

Disadvantages of DFS+: 
- Has a larger search space compared to AStar+
- Performs worse than AStar+ in extensive empirical studies"
What type of group schemes are considered in the context of vanishing theorems for stacks?,3a09c3ce,"[{'abstract': '  We prove that the homotopy algebraic K-theory of tame quasi-DM stacks\nsatisfies cdh-descent. We apply this descent result to prove that if X is a\nNoetherian tame quasi-DM stack and i < -dim(X), then K_i(X)[1/n] = 0 (resp.\nK_i(X, Z/n) = 0) provided that n is nilpotent on X (resp. is invertible on X).\nOur descent and vanishing results apply more generally to certain Artin stacks\nwhose stabilizers are extensions of finite group schemes by group schemes of\nmultiplicative type.\n', 'title': 'Vanishing theorems for the negative K-theory of stacks'}]","Title: Vanishing theorems for the negative K-theory of stacks

Abstract: We prove that the homotopy algebraic K-theory of tame quasi-DM stacks satisfies cdh-descent. We apply this descent result to prove that if X is a Noetherian tame quasi-DM stack and i < -dim(X), then K_i(X)[1/n] = 0 (resp. K_i(X, Z/n) = 0) provided that n is nilpotent on X (resp. is invertible on X). Our descent and vanishing results apply more generally to certain Artin stacks whose stabilizers are extensions of finite group schemes by group schemes of multiplicative type.

The type of group schemes considered in this context are finite group schemes and group schemes of multiplicative type."
What is the remaining unidentified component of the large deviation function for general weak solutions?,93f15441,"[{'abstract': '  We investigate the density large deviation function for a multidimensional\nconservation law in the vanishing viscosity limit, when the probability\nconcentrates on weak solutions of a hyperbolic conservation law conservation\nlaw. When the conductivity and dif-fusivity matrices are proportional, i.e. an\nEinstein-like relation is satisfied, the problem has been solved in [4]. When\nthis proportionality does not hold, we compute explicitly the large deviation\nfunction for a step-like density profile, and we show that the associated\noptimal current has a non trivial structure. We also derive a lower bound for\nthe large deviation function, valid for a general weak solution, and leave the\ngeneral large deviation function upper bound as a conjecture.\n', 'title': 'Density large deviations for multidimensional stochastic hyperbolic conservation laws'}]","Based on the context, I found the relevant paper:

Title: Density large deviations for multidimensional stochastic hyperbolic conservation laws

Abstract: We investigate the density large deviation function for a multidimensional conservation law in the vanishing viscosity limit, when the probability concentrates on weak solutions of a hyperbolic conservation law conservation law. When the conductivity and dif-fusivity matrices are proportional, i.e. an Einstein-like relation is satisfied, the problem has been solved in [4]. When this proportionality does not hold, we compute explicitly the large deviation function for a step-like density profile, and we show that the associated optimal current has a non-trivial structure. We also derive a lower bound for the large deviation function, valid for a general weak solution, and leave the general large deviation function upper bound as a conjecture.

The remaining unidentified component of the large deviation function is the upper bound for general weak solutions, which is left as a conjecture in this paper."
Can you find a paper discussing fixed points in Banach spaces that uses the property of compact order intervals?,09345cd0,"[{'abstract': ""  Let $X$ be a partially ordered set with the property that each family of\norder intervals of the form $[a,b],[a,\\rightarrow )$ with the finite\nintersection property has a nonempty intersection. We show that every directed\nsubset of $X$ has a supremum. Then we apply the above result to prove that if\n$X$ is a topological space with a partial order $\\preceq $ for which the order\nintervals are compact, $\\mathcal{F}$ a nonempty commutative family of monotone\nmaps from $X$ into $X$ and there exists $c\\in X$ such that $c\\preceq Tc$ for\nevery $T\\in \\mathcal{F}$, then the set of common fixed points of $\\mathcal{F}$\nis nonempty and has a maximal element. The result, specialized to the case of\nBanach spaces gives a general fixed point theorem that drops almost all\nassumptions from the recent results in this area. An application to the theory\nof integral equations of Urysohn's type is also given.\n"", 'title': 'The Knaster-Tarski theorem versus monotone nonexpansive mappings'}]","I think I can help you with that!

Title: The Knaster-Tarski theorem versus monotone nonexpansive mappings

Abstract:   Let $X$ be a partially ordered set with the property that each family of
order intervals of the form $[a,b],[a,\rightarrow )$ with the finite
intersection property has a nonempty intersection. We show that every directed
subset of $X$ has a supremum. Then we apply the above result to prove that if
$X$ is a topological space with a partial order $\preceq $ for which the order
intervals are compact, $\mathcal{F}$ a nonempty commutative family of monotone
maps from $X$ into $X$ and there exists $c\in X$ such that $c\preceq Tc$ for
every $T\in \mathcal{F}$, then the set of common fixed points of $\mathcal{F}$
is nonempty and has a maximal element. The result, specialized to the case of
Banach spaces gives a general fixed point theorem that drops almost all
assumptions from the recent results in this area. An application to the theory
of integral equations of Urysohn's type is also given."
What are the similarities between transportation networks of Brazilian cities and foreign cities?,1f3e7c4a,"[{'abstract': '  In this paper, a comparative study was conducted between complex networks\nrepresenting origin and destination survey data. Similarities were found\nbetween the characteristics of the networks of Brazilian cities with networks\nof foreign cities. Power laws were found in the distributions of edge weights\nand this scale - free behavior can occur due to the economic characteristics of\nthe cities.\n', 'title': 'Análise comparativa de pesquisas de origens e destinos: uma abordagem baseada em Redes Complexas'}]","Title: Análise comparativa de pesquisas de origens e destinos: uma abordagem baseada em Redes Complexas

Abstract: In this paper, a comparative study was conducted between complex networks representing origin and destination survey data. Similarities were found between the characteristics of the networks of Brazilian cities with networks of foreign cities. Power laws were found in the distributions of edge weights and this scale-free behavior can occur due to the economic characteristics of the cities.

The answer can be found in the abstract of this paper, which states that similarities were found between the characteristics of the networks of Brazilian cities and foreign cities, but it does not specify what these similarities are."
How have GBDTs been applied to solve problems in Weyl-Titchmarsh theory?,ac47e003,"[{'abstract': '  Generalized Bäcklund-Darboux transformations (GBDTs) of discrete\nskew-selfadjoint Dirac systems have been successfully used for explicit solving\nof direct and inverse problems of Weyl-Titchmarsh theory. During explicit\nsolving of the direct and inverse problems, we considered GBDTs of the trivial\ninitial systems. However, GBDTs of arbitrary discrete skew-selfadjoint Dirac\nsystems are important as well and we introduce these transformations in the\npresent paper. The obtained results are applied to the construction of explicit\nsolutions of the interesting related non-stationary systems.\n', 'title': 'GBDT of discrete skew-selfadjoint Dirac systems and explicit solutions of the corresponding non-stationary problems'}]","Title: GBDT of discrete skew-selfadjoint Dirac systems and explicit solutions of the corresponding non-stationary problems

Abstract: Generalized Bäcklund-Darboux transformations (GBDTs) of discrete skew-selfadjoint Dirac systems have been successfully used for explicit solving of direct and inverse problems of Weyl-Titchmarsh theory. During explicit solving of the direct and inverse problems, we considered GBDTs of the trivial initial systems. However, GBDTs of arbitrary discrete skew-selfadjoint Dirac systems are important as well and we introduce these transformations in the present paper. The obtained results are applied to the construction of explicit solutions of the interesting related non-stationary systems."
How does the proposed Wikipedia Index (WI) differ from traditional citation indices like the Hirsch index?,a1f042a0,"[{'abstract': '  The new index of the author\'s popularity estimation is represented in the\npaper. The index is calculated on the basis of Wikipedia encyclopedia analysis\n(Wikipedia Index - WI). Unlike the conventional existed citation indices, the\nsuggested mark allows to evaluate not only the popularity of the author, as it\ncan be done by means of calculating the general citation number or by the\nHirsch index, which is often used to measure the author\'s research rate. The\nindex gives an opportunity to estimate the author\'s popularity, his/her\ninfluence within the sought-after area ""knowledge area"" in the Internet - in\nthe Wikipedia. The suggested index is supposed to be calculated in frames of\nthe subject domain, and it, on the one hand, avoids the mistaken computation of\nthe homonyms, and on the other hand - provides the entirety of the subject\narea. There are proposed algorithms and the technique of the Wikipedia Index\ncalculation through the network encyclopedia sounding, the exemplified\ncalculations of the index for the prominent researchers, and also the methods\nof the information networks formation - models of the subject domains by the\nautomatic monitoring and networks information reference resources analysis. The\nconsidered in the paper notion network corresponds the terms-heads of the\nWikipedia articles.\n', 'title': 'Wiki-index of authors popularity'}]","Title: Wiki-index of authors popularity

Abstract: The new index of the author's popularity estimation is represented in the paper. The index is calculated on the basis of Wikipedia encyclopedia analysis (Wikipedia Index - WI). Unlike the conventional existed citation indices, the suggested mark allows to evaluate not only the popularity of the author, as it can be done by means of calculating the general citation number or by the Hirsch index, which is often used to measure the author's research rate. The index gives an opportunity to estimate the author's popularity, his/her influence within the sought-after area ""knowledge area"" in the Internet - in the Wikipedia. 

The difference mentioned in this abstract is that the Wikipedia Index (WI) allows to evaluate not only the popularity of the author but also his/her influence within a specific knowledge area in the Internet, which is not covered by traditional citation indices like the Hirsch index."
What is the computational complexity of determining if a matrix satisfies the Restricted Isometry Property for any accuracy?,12f63884,"[{'abstract': '  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts\nas an approximate isometry when restricted to sparse vectors. Previous work has\nshown it to be NP-hard to determine whether a matrix possess this property, but\nonly in a narrow range of parameters. In this work, we show that it is NP-hard\nto make this determination for any accuracy parameter, even when we restrict\nourselves to instances which are either RIP or far from being RIP. This result\nimplies that it is NP-hard to approximate the range of parameters for which a\nmatrix possesses the Restricted Isometry Property with accuracy better than\nsome constant. Ours is the first work to prove such a claim without any\nadditional assumptions.\n', 'title': 'Approximately certifying the restricted isometry property is hard'}]","Title: Approximately certifying the restricted isometry property is hard

Abstract: 
A matrix is said to possess the Restricted Isometry Property (RIP) if it acts as an approximate isometry when restricted to sparse vectors. Previous work has shown it to be NP-hard to determine whether a matrix possess this property, but only in a narrow range of parameters. In this work, we show that it is NP-hard to make this determination for any accuracy parameter, even when we restrict ourselves to instances which are either RIP or far from being RIP. This result implies that it is NP-hard to approximate the range of parameters for which a matrix possesses the Restricted Isometry Property with accuracy better than some constant. Ours is the first work to prove such a claim without any additional assumptions."
How does phaseless super-resolution relate to the problems of phase retrieval and super-resolution?,6f4a97ca,"[{'abstract': '  Phaseless super-resolution is the problem of recovering an unknown signal\nfrom measurements of the magnitudes of the low frequency Fourier transform of\nthe signal. This problem arises in applications where measuring the phase, and\nmaking high-frequency measurements, are either too costly or altogether\ninfeasible. The problem is especially challenging because it combines the\ndifficult problems of phase retrieval and classical super-resolution\n', 'title': 'Multiple Illumination Phaseless Super-Resolution (MIPS) with Applications To Phaseless DOA Estimation and Diffraction Imaging'}]","The researcher is looking for information on how phaseless super-resolution relates to the problems of phase retrieval and super-resolution.

Title: Multiple Illumination Phaseless Super-Resolution (MIPS) with Applications To Phaseless DOA Estimation and Diffraction Imaging

Abstract: Phaseless super-resolution is the problem of recovering an unknown signal from measurements of the magnitudes of the low frequency Fourier transform of the signal. This problem arises in applications where measuring the phase, and making high-frequency measurements, are either too costly or altogether infeasible. The problem is especially challenging because it combines the difficult problems of phase retrieval and classical super-resolution.

According to the abstract, phaseless super-resolution combines the problems of phase retrieval and classical super-resolution, making it especially challenging."
What kind of practical experiments have been conducted to demonstrate the failures of gradient-based deep learning?,de83307e,"[{'abstract': '  In recent years, Deep Learning has become the go-to solution for a broad\nrange of applications, often outperforming state-of-the-art. However, it is\nimportant, for both theoreticians and practitioners, to gain a deeper\nunderstanding of the difficulties and limitations associated with common\napproaches and algorithms. We describe four types of simple problems, for which\nthe gradient-based algorithms commonly used in deep learning either fail or\nsuffer from significant difficulties. We illustrate the failures through\npractical experiments, and provide theoretical insights explaining their\nsource, and how they might be remedied.\n', 'title': 'Failures of Gradient-Based Deep Learning'}]","Based on the provided context, I found that the paper ""Failures of Gradient-Based Deep Learning"" mentions that they ""illustrate the failures through practical experiments"". However, it doesn't provide specific details about the experiments.

If you're looking for practical experiments, you might be interested in the paper ""The Shattered Gradients Problem: If resnets are the answer, then what is the question?"". This paper provides ""detailed empirical evidence"" in support of their analysis, on both fully-connected networks and convnets.

Here is the relevant information:

Title: The Shattered Gradients Problem: If resnets are the answer, then what is the question?

Abstract: A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new ""looks linear"" (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections."
"What are some open research directions in systems, architectures, and security for AI?",980d0a2b,"[{'abstract': ""  With the increasing commoditization of computer vision, speech recognition\nand machine translation systems and the widespread deployment of learning-based\nback-end technologies such as digital advertising and intelligent\ninfrastructures, AI (Artificial Intelligence) has moved from research labs to\nproduction. These changes have been made possible by unprecedented levels of\ndata and computation, by methodological advances in machine learning, by\ninnovations in systems software and architectures, and by the broad\naccessibility of these technologies.\nThe next generation of AI systems promises to accelerate these developments\nand increasingly impact our lives via frequent interactions and making (often\nmission-critical) decisions on our behalf, often in highly personalized\ncontexts. Realizing this promise, however, raises daunting challenges. In\nparticular, we need AI systems that make timely and safe decisions in\nunpredictable environments, that are robust against sophisticated adversaries,\nand that can process ever increasing amounts of data across organizations and\nindividuals without compromising confidentiality. These challenges will be\nexacerbated by the end of the Moore's Law, which will constrain the amount of\ndata these technologies can store and process. In this paper, we propose\nseveral open research directions in systems, architectures, and security that\ncan address these challenges and help unlock AI's potential to improve lives\nand society.\n"", 'title': 'A Berkeley View of Systems Challenges for AI'}]","Title: A Berkeley View of Systems Challenges for AI

Abstract:   With the increasing commoditization of computer vision, speech recognition
and machine translation systems and the widespread deployment of learning-based
back-end technologies such as digital advertising and intelligent
infrastructures, AI (Artificial Intelligence) has moved from research labs to
production. These changes have been made possible by unprecedented levels of
data and computation, by methodological advances in machine learning, by
innovations in systems software and architectures, and by the broad
accessibility of these technologies.
The next generation of AI systems promises to accelerate these developments
and increasingly impact our lives via frequent interactions and making (often
mission-critical) decisions on our behalf, often in highly personalized
contexts. Realizing this promise, however, raises daunting challenges. In
particular, we need AI systems that make timely and safe decisions in
unpredictable environments, that are robust against sophisticated adversaries,
and that can process ever increasing amounts of data across organizations and
individuals without compromising confidentiality. These challenges will be
exacerbated by the end of the Moore's Law, which will constrain the amount of
data these technologies can store and process. In this paper, we propose
several open research directions in systems, architectures, and security that
can address these challenges and help unlock AI's potential to improve lives
and society."
Is there a survey paper explaining self-affine sets for non-experts?,de510dd0,"[{'abstract': '  We survey the dimension theory of self-affine sets for general mathematical\naudience. The article is in Finnish.\n', 'title': 'On self-affine sets'}]","You're looking for a survey paper on self-affine sets that's accessible to non-experts!

I think I have just the thing for you:

Title: On self-affine sets

Abstract: We survey the dimension theory of self-affine sets for general mathematical audience. The article is in Finnish.

This paper seems to fit your requirements, as it's a survey paper on self-affine sets intended for a general mathematical audience, which implies that it's not too technical or specialized."
How is multiplication  utilized in the opposite bi-free partial S-transform described in this paper?,2dcdfe77,"[{'abstract': '  In this paper, we present a combinatorial approach to the opposite 2-variable\nbi-free partial $S$-transforms where the opposite multiplication is used on the\nright. In addition, extensions of this partial $S$-transforms to the\nconditional bi-free and operator-valued bi-free settings are discussed.\n', 'title': 'A Combinatorial Approach to the Opposite Bi-Free Partial $S$-Transform'}]","I think I can help you with that!

Title: A Combinatorial Approach to the Opposite Bi-Free Partial $S$-Transform

Abstract: In this paper, we present a combinatorial approach to the opposite 2-variable bi-free partial $S$-transforms where the opposite multiplication is used on the right. In addition, extensions of this partial $S$-transforms to the conditional bi-free and operator-valued bi-free settings are discussed.

According to the abstract, the opposite multiplication is used on the right in the opposite bi-free partial $S$-transforms. Unfortunately, the abstract doesn't provide more details on how exactly the multiplication is utilized."
Does the number of nearby galaxies impact the accuracy of Hubble constant estimates from strong lensing observations?,9dc57430,"[{'abstract': '  Cosmological parameter constraints from observations of time-delay lenses are\nbecoming increasingly precise. However, there may be significant bias and\nscatter in these measurements due to, among other things, the so-called\nmass-sheet degeneracy. To estimate these uncertainties, we analyze strong\nlenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet\ntransformation to the radial density profiles of lenses, and by selecting\nlenses near isothermality, we find that the bias on H0 can be reduced to 5%\nwith an intrinsic scatter of 10%, confirming previous results performed on a\ndifferent simulation data set. We further investigate whether combining lensing\nobservables with kinematic constraints helps to minimize this bias. We do not\ndetect any significant dependence of the bias on lens model parameters or\nobservational properties of the galaxy, but depending on the source--lens\nconfiguration, a bias may still exist. Cross lenses provide an accurate\nestimate of the Hubble constant, while fold (double) lenses tend to be biased\nlow (high). With kinematic constraints, double lenses show bias and intrinsic\nscatter of 6% and 10%, respectively, while quad lenses show bias and intrinsic\nscatter of 0.5% and 10%, respectively. For lenses with a reduced $\\chi^2 > 1$,\na power-law dependence of the $\\chi^2$ on the lens environment (number of\nnearby galaxies) is seen. Lastly, we model, in greater detail, the cases of two\ndouble lenses that are significantly biased. We are able to remove the bias,\nsuggesting that the remaining biases could also be reduced by carefully taking\ninto account additional sources of systematic uncertainty.\n', 'title': 'Reducing biases on $H_0$ measurements using strong lensing and galaxy dynamics: results from the EAGLE simulation'}]","Here is the relevant information:

Title: Reducing biases on $H_0$ measurements using strong lensing and galaxy dynamics: results from the EAGLE simulation

Abstract: ...For lenses with a reduced $\chi^2 > 1$, a power-law dependence of the $\chi^2$ on the lens environment (number of nearby galaxies) is seen.

According to this abstract, the answer is yes, the number of nearby galaxies does impact the accuracy of Hubble constant estimates from strong lensing observations."
What novel unfolding algorithm is presented for abstract interpretation that utilizes independence to improve efficiency?,812f8ea5,"[{'abstract': '  We present and evaluate a technique for computing path-sensitive interference\nconditions during abstract interpretation of concurrent programs. In lieu of\nfixed point computation, we use prime event structures to compactly represent\ncausal dependence and interference between sequences of transformers. Our main\ncontribution is an unfolding algorithm that uses a new notion of independence\nto avoid redundant transformer application, thread-local fixed points to reduce\nthe size of the unfolding, and a novel cutoff criterion based on subsumption to\nguarantee termination of the analysis. Our experiments show that the abstract\nunfolding produces an order of magnitude fewer false alarms than a mature\nabstract interpreter, while being several orders of magnitude faster than\nsolver-based tools that have the same precision.\n', 'title': 'Abstract Interpretation with Unfoldings'}]","Title: Abstract Interpretation with Unfoldings

Abstract: We present and evaluate a technique for computing path-sensitive interference conditions during abstract interpretation of concurrent programs. In lieu of fixed point computation, we use prime event structures to compactly represent causal dependence and interference between sequences of transformers. Our main contribution is an unfolding algorithm that uses a new notion of independence to avoid redundant transformer application, thread-local fixed points to reduce the size of the unfolding, and a novel cutoff criterion based on subsumption to guarantee termination of the analysis. Our experiments show that the abstract unfolding produces an order of magnitude fewer false alarms than a mature abstract interpreter, while being several orders of magnitude faster than solver-based tools that have the same precision."
What is a method for detecting changes in a large approximate factor model?,1191a249,"[{'abstract': '  We develop an on-line monitoring procedure to detect a change in a large\napproximate factor model. Our statistics are based on a well-known property of\nthe $% \\left( r+1\\right) $-th eigenvalue of the sample covariance matrix of the\ndata (having defined $r$ as the number of common factors): whilst under the\nnull the $\\left( r+1\\right) $-th eigenvalue is bounded, under the alternative\nof a change (either in the loadings, or in the number of factors itself) it\nbecomes spiked. Given that the sample eigenvalue cannot be estimated\nconsistently under the null, we regularise the problem by randomising the test\nstatistic in conjunction with sample conditioning, obtaining a sequence of\n\\textit{i.i.d.}, asymptotically chi-square statistics which are then employed\nto build the monitoring scheme. Numerical evidence shows that our procedure\nworks very well in finite samples, with a very small probability of false\ndetections and tight detection times in presence of a genuine change-point.\n', 'title': 'Sequential testing for structural stability in approximate factor models'}]","I think I can help you with that!

The method for detecting changes in a large approximate factor model is described in the paper:

Title: Sequential testing for structural stability in approximate factor models

Abstract: We develop an on-line monitoring procedure to detect a change in a large approximate factor model. Our statistics are based on a well-known property of the $(r+1)$-th eigenvalue of the sample covariance matrix of the data (having defined $r$ as the number of common factors): whilst under the null the $(r+1)$-th eigenvalue is bounded, under the alternative of a change (either in the loadings, or in the number of factors itself) it becomes spiked. Given that the sample eigenvalue cannot be estimated consistently under the null, we regularise the problem by randomising the test statistic in conjunction with sample conditioning, obtaining a sequence of _i.i.d._, asymptotically chi-square statistics which are then employed to build the monitoring scheme. Numerical evidence shows that our procedure works very well in finite samples, with a very small probability of false detections and tight detection times in presence of a genuine change-point."
What type of functions do these Poisson kernels apply to?,e2508c43,"[{'abstract': '  We introduce and develop the notion of spherical polyharmonics, which are a\nnatural generalisation of spherical harmonics. In particular we study the\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\nto construct Poisson kernels for polyharmonic functions on the union of rotated\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\nterms of the Gegenbauer polynomials. We show the connection between the\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\nkernel for holomorphic functions on the Lie ball.\n', 'title': 'Spherical polyharmonics and Poisson kernels for polyharmonic functions'}]","Title: Spherical polyharmonics and Poisson kernels for polyharmonic functions

Abstract: We introduce and develop the notion of spherical polyharmonics, which are a natural generalisation of spherical harmonics. In particular we study the theory of zonal polyharmonics, which allows us, analogously to zonal harmonics, to construct Poisson kernels for polyharmonic functions on the union of rotated balls. We find the representation of Poisson kernels and zonal polyharmonics in terms of the Gegenbauer polynomials. We show the connection between the classical Poisson kernel for harmonic functions on the ball, Poisson kernels for polyharmonic functions on the union of rotated balls, and the Cauchy-Hua kernel for holomorphic functions on the Lie ball.

The Poisson kernels in this paper apply to polyharmonic functions."
What is coreness and how can it be applied to define a novel neighborhood concept in network representation?,fe90413a,"[{'abstract': '  Recent advances in the field of network representation learning are mostly\nattributed to the application of the skip-gram model in the context of graphs.\nState-of-the-art analogues of skip-gram model in graphs define a notion of\nneighbourhood and aim to find the vector representation for a node, which\nmaximizes the likelihood of preserving this neighborhood.\nIn this paper, we take a drastic departure from the existing notion of\nneighbourhood of a node by utilizing the idea of coreness. More specifically,\nwe utilize the well-established idea that nodes with similar core numbers play\nequivalent roles in the network and hence induce a novel and an organic notion\nof neighbourhood. Based on this idea, we propose core2vec, a new algorithmic\nframework for learning low dimensional continuous feature mapping for a node.\nConsequently, the nodes having similar core numbers are relatively closer in\nthe vector space that we learn.\nWe further demonstrate the effectiveness of core2vec by comparing word\nsimilarity scores obtained by our method where the node representations are\ndrawn from standard word association graphs against scores computed by other\nstate-of-the-art network representation techniques like node2vec, DeepWalk and\nLINE. Our results always outperform these existing methods\n', 'title': 'Core2Vec: A core-preserving feature learning framework for networks'}]","Title: Core2Vec: A core-preserving feature learning framework for networks

Abstract: Recent advances in the field of network representation learning are mostly attributed to the application of the skip-gram model in the context of graphs.
State-of-the-art analogues of skip-gram model in graphs define a notion of neighbourhood and aim to find the vector representation for a node, which maximizes the likelihood of preserving this neighborhood.
In this paper, we take a drastic departure from the existing notion of neighbourhood of a node by utilizing the idea of coreness. More specifically, we utilize the well-established idea that nodes with similar core numbers play equivalent roles in the network and hence induce a novel and an organic notion of neighbourhood. Based on this idea, we propose core2vec, a new algorithmic framework for learning low dimensional continuous feature mapping for a node.
Consequently, the nodes having similar core numbers are relatively closer in the vector space that we learn.
We further demonstrate the effectiveness of core2vec by comparing word similarity scores obtained by our method where the node representations are drawn from standard word association graphs against scores computed by other state-of-the-art network representation techniques like node2vec, DeepWalk and LINE. Our results always outperform these existing methods"
Does the incorporation of offshore entities by national agents increase following news reports about asset confiscation?,704a6421,"[{'abstract': '  Using the Panama Papers, we show that the beginning of media reporting on\nexpropriations and property confiscations in a country increases the\nprobability that offshore entities are incorporated by agents from the same\ncountry in the same month. This result is robust to the use of country-year\nfixed effects and the exclusion of tax havens. Further analysis shows that the\neffect is driven by countries with non-corrupt and effective governments, which\nsupports the notion that offshore entities are incorporated when reasonably\nwell-intended and well-functioning governments become more serious about\nfighting organized crime by confiscating proceeds of crime.\n', 'title': 'Expropriations, Property Confiscations and New Offshore Entities: Evidence from the Panama Papers'}]","Here's an answer to your question:

Title: Expropriations, Property Confiscations and New Offshore Entities: Evidence from the Panama Papers

Abstract: Using the Panama Papers, we show that the beginning of media reporting on expropriations and property confiscations in a country increases the probability that offshore entities are incorporated by agents from the same country in the same month. This result is robust to the use of country-year fixed effects and the exclusion of tax havens. Further analysis shows that the effect is driven by countries with non-corrupt and effective governments, which supports the notion that offshore entities are incorporated when reasonably well-intended and well-functioning governments become more serious about fighting organized crime by confiscating proceeds of crime."
What specific modification is made to the random walk in VEC-NBT?,3d0022ac,"[{'abstract': '  This paper presents VEC-NBT, a variation on the unsupervised graph clustering\ntechnique VEC, which improves upon the performance of the original algorithm\nsignificantly for sparse graphs. VEC employs a novel application of the\nstate-of-the-art word2vec model to embed a graph in Euclidean space via random\nwalks on the nodes of the graph. In VEC-NBT, we modify the original algorithm\nto use a non-backtracking random walk instead of the normal backtracking random\nwalk used in VEC. We introduce a modification to a non-backtracking random\nwalk, which we call a begrudgingly-backtracking random walk, and show\nempirically that using this model of random walks for VEC-NBT requires shorter\nwalks on the graph to obtain results with comparable or greater accuracy than\nVEC, especially for sparser graphs.\n', 'title': 'Faster Clustering via Non-Backtracking Random Walks'}]","Title: Faster Clustering via Non-Backtracking Random Walks

Abstract: This paper presents VEC-NBT, a variation on the unsupervised graph clustering technique VEC, which improves upon the performance of the original algorithm significantly for sparse graphs. VEC employs a novel application of the state-of-the-art word2vec model to embed a graph in Euclidean space via random walks on the nodes of the graph. In VEC-NBT, we modify the original algorithm to use a non-backtracking random walk instead of the normal backtracking random walk used in VEC. We introduce a modification to a non-backtracking random walk, which we call a begrudgingly-backtracking random walk, and show empirically that using this model of random walks for VEC-NBT requires shorter walks on the graph to obtain results with comparable or greater accuracy than VEC, especially for sparser graphs.

Answer: The specific modification made to the random walk in VEC-NBT is the use of a non-backtracking random walk, which is further modified to a begrudgingly-backtracking random walk."
Are there universal adelic point groups for elliptic curves over number fields of a specific degree?,6da9852a,"[{'abstract': '  We show that for an elliptic curve E defined over a number field K, the group\nE(A) of points of E over the adele ring A of K is a topological group that can\nbe analyzed in terms of the Galois representation associated to the torsion\npoints of E. An explicit description of E(A) is given, and we prove that for K\nof degree n, almost all elliptic curves over K have an adelic point group\ntopologically isomorphic to a universal group depending on n. We also show that\nthere exist infinitely many elliptic curves over K having a different adelic\npoint group.\n', 'title': 'Adelic point groups of elliptic curves'}]","Title: Adelic point groups of elliptic curves

Abstract: We show that for an elliptic curve E defined over a number field K, the group E(A) of points of E over the adele ring A of K is a topological group that can be analyzed in terms of the Galois representation associated to the torsion points of E. An explicit description of E(A) is given, and we prove that for K of degree n, almost all elliptic curves over K have an adelic point group topologically isomorphic to a universal group depending on n. We also show that there exist infinitely many elliptic curves over K having a different adelic point group."
What is a quantum metric information used for and how is it connected to a metric?,a351cef7,"[{'abstract': '  The modular Gromov-Hausdorff propinquity is a distance on classes of modules\nendowed with quantum metric information, in the form of a metric form of a\nconnection and a left Hilbert module structure. This paper proves that the\nfamily of Heisenberg modules over quantum two tori, when endowed with their\ncanonical connections, form a family of metrized quantum vector bundles, as a\nfirst step in proving that Heisenberg modules form a continuous family for the\nmodular Gromov-Hausdorff propinquity.\n', 'title': 'Heisenberg Modules over Quantum 2-tori are metrized quantum vector bundles'}]","I think I can help you with that!

Based on the provided abstracts, I found a relevant paper that discusses quantum metrics and their connection to classical metrics.

Title: A New Family of Near-metrics for Universal Similarity

Abstract: We propose a family of near-metrics based on local graph diffusion to capture similarity for a wide class of data sets. These quasi-metametrics, as their names suggest, dispense with one or two standard axioms of metric spaces, specifically distinguishability and symmetry, so that similarity between data points of arbitrary type and form could be measured broadly and effectively. The proposed near-metric family includes the forward k-step diffusion and its reverse, typically on the graph consisting of data objects and their features. By construction, this family of near-metrics is particularly appropriate for categorical data, continuous data, and vector representations of images and text extracted via deep learning approaches. We conduct extensive experiments to evaluate the performance of this family of similarity measures and compare and contrast with traditional measures of similarity used for each specific application and with the ground truth when available. We show that for structured data including categorical and continuous data, the near-metrics corresponding to normalized forward k-step diffusion (k small) work as one of the best performing similarity measures; for vector representations of text and images including those extracted from deep learning, the near-metrics derived from normalized and reverse k-step graph diffusion (k very small) exhibit outstanding ability to distinguish data points from different classes.

This paper discusses a new family of near-metrics, which can be seen as a relaxation of classical metrics. While classical metrics follow certain axioms (distinguishability and symmetry), these near-metrics dispense with one or two of these axioms to capture similarity between data points in a more flexible way."
What are some novel tuning parameter approaches for CS algorithms that adapt to SNR?,73dca852,"[{'abstract': '  High signal to noise ratio (SNR) consistency of model selection criteria in\nlinear regression models has attracted a lot of attention recently. However,\nmost of the existing literature on high SNR consistency deals with model order\nselection. Further, the limited literature available on the high SNR\nconsistency of subset selection procedures (SSPs) is applicable to linear\nregression with full rank measurement matrices only. Hence, the performance of\nSSPs used in underdetermined linear models (a.k.a compressive sensing (CS)\nalgorithms) at high SNR is largely unknown. This paper fills this gap by\nderiving necessary and sufficient conditions for the high SNR consistency of\npopular CS algorithms like $l_0$-minimization, basis pursuit de-noising or\nLASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions\nanalytically establish the high SNR inconsistency of CS algorithms when used\nwith the tuning parameters discussed in literature. Novel tuning parameters\nwith SNR adaptations are developed using the sufficient conditions and the\nchoice of SNR adaptations are discussed analytically using convergence rate\nanalysis. CS algorithms with the proposed tuning parameters are numerically\nshown to be high SNR consistent and outperform existing tuning parameters in\nthe moderate to high SNR regime.\n', 'title': 'High SNR Consistent Compressive Sensing'}]","Title: High SNR Consistent Compressive Sensing

Abstract: High signal to noise ratio (SNR) consistency of model selection criteria in
linear regression models has attracted a lot of attention recently. However,
most of the existing literature on high SNR consistency deals with model order
selection. Further, the limited literature available on the high SNR
consistency of subset selection procedures (SSPs) is applicable to linear
regression with full rank measurement matrices only. Hence, the performance of
SSPs used in underdetermined linear models (a.k.a compressive sensing (CS)
algorithms) at high SNR is largely unknown. This paper fills this gap by
deriving necessary and sufficient conditions for the high SNR consistency of
popular CS algorithms like $l_0$-minimization, basis pursuit de-noising or
LASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions
analytically establish the high SNR inconsistency of CS algorithms when used
with the tuning parameters discussed in literature. Novel tuning parameters
with SNR adaptations are developed using the sufficient conditions and the
choice of SNR adaptations are discussed analytically using convergence rate
analysis. CS algorithms with the proposed tuning parameters are numerically
shown to be high SNR consistent and outperform existing tuning parameters in
the moderate to high SNR regime."
In what ways can legal and risk departments impact the development of machine learning applications?,71c1f1df,"[{'abstract': '  Recently software development companies started to embrace Machine Learning\n(ML) techniques for introducing a series of advanced functionality in their\nproducts such as personalisation of the user experience, improved search,\ncontent recommendation and automation. The technical challenges for tackling\nthese problems are heavily researched in literature. A less studied area is a\npragmatic approach to the role of humans in a complex modern industrial\nenvironment where ML based systems are developed. Key stakeholders affect the\nsystem from inception and up to operation and maintenance. Product managers\nwant to embed ""smart"" experiences for their users and drive the decisions on\nwhat should be built next; software engineers are challenged to build or\nutilise ML software tools that require skills that are well outside of their\ncomfort zone; legal and risk departments may influence design choices and data\naccess; operations teams are requested to maintain ML systems which are\nnon-stationary in their nature and change behaviour over time; and finally ML\npractitioners should communicate with all these stakeholders to successfully\nbuild a reliable system. This paper discusses some of the challenges we faced\nin Atlassian as we started investing more in the ML space.\n', 'title': 'Beyond the technical challenges for deploying Machine Learning solutions in a software company'}]","Title: Beyond the technical challenges for deploying Machine Learning solutions in a software company

Abstract:  Recently software development companies started to embrace Machine Learning
(ML) techniques for introducing a series of advanced functionality in their
products such as personalisation of the user experience, improved search,
content recommendation and automation. The technical challenges for tackling
these problems are heavily researched in literature. A less studied area is a
pragmatic approach to the role of humans in a complex modern industrial
environment where ML based systems are developed. Key stakeholders affect the
system from inception and up to operation and maintenance. Product managers
want to embed ""smart"" experiences for their users and drive the decisions on
what should be built next; software engineers are challenged to build or
utilise ML software tools that require skills that are well outside of their
comfort zone; legal and risk departments may influence design choices and data
access; operations teams are requested to maintain ML systems which are
non-stationary in their nature and change behaviour over time; and finally ML
practitioners should communicate with all these stakeholders to successfully
build a reliable system. This paper discusses some of the challenges we faced
in Atlassian as we started investing more in the ML space.

According to this paper, legal and risk departments may impact the development of machine learning applications by influencing design choices and data access."
What statistical software package analyzes multivariate data and repeated measures designs without assuming multivariate normality?,7ee7399c,"[{'abstract': '  The numerical availability of statistical inference methods for a modern and\nrobust analysis of longitudinal- and multivariate data in factorial experiments\nis an essential element in research and education. While existing approaches\nthat rely on specific distributional assumptions of the data (multivariate\nnormality and/or characteristic covariance matrices) are implemented in\nstatistical software packages, there is a need for user-friendly software that\ncan be used for the analysis of data that do not fulfill the aforementioned\nassumptions and provide accurate p-value and confidence interval estimates.\nTherefore, newly developed statistical methods for the analysis of repeated\nmeasures designs and multivariate data that neither assume multivariate\nnormality nor specific covariance matrices have been implemented in the freely\navailable R-package MANOVA.RM. The package is equipped with a graphical user\ninterface for plausible applications in academia and other educational purpose.\nSeveral motivating examples illustrate the application of the methods.\n', 'title': 'Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM'}]","Title: Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM

Abstract:  The numerical availability of statistical inference methods for a modern and
robust analysis of longitudinal- and multivariate data in factorial experiments
is an essential element in research and education. While existing approaches
that rely on specific distributional assumptions of the data (multivariate
normality and/or characteristic covariance matrices) are implemented in
statistical software packages, there is a need for user-friendly software that
can be used for the analysis of data that do not fulfill the aforementioned
assumptions and provide accurate p-value and confidence interval estimates.
Therefore, newly developed statistical methods for the analysis of repeated
measures designs and multivariate data that neither assume multivariate
normality nor specific covariance matrices have been implemented in the freely
available R-package MANOVA.RM. The package is equipped with a graphical user
interface for plausible applications in academia and other educational purpose.
Several motivating examples illustrate the application of the methods."
Classification of pointed Hopf algebras over an algebraically closed field,dfbf14cb,"[{'abstract': '  Let $\\K$ be an algebraically closed field of positive characteristic $p$. We\nmainly classify pointed Hopf algebras over $\\K$ of dimension $p^2q$, $pq^2$ and\n$pqr$ where $p,q,r$ are distinct prime numbers. We obtain a complete\nclassification of such Hopf algebras except two subcases when they are not\ngenerated by the first terms of coradical filtration. In particular, we obtain\nmany new examples of non-commutative and non-cocommutative finite-dimensional\nHopf algebras.\n', 'title': 'Pointed $p^2q$-dimensional Hopf algebras in positive characteristic'}]","I think I can help you with that!

Title: Pointed $p^2q$-dimensional Hopf algebras in positive characteristic

Abstract: Let $\K$ be an algebraically closed field of positive characteristic $p$. We mainly classify pointed Hopf algebras over $\K$ of dimension $p^2q$, $pq^2$ and $pqr$ where $p,q,r$ are distinct prime numbers. We obtain a complete classification of such Hopf algebras except two subcases when they are not generated by the first terms of coradical filtration. In particular, we obtain many new examples of non-commutative and non-cocommutative finite-dimensional Hopf algebras."
What is the effect of chemical doping on the Fermi energy and superconducting gap in Fe_{1+y}Se_xTe_{1-x}?,4aa3217c,"[{'abstract': '  The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to\nBose-Einstein condensation (BEC) is difficult to realize in quantum materials\nbecause, unlike in ultracold atoms, one cannot tune the pairing interaction. We\nrealize the BCS-BEC crossover in a nearly compensated semimetal\nFe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\\epsilon_F$, via\nchemical doping, which permits us to systematically change $\\Delta /\n\\epsilon_F$ from 0.16 to 0.5 were $\\Delta$ is the superconducting (SC) gap. We\nuse angle-resolved photoemission spectroscopy to measure the Fermi energy, the\nSC gap and characteristic changes in the SC state electronic dispersion as the\nsystem evolves from a BCS to a BEC regime. Our results raise important\nquestions about the crossover in multiband superconductors which go beyond\nthose addressed in the context of cold atoms.\n', 'title': 'Tuning across the BCS-BEC crossover in the multiband superconductor Fe$_{1+y}$Se$_x$Te$_{1-x}$ : An angle-resolved photoemission study'}]","Title: Tuning across the BCS-BEC crossover in the multiband superconductor Fe$_{1+y}$Se$_x$Te$_{1-x}$ : An angle-resolved photoemission study

Abstract: The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to Bose-Einstein condensation (BEC) is difficult to realize in quantum materials because, unlike in ultracold atoms, one cannot tune the pairing interaction. We realize the BCS-BEC crossover in a nearly compensated semimetal Fe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\epsilon_F$, via chemical doping, which permits us to systematically change $\Delta / \epsilon_F$ from 0.16 to 0.5 were $\Delta$ is the superconducting (SC) gap."
How many authors created the Belyi map for the Janko group J1?,e0eeb29f,"[{'abstract': '  We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree\n266 and describe the applied method. This yields explicit polynomials having J1\nas a Galois group over K(t), [K:Q] = 7.\n', 'title': 'Belyi map for the sporadic group J1'}]","Based on the context, I found the relevant paper:

Title: Belyi map for the sporadic group J1

Abstract:   We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree
266 and describe the applied method. This yields explicit polynomials having J1
as a Galois group over K(t), [K:Q] = 7.

Unfortunately, the abstract does not provide information about the authors of the paper. Therefore, I cannot determine how many authors created the Belyi map for the Janko group J1 based on the provided context."
What algorithm approximates maximum weight flow in small depth networks considering concave functions,c2bf6ede,"[{'abstract': '  Small depth networks arise in a variety of network related applications,\noften in the form of maximum flow and maximum weighted matching. Recent works\nhave generalized such methods to include costs arising from concave functions.\nIn this paper we give an algorithm that takes a depth $D$ network and strictly\nincreasing concave weight functions of flows on the edges and computes a $(1 -\n\\epsilon)$-approximation to the maximum weight flow in time $mD \\epsilon^{-1}$\ntimes an overhead that is logarithmic in the various numerical parameters\nrelated to the magnitudes of gradients and capacities.\nOur approach is based on extending the scaling algorithm for approximate\nmaximum weighted matchings by [Duan-Pettie JACM`14] to the setting of small\ndepth networks, and then generalizing it to concave functions. In this more\nrestricted setting of linear weights in the range $[w_{\\min}, w_{\\max}]$, it\nproduces a $(1 - \\epsilon)$-approximation in time $O(mD \\epsilon^{-1} \\log(\nw_{\\max} /w_{\\min}))$. The algorithm combines a variety of tools and provides a\nunified approach towards several problems involving small depth networks.\n', 'title': 'Concave Flow on Small Depth Directed Networks'}]","Based on the provided context, I found a match for your question:

Title: Concave Flow on Small Depth Directed Networks

Abstract:   Small depth networks arise in a variety of network related applications,
often in the form of maximum flow and maximum weighted matching. Recent works
have generalized such methods to include costs arising from concave functions.
In this paper we give an algorithm that takes a depth $D$ network and strictly
increasing concave weight functions of flows on the edges and computes a $(1 -
\epsilon)$-approximation to the maximum weight flow in time $mD \epsilon^{-1}$
times an overhead that is logarithmic in the various numerical parameters
related to the magnitudes of gradients and capacities.
Our approach is based on extending the scaling algorithm for approximate
maximum weighted matchings by [Duan-Pettie JACM`14] to the setting of small
depth networks, and then generalizing it to concave functions. In this more
restricted setting of linear weights in the range $[w_{\min}, w_{\max}]$, it
produces a $(1 - \epsilon)$-approximation in time $O(mD \epsilon^{-1} \log(
w_{\max} /w_{\min}))$. The algorithm combines a variety of tools and provides a
unified approach towards several problems involving small depth networks."
What are Green's functions in the context of generating synthetic cyclotron lines?,471aede4,"[{'abstract': ""  Cyclotron resonant scattering features (CRSFs) are formed by scattering of\nX-ray photons off quantized plasma electrons in the strong magnetic field (of\nthe order 10^12 G) close to the surface of an accreting X-ray pulsar. The line\nprofiles of CRSFs cannot be described by an analytic expression. Numerical\nmethods such as Monte Carlo (MC) simulations of the scattering processes are\nrequired in order to predict precise line shapes for a given physical setup,\nwhich can be compared to observations to gain information about the underlying\nphysics in these systems.\nA versatile simulation code is needed for the generation of synthetic\ncyclotron lines. Sophisticated geometries should be investigatable by making\ntheir simulation possible for the first time.\nThe simulation utilizes the mean free path tables described in the first\npaper of this series for the fast interpolation of propagation lengths. The\ncode is parallelized to make the very time consuming simulations possible on\nconvenient time scales. Furthermore, it can generate responses to\nmono-energetic photon injections, producing Green's functions, which can be\nused later to generate spectra for arbitrary continua.\nWe develop a new simulation code to generate synthetic cyclotron lines for\ncomplex scenarios, allowing for unprecedented physical interpretation of the\nobserved data. An associated XSPEC model implementation is used to fit\nsynthetic line profiles to NuSTAR data of Cep X-4. The code has been developed\nwith the main goal of overcoming previous geometrical constraints in MC\nsimulations of CRSFs. By applying this code also to more simple, classic\ngeometries used in previous works, we furthermore address issues of code\nverification and cross-comparison of various models. The XSPEC model and the\nGreen's function tables are available online at\nthis http URL .\n"", 'title': 'Cyclotron resonant scattering feature simulations. II. Description of the CRSF simulation process'}]","Title: Cyclotron resonant scattering feature simulations. II. Description of the CRSF simulation process

Abstract: Cyclotron resonant scattering features (CRSFs) are formed by scattering of X-ray photons off quantized plasma electrons in the strong magnetic field (of the order 10^12 G) close to the surface of an accreting X-ray pulsar. The line profiles of CRSFs cannot be described by an analytic expression. Numerical methods such as Monte Carlo (MC) simulations of the scattering processes are required in order to predict precise line shapes for a given physical setup, which can be compared to observations to gain information about the underlying physics in these systems.

In this context, Green's functions refer to responses to mono-energetic photon injections, which can be used later to generate spectra for arbitrary continua."
How can a long short-term memory neural network be used to create phishing posts?,54e1e293,"[{'abstract': ""  Historically, machine learning in computer security has prioritized defense:\nthink intrusion detection systems, malware classification, and botnet traffic\nidentification. Offense can benefit from data just as well. Social networks,\nwith their access to extensive personal data, bot-friendly APIs, colloquial\nsyntax, and prevalence of shortened links, are the perfect venues for spreading\nmachine-generated malicious content. We aim to discover what capabilities an\nadversary might utilize in such a domain. We present a long short-term memory\n(LSTM) neural network that learns to socially engineer specific users into\nclicking on deceptive URLs. The model is trained with word vector\nrepresentations of social media posts, and in order to make a click-through\nmore likely, it is dynamically seeded with topics extracted from the target's\ntimeline. We augment the model with clustering to triage high value targets\nbased on their level of social engagement, and measure success of the LSTM's\nphishing expedition using click-rates of IP-tracked links. We achieve state of\nthe art success rates, tripling those of historic email attack campaigns, and\noutperform humans manually performing the same task.\n"", 'title': 'Generative Models for Spear Phishing Posts on Social Media'}]","I think I can help you with that!

Title: Generative Models for Spear Phishing Posts on Social Media

Abstract: Historically, machine learning in computer security has prioritized defense: think intrusion detection systems, malware classification, and botnet traffic identification. Offense can benefit from data just as well. Social networks, with their access to extensive personal data, bot-friendly APIs, colloquial syntax, and prevalence of shortened links, are the perfect venues for spreading machine-generated malicious content. We aim to discover what capabilities an adversary might utilize in such a domain. We present a long short-term memory (LSTM) neural network that learns to socially engineer specific users into clicking on deceptive URLs. The model is trained with word vector representations of social media posts, and in order to make a click-through more likely, it is dynamically seeded with topics extracted from the target's timeline. We augment the model with clustering to triage high-value targets based on their level of social engagement, and measure success of the LSTM's phishing expedition using click-rates of IP-tracked links. We achieve state-of-the-art success rates, tripling those of historic email attack campaigns, and outperform humans manually performing the same task."
In what ways does theImproved Phantom cell structure perform better than conventional HetNets regarding handover?,d777aac0,"[{'abstract': '  Improved Phantom cell is a new scenario which has been introduced recently to\nenhance the capacity of Heterogeneous Networks (HetNets). The main trait of\nthis scenario is that, besides maximizing the total network capacity in both\nindoor and outdoor environments, it claims to reduce the handover number\ncompared to the conventional scenarios. In this paper, by a comprehensive\nreview of the Improved Phantom cells structure, an appropriate algorithm will\nbe introduced for the handover procedure of this scenario. To reduce the number\nof handover in the proposed algorithm, various parameters such as the received\nSignal to Interference plus Noise Ratio (SINR) at the user equipment (UE),\nusers access conditions to the phantom cells, and users staying time in the\ntarget cell based on its velocity, has been considered. Theoretical analyses\nand simulation results show that applying the suggested algorithm the improved\nphantom cell structure has a much better performance than conventional HetNets\nin terms of the number of handover.\n', 'title': 'Handover analysis of the Improved Phantom Cells'}]","Title: Handover analysis of the Improved Phantom Cells

Abstract: Improved Phantom cell is a new scenario which has been introduced recently to enhance the capacity of Heterogeneous Networks (HetNets). The main trait of this scenario is that, besides maximizing the total network capacity in both indoor and outdoor environments, it claims to reduce the handover number compared to the conventional scenarios. In this paper, by a comprehensive review of the Improved Phantom cells structure, an appropriate algorithm will be introduced for the handover procedure of this scenario. To reduce the number of handover in the proposed algorithm, various parameters such as the received Signal to Interference plus Noise Ratio (SINR) at the user equipment (UE), users access conditions to the phantom cells, and users staying time in the target cell based on its velocity, has been considered. Theoretical analyses and simulation results show that applying the suggested algorithm the improved phantom cell structure has a much better performance than conventional HetNets in terms of the number of handover.

The Improved Phantom cell structure performs better than conventional HetNets regarding handover in terms of:

* Reducing the handover number
* Improving performance in terms of the number of handover"
Can non-exponential discount functions lead to the violation of a condition necessary for the smooth pasting principle to apply?,fd335522,"[{'abstract': '  This paper considers a time-inconsistent stopping problem in which the\ninconsistency arises from non-constant time preference rates. We show that the\nsmooth pasting principle, the main approach that has been used to construct\nexplicit solutions for conventional time-consistent optimal stopping problems,\nmay fail under time-inconsistency. Specifically, we prove that the smooth\npasting principle solves a time-inconsistent problem within the intra-personal\ngame theoretic framework if and only if a certain inequality on the model\nprimitives is satisfied. We show that the violation of this inequality can\nhappen even for very simple non-exponential discount functions. Moreover, we\ndemonstrate that the stopping problem does not admit any intra-personal\nequilibrium whenever the smooth pasting principle fails. The ""negative"" results\nin this paper caution blindly extending the classical approaches for\ntime-consistent stopping problems to their time-inconsistent counterparts.\n', 'title': 'Failure of Smooth Pasting Principle and Nonexistence of Equilibrium Stopping Rules under Time-Inconsistency'}]","Title: Failure of Smooth Pasting Principle and Nonexistence of Equilibrium Stopping Rules under Time-Inconsistency

Abstract: This paper considers a time-inconsistent stopping problem in which the inconsistency arises from non-constant time preference rates. We show that the smooth pasting principle, the main approach that has been used to construct explicit solutions for conventional time-consistent optimal stopping problems, may fail under time-inconsistency. Specifically, we prove that the smooth pasting principle solves a time-inconsistent problem within the intra-personal game theoretic framework if and only if a certain inequality on the model primitives is satisfied. We show that the violation of this inequality can happen even for very simple non-exponential discount functions. Moreover, we demonstrate that the stopping problem does not admit any intra-personal equilibrium whenever the smooth pasting principle fails. The ""negative"" results in this paper caution blindly extending the classical approaches for time-consistent stopping problems to their time-inconsistent counterparts."
What restrictions are there on the image of the classifying map of a surface group in a  Kodaira fibration setting?,f0510f37,"[{'abstract': '  The fundamental group $\\pi$ of a Kodaira fibration is, by definition, the\nextension of a surface group $\\Pi_b$ by another surface group $\\Pi_g$, i.e. \\[\n1 \\rightarrow \\Pi_g \\rightarrow \\pi \\rightarrow \\Pi_b \\rightarrow 1. \\]\nConversely, we can inquire about what conditions need to be satisfied by a\ngroup of that sort in order to be the fundamental group of a Kodaira fibration.\nIn this short note we collect some restriction on the image of the classifying\nmap $m \\colon \\Pi_b \\to \\Gamma_g$ in terms of the coinvariant homology of\n$\\Pi_g$. In particular, we observe that if $\\pi$ is the fundamental group of a\nKodaira fibration with relative irregularity $g-s$, then $g \\leq 1+ 6s$, and we\nshow that this effectively constrains the possible choices for $\\pi$, namely\nthat there are group extensions as above that fail to satisfy this bound, hence\ncannot be the fundamental group of a Kodaira fibration. In particular this\nprovides examples of symplectic $4$--manifolds that fail to admit a Kähler\nstructure for reasons that eschew the usual obstructions.\n', 'title': 'A note on the fundamental group of Kodaira fibrations'}]","Title: A note on the fundamental group of Kodaira fibrations

Abstract:   The fundamental group $\pi$ of a Kodaira fibration is, by definition, the
extension of a surface group $\Pi_b$ by another surface group $\Pi_g$, i.e. \[
1 \rightarrow \Pi_g \rightarrow \pi \rightarrow \Pi_b \rightarrow 1. \]
Conversely, we can inquire about what conditions need to be satisfied by a
group of that sort in order to be the fundamental group of a Kodaira fibration.
In this short note we collect some restriction on the image of the classifying
map $m \colon \Pi_b \to \Gamma_g$ in terms of the coinvariant homology of
$\Pi_g$. In particular, we observe that if $\pi$ is the fundamental group of a
Kodaira fibration with relative irregularity $g-s$, then $g \leq 1+ 6s$, and we
show that this effectively constrains the possible choices for $\pi$, namely
that there are group extensions as above that fail to satisfy this bound, hence
cannot be the fundamental group of a Kodaira fibration. In particular this
provides examples of symplectic $4$--manifolds that fail to admit a Kähler
structure for reasons that eschew the usual obstructions."
How can acoustic impedance be determined at unknown boundaries of a given domain given a real-valued frequency?,423ccaae,"[{'abstract': ""  Assigning homogeneous boundary conditions, such as acoustic impedance, to the\nthermoviscous wave equations (TWE) derived by transforming the linearized\nNavier-Stokes equations (LNSE) to the frequency domain yields a so-called\nHelmholtz solver, whose output is a discrete set of complex eigenfunction and\neigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --\nreverses such procedure by returning the value of acoustic impedance at one or\nmore unknown impedance boundaries (IBs) of a given domain via spatial\nintegration of the TWE for a given real-valued frequency with assigned\nconditions on other boundaries. The iHS procedure is applied to a second-order\nspatial discretization of the TWEs derived on an unstructured grid with\nstaggered grid arrangement. The momentum equation only is extended to the\ncenter of each IB face where pressure and velocity components are co-located\nand treated as unknowns. One closure condition considered for the iHS is the\nassignment of the surface gradient of pressure phase over the IBs,\ncorresponding to assigning the shape of the acoustic waveform at the IB. The\niHS procedure is carried out independently for each frequency in order to\nreturn the complete broadband complex impedance distribution at the IBs in any\ndesired frequency range. The iHS approach is first validated against Rott's\ntheory for both inviscid and viscous, rectangular and circular ducts. The\nimpedance of a geometrically complex toy cavity is then reconstructed and\nverified against companion full compressible unstructured Navier-Stokes\nsimulations resolving the cavity geometry and one-dimensional impedance test\ntube calculations based on time-domain impedance boundary conditions (TDIBC).\nThe iHS methodology is also shown to capture thermoacoustic effects, with\nreconstructed impedance values quantitatively in agreement with thermoacoustic\ngrowth rates.\n"", 'title': 'Acoustic Impedance Calculation via Numerical Solution of the Inverse Helmholtz Problem'}]","Title: Acoustic Impedance Calculation via Numerical Solution of the Inverse Helmholtz Problem

Abstract: Assigning homogeneous boundary conditions, such as acoustic impedance, to the thermoviscous wave equations (TWE) derived by transforming the linearized Navier-Stokes equations (LNSE) to the frequency domain yields a so-called Helmholtz solver, whose output is a discrete set of complex eigenfunction and eigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) -- reverses such procedure by returning the value of acoustic impedance at one or more unknown impedance boundaries (IBs) of a given domain via spatial integration of the TWE for a given real-valued frequency with assigned conditions on other boundaries. The iHS procedure is applied to a second-order spatial discretization of the TWEs derived on an unstructured grid with staggered grid arrangement. The momentum equation only is extended to the center of each IB face where pressure and velocity components are co-located and treated as unknowns. One closure condition considered for the iHS is the assignment of the surface gradient of pressure phase over the IBs, corresponding to assigning the shape of the acoustic waveform at the IB. The iHS procedure is carried out independently for each frequency in order to return the complete broadband complex impedance distribution at the IBs in any desired frequency range."
How does the bias of the maximum likelihood estimator for the extreme value index change based on the extreme value index and the second order parameter?,cd330b15,"[{'abstract': ""  The extreme value index is a fundamental parameter in univariate Extreme\nValue Theory (EVT). It captures the tail behavior of a distribution and is\ncentral in the extrapolation beyond observed data. Among other semi-parametric\nmethods (such as the popular Hill's estimator), the Block Maxima (BM) and\nPeaks-Over-Threshold (POT) methods are widely used for assessing the extreme\nvalue index and related normalizing constants. We provide asymptotic theory for\nthe maximum likelihood estimators (MLE) based on the BM method. Our main result\nis the asymptotic normality of the MLE with a non-trivial bias depending on the\nextreme value index and on the so-called second order parameter. Our approach\ncombines asymptotic expansions of the likelihood process and of the empirical\nquantile process of block maxima. The results permit to complete the comparison\nof most common semi-parametric estimators in EVT (MLE and probability weighted\nmoment estimators based on the POT or BM methods) through their asymptotic\nvariances, biases and optimal mean square errors.\n"", 'title': 'Maximum likelihood estimators based on the block maxima method'}]","I think I can help you with that!

Title: Maximum likelihood estimators based on the block maxima method

Abstract: The extreme value index is a fundamental parameter in univariate Extreme Value Theory (EVT). It captures the tail behavior of a distribution and is central in the extrapolation beyond observed data. Among other semi-parametric methods (such as the popular Hill's estimator), the Block Maxima (BM) and Peaks-Over-Threshold (POT) methods are widely used for assessing the extreme value index and related normalizing constants. We provide asymptotic theory for the maximum likelihood estimators (MLE) based on the BM method. Our main result is the asymptotic normality of the MLE with a non-trivial bias depending on the extreme value index and on the so-called second order parameter. Our approach combines asymptotic expansions of the likelihood process and of the empirical quantile process of block maxima. The results permit to complete the comparison of most common semi-parametric estimators in EVT (MLE and probability weighted moment estimators based on the POT or BM methods) through their asymptotic variances, biases and optimal mean square errors.

According to this paper, the bias of the maximum likelihood estimator (MLE) for the extreme value index depends on the extreme value index and the second order parameter."
What are the boundary conditions used in a velocity-vorticity formulation of the Navier-Stokes equations?,731e27a9,"[{'abstract': '  We study well-posedness of a velocity-vorticity formulation of the\nNavier--Stokes equations, supplemented with no-slip velocity boundary\nconditions, a no-penetration vorticity boundary condition, along with a natural\nvorticity boundary condition depending on a pressure functional. In the\nstationary case we prove existence and uniqueness of a suitable weak solution\nto the system under a small data condition. The topic of the paper is driven by\nrecent developments of vorticity based numerical methods for the Navier--Stokes\nequations.\n', 'title': 'On well-posedness of a velocity-vorticity formulation of the Navier-Stokes equations with no-slip boundary conditions'}]","I think I can help the researcher with that!

The answer can be found in the paper titled:

Title: On well-posedness of a velocity-vorticity formulation of the Navier-Stokes equations with no-slip boundary conditions

Abstract: We study well-posedness of a velocity-vorticity formulation of the 
Navier--Stokes equations, supplemented with no-slip velocity boundary 
conditions, a no-penetration vorticity boundary condition, along with a natural 
vorticity boundary condition depending on a pressure functional. In the 
stationary case we prove existence and uniqueness of a suitable weak solution 
to the system under a small data condition. The topic of the paper is driven by 
recent developments of vorticity based numerical methods for the Navier--Stokes 
equations.

So, the boundary conditions used are:

* No-slip velocity boundary conditions
* No-penetration vorticity boundary condition
* Natural vorticity boundary condition depending on a pressure functional."
In what sense does the almost-Euclidean isoperimetric inequality hold in a smaller ball?,e605021c,"[{'abstract': ""  Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove\nthat if a Riemannian manifold has Ricci curvature bounded below in a metric\nball which moreover has almost maximal volume, then in a smaller ball (in a\nquantified sense) it holds an almost-euclidean isoperimetric inequality. The\nresult is actually established in the more general framework of non-smooth\nspaces satisfying local Ricci curvature lower bounds in a synthetic sense via\noptimal transportation.\n"", 'title': 'Almost euclidean Isoperimetric Inequalities in spaces satisfying local Ricci curvature lower bounds'}]","I think I can help you with that!

Title: Almost euclidean Isoperimetric Inequalities in spaces satisfying local Ricci curvature lower bounds

Abstract: Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove that if a Riemannian manifold has Ricci curvature bounded below in a metric ball which moreover has almost maximal volume, then in a smaller ball (in a quantified sense) it holds an almost-euclidean isoperimetric inequality. The result is actually established in the more general framework of non-smooth spaces satisfying local Ricci curvature lower bounds in a synthetic sense via optimal transportation.

According to the abstract, the almost-Euclidean isoperimetric inequality holds in a smaller ball ""in a quantified sense"". Unfortunately, the abstract does not provide more details on what this ""quantified sense"" is."
How do Blum and Vempala's password strategies compare to traditional password methods?,9c314f2d,"[{'abstract': '  Reusing passwords across multiple websites is a common practice that\ncompromises security. Recently, Blum and Vempala have proposed password\nstrategies to help people calculate, in their heads, passwords for different\nsites without dependence on third-party tools or external devices. Thus far,\nthe security and efficiency of these ""mental algorithms"" has been analyzed only\ntheoretically. But are such methods usable? We present the first usability\nstudy of humanly computable password strategies, involving a learning phase (to\nlearn a password strategy), then a rehearsal phase (to login to a few\nwebsites), and multiple follow-up tests. In our user study, with training,\nparticipants were able to calculate a deterministic eight-character password\nfor an arbitrary new website in under 20 seconds.\n', 'title': 'Usability of Humanly Computable Passwords'}]","I'm happy to help you with that!

However, I must inform you that the provided context doesn't directly answer your question. The first paper, ""Usability of Humanly Computable Passwords,"" mentions Blum and Vempala's password strategies, but it doesn't compare them to traditional password methods.

Here's the relevant paper:

Title: Usability of Humanly Computable Passwords

Abstract: Reusing passwords across multiple websites is a common practice that compromises security. Recently, Blum and Vempala have proposed password strategies to help people calculate, in their heads, passwords for different sites without dependence on third-party tools or external devices. Thus far, the security and efficiency of these ""mental algorithms"" has been analyzed only theoretically. But are such methods usable? We present the first usability study of humanly computable password strategies, involving a learning phase (to learn a password strategy), then a rehearsal phase (to login to a few websites), and multiple follow-up tests. In our user study, with training, participants were able to calculate a deterministic eight-character password for an arbitrary new website in under 20 seconds.

If you'd like to explore more papers or clarify your question, I'm here to assist you!"
How is Lefschetz duality applied in the context of partial pseudomanifolds?,d3822449,"[{'abstract': '  We prove the Lefschetz duality for intersection (co)homology in the framework\nof $\\partial$-pesudomanifolds. We work with general perversities and without\nrestriction on the coefficient ring.\n', 'title': 'Lefschetz duality for intersection (co)homology'}]","Title: Lefschetz duality for intersection (co)homology

Abstract: We prove the Lefschetz duality for intersection (co)homology in the framework of $\partial$-pseudomanifolds. We work with general perversities and without restriction on the coefficient ring."
What are some new mechanisms proposed to enhance Pareto Local Search for handling multiple objectives?,6dac50d5,"[{'abstract': '  We propose a new Pareto Local Search Algorithm for the many-objective\ncombinatorial optimization. Pareto Local Search proved to be a very effective\ntool in the case of the bi-objective combinatorial optimization and it was used\nin a number of the state-of-the-art algorithms for problems of this kind. On\nthe other hand, the standard Pareto Local Search algorithm becomes very\ninefficient for problems with more than two objectives. We build an effective\nMany-Objective Pareto Local Search algorithm using three new mechanisms: the\nefficient update of large Pareto archives with ND-Tree data structure, a new\nmechanism for the selection of the promising solutions for the neighborhood\nexploration, and a partial exploration of the neighborhoods. We apply the\nproposed algorithm to the instances of two different problems, i.e. the\ntraveling salesperson problem and the traveling salesperson problem with\nprofits with up to 5 objectives showing high effectiveness of the proposed\nalgorithm.\n', 'title': 'Many-Objective Pareto Local Search'}]","Title: Many-Objective Pareto Local Search

Abstract: We propose a new Pareto Local Search Algorithm for the many-objective combinatorial optimization. Pareto Local Search proved to be a very effective tool in the case of the bi-objective combinatorial optimization and it was used in a number of the state-of-the-art algorithms for problems of this kind. On the other hand, the standard Pareto Local Search algorithm becomes very inefficient for problems with more than two objectives. We build an effective Many-Objective Pareto Local Search algorithm using three new mechanisms: the efficient update of large Pareto archives with ND-Tree data structure, a new mechanism for the selection of the promising solutions for the neighborhood exploration, and a partial exploration of the neighborhoods. We apply the proposed algorithm to the instances of two different problems, i.e. the traveling salesperson problem and the traveling salesperson problem with profits with up to 5 objectives showing high effectiveness of the proposed algorithm."
How can a family of Heisenberg modules form a family of metrized quantum vector bundles?,a351cef7,"[{'abstract': '  The modular Gromov-Hausdorff propinquity is a distance on classes of modules\nendowed with quantum metric information, in the form of a metric form of a\nconnection and a left Hilbert module structure. This paper proves that the\nfamily of Heisenberg modules over quantum two tori, when endowed with their\ncanonical connections, form a family of metrized quantum vector bundles, as a\nfirst step in proving that Heisenberg modules form a continuous family for the\nmodular Gromov-Hausdorff propinquity.\n', 'title': 'Heisenberg Modules over Quantum 2-tori are metrized quantum vector bundles'}]","Title: Heisenberg Modules over Quantum 2-tori are metrized quantum vector bundles

Abstract: The modular Gromov-Hausdorff propinquity is a distance on classes of modules 
endowed with quantum metric information, in the form of a metric form of a 
connection and a left Hilbert module structure. This paper proves that the 
family of Heisenberg modules over quantum two tori, when endowed with their 
canonical connections, form a family of metrized quantum vector bundles, as a 
first step in proving that Heisenberg modules form a continuous family for the 
modular Gromov-Hausdorff propinquity."
What are some of the challenges in verifying deep neural networks (DNNs)?,ce7a196c,"[{'abstract': '  In recent years, a number of methods for verifying DNNs have been developed.\nBecause the approaches of the methods differ and have their own limitations, we\nthink that a number of verification methods should be applied to a developed\nDNN. To apply a number of methods to the DNN, it is necessary to translate\neither the implementation of the DNN or the verification method so that one\nruns in the same environment as the other. Since those translations are\ntime-consuming, a utility tool, named DeepSaucer, which helps to retain and\nreuse implementations of DNNs, verification methods, and their environments, is\nproposed. In DeepSaucer, code snippets of loading DNNs, running verification\nmethods, and creating their environments are retained and reused as software\nassets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer\nis confirmed by implementing it on the basis of Anaconda, which provides\nvirtual environment for loading a DNN and running a verification method. In\naddition, the effectiveness of DeepSaucer is demonstrated by usecase examples.\n', 'title': 'DeepSaucer: Unified Environment for Verifying Deep Neural Networks'}]","Based on the context, I found an abstract that discusses the challenges in verifying deep neural networks (DNNs):

Title: DeepSaucer: Unified Environment for Verifying Deep Neural Networks

Abstract: In recent years, a number of methods for verifying DNNs have been developed. Because the approaches of the methods differ and have their own limitations, we think that a number of verification methods should be applied to a developed DNN. To apply a number of methods to the DNN, it is necessary to translate either the implementation of the DNN or the verification method so that one runs in the same environment as the other. Since those translations are time-consuming, a utility tool, named DeepSaucer, which helps to retain and reuse implementations of DNNs, verification methods, and their environments, is proposed. In DeepSaucer, code snippets of loading DNNs, running verification methods, and creating their environments are retained and reused as software assets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer is confirmed by implementing it on the basis of Anaconda, which provides virtual environment for loading a DNN and running a verification method. In addition, the effectiveness of DeepSaucer is demonstrated by usecase examples.

The challenge mentioned is the need to translate either the implementation of the DNN or the verification method so that one runs in the same environment as the other, which is time-consuming."
What types of information can be efficiently transmitted between robots in a swarm using simple communication protocols?,72a15f06,"[{'abstract': '  Swarm systems constitute a challenging problem for reinforcement learning\n(RL) as the algorithm needs to learn decentralized control policies that can\ncope with limited local sensing and communication abilities of the agents.\nWhile it is often difficult to directly define the behavior of the agents,\nsimple communication protocols can be defined more easily using prior knowledge\nabout the given task. In this paper, we propose a number of simple\ncommunication protocols that can be exploited by deep reinforcement learning to\nfind decentralized control policies in a multi-robot swarm environment. The\nprotocols are based on histograms that encode the local neighborhood relations\nof the agents and can also transmit task-specific information, such as the\nshortest distance and direction to a desired target. In our framework, we use\nan adaptation of Trust Region Policy Optimization to learn complex\ncollaborative tasks, such as formation building and building a communication\nlink. We evaluate our findings in a simulated 2D-physics environment, and\ncompare the implications of different communication protocols.\n', 'title': 'Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning'}]","Title: Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning

Abstract: Swarm systems constitute a challenging problem for reinforcement learning (RL) as the algorithm needs to learn decentralized control policies that can cope with limited local sensing and communication abilities of the agents. While it is often difficult to directly define the behavior of the agents, simple communication protocols can be defined more easily using prior knowledge about the given task. In this paper, we propose a number of simple communication protocols that can be exploited by deep reinforcement learning to find decentralized control policies in a multi-robot swarm environment. The protocols are based on histograms that encode the local neighborhood relations of the agents and can also transmit task-specific information, such as the shortest distance and direction to a desired target."
What are the conditions under which Q-Fano varieties with a fixed dimension form a bounded family?,fc2aeee2,"[{'abstract': '  We show that $\\mathbb{Q}$-Fano varieties of fixed dimension with\nanti-canonical degrees and alpha-invariants bounded from below form a bounded\nfamily. As a corollary, K-semistable $\\mathbb{Q}$-Fano varieties of fixed\ndimension with anti-canonical degrees bounded from below form a bounded family.\n', 'title': 'Boundedness of $\\mathbb{Q}$-Fano varieties with degrees and alpha-invariants bounded from below'}]","Title: Boundedness of $\mathbb{Q}$-Fano varieties with degrees and alpha-invariants bounded from below

Abstract: We show that $\mathbb{Q}$-Fano varieties of fixed dimension with anti-canonical degrees and alpha-invariants bounded from below form a bounded family. As a corollary, K-semistable $\mathbb{Q}$-Fano varieties of fixed dimension with anti-canonical degrees bounded from below form a bounded family."
What type of conditions on y are needed to derive minimax rates in uncoupled isotonic regression?,9ed24091,"[{'abstract': '  Isotonic regression is a standard problem in shape-constrained estimation\nwhere the goal is to estimate an unknown nondecreasing regression function $f$\nfrom independent pairs $(x_i, y_i)$ where $\\mathbb{E}[y_i]=f(x_i), i=1, \\ldots\nn$. While this problem is well understood both statistically and\ncomputationally, much less is known about its uncoupled counterpart where one\nis given only the unordered sets $\\{x_1, \\ldots, x_n\\}$ and $\\{y_1, \\ldots,\ny_n\\}$. In this work, we leverage tools from optimal transport theory to derive\nminimax rates under weak moments conditions on $y_i$ and to give an efficient\nalgorithm achieving optimal rates. Both upper and lower bounds employ\nmoment-matching arguments that are also pertinent to learning mixtures of\ndistributions and deconvolution.\n', 'title': 'Uncoupled isotonic regression via minimum Wasserstein deconvolution'}]","Title: Uncoupled isotonic regression via minimum Wasserstein deconvolution

Abstract:  Isotonic regression is a standard problem in shape-constrained estimation
where the goal is to estimate an unknown nondecreasing regression function $f$
from independent pairs $(x_i, y_i)$ where $\mathbb{E}[y_i]=f(x_i), i=1, \ldots
n$. While this problem is well understood both statistically and
computationally, much less is known about its uncoupled counterpart where one
is given only the unordered sets $\{x_1, \ldots, x_n\}$ and $\{y_1, \ldots,
y_n\}$. In this work, we leverage tools from optimal transport theory to derive
minimax rates under weak moments conditions on $y_i$ and to give an efficient
algorithm achieving optimal rates. Both upper and lower bounds employ
moment-matching arguments that are also pertinent to learning mixtures of
distributions and deconvolution."
What mathematical model was created to predict drug diffusion within a spherical tumor?,08e1751b,"[{'abstract': ""  It has been shown recently that changing the fluidic properties of a drug can\nimprove its efficacy in ablating solid tumors. We develop a modeling framework\nfor tumor ablation, and present the simplest possible model for drug diffusion\nin a spherical tumor with leaky boundaries and assuming cell death eventually\nleads to ablation of that cell effectively making the two quantities\nnumerically equivalent. The death of a cell after a given exposure time depends\non both the concentration of the drug and the amount of oxygen available to the\ncell. Higher oxygen availability leads to cell death at lower drug\nconcentrations. It can be assumed that a minimum concentration is required for\na cell to die, effectively connecting diffusion with efficacy. The\nconcentration threshold decreases as exposure time increases, which allows us\nto compute dose-response curves. Furthermore, these curves can be plotted at\nmuch finer time intervals compared to that of experiments, which is used to\nproduce a dose-threshold-response surface giving an observer a complete picture\nof the drug's efficacy for an individual. In addition, since the diffusion,\nleak coefficients, and the availability of oxygen is different for different\nindividuals and tumors, we produce artificial replication data through\nbootstrapping to simulate error. While the usual data-driven model with\nSigmoidal curves use 12 free parameters, our mechanistic model only has two\nfree parameters, allowing it to be open to scrutiny rather than forcing\nagreement with data. Even so, the simplest model in our framework, derived\nhere, shows close agreement with the bootstrapped curves, and reproduces well\nestablished relations, such as Haber's rule.\n"", 'title': 'Modeling of drug diffusion in a solid tumor leading to tumor cell death'}]","Title: Modeling of drug diffusion in a solid tumor leading to tumor cell death

Abstract:   It has been shown recently that changing the fluidic properties of a drug can
improve its efficacy in ablating solid tumors. We develop a modeling framework
for tumor ablation, and present the simplest possible model for drug diffusion
in a spherical tumor with leaky boundaries and assuming cell death eventually
leads to ablation of that cell effectively making the two quantities
numerically equivalent. The death of a cell after a given exposure time depends
on both the concentration of the drug and the amount of oxygen available to the
cell. Higher oxygen availability leads to cell death at lower drug
concentrations. It can be assumed that a minimum concentration is required for
a cell to die, effectively connecting diffusion with efficacy. The
concentration threshold decreases as exposure time increases, which allows us
to compute dose-response curves. Furthermore, these curves can be plotted at
much finer time intervals compared to that of experiments, which is used to
produce a dose-threshold-response surface giving an observer a complete picture
of the drug's efficacy for an individual. In addition, since the diffusion,
leak coefficients, and the availability of oxygen is different for different
individuals and tumors, we produce artificial replication data through
bootstrapping to simulate error. While the usual data-driven model with
Sigmoidal curves use 12 free parameters, our mechanistic model only has two
free parameters, allowing it to be open to scrutiny rather than forcing
agreement with data. Even so, the simplest model in our framework, derived
here, shows close agreement with the bootstrapped curves, and reproduces well
established relations, such as Haber's rule."
Why are scatter plots with linear scales considered challenging for automated data extraction?,44e4ea99,"[{'abstract': '  Charts are an excellent way to convey patterns and trends in data, but they\ndo not facilitate further modeling of the data or close inspection of\nindividual data points. We present a fully automated system for extracting the\nnumerical values of data points from images of scatter plots. We use deep\nlearning techniques to identify the key components of the chart, and optical\ncharacter recognition together with robust regression to map from pixels to the\ncoordinate system of the chart. We focus on scatter plots with linear scales,\nwhich already have several interesting challenges. Previous work has done fully\nautomatic extraction for other types of charts, but to our knowledge this is\nthe first approach that is fully automatic for scatter plots. Our method\nperforms well, achieving successful data extraction on 89% of the plots in our\ntest set.\n', 'title': 'Scatteract: Automated extraction of data from scatter plots'}]","Based on the context, I found an answer to your question:

Title: Scatteract: Automated extraction of data from scatter plots

Abstract: Charts are an excellent way to convey patterns and trends in data, but they do not facilitate further modeling of the data or close inspection of individual data points. We present a fully automated system for extracting the numerical values of data points from images of scatter plots. We use deep learning techniques to identify the key components of the chart, and optical character recognition together with robust regression to map from pixels to the coordinate system of the chart. We focus on scatter plots with linear scales, which already have several interesting challenges. Previous work has done fully automatic extraction for other types of charts, but to our knowledge this is the first approach that is fully automatic for scatter plots. Our method performs well, achieving successful data extraction on 89% of the plots in our test set.

The answer is implicit in the abstract, but it doesn't explicitly state why scatter plots with linear scales are challenging. However, we can infer that the authors consider them challenging because they mention that ""scatter plots with linear scales, which already have several interesting challenges"" and that their approach is the first to tackle this specific type of chart. If you'd like, I can try to help you with other questions or assist in finding relevant papers!"
What discussion topics were covered in a BoF about astronomy software metadata? ,14db7edd,"[{'abstract': '  Broad efforts are underway to capture metadata about research software and\nretain it across services; notable in this regard is the CodeMeta project. What\nmetadata are important to have about (research) software? What metadata are\nuseful for searching for codes? What would you like to learn about astronomy\nsoftware? This BoF sought to gather information on metadata most desired by\nresearchers and users of astro software and others interested in registering,\nindexing, capturing, and doing research on this software. Information from this\nBoF could conceivably result in changes to the Astrophysics Source Code Library\n(ASCL) or other resources for the benefit of the community or provide input\ninto other projects concerned with software metadata.\n', 'title': 'Software metadata: How much is enough?'}]","Title: Software metadata: How much is enough?

Abstract: Broad efforts are underway to capture metadata about research software and retain it across services; notable in this regard is the CodeMeta project. What metadata are important to have about (research) software? What metadata are useful for searching for codes? What would you like to learn about astronomy software? This BoF sought to gather information on metadata most desired by researchers and users of astro software and others interested in registering, indexing, capturing, and doing research on this software. Information from this BoF could conceivably result in changes to the Astrophysics Source Code Library (ASCL) or other resources for the benefit of the community or provide input into other projects concerned with software metadata."
How can Bayesian methods be used to determine the full distribution of fluxes in a genome-scale metabolic system?,ab6a9c9c,"[{'abstract': '  Metabolic flux balance analyses are a standard tool in analysing metabolic\nreaction rates compatible with measurements, steady-state and the metabolic\nreaction network stoichiometry. Flux analysis methods commonly place\nunrealistic assumptions on fluxes due to the convenience of formulating the\nproblem as a linear programming model, and most methods ignore the notable\nuncertainty in flux estimates. We introduce a novel paradigm of Bayesian\nmetabolic flux analysis that models the reactions of the whole genome-scale\ncellular system in probabilistic terms, and can infer the full flux vector\ndistribution of genome-scale metabolic systems based on exchange and\nintracellular (e.g. 13C) flux measurements, steady-state assumptions, and\ntarget function assumptions. The Bayesian model couples all fluxes jointly\ntogether in a simple truncated multivariate posterior distribution, which\nreveals informative flux couplings. Our model is a plug-in replacement to\nconventional metabolic balance methods, such as flux balance analysis (FBA).\nOur experiments indicate that we can characterise the genome-scale flux\ncovariances, reveal flux couplings, and determine more intracellular unobserved\nfluxes in C. acetobutylicum from 13C data than flux variability analysis. The\nCOBRA compatible software is available at github.com/markusheinonen/bamfa\n', 'title': 'Bayesian Metabolic Flux Analysis reveals intracellular flux couplings'}]","I think I can help you with that!

Title: Bayesian Metabolic Flux Analysis reveals intracellular flux couplings

Abstract: Metabolic flux balance analyses are a standard tool in analysing metabolic
reaction rates compatible with measurements, steady-state and the metabolic
reaction network stoichiometry. Flux analysis methods commonly place
unrealistic assumptions on fluxes due to the convenience of formulating the
problem as a linear programming model, and most methods ignore the notable
uncertainty in flux estimates. We introduce a novel paradigm of Bayesian
metabolic flux analysis that models the reactions of the whole genome-scale
cellular system in probabilistic terms, and can infer the full flux vector
distribution of genome-scale metabolic systems based on exchange and
intracellular (e.g. 13C) flux measurements, steady-state assumptions, and
target function assumptions. The Bayesian model couples all fluxes jointly
together in a simple truncated multivariate posterior distribution, which
reveals informative flux couplings. Our model is a plug-in replacement to
conventional metabolic balance methods, such as flux balance analysis (FBA).
Our experiments indicate that we can characterise the genome-scale flux
covariances, reveal flux couplings, and determine more intracellular unobserved
fluxes in C. acetobutylicum from 13C data than flux variability analysis. The
COBRA compatible software is available at github.com/markusheinonen/bamfa"
How does a top-down algorithm handle the rejection of hypotheses in a directed acyclic graph?,6cb89e78,"[{'abstract': ""  We propose a linear-time, single-pass, top-down algorithm for multiple\ntesting on directed acyclic graphs (DAGs), where nodes represent hypotheses and\nedges specify a partial ordering in which hypotheses must be tested. The\nprocedure is guaranteed to reject a sub-DAG with bounded false discovery rate\n(FDR) while satisfying the logical constraint that a rejected node's parents\nmust also be rejected. It is designed for sequential testing settings, when the\nDAG structure is known a priori, but the $p$-values are obtained selectively\n(such as in a sequence of experiments), but the algorithm is also applicable in\nnon-sequential settings when all $p$-values can be calculated in advance (such\nas variable/model selection). Our DAGGER algorithm, shorthand for Greedily\nEvolving Rejections on DAGs, provably controls the false discovery rate under\nindependence, positive dependence or arbitrary dependence of the $p$-values.\nThe DAGGER procedure specializes to known algorithms in the special cases of\ntrees and line graphs, and simplifies to the classical Benjamini-Hochberg\nprocedure when the DAG has no edges. We explore the empirical performance of\nDAGGER using simulations, as well as a real dataset corresponding to a gene\nontology, showing favorable performance in terms of time and power.\n"", 'title': 'DAGGER: A sequential algorithm for FDR control on DAGs'}]","Title: DAGGER: A sequential algorithm for FDR control on DAGs

Abstract: We propose a linear-time, single-pass, top-down algorithm for multiple testing on directed acyclic graphs (DAGs), where nodes represent hypotheses and edges specify a partial ordering in which hypotheses must be tested. The procedure is guaranteed to reject a sub-DAG with bounded false discovery rate (FDR) while satisfying the logical constraint that a rejected node's parents must also be rejected. It is designed for sequential testing settings, when the DAG structure is known a priori, but the p-values are obtained selectively (such as in a sequence of experiments), but the algorithm is also applicable in non-sequential settings when all p-values can be calculated in advance (such as variable/model selection). Our DAGGER algorithm, shorthand for Greedily Evolving Rejections on DAGs, provably controls the false discovery rate under independence, positive dependence or arbitrary dependence of the p-values. The DAGGER procedure specializes to known algorithms in the special cases of trees and line graphs, and simplifies to the classical Benjamini-Hochberg procedure when the DAG has no edges. We explore the empirical performance of DAGGER using simulations, as well as a real dataset corresponding to a gene ontology, showing favorable performance in terms of time and power."
What does this paper say about  $epsilon$-over-recurrence and  ergodic measure preserving invertible transformations?,8a62348f,"[{'abstract': '  We show that every invertible strong mixing transformation on a Lebesgue\nspace has strictly over-recurrent sets. Also, we give an explicit procedure for\nconstructing strong mixing transformations with no under-recurrent sets. This\nanswers both parts of a question of V. Bergelson.\nWe define $\\epsilon$-over-recurrence and show that given $\\epsilon > 0$, any\nergodic measure preserving invertible transformation (including discrete\nspectrum) has $\\epsilon$-over-recurrent sets of arbitrarily small measure.\nDiscrete spectrum transformations and rotations do not have over-recurrent\nsets, but we construct a weak mixing rigid transformation with strictly\nover-recurrent sets.\n', 'title': 'Over Recurrence for Mixing Transformations'}]","Title: Over Recurrence for Mixing Transformations

Abstract: We show that every invertible strong mixing transformation on a Lebesgue space has strictly over-recurrent sets. Also, we give an explicit procedure for constructing strong mixing transformations with no under-recurrent sets. This answers both parts of a question of V. Bergelson. We define ε-over-recurrence and show that given ε > 0, any ergodic measure preserving invertible transformation (including discrete spectrum) has ε-over-recurrent sets of arbitrarily small measure."
What is the minimum number of fault-free neighbors required for a vertex in a system?,eef2e7b6,"[{'abstract': '  In the work of Peng et al. in 2012, a new measure was proposed for fault\ndiagnosis of systems: namely, g-good-neighbor conditional diagnosability, which\nrequires that any fault-free vertex has at least g fault-free neighbors in the\nsystem. In this paper, we establish the g-good-neighbor conditional\ndiagnosability of locally twisted cubes under the PMC model and the MM^* model.\n', 'title': 'The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes'}]","Title: The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes

Abstract: In the work of Peng et al. in 2012, a new measure was proposed for fault diagnosis of systems: namely, g-good-neighbor conditional diagnosability, which requires that any fault-free vertex has at least g fault-free neighbors in the system. In this paper, we establish the g-good-neighbor conditional diagnosability of locally twisted cubes under the PMC model and the MM^* model.

The answer is ""at least g"" fault-free neighbors."
What is the effect of soap film thickness on surface tension?,9cc404f5,"[{'abstract': '  The surface tension of flowing soap films is measured with respect to the\nfilm thickness and the concentration of soap solution. We perform this\nmeasurement by measuring the curvature of the nylon wires that bound the soap\nfilm channel and use the measured curvature to parametrize the relation between\nthe surface tension and the tension of the wire. We find the surface tension of\nour soap films increases when the film is relatively thin or made of soap\nsolution of low concentration, otherwise it approaches an asymptotic value 30\nmN/m. A simple adsorption model with only two parameters describes our\nobservations reasonably well. With our measurements, we are also able to\nmeasure Gibbs elasticity for our soap film.\n', 'title': 'Surface tension of flowing soap films'}]","Title: Surface tension of flowing soap films

Abstract: The surface tension of flowing soap films is measured with respect to the film thickness and the concentration of soap solution. We perform this measurement by measuring the curvature of the nylon wires that bound the soap film channel and use the measured curvature to parametrize the relation between the surface tension and the tension of the wire. We find the surface tension of our soap films increases when the film is relatively thin or made of soap solution of low concentration, otherwise it approaches an asymptotic value 30 mN/m. A simple adsorption model with only two parameters describes our observations reasonably well. With our measurements, we are also able to measure Gibbs elasticity for our soap film."
How does the PULSE method address the challenge of non-Markovian dependencies in music?,6c996f7a,"[{'abstract': '  Predictive models for music are studied by researchers of algorithmic\ncomposition, the cognitive sciences and machine learning. They serve as base\nmodels for composition, can simulate human prediction and provide a\nmultidisciplinary application domain for learning algorithms. A particularly\nwell established and constantly advanced subtask is the prediction of\nmonophonic melodies. As melodies typically involve non-Markovian dependencies\ntheir prediction requires a capable learning algorithm. In this thesis, I apply\nthe recent feature discovery and learning method PULSE to the realm of symbolic\nmusic modeling. PULSE is comprised of a feature generating operation and\nL1-regularized optimization. These are used to iteratively expand and cull the\nfeature set, effectively exploring feature spaces that are too large for common\nfeature selection approaches. I design a general Python framework for PULSE,\npropose task-optimized feature generating operations and various\nmusic-theoretically motivated features that are evaluated on a standard corpus\nof monophonic folk and chorale melodies. The proposed method significantly\noutperforms comparable state-of-the-art models. I further discuss the free\nparameters of the learning algorithm and analyze the feature composition of the\nlearned models. The models learned by PULSE afford an easy inspection and are\nmusicologically interpreted for the first time.\n', 'title': 'Learning a Predictive Model for Music Using PULSE'}]","Title: Learning a Predictive Model for Music Using PULSE

Abstract: Predictive models for music are studied by researchers of algorithmic
composition, the cognitive sciences and machine learning. They serve as base
models for composition, can simulate human prediction and provide a
multidisciplinary application domain for learning algorithms. A particularly
well established and constantly advanced subtask is the prediction of
monophonic melodies. As melodies typically involve non-Markovian dependencies
their prediction requires a capable learning algorithm. In this thesis, I apply
the recent feature discovery and learning method PULSE to the realm of symbolic
music modeling. PULSE is comprised of a feature generating operation and
L1-regularized optimization. These are used to iteratively expand and cull the
feature set, effectively exploring feature spaces that are too large for common
feature selection approaches."
Describe the connection between the classification of pro-$p$ Poincaré duality pairs and the separability of the mapping class group.,2ec884d4,"[{'abstract': '  We classify pro-$p$ Poincaré duality pairs in dimension two. We then use\nthis classification to build a pro-$p$ analogue of the curve complex and\nestablish its basic properties. We conclude with some statements concerning\nseparability properties of the mapping class group.\n', 'title': 'Classification of pro-$p$ PD$^2$ pairs and the pro-$p$ curve complex'}]","Based on the context, the connection between the classification of pro-$p$ Poincaré duality pairs and the separability of the mapping class group can be found in the first paper:

Title: Classification of pro-$p$ PD$^2$ pairs and the pro-$p$ curve complex

Abstract:  We classify pro-$p$ Poincaré duality pairs in dimension two. We then use
this classification to build a pro-$p$ analogue of the curve complex and
establish its basic properties. We conclude with some statements concerning
separability properties of the mapping class group.

The abstract suggests that the classification of pro-$p$ Poincaré duality pairs is used to build a pro-$p$ analogue of the curve complex, and this is then used to establish separability properties of the mapping class group."
How do electron energy increases and magnetic field conditions vary within foreshock transient cores that exhibit electron acceleration?,9e86d693,"[{'abstract': ""  Foreshock transients upstream of Earth's bow shock have been recently\nobserved to accelerate electrons to many times their thermal energy. How such\nacceleration occurs is unknown, however. Using THEMIS case studies, we examine\na subset of acceleration events (31 of 247 events) in foreshock transients with\ncores that exhibit gradual electron energy increases accompanied by low\nbackground magnetic field strength and large-amplitude magnetic fluctuations.\nUsing the evolution of electron distributions and the energy increase rates at\nmultiple spacecraft, we suggest that Fermi acceleration between a converging\nforeshock transient's compressional boundary and the bow shock is responsible\nfor the observed electron acceleration. We then show that a one-dimensional\ntest particle simulation of an ideal Fermi acceleration model in fluctuating\nfields prescribed by the observations can reproduce the observed evolution of\nelectron distributions, energy increase rate, and pitch-angle isotropy,\nproviding further support for our hypothesis. Thus, Fermi acceleration is\nlikely the principal electron acceleration mechanism in at least this subset of\nforeshock transient cores.\n"", 'title': 'Fermi acceleration of electrons inside foreshock transient cores'}]","Title: Fermi acceleration of electrons inside foreshock transient cores

Abstract: Foreshock transients upstream of Earth's bow shock have been recently observed to accelerate electrons to many times their thermal energy. How such acceleration occurs is unknown, however. Using THEMIS case studies, we examine a subset of acceleration events (31 of 247 events) in foreshock transients with cores that exhibit gradual electron energy increases accompanied by low background magnetic field strength and large-amplitude magnetic fluctuations. Using the evolution of electron distributions and the energy increase rates at multiple spacecraft, we suggest that Fermi acceleration between a converging foreshock transient's compressional boundary and the bow shock is responsible for the observed electron acceleration. We then show that a one-dimensional test particle simulation of an ideal Fermi acceleration model in fluctuating fields prescribed by the observations can reproduce the observed evolution of electron distributions, energy increase rate, and pitch-angle isotropy, providing further support for our hypothesis. Thus, Fermi acceleration is likely the principal electron acceleration mechanism in at least this subset of foreshock transient cores."
What type of semimetal does the Ir conduction electrons form in Pr2Ir2O7?,0483254a,"[{'abstract': '  We study the band structure topology and engineering from the interplay\nbetween local moments and itinerant electrons in the context of pyrochlore\niridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction\nelectrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.\nWhile the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned\ninto an ordered spin ice with a finite ordering wavevector, dubbed\n""Melko-Hertog-Gingras"" state, by varying Ir and O contents. We point out that\nthe ordered spin ice of the Pr local moments generates an internal magnetic\nfield that reconstructs the band structure of the Luttinger semimetal. Besides\nthe broad existence of Weyl nodes, we predict that the magnetic translation of\nthe ""Melko-Hertog-Gingras"" state for the Pr moments protects the Dirac band\ntouching at certain time reversal invariant momenta for the Ir conduction\nelectrons. We propose the magnetic fields to control the Pr magnetic structure\nand thereby indirectly influence the topological and other properties of the Ir\nelectrons. Our prediction may be immediately tested in the ordered\nPr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed\nexamination of the band structure, magneto-transport, and other properties of\nPr$_2$Ir$_2$O$_7$.\n', 'title': 'Pr$_2$Ir$_2$O$_7$: when Luttinger semimetal meets Melko-Hertog-Gingras spin ice state'}]","Title: Pr$_2$Ir$_2$O$_7$: when Luttinger semimetal meets Melko-Hertog-Gingras spin ice state

Abstract:   We study the band structure topology and engineering from the interplay
between local moments and itinerant electrons in the context of pyrochlore
iridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction
electrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.
While the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned
into an ordered spin ice with a finite ordering wavevector, dubbed
""Melko-Hertog-Gingras"" state, by varying Ir and O contents. We point out that
the ordered spin ice of the Pr local moments generates an internal magnetic
field that reconstructs the band structure of the Luttinger semimetal. Besides
the broad existence of Weyl nodes, we predict that the magnetic translation of
the ""Melko-Hertog-Gingras"" state for the Pr moments protects the Dirac band
touching at certain time reversal invariant momenta for the Ir conduction
electrons. We propose the magnetic fields to control the Pr magnetic structure
and thereby indirectly influence the topological and other properties of the Ir
electrons. Our prediction may be immediately tested in the ordered
Pr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed
examination of the band structure, magneto-transport, and other properties of
Pr$_2$Ir$_2$O$_7$."
Can quadratic growth of the objective functional be guaranteed for non-bang-bang controls?,b2198912,"[{'abstract': '  We consider bilinear optimal control problems, whose objective functionals do\nnot depend on the controls. Hence, bang-bang solutions will appear. We\ninvestigate sufficient second-order conditions for bang-bang controls, which\nguarantee local quadratic growth of the objective functional in $L^1$. In\naddition, we prove that for controls that are not bang-bang, no such growth can\nbe expected. Finally, we study the finite-element discretization, and prove\nerror estimates of bang-bang controls in $L^1$-norms.\n', 'title': 'Second-Order Analysis and Numerical Approximation for Bang-Bang Bilinear Control Problems'}]","Title: Second-Order Analysis and Numerical Approximation for Bang-Bang Bilinear Control Problems

Abstract: We consider bilinear optimal control problems, whose objective functionals do not depend on the controls. Hence, bang-bang solutions will appear. We investigate sufficient second-order conditions for bang-bang controls, which guarantee local quadratic growth of the objective functional in $L^1$. In addition, we prove that for controls that are not bang-bang, no such growth can be expected. Finally, we study the finite-element discretization, and prove error estimates of bang-bang controls in $L^1$-norms.

The answer to your question is ""no"", quadratic growth of the objective functional cannot be guaranteed for non-bang-bang controls."
How can new p-sets be used to construct point sets with cardinality other than a prime number?,509cefeb,"[{'abstract': ""  The $p$-set, which is in a simple analytic form, is well distributed in unit\ncubes. The well-known Weil's exponential sum theorem presents an upper bound of\nthe exponential sum over the $p$-set. Based on the result, one shows that the\n$p$-set performs well in numerical integration, in compressed sensing as well\nas in UQ. However, $p$-set is somewhat rigid since the cardinality of the\n$p$-set is a prime $p$ and the set only depends on the prime number $p$. The\npurpose of this paper is to present generalizations of $p$-sets, say\n$\\mathcal{P}_{d,p}^{{\\mathbf a},\\epsilon}$, which is more flexible.\nParticularly, when a prime number $p$ is given, we have many different choices\nof the new $p$-sets. Under the assumption that Goldbach conjecture holds, for\nany even number $m$, we present a point set, say ${\\mathcal L}_{p,q}$, with\ncardinality $m-1$ by combining two different new $p$-sets, which overcomes a\nmajor bottleneck of the $p$-set. We also present the upper bounds of the\nexponential sums over $\\mathcal{P}_{d,p}^{{\\mathbf a},\\epsilon}$ and ${\\mathcal\nL}_{p,q}$, which imply these sets have many potential applications.\n"", 'title': 'On generalizations of $p$-sets and their applications'}]","Title: On generalizations of $p$-sets and their applications

Abstract:   The $p$-set, which is in a simple analytic form, is well distributed in unit
cubes. The well-known Weil's exponential sum theorem presents an upper bound of
the exponential sum over the $p$-set. Based on the result, one shows that the
$p$-set performs well in numerical integration, in compressed sensing as well
as in UQ. However, $p$-set is somewhat rigid since the cardinality of the
$p$-set is a prime $p$ and the set only depends on the prime number $p$. The
purpose of this paper is to present generalizations of $p$-sets, say
$\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$, which is more flexible.
Particularly, when a prime number $p$ is given, we have many different choices
of the new $p$-sets. Under the assumption that Goldbach conjecture holds, for
any even number $m$, we present a point set, say ${\mathcal L}_{p,q}$, with
cardinality $m-1$ by combining two different new $p$-sets, which overcomes a
major bottleneck of the $p$-set. We also present the upper bounds of the
exponential sums over $\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$ and ${\mathcal
L}_{p,q}$, which imply these sets have many potential applications."
How does the Relativistic Transactional Interpretation (RTI) address the issue of absorption in quantum mechanics?,995404ab,"[{'abstract': ""  In view of a resurgence of concern about the measurement problem, it is\npointed out that the Relativistic Transactional Interpretation (RTI) remedies\nissues previously considered as drawbacks or refutations of the original TI.\nSpecifically, once one takes into account relativistic processes that are not\nrepresentable at the non-relativistic level (such as particle creation and\nannihilation, and virtual propagation), absorption is quantitatively defined in\nunambiguous physical terms. In addition, specifics of the relativistic\ntransactional model demonstrate that the Maudlin `contingent absorber'\nchallenge to the original TI cannot even be mounted: basic features of\nestablished relativistic field theories (in particular, the asymmetry between\nfield sources and the bosonic fields, and the fact that slow-moving bound\nstates, such as atoms, are not offer waves) dictate that the `slow-moving offer\nwave' required for the challenge scenario cannot exist. It is concluded that\nissues previously considered obstacles for TI are no longer legitimately viewed\nas such, and that reconsideration of the transactional picture is warranted in\nconnection with solving the measurement problem.\n"", 'title': 'On the Status of the Measurement Problem: Recalling the Relativistic Transactional Interpretation'}]","Title: On the Status of the Measurement Problem: Recalling the Relativistic Transactional Interpretation

Abstract:   In view of a resurgence of concern about the measurement problem, it is
pointed out that the Relativistic Transactional Interpretation (RTI) remedies
issues previously considered as drawbacks or refutations of the original TI.
Specifically, once one takes into account relativistic processes that are not
representable at the non-relativistic level (such as particle creation and
annihilation, and virtual propagation), absorption is quantitatively defined in
unambiguous physical terms."
What are the computational differences between using sparse matrices and dense matrices in the discrete Schrödinger equation?,1e0977bf,"[{'abstract': '  In the present paper we consider numerical methods to solve the discrete\nSchrödinger equation with a time dependent Hamiltonian (motivated by problems\nencountered in the study of spin systems). We will consider both short-range\ninteractions, which lead to evolution equations involving sparse matrices, and\nlong-range interactions, which lead to dense matrices. Both of these settings\nshow very different computational characteristics. We use Magnus integrators\nfor time integration and employ a framework based on Leja interpolation to\ncompute the resulting action of the matrix exponential. We consider both\ntraditional Magnus integrators (which are extensively used for these types of\nproblems in the literature) as well as the recently developed commutator-free\nMagnus integrators and implement them on modern CPU and GPU (graphics\nprocessing unit) based systems.\nWe find that GPUs can yield a significant speed-up (up to a factor of $10$ in\nthe dense case) for these types of problems. In the sparse case GPUs are only\nadvantageous for large problem sizes and the achieved speed-ups are more\nmodest. In most cases the commutator-free variant is superior but especially on\nthe GPU this advantage is rather small. In fact, none of the advantage of\ncommutator-free methods on GPUs (and on multi-core CPUs) is due to the\nelimination of commutators. This has important consequences for the design of\nmore efficient numerical methods.\n', 'title': 'Magnus integrators on multicore CPUs and GPUs'}]","Title: Magnus integrators on multicore CPUs and GPUs

Abstract:   In the present paper we consider numerical methods to solve the discrete
Schrödinger equation with a time dependent Hamiltonian (motivated by problems
encountered in the study of spin systems). We will consider both short-range
interactions, which lead to evolution equations involving sparse matrices, and
long-range interactions, which lead to dense matrices. Both of these settings
show very different computational characteristics. We use Magnus integrators
for time integration and employ a framework based on Leja interpolation to
compute the resulting action of the matrix exponential. We consider both
traditional Magnus integrators (which are extensively used for these types of
problems in the literature) as well as the recently developed commutator-free
Magnus integrators and implement them on modern CPU and GPU (graphics
processing unit) based systems.
We find that GPUs can yield a significant speed-up (up to a factor of $10$ in
the dense case) for these types of problems. In the sparse case GPUs are only
advantageous for large problem sizes and the achieved speed-ups are more
modest. In most cases the commutator-free variant is superior but especially on
the GPU this advantage is rather small. In fact, none of the advantage of
commutator-free methods on GPUs (and on multi-core CPUs) is due to the
elimination of commutators. This has important consequences for the design of
more efficient numerical methods."
Can statistical depth functions be applied in a regression setting?,729738e3,"[{'abstract': '  As a measure for the centrality of a point in a set of multivariate data,\nstatistical depth functions play important roles in multivariate analysis,\nbecause one may conveniently construct descriptive as well as inferential\nprocedures relying on them. Many depth notions have been proposed in the\nliterature to fit to different applications. However, most of them are mainly\ndeveloped for the location setting. In this paper, we discuss the possibility\nof extending some of them into the regression setting. A general concept of\nregression depth function is also provided.\n', 'title': 'General notions of regression depth function'}]","Title: General notions of regression depth function

Abstract:  As a measure for the centrality of a point in a set of multivariate data, 
statistical depth functions play important roles in multivariate analysis, 
because one may conveniently construct descriptive as well as inferential 
procedures relying on them. Many depth notions have been proposed in the 
literature to fit to different applications. However, most of them are mainly 
developed for the location setting. In this paper, we discuss the possibility 
of extending some of them into the regression setting. A general concept of 
regression depth function is also provided."
How does Trimmed Hard Thresholding work as a robust variant of Iterative Hard Thresholding (IHT) in the context of M-estimation?,6a050080,"[{'abstract': '  We study the problem of sparsity constrained $M$-estimation with arbitrary\ncorruptions to both {\\em explanatory and response} variables in the\nhigh-dimensional regime, where the number of variables $d$ is larger than the\nsample size $n$. Our main contribution is a highly efficient gradient-based\noptimization algorithm that we call Trimmed Hard Thresholding -- a robust\nvariant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient\ncomputations. Our algorithm can deal with a wide class of sparsity constrained\n$M$-estimation problems, and we can tolerate a nearly dimension independent\nfraction of arbitrarily corrupted samples. More specifically, when the\ncorrupted fraction satisfies $\\epsilon \\lesssim {1} /\\left({\\sqrt{k} \\log\n(nd)}\\right)$, where $k$ is the sparsity of the parameter, we obtain accurate\nestimation and model selection guarantees with optimal sample complexity.\nFurthermore, we extend our algorithm to sparse Gaussian graphical model\n(precision matrix) estimation via a neighborhood selection approach. We\ndemonstrate the effectiveness of robust estimation in sparse linear, logistic\nregression, and sparse precision matrix estimation on synthetic and real-world\nUS equities data.\n', 'title': 'High Dimensional Robust Estimation of Sparse Models via Trimmed Hard Thresholding'}]","Title: High Dimensional Robust Estimation of Sparse Models via Trimmed Hard Thresholding

Abstract:   We study the problem of sparsity constrained $M$-estimation with arbitrary
corruptions to both {\em explanatory and response} variables in the
high-dimensional regime, where the number of variables $d$ is larger than the
sample size $n$. Our main contribution is a highly efficient gradient-based
optimization algorithm that we call Trimmed Hard Thresholding -- a robust
variant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient
computations. Our algorithm can deal with a wide class of sparsity constrained
$M$-estimation problems, and we can tolerate a nearly dimension independent
fraction of arbitrarily corrupted samples. More specifically, when the
corrupted fraction satisfies $\epsilon \lesssim {1} /\left({\sqrt{k} \log
(nd)}\right)$, where $k$ is the sparsity of the parameter, we obtain accurate
estimation and model selection guarantees with optimal sample complexity.
Furthermore, we extend our algorithm to sparse Gaussian graphical model
(precision matrix) estimation via a neighborhood selection approach. We
demonstrate the effectiveness of robust estimation in sparse linear, logistic
regression, and sparse precision matrix estimation on synthetic and real-world
US equities data."
What are the advantages of using approximate submodular maximization in coupon allocation?,97ad1ed3,"[{'abstract': '  CMO Council reports that 71\\% of internet users in the U.S. were influenced\nby coupons and discounts when making their purchase decisions. It has also been\nshown that offering coupons to a small fraction of users (called seed users)\nmay affect the purchase decisions of many other users in a social network. This\nmotivates us to study the optimal coupon allocation problem, and our objective\nis to allocate coupons to a set of users so as to maximize the expected\ncascade. Different from existing studies on influence maximizaton (IM), our\nframework allows a general utility function and a more complex set of\nconstraints. In particular, we formulate our problem as an approximate\nsubmodular maximization problem subject to matroid and knapsack constraints.\nExisting techniques relying on the submodularity of the utility function, such\nas greedy algorithm, can not work directly on a non-submodular function. We use\n$\\epsilon$ to measure the difference between our function and its closest\nsubmodular function and propose a novel approximate algorithm with\napproximation ratio $\\beta(\\epsilon)$ with $\\lim_{\\epsilon\\rightarrow\n0}\\beta(\\epsilon)=1-1/e$. This is the best approximation guarantee for\napproximate submodular maximization subject to a partition matroid and knapsack\nconstraints, our results apply to a broad range of optimization problems that\ncan be formulated as an approximate submodular maximization problem.\n', 'title': 'Toward Optimal Coupon Allocation in Social Networks: An Approximate Submodular Optimization Approach'}]","Title: Toward Optimal Coupon Allocation in Social Networks: An Approximate Submodular Optimization Approach

Abstract: CMO Council reports that 71\% of internet users in the U.S. were influenced by coupons and discounts when making their purchase decisions. It has also been shown that offering coupons to a small fraction of users (called seed users) may affect the purchase decisions of many other users in a social network. This motivates us to study the optimal coupon allocation problem, and our objective is to allocate coupons to a set of users so as to maximize the expected cascade. Different from existing studies on influence maximization (IM), our framework allows a general utility function and a more complex set of constraints. In particular, we formulate our problem as an approximate submodular maximization problem subject to matroid and knapsack constraints. Existing techniques relying on the submodularity of the utility function, such as greedy algorithm, can not work directly on a non-submodular function. We use $\epsilon$ to measure the difference between our function and its closest submodular function and propose a novel approximate algorithm with approximation ratio $\beta(\epsilon)$ with $\lim_{\epsilon\rightarrow 0}\beta(\epsilon)=1-1/e$. This is the best approximation guarantee for approximate submodular maximization subject to a partition matroid and knapsack constraints, our results apply to a broad range of optimization problems that can be formulated as an approximate submodular maximization problem.

The advantages of using approximate submodular maximization in coupon allocation are:

* It allows for a general utility function and more complex constraints, making it more flexible than existing studies on influence maximization.
* It provides a novel approximate algorithm with a good approximation ratio, making it a more efficient and effective solution than existing techniques.
* The results apply to a broad range of optimization problems, making it a versatile approach."
How did the global financial crisis impact the core-periphery structure of the interbank market?,4f903d8a,"[{'abstract': '  Interbank markets are often characterised in terms of a core-periphery\nnetwork structure, with a highly interconnected core of banks holding the\nmarket together, and a periphery of banks connected mostly to the core but not\ninternally. This paradigm has recently been challenged for short time scales,\nwhere interbank markets seem better characterised by a bipartite structure with\nmore core-periphery connections than inside the core. Using a novel\ncore-periphery detection method on the eMID interbank market, we enrich this\npicture by showing that the network is actually characterised by multiple\ncore-periphery pairs. Moreover, a transition from core-periphery to bipartite\nstructures occurs by shortening the temporal scale of data aggregation. We\nfurther show how the global financial crisis transformed the market, in terms\nof composition, multiplicity and internal organisation of core-periphery pairs.\nBy unveiling such a fine-grained organisation and transformation of the\ninterbank market, our method can find important applications in the\nunderstanding of how distress can propagate over financial networks.\n', 'title': 'Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis'}]","Here's the answer:

Title: Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis

Abstract: Interbank markets are often characterised in terms of a core-periphery network structure, with a highly interconnected core of banks holding the market together, and a periphery of banks connected mostly to the core but not internally. This paradigm has recently been challenged for short time scales, where interbank markets seem better characterised by a bipartite structure with more core-periphery connections than inside the core. Using a novel core-periphery detection method on the eMID interbank market, we enrich this picture by showing that the network is actually characterised by multiple core-periphery pairs. Moreover, a transition from core-periphery to bipartite structures occurs by shortening the temporal scale of data aggregation. We further show how the global financial crisis transformed the market, in terms of composition, multiplicity and internal organisation of core-periphery pairs. By unveiling such a fine-grained organisation and transformation of the interbank market, our method can find important applications in the understanding of how distress can propagate over financial networks."
What specific challenges are addressed in the software processing pipeline for  inhomogeneous radio telescope data?,7e326d4e,"[{'abstract': ""  The Large European Array for Pulsars combines Europe's largest radio\ntelescopes to form a tied-array telescope that provides high signal-to-noise\nobservations of millisecond pulsars (MSPs) with the objective to increase the\nsensitivity of detecting low-frequency gravitational waves. As part of this\nendeavor we have developed a software correlator and beamformer which enables\nthe formation of a tied-array beam from the raw voltages from each of\ntelescopes. We explain the concepts and techniques involved in the process of\nadding the raw voltages coherently. We further present the software processing\npipeline that is specifically designed to deal with data from widely spaced,\ninhomogeneous radio telescopes and describe the steps involved in preparing,\ncorrelating and creating the tied-array beam. This includes polarization\ncalibration, bandpass correction, frequency dependent phase correction,\ninterference mitigation and pulsar gating. A link is provided where the\nsoftware can be obtained.\n"", 'title': 'The beamformer and correlator for the Large European Array for Pulsars'}]","Title: The beamformer and correlator for the Large European Array for Pulsars

Abstract:   The Large European Array for Pulsars combines Europe's largest radio
telescopes to form a tied-array telescope that provides high signal-to-noise
observations of millisecond pulsars (MSPs) with the objective to increase the
sensitivity of detecting low-frequency gravitational waves. As part of this
endeavor we have developed a software correlator and beamformer which enables
the formation of a tied-array beam from the raw voltages from each of
telescopes. We explain the concepts and techniques involved in the process of
adding the raw voltages coherently. We further present the software processing
pipeline that is specifically designed to deal with data from widely spaced,
inhomogeneous radio telescopes and describe the steps involved in preparing,
correlating and creating the tied-array beam. This includes polarization
calibration, bandpass correction, frequency dependent phase correction,
interference mitigation and pulsar gating. A link is provided where the
software can be obtained.

In this paper, the specific challenges addressed in the software processing pipeline for inhomogeneous radio telescope data are:

* Coherently adding raw voltages from individual telescopes
* Polarization calibration
* Bandpass correction
* Frequency dependent phase correction
* Interference mitigation
* Pulsar gating"
What makes the proposed approach in the paper better suited for exploring complex multimodal posteriors?,983132ab,"[{'abstract': '  Recent advances in stochastic gradient techniques have made it possible to\nestimate posterior distributions from large datasets via Markov Chain Monte\nCarlo (MCMC). However, when the target posterior is multimodal, mixing\nperformance is often poor. This results in inadequate exploration of the\nposterior distribution. A framework is proposed to improve the sampling\nefficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A\ngeneralized kinetic function is leveraged, delivering superior stationary\nmixing, especially for multimodal distributions. Techniques are also discussed\nto overcome the practical issues introduced by this generalization. It is shown\nthat the proposed approach is better at exploring complex multimodal posterior\ndistributions, as demonstrated on multiple applications and in comparison with\nother stochastic gradient MCMC methods.\n', 'title': 'Stochastic Gradient Monomial Gamma Sampler'}]","The question is asking about the paper ""Stochastic Gradient Monomial Gamma Sampler"".

Title: Stochastic Gradient Monomial Gamma Sampler

Abstract: Recent advances in stochastic gradient techniques have made it possible to
estimate posterior distributions from large datasets via Markov Chain Monte
Carlo (MCMC). However, when the target posterior is multimodal, mixing
performance is often poor. This results in inadequate exploration of the
posterior distribution. A framework is proposed to improve the sampling
efficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A
generalized kinetic function is leveraged, delivering superior stationary
mixing, especially for multimodal distributions. Techniques are also discussed
to overcome the practical issues introduced by this generalization. It is shown
that the proposed approach is better at exploring complex multimodal posterior
distributions, as demonstrated on multiple applications and in comparison with
other stochastic gradient MCMC methods.

The proposed approach is better suited for exploring complex multimodal posteriors because a generalized kinetic function is leveraged, which delivers superior stationary mixing, especially for multimodal distributions. This helps to improve the sampling efficiency of stochastic gradient MCMC and enables better exploration of complex multimodal posterior distributions."
What statistical properties of Quick Shift are discussed in the paper 'On the Consistency of Quick Shift'?,28ce66d4,"[{'abstract': '  Quick Shift is a popular mode-seeking and clustering algorithm. We present\nfinite sample statistical consistency guarantees for Quick Shift on mode and\ncluster recovery under mild distributional assumptions. We then apply our\nresults to construct a consistent modal regression algorithm.\n', 'title': 'On the Consistency of Quick Shift'}]","Title: On the Consistency of Quick Shift

Abstract: Quick Shift is a popular mode-seeking and clustering algorithm. We present finite sample statistical consistency guarantees for Quick Shift on mode and cluster recovery under mild distributional assumptions. We then apply our results to construct a consistent modal regression algorithm.

The statistical properties of Quick Shift discussed in the paper are its finite sample statistical consistency guarantees for mode and cluster recovery."
What are the key benefits of DeepFense in terms of performance and deployment?,ffdaedd9,"[{'abstract': '  Recent advances in adversarial Deep Learning (DL) have opened up a largely\nunexplored surface for malicious attacks jeopardizing the integrity of\nautonomous DL systems. With the wide-spread usage of DL in critical and\ntime-sensitive applications, including unmanned vehicles, drones, and video\nsurveillance systems, online detection of malicious inputs is of utmost\nimportance. We propose DeepFense, the first end-to-end automated framework that\nsimultaneously enables efficient and safe execution of DL models. DeepFense\nformalizes the goal of thwarting adversarial attacks as an optimization problem\nthat minimizes the rarely observed regions in the latent feature space spanned\nby a DL network. To solve the aforementioned minimization problem, a set of\ncomplementary but disjoint modular redundancies are trained to validate the\nlegitimacy of the input samples in parallel with the victim DL model. DeepFense\nleverages hardware/software/algorithm co-design and customized acceleration to\nachieve just-in-time performance in resource-constrained settings. The proposed\ncountermeasure is unsupervised, meaning that no adversarial sample is leveraged\nto train modular redundancies. We further provide an accompanying API to reduce\nthe non-recurring engineering cost and ensure automated adaptation to various\nplatforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders\nof magnitude performance improvement while enabling online adversarial sample\ndetection.\n', 'title': 'DeepFense: Online Accelerated Defense Against Adversarial Deep Learning'}]","Title: DeepFense: Online Accelerated Defense Against Adversarial Deep Learning

Abstract: Recent advances in adversarial Deep Learning (DL) have opened up a largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. With the wide-spread usage of DL in critical and time-sensitive applications, including unmanned vehicles, drones, and video surveillance systems, online detection of malicious inputs is of utmost importance. We propose DeepFense, the first end-to-end automated framework that simultaneously enables efficient and safe execution of DL models. DeepFense formalizes the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples in parallel with the victim DL model. DeepFense leverages hardware/software/algorithm co-design and customized acceleration to achieve just-in-time performance in resource-constrained settings. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. We further provide an accompanying API to reduce the non-recurring engineering cost and ensure automated adaptation to various platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders of magnitude performance improvement while enabling online adversarial sample detection.

The key benefits of DeepFense are:

* Up to two orders of magnitude performance improvement
* Enables online adversarial sample detection
* Provides just-in-time performance in resource-constrained settings
* Offers automated adaptation to various platforms through an accompanying API."
In what temperature and magnetic field regions do scaling laws hold for three-body collisions?,b1311b23,"[{'abstract': '  We experimentally confirmed the threshold behavior and scattering length\nscaling law of the three-body loss coefficients in an ultracold spin-polarized\ngas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the\nthree-body loss coefficients as functions of temperature and scattering volume,\nand found that the threshold law and the scattering length scaling law hold in\nlimited temperature and magnetic field regions. We also found that the\nbreakdown of the scaling laws is due to the emergence of the effective-range\nterm. This work is an important first step toward full understanding of the\nloss of identical fermions with $p$-wave interactions.\n', 'title': 'Scaling Law for Three-body Collisions in Identical Fermions with $p$-wave Interactions'}]","Title: Scaling Law for Three-body Collisions in Identical Fermions with $p$-wave Interactions

Abstract: We experimentally confirmed the threshold behavior and scattering length scaling law of the three-body loss coefficients in an ultracold spin-polarized gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the three-body loss coefficients as functions of temperature and scattering volume, and found that the threshold law and the scattering length scaling law hold in limited temperature and magnetic field regions. We also found that the breakdown of the scaling laws is due to the emergence of the effective-range term. This work is an important first step toward full understanding of the loss of identical fermions with $p$-wave interactions.

The scaling laws hold in limited temperature and magnetic field regions, but the exact ranges are not specified in the abstract."
What is the primary focus of this paper regarding metric learning in speaker diarization?,b0f33ae5,"[{'abstract': '  In automatic speech processing systems, speaker diarization is a crucial\nfront-end component to separate segments from different speakers. Inspired by\nthe recent success of deep neural networks (DNNs) in semantic inferencing,\ntriplet loss-based architectures have been successfully used for this problem.\nHowever, existing work utilizes conventional i-vectors as the input\nrepresentation and builds simple fully connected networks for metric learning,\nthus not fully leveraging the modeling power of DNN architectures. This paper\ninvestigates the importance of learning effective representations from the\nsequences directly in metric learning pipelines for speaker diarization. More\nspecifically, we propose to employ attention models to learn embeddings and the\nmetric jointly in an end-to-end fashion. Experiments are conducted on the\nCALLHOME conversational speech corpus. The diarization results demonstrate\nthat, besides providing a unified model, the proposed approach achieves\nimproved performance when compared against existing approaches.\n', 'title': 'Triplet Network with Attention for Speaker Diarization'}]","Title: Triplet Network with Attention for Speaker Diarization

Abstract: In automatic speech processing systems, speaker diarization is a crucial front-end component to separate segments from different speakers. Inspired by the recent success of deep neural networks (DNNs) in semantic inferencing, triplet loss-based architectures have been successfully used for this problem. However, existing work utilizes conventional i-vectors as the input representation and builds simple fully connected networks for metric learning, thus not fully leveraging the modeling power of DNN architectures. This paper investigates the importance of learning effective representations from the sequences directly in metric learning pipelines for speaker diarization. More specifically, we propose to employ attention models to learn embeddings and the metric jointly in an end-to-end fashion. Experiments are conducted on the CALLHOME conversational speech corpus. The diarization results demonstrate that, besides providing a unified model, the proposed approach achieves improved performance when compared against existing approaches."
What are factor models used for in analyzing the daily returns of cryptocurrencies?,9cbf9db9,"[{'abstract': '  We propose factor models for the cross-section of daily cryptoasset returns\nand provide source code for data downloads, computing risk factors and\nbacktesting them out-of-sample. In ""cryptoassets"" we include all\ncryptocurrencies and a host of various other digital assets (coins and tokens)\nfor which exchange market data is available. Based on our empirical analysis,\nwe identify the leading factor that appears to strongly contribute into daily\ncryptoasset returns. Our results suggest that cross-sectional statistical\narbitrage trading may be possible for cryptoassets subject to efficient\nexecutions and shorting.\n', 'title': 'Cryptoasset Factor Models'}]","Title: Cryptoasset Factor Models

Abstract: We propose factor models for the cross-section of daily cryptoasset returns and provide source code for data downloads, computing risk factors and backtesting them out-of-sample. In ""cryptoassets"" we include all cryptocurrencies and a host of various other digital assets (coins and tokens) for which exchange market data is available. Based on our empirical analysis, we identify the leading factor that appears to strongly contribute into daily cryptoasset returns. Our results suggest that cross-sectional statistical arbitrage trading may be possible for cryptoassets subject to efficient executions and shorting."
How can a neural network using a cyclic convolutional layer be used for object detection?,692d6bcc,"[{'abstract': '  Rotation invariance and translation invariance have great values in image\nrecognition tasks. In this paper, we bring a new architecture in convolutional\nneural network (CNN) named cyclic convolutional layer to achieve rotation\ninvariance in 2-D symbol recognition. We can also get the position and\norientation of the 2-D symbol by the network to achieve detection purpose for\nmultiple non-overlap target. Last but not least, this architecture can achieve\none-shot learning in some cases using those invariance.\n', 'title': 'Rotation Invariance Neural Network'}]","I think I can help you with that!

Title: Rotation Invariance Neural Network

Abstract: Rotation invariance and translation invariance have great values in image recognition tasks. In this paper, we bring a new architecture in convolutional neural network (CNN) named cyclic convolutional layer to achieve rotation invariance in 2-D symbol recognition. We can also get the position and orientation of the 2-D symbol by the network to achieve detection purpose for multiple non-overlap target. Last but not least, this architecture can achieve one-shot learning in some cases using those invariance.

This paper suggests that a neural network using a cyclic convolutional layer can be used for object detection, specifically for 2-D symbol recognition, and can even achieve one-shot learning in some cases."
"What specific adjustments were made to the ion source timing, discharge current, gas pressure, and cesium delivery system in response to the 2012 upgrade?",5ae3c478,"[{'abstract': '  This paper will detail changes in the operational paradigm of the Fermi\nNational Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to\nupgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions\nfor High Energy Physics (HEP) experiments were extracted at ~18 keV vertically\ndownward into a 90 degree bending magnet and accelerated through a\nCockcroft-Walton accelerating column to 750 keV. Following the upgrade in the\nfall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35\nkeV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This\nchange in extraction energy as well as the orientation of the ion source\nrequired not only a redesign of the ion source, but an updated understanding of\nits operation at these new values. Discussed in detail are the changes to the\nion source timing, arc discharge current, hydrogen gas pressure, and cesium\ndelivery system that were needed to maintain consistent operation at >99%\nuptime for HEP, with an increased ion source lifetime of over 9 months.\n', 'title': 'Recent Operation of the FNAL Magnetron $H^{-}$ Ion Source'}]","Title: Recent Operation of the FNAL Magnetron $H^{-}$ Ion Source

Abstract: This paper will detail changes in the operational paradigm of the Fermi National Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to upgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions for High Energy Physics (HEP) experiments were extracted at ~18 keV vertically downward into a 90 degree bending magnet and accelerated through a Cockcroft-Walton accelerating column to 750 keV. Following the upgrade in the fall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35 keV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This change in extraction energy as well as the orientation of the ion source required not only a redesign of the ion source, but an updated understanding of its operation at these new values. Discussed in detail are the changes to the ion source timing, arc discharge current, hydrogen gas pressure, and cesium delivery system that were needed to maintain consistent operation at >99% uptime for HEP, with an increased ion source lifetime of over 9 months.

The adjustments made to the ion source timing, discharge current, gas pressure, and cesium delivery system are not explicitly stated in the abstract. However, it is mentioned that changes were made to these parameters in response to the 2012 upgrade, and that these changes were necessary to maintain consistent operation and increase the ion source lifetime."
What is the upper bound for list chromatic number of a K_r free graph?,38f89bf4,"[{'abstract': '  We prove that every triangle-free graph with maximum degree $\\Delta$ has list\nchromatic number at most $(1+o(1))\\frac{\\Delta}{\\ln \\Delta}$. This matches the\nbest-known bound for graphs of girth at least 5. We also provide a new proof\nthat for any $r\\geq 4$ every $K_r$-free graph has list-chromatic number at most\n$200r\\frac{\\Delta\\ln\\ln\\Delta}{\\ln\\Delta}$.\n', 'title': 'The list chromatic number of graphs with small clique number'}]","Title: The list chromatic number of graphs with small clique number

Abstract: We prove that every triangle-free graph with maximum degree Δ has list chromatic number at most (1+o(1))Δ/ln Δ. This matches the best-known bound for graphs of girth at least 5. We also provide a new proof that for any r≥4 every Kr-free graph has list-chromatic number at most 200rΔ ln ln Δ / ln Δ.

The upper bound for the list chromatic number of a Kr-free graph is 200rΔ ln ln Δ / ln Δ."
How does past experience play a role in the decision-making process of investors and project managers?,1401f7e7,"[{'abstract': '  All people have to make risky decisions in everyday life. And we do not know\nhow true they are. But is it possible to mathematically assess the correctness\nof our choice? This article discusses the model of decision making under risk\non the example of project management. This is a game with two players, one of\nwhich is Investor, and the other is the Project Manager. Each player makes a\nrisky decision for himself, based on his past experience. With the help of a\nmathematical model, the players form a level of confidence, depending on who\nthe player accepts the strategy or does not accept. The project manager\nassesses the costs and compares them with the level of confidence. An investor\nevaluates past results. Also visit the case where the strategy of the player\naccepts the part.\n', 'title': 'Psychological model of the investor and manager behavior in risk'}]","Title: Psychological model of the investor and manager behavior in risk

Abstract:  All people have to make risky decisions in everyday life. And we do not know
how true they are. But is it possible to mathematically assess the correctness
of our choice? This article discusses the model of decision making under risk
on the example of project management. This is a game with two players, one of
which is Investor, and the other is the Project Manager. Each player makes a
risky decision for himself, based on his past experience. With the help of a
mathematical model, the players form a level of confidence, depending on who
the player accepts the strategy or does not accept. The project manager
assesses the costs and compares them with the level of confidence. An investor
evaluates past results. Also visit the case where the strategy of the player
accepts the part."
What is the benefit of using this framework for multiple instance learning tasks compared to previous approaches?,21d71280,"[{'abstract': '  We describe a novel weakly supervised deep learning framework that combines\nboth the discriminative and generative models to learn meaningful\nrepresentation in the multiple instance learning (MIL) setting. MIL is a weakly\nsupervised learning problem where labels are associated with groups of\ninstances (referred as bags) instead of individual instances. To address the\nessential challenge in MIL problems raised from the uncertainty of positive\ninstances label, we use a discriminative model regularized by variational\nautoencoders (VAEs) to maximize the differences between latent representations\nof all instances and negative instances. As a result, the hidden layer of the\nvariational autoencoder learns meaningful representation. This representation\ncan effectively be used for MIL problems as illustrated by better performance\non the standard benchmark datasets comparing to the state-of-the-art\napproaches. More importantly, unlike most related studies, the proposed\nframework can be easily scaled to large dataset problems, as illustrated by the\naudio event detection and segmentation task. Visualization also confirms the\neffectiveness of the latent representation in discriminating positive and\nnegative classes.\n', 'title': 'Deep Multiple Instance Feature Learning via Variational Autoencoder'}]","Title: Deep Multiple Instance Feature Learning via Variational Autoencoder

Abstract:   We describe a novel weakly supervised deep learning framework that combines
both the discriminative and generative models to learn meaningful
representation in the multiple instance learning (MIL) setting. MIL is a weakly
supervised learning problem where labels are associated with groups of
instances (referred as bags) instead of individual instances. To address the
essential challenge in MIL problems raised from the uncertainty of positive
instances label, we use a discriminative model regularized by variational
autoencoders (VAEs) to maximize the differences between latent representations
of all instances and negative instances. As a result, the hidden layer of the
variational autoencoder learns meaningful representation. This representation
can effectively be used for MIL problems as illustrated by better performance
on the standard benchmark datasets comparing to the state-of-the-art
approaches. More importantly, unlike most related studies, the proposed
framework can be easily scaled to large dataset problems, as illustrated by the
audio event detection and segmentation task. Visualization also confirms the
effectiveness of the latent representation in discriminating positive and
negative classes.

The benefit of using this framework for multiple instance learning tasks is that it can be easily scaled to large dataset problems, unlike most related studies."
What are the potential applications of local hybrid methods in numerical weather prediction?,f1255665,"[{'abstract': '  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)\nare a key component of probabilistic weather forecasting. They represent the\nuncertainty in the initial conditions by an ensemble which incorporates\ninformation coming from the physical model with the latest observations.\nHigh-resolution numerical weather prediction models ran at operational centers\nare able to resolve non-linear and non-Gaussian physical phenomena such as\nconvection. There is therefore a growing need to develop ensemble assimilation\nalgorithms able to deal with non-Gaussianity while staying computationally\nfeasible. In the present paper we address some of these needs by proposing a\nnew hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully\nformulated in ensemble space and uses a deterministic scheme such that it has\nthe ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a\nlimiting case. A new criterion for choosing the proportion of particle filter\nand ETKF update is also proposed. The new algorithm is implemented in the COSMO\nframework and numerical experiments in a quasi-operational convective-scale\nsetup are conducted. The results show the feasibility of the new algorithm in\npractice and indicate a strong potential for such local hybrid methods, in\nparticular for forecasting non-Gaussian variables such as wind and hourly\nprecipitation.\n', 'title': 'A local ensemble transform Kalman particle filter for convective scale data assimilation'}]","Title: A local ensemble transform Kalman particle filter for convective scale data assimilation

Abstract:   Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)
are a key component of probabilistic weather forecasting. They represent the
uncertainty in the initial conditions by an ensemble which incorporates
information coming from the physical model with the latest observations.
High-resolution numerical weather prediction models ran at operational centers
are able to resolve non-linear and non-Gaussian physical phenomena such as
convection. There is therefore a growing need to develop ensemble assimilation
algorithms able to deal with non-Gaussianity while staying computationally
feasible. In the present paper we address some of these needs by proposing a
new hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully
formulated in ensemble space and uses a deterministic scheme such that it has
the ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a
limiting case. A new criterion for choosing the proportion of particle filter
and ETKF update is also proposed. The new algorithm is implemented in the COSMO
framework and numerical experiments in a quasi-operational convective-scale
setup are conducted. The results show the feasibility of the new algorithm in
practice and indicate a strong potential for such local hybrid methods, in
particular for forecasting non-Gaussian variables such as wind and hourly
precipitation."
What factors explain the experimental observations of Klinkenberg's apparent gas permeability?,1affc023,"[{'abstract': ""  The apparent gas permeability of the porous medium is an important parameter\nin the prediction of unconventional gas production, which was first\ninvestigated systematically by Klinkenberg in 1941 and found to increase with\nthe reciprocal mean gas pressure (or equivalently, the Knudsen number).\nAlthough the underlying rarefaction effects are well-known, the reason that the\ncorrection factor in Klinkenberg's famous equation decreases when the Knudsen\nnumber increases has not been fully understood. Most of the studies idealize\nthe porous medium as a bundle of straight cylindrical tubes, however, according\nto the gas kinetic theory, this only results in an increase of the correction\nfactor with the Knudsen number, which clearly contradicts Klinkenberg's\nexperimental observations. Here, by solving the Bhatnagar-Gross-Krook equation\nin simplified (but not simple) porous media, we identify, for the first time,\ntwo key factors that can explain Klinkenberg's experimental results: the\ntortuous flow path and the non-unitary tangential momentum accommodation\ncoefficient for the gas-surface interaction. Moreover, we find that\nKlinkenberg's results can only be observed when the ratio between the apparent\nand intrinsic permeabilities is $\\lesssim30$; at large ratios (or Knudsen\nnumbers) the correction factor increases with the Knudsen number. Our numerical\nresults could also serve as benchmarking cases to assess the accuracy of\nmacroscopic models and/or numerical schemes for the modeling/simulation of\nrarefied gas flows in complex geometries over a wide range of gas rarefaction.\n"", 'title': 'On the apparent permeability of porous media in rarefied gas flows'}]","Title: On the apparent permeability of porous media in rarefied gas flows

Abstract: The apparent gas permeability of the porous medium is an important parameter
in the prediction of unconventional gas production, which was first
investigated systematically by Klinkenberg in 1941 and found to increase with
the reciprocal mean gas pressure (or equivalently, the Knudsen number).
Although the underlying rarefaction effects are well-known, the reason that the
correction factor in Klinkenberg's famous equation decreases when the Knudsen
number increases has not been fully understood. Most of the studies idealize
the porous medium as a bundle of straight cylindrical tubes, however, according
to the gas kinetic theory, this only results in an increase of the correction
factor with the Knudsen number, which clearly contradicts Klinkenberg's
experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
in simplified (but not simple) porous media, we identify, for the first time,
two key factors that can explain Klinkenberg's experimental results: the
tortuous flow path and the non-unitary tangential momentum accommodation
coefficient for the gas-surface interaction. Moreover, we find that
Klinkenberg's results can only be observed when the ratio between the apparent
and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
numbers) the correction factor increases with the Knudsen number. Our numerical
results could also serve as benchmarking cases to assess the accuracy of
macroscopic models and/or numerical schemes for the modeling/simulation of
rarefied gas flows in complex geometries over a wide range of gas rarefaction.

The two factors that explain Klinkenberg's experimental observations are:

1. Tortuous flow path
2. Non-unitary tangential momentum accommodation coefficient for the gas-surface interaction."
What is the role of the linearized nonlocal mean curvature operator in proving the flatness of nonlocal minimal cones?,264dc421,"[{'abstract': '  We show that nonlocal minimal cones which are non-singular subgraphs outside\nthe origin are necessarily halfspaces.\nThe proof is based on classical ideas of~\\cite{DG1} and on the computation of\nthe linearized nonlocal mean curvature operator, which is proved to satisfy a\nsuitable maximum principle.\nWith this, we obtain new, and somehow simpler, proofs of the Bernstein-type\nresults for nonlocal minimal surfaces which have been recently established\nin~\\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type\nresult which classifies Lipschitz nonlocal minimal subgraphs outside a ball.\n', 'title': 'Flatness results for nonlocal minimal cones and subgraphs'}]","Title: Flatness results for nonlocal minimal cones and subgraphs

Abstract: We show that nonlocal minimal cones which are non-singular subgraphs outside the origin are necessarily halfspaces. The proof is based on classical ideas of~\cite{DG1} and on the computation of the linearized nonlocal mean curvature operator, which is proved to satisfy a suitable maximum principle."
How can the classification of pro-$p$ Poincaré duality pairs be used to construct a new mathematical object?,2ec884d4,"[{'abstract': '  We classify pro-$p$ Poincaré duality pairs in dimension two. We then use\nthis classification to build a pro-$p$ analogue of the curve complex and\nestablish its basic properties. We conclude with some statements concerning\nseparability properties of the mapping class group.\n', 'title': 'Classification of pro-$p$ PD$^2$ pairs and the pro-$p$ curve complex'}]","Title: Classification of pro-$p$ PD$^2$ pairs and the pro-$p$ curve complex

Abstract: We classify pro-$p$ Poincaré duality pairs in dimension two. We then use this classification to build a pro-$p$ analogue of the curve complex and establish its basic properties. We conclude with some statements concerning separability properties of the mapping class group.

This paper describes how the classification of pro-$p$ Poincaré duality pairs can be used to construct a new mathematical object, specifically a pro-$p$ analogue of the curve complex."
Study the specific case of a family of holomorphic functions within the unit disk where all zeros are on a ray and all 1-points are on a distinct ray.,fd28ed35,"[{'abstract': '  Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let\n${\\mathcal F}$ be the family of all functions holomorphic in the unit disk\n${\\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on\n$L_1$. It is shown that ${\\mathcal F}$ is normal in ${\\mathbb\nD}\\backslash\\{0\\}$. The case where $L_0$ is the positive real axis and $L_1$ is\nthe negative real axis is studied in more detail.\n', 'title': 'Radially distributed values and normal families'}]","I think I can help you with that!

Title: Radially distributed values and normal families

Abstract: Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let ${\mathcal F}$ be the family of all functions holomorphic in the unit disk ${\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on $L_1$. It is shown that ${\mathcal F}$ is normal in ${\mathbb D}\backslash\{0\}$. The case where $L_0$ is the positive real axis and $L_1$ is the negative real axis is studied in more detail."
What type of feedback mechanism is used to ensure the diver is in control of the AUV throughout the operation?,b6a0c3c5,"[{'abstract': '  We propose a robust gesture-based communication pipeline for divers to\ninstruct an Autonomous Underwater Vehicle (AUV) to assist them in performing\nhigh-risk tasks and helping in case of emergency. A gesture communication\nlanguage (CADDIAN) is developed, based on consolidated and standardized diver\ngestures, including an alphabet, syntax and semantics, ensuring a logical\nconsistency. A hierarchical classification approach is introduced for hand\ngesture recognition based on stereo imagery and multi-descriptor aggregation to\nspecifically cope with underwater image artifacts, e.g. light backscatter or\ncolor attenuation. Once the classification task is finished, a syntax check is\nperformed to filter out invalid command sequences sent by the diver or\ngenerated by errors in the classifier. Throughout this process, the diver\nreceives constant feedback from an underwater tablet to acknowledge or abort\nthe mission at any time. The objective is to prevent the AUV from executing\nunnecessary, infeasible or potentially harmful motions. Experimental results\nunder different environmental conditions in archaeological exploration and\nbridge inspection applications show that the system performs well in the field.\n', 'title': 'Robust Gesture-Based Communication for Underwater Human-Robot Interaction in the context of Search and Rescue Diver Missions'}]","Title: Robust Gesture-Based Communication for Underwater Human-Robot Interaction in the context of Search and Rescue Diver Missions

Abstract: We propose a robust gesture-based communication pipeline for divers to instruct an Autonomous Underwater Vehicle (AUV) to assist them in performing high-risk tasks and helping in case of emergency. A gesture communication language (CADDIAN) is developed, based on consolidated and standardized diver gestures, including an alphabet, syntax and semantics, ensuring a logical consistency. A hierarchical classification approach is introduced for hand gesture recognition based on stereo imagery and multi-descriptor aggregation to specifically cope with underwater image artifacts, e.g. light backscatter or color attenuation. Once the classification task is finished, a syntax check is performed to filter out invalid command sequences sent by the diver or generated by errors in the classifier. Throughout this process, the diver receives constant feedback from an underwater tablet to acknowledge or abort the mission at any time. The objective is to prevent the AUV from executing unnecessary, infeasible or potentially harmful motions. Experimental results under different environmental conditions in archaeological exploration and bridge inspection applications show that the system performs well in the field.

The feedback mechanism used to ensure the diver is in control of the AUV throughout the operation is the constant feedback from an underwater tablet, which allows the diver to acknowledge or abort the mission at any time."
What type of outcome data is examined in the context of sequential learning and posterior dominance?,5ffee714,"[{'abstract': '  We consider the problem of sequential learning from categorical observations\nbounded in [0,1]. We establish an ordering between the Dirichlet posterior over\ncategorical outcomes and a Gaussian posterior under observations with N(0,1)\nnoise. We establish that, conditioned upon identical data with at least two\nobservations, the posterior mean of the categorical distribution will always\nsecond-order stochastically dominate the posterior mean of the Gaussian\ndistribution. These results provide a useful tool for the analysis of\nsequential learning under categorical outcomes.\n', 'title': 'Gaussian-Dirichlet Posterior Dominance in Sequential Learning'}]","Title: Gaussian-Dirichlet Posterior Dominance in Sequential Learning

Abstract: We consider the problem of sequential learning from categorical observations bounded in [0,1]. We establish an ordering between the Dirichlet posterior over categorical outcomes and a Gaussian posterior under observations with N(0,1) noise. We establish that, conditioned upon identical data with at least two observations, the posterior mean of the categorical distribution will always second-order stochastically dominate the posterior mean of the Gaussian distribution. These results provide a useful tool for the analysis of sequential learning under categorical outcomes."
What is the impact of prior probabilities on uncertainty quantification using small datasets?,1705deba,"[{'abstract': '  This paper outlines a methodology for Bayesian multimodel uncertainty\nquantification (UQ) and propagation and presents an investigation into the\neffect of prior probabilities on the resulting uncertainties. The UQ\nmethodology is adapted from the information-theoretic method previously\npresented by the authors (Zhang and Shields, 2018) to a fully Bayesian\nconstruction that enables greater flexibility in quantifying uncertainty in\nprobability model form. Being Bayesian in nature and rooted in UQ from small\ndatasets, prior probabilities in both probability model form and model\nparameters are shown to have a significant impact on quantified uncertainties\nand, consequently, on the uncertainties propagated through a physics-based\nmodel. These effects are specifically investigated for a simplified plate\nbuckling problem with uncertainties in material properties derived from a small\nnumber of experiments using noninformative priors and priors derived from past\nstudies of varying appropriateness. It is illustrated that prior probabilities\ncan have a significant impact on multimodel UQ for small datasets and\ninappropriate (but seemingly reasonable) priors may even have lingering effects\nthat bias probabilities even for large datasets. When applied to uncertainty\npropagation, this may result in probability bounds on response quantities that\ndo not include the true probabilities.\n', 'title': 'The effect of prior probabilities on quantification and propagation of imprecise probabilities resulting from small datasets'}]","I think I can help you with that!

Title: The effect of prior probabilities on quantification and propagation of imprecise probabilities resulting from small datasets

Abstract: This paper outlines a methodology for Bayesian multimodel uncertainty quantification (UQ) and propagation and presents an investigation into the effect of prior probabilities on the resulting uncertainties. The UQ methodology is adapted from the information-theoretic method previously presented by the authors (Zhang and Shields, 2018) to a fully Bayesian construction that enables greater flexibility in quantifying uncertainty in probability model form. Being Bayesian in nature and rooted in UQ from small datasets, prior probabilities in both probability model form and model parameters are shown to have a significant impact on quantified uncertainties and, consequently, on the uncertainties propagated through a physics-based model. These effects are specifically investigated for a simplified plate buckling problem with uncertainties in material properties derived from a small number of experiments using noninformative priors and priors derived from past studies of varying appropriateness. It is illustrated that prior probabilities can have a significant impact on multimodel UQ for small datasets and inappropriate (but seemingly reasonable) priors may even have lingering effects that bias probabilities even for large datasets. When applied to uncertainty propagation, this may result in probability bounds on response quantities that do not include the true probabilities."
What are the benefits of using RGBD images for 3D human pose estimation compared to color or depth-only methods?,2c4e66c7,"[{'abstract': '  We propose an approach to estimate 3D human pose in real world units from a\nsingle RGBD image and show that it exceeds performance of monocular 3D pose\nestimation approaches from color as well as pose estimation exclusively from\ndepth. Our approach builds on robust human keypoint detectors for color images\nand incorporates depth for lifting into 3D. We combine the system with our\nlearning from demonstration framework to instruct a service robot without the\nneed of markers. Experiments in real world settings demonstrate that our\napproach enables a PR2 robot to imitate manipulation actions observed from a\nhuman teacher.\n', 'title': '3D Human Pose Estimation in RGBD Images for Robotic Task Learning'}]","Here is the answer:

Title: 3D Human Pose Estimation in RGBD Images for Robotic Task Learning

Abstract: We propose an approach to estimate 3D human pose in real world units from a single RGBD image and show that it exceeds performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth. Our approach builds on robust human keypoint detectors for color images and incorporates depth for lifting into 3D. We combine the system with our learning from demonstration framework to instruct a service robot without the need of markers. Experiments in real world settings demonstrate that our approach enables a PR2 robot to imitate manipulation actions observed from a human teacher.

The benefits of using RGBD images for 3D human pose estimation are that it exceeds the performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth."
What are the limitations of neural network based generative models with discriminative components for semi-supervised learning?,78357dd1,"[{'abstract': ""  Neural network based generative models with discriminative components are a\npowerful approach for semi-supervised learning. However, these techniques a)\ncannot account for model uncertainty in the estimation of the model's\ndiscriminative component and b) lack flexibility to capture complex stochastic\npatterns in the label generation process. To avoid these problems, we first\npropose to use a discriminative component with stochastic inputs for increased\nnoise flexibility. We show how an efficient Gibbs sampling procedure can\nmarginalize the stochastic inputs when inferring missing labels in this model.\nFollowing this, we extend the discriminative component to be fully Bayesian and\nproduce estimates of uncertainty in its parameter values. This opens the door\nfor semi-supervised Bayesian active learning.\n"", 'title': 'Bayesian Semisupervised Learning with Deep Generative Models'}]","Title: Bayesian Semisupervised Learning with Deep Generative Models

Abstract: Neural network based generative models with discriminative components are a powerful approach for semi-supervised learning. However, these techniques a) cannot account for model uncertainty in the estimation of the model's discriminative component and b) lack flexibility to capture complex stochastic patterns in the label generation process."
How do individuals who choose unconventional options influence the popularity of products in a social network?,ecf1c3c3,"[{'abstract': '  The spread of opinions, memes, diseases, and ""alternative facts"" in a\npopulation depends both on the details of the spreading process and on the\nstructure of the social and communication networks on which they spread. In\nthis paper, we explore how \\textit{anti-establishment} nodes (e.g.,\n\\textit{hipsters}) influence the spreading dynamics of two competing products.\nWe consider a model in which spreading follows a deterministic rule for\nupdating node states (which describe which product has been adopted) in which\nan adjustable fraction $p_{\\rm Hip}$ of the nodes in a network are hipsters,\nwho choose to adopt the product that they believe is the less popular of the\ntwo. The remaining nodes are conformists, who choose which product to adopt by\nconsidering which products their immediate neighbors have adopted. We simulate\nour model on both synthetic and real networks, and we show that the hipsters\nhave a major effect on the final fraction of people who adopt each product:\neven when only one of the two products exists at the beginning of the\nsimulations, a very small fraction of hipsters in a network can still cause the\nother product to eventually become the more popular one. To account for this\nbehavior, we construct an approximation for the steady-state adoption fraction\non $k$-regular trees in the limit of few hipsters. Additionally, our\nsimulations demonstrate that a time delay $\\tau$ in the knowledge of the\nproduct distribution in a population, as compared to immediate knowledge of\nproduct adoption among nearest neighbors, can have a large effect on the final\ndistribution of product adoptions. Our simple model and analysis may help shed\nlight on the road to success for anti-establishment choices in elections, as\nsuch success can arise rather generically in our model from a small number of\nanti-establishment individuals and ordinary processes of social influence on\nnormal individuals.\n', 'title': 'Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority'}]","Title: Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority

Abstract: The spread of opinions, memes, diseases, and ""alternative facts"" in a population depends both on the details of the spreading process and on the structure of the social and communication networks on which they spread. In this paper, we explore how anti-establishment nodes (e.g., hipsters) influence the spreading dynamics of two competing products. We consider a model in which spreading follows a deterministic rule for updating node states (which describe which product has been adopted) in which an adjustable fraction p_Hip of the nodes in a network are hipsters, who choose to adopt the product that they believe is the less popular of the two. The remaining nodes are conformists, who choose which product to adopt by considering which products their immediate neighbors have adopted. We simulate our model on both synthetic and real networks, and we show that the hipsters have a major effect on the final fraction of people who adopt each product: even when only one of the two products exists at the beginning of the simulations, a very small fraction of hipsters in a network can still cause the other product to eventually become the more popular one. To account for this behavior, we construct an approximation for the steady-state adoption fraction on k-regular trees in the limit of few hipsters. Additionally, our simulations demonstrate that a time delay τ in the knowledge of the product distribution in a population, as compared to immediate knowledge of product adoption among nearest neighbors, can have a large effect on the final distribution of product adoptions. Our simple model and analysis may help shed light on the road to success for anti-establishment choices in elections, as such success can arise rather generically in our model from a small number of anti-establishment individuals and ordinary processes of social influence on normal individuals."
How does long-term analysis of online communication networks differ from short-term analysis?,c227e9a8,"[{'abstract': '  Social media has changed the ways of communication, where everyone is\nequipped with the power to express their opinions to others in online\ndiscussion platforms. Previously, a number of stud- ies have been presented to\nidentify opinion leaders in online discussion networks. Feng (""Are you\nconnected? Evaluating information cascade in online discussion about the\n#RaceTogether campaign"", Computers in Human Behavior, 2016) identified five\ntypes of central users and their communication patterns in an online\ncommunication network of a limited time span. However, to trace the change in\ncommunication pattern, a long-term analysis is required. In this study, we\ncritically analyzed framework presented by Feng based on five types of central\nusers in online communication network and their communication pattern in a\nlong-term manner. We take another case study presented by Udnor et al.\n(""Determining social media impact on the politics of developing countries using\nsocial network analytics"", Program, 2016) to further understand the dynamics as\nwell as to perform validation . Results indicate that there may not exist all\nof these central users in an online communication network in a long-term\nmanner. Furthermore, we discuss the changing positions of opinion leaders and\ntheir power to keep isolates interested in an online discussion network.\n', 'title': 'Evaluating Roles of Central Users in Online Communication Networks: A Case Study of #PanamaLeaks'}]","Title: Evaluating Roles of Central Users in Online Communication Networks: A Case Study of #PanamaLeaks

Abstract: Social media has changed the ways of communication, where everyone is equipped with the power to express their opinions to others in online discussion platforms. Previously, a number of studies have been presented to identify opinion leaders in online discussion networks. Feng (""Are you connected? Evaluating information cascade in online discussion about the #RaceTogether campaign"", Computers in Human Behavior, 2016) identified five types of central users and their communication patterns in an online communication network of a limited time span. However, to trace the change in communication pattern, a long-term analysis is required. In this study, we critically analyzed framework presented by Feng based on five types of central users in online communication network and their communication pattern in a long-term manner. We take another case study presented by Udnor et al. (""Determining social media impact on the politics of developing countries using social network analytics"", Program, 2016) to further understand the dynamics as well as to perform validation. Results indicate that there may not exist all of these central users in an online communication network in a long-term manner. Furthermore, we discuss the changing positions of opinion leaders and their power to keep isolates interested in an online discussion network.

In this study, the researcher mentions that a long-term analysis of online communication networks is required to trace the change in communication pattern, which differs from the limited time span analysis conducted by Feng (2016)."
What was the focus of the study regarding political parties and Twitter use in the 2017 Japanese election?,68a28b02,"[{'abstract': ""  In modern election campaigns, political parties utilize social media to\nadvertise their policies and candidates and to communicate to the electorate.\nIn Japan's latest general election in 2017, the 48th general election for the\nLower House, social media, especially Twitter, was actively used. In this\npaper, we analyze the users who retweeted tweets of political parties on\nTwitter during the election. Our aim is to clarify what kinds of users are\ndiffusing (retweeting) tweets of political parties. The results indicate that\nthe characteristics of retweeters of the largest ruling party (Liberal\nDemocratic Party of Japan) and the largest opposition party (The Constitutional\nDemocratic Party of Japan) were similar, even though the retweeters did not\noverlap each other. We also found that a particular opposition party (Japanese\nCommunist Party) had quite different characteristics from other political\nparties.\n"", 'title': ""Analysis of Political Party Twitter Accounts' Retweeters During Japan's 2017 Election""}]","Title: Analysis of Political Party Twitter Accounts' Retweeters During Japan's 2017 Election

Abstract: In modern election campaigns, political parties utilize social media to advertise their policies and candidates and to communicate to the electorate. In Japan's latest general election in 2017, the 48th general election for the Lower House, social media, especially Twitter, was actively used. In this paper, we analyze the users who retweeted tweets of political parties on Twitter during the election. Our aim is to clarify what kinds of users are diffusing (retweeting) tweets of political parties. The results indicate that the characteristics of retweeters of the largest ruling party (Liberal Democratic Party of Japan) and the largest opposition party (The Constitutional Democratic Party of Japan) were similar, even though the retweeters did not overlap each other. We also found that a particular opposition party (Japanese Communist Party) had quite different characteristics from other political parties."
How does the proposed algorithm address the stability of Support Vector Machines with respect to regularization hyper-parameters? ,dc660c88,"[{'abstract': '  This work proposes a new algorithm for training a re-weighted L2 Support\nVector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès\net al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In\nparticular, the margin required for each training vector is set independently,\ndefining a new weighted SVM model. These weights are selected to be binary, and\nthey are automatically adapted during the training of the model, resulting in a\nvariation of the Frank-Wolfe optimization algorithm with essentially the same\ncomputational complexity as the original algorithm. As shown experimentally,\nthis algorithm is computationally cheaper to apply since it requires less\niterations to converge, and it produces models with a sparser representation in\nterms of support vectors and which are more stable with respect to the\nselection of the regularization hyper-parameter.\n', 'title': 'Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers'}]","Title: Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers

Abstract:   This work proposes a new algorithm for training a re-weighted L2 Support Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In particular, the margin required for each training vector is set independently, defining a new weighted SVM model. These weights are selected to be binary, and they are automatically adapted during the training of the model, resulting in a variation of the Frank-Wolfe optimization algorithm with essentially the same computational complexity as the original algorithm. As shown experimentally, this algorithm is computationally cheaper to apply since it requires less iterations to converge, and it produces models with a sparser representation in terms of support vectors and which are more stable with respect to the selection of the regularization hyper-parameter."
Does the presence of randomly placed buildings affect the performance of ultra-dense networks with elevated base stations?,f3caf691,"[{'abstract': '  This paper analyzes the downlink performance of ultra-dense networks with\nelevated base stations (BSs). We consider a general dual-slope pathloss model\nwith distance-dependent probability of line-of-sight (LOS) transmission between\nBSs and receivers. Specifically, we consider the scenario where each link may\nbe obstructed by randomly placed buildings. Using tools from stochastic\ngeometry, we show that both coverage probability and area spectral efficiency\ndecay to zero as the BS density grows large. Interestingly, we show that the BS\nheight alone has a detrimental effect on the system performance even when the\nstandard single-slope pathloss model is adopted.\n', 'title': 'Performance Analysis of Ultra-Dense Networks with Elevated Base Stations'}]","Title: Performance Analysis of Ultra-Dense Networks with Elevated Base Stations

Abstract: This paper analyzes the downlink performance of ultra-dense networks with elevated base stations (BSs). We consider a general dual-slope pathloss model with distance-dependent probability of line-of-sight (LOS) transmission between BSs and receivers. Specifically, we consider the scenario where each link may be obstructed by randomly placed buildings. Using tools from stochastic geometry, we show that both coverage probability and area spectral efficiency decay to zero as the BS density grows large. Interestingly, we show that the BS height alone has a detrimental effect on the system performance even when the standard single-slope pathloss model is adopted."
How can topological data analysis be used to understand traffic congestion?,aadfb609,"[{'abstract': '  This work presents a new method to quantify connectivity in transportation\nnetworks. Inspired by the field of topological data analysis, we propose a\nnovel approach to explore the robustness of road network connectivity in the\npresence of congestion on the roadway. The robustness of the pattern is\nsummarized in a congestion barcode, which can be constructed directly from\ntraffic datasets commonly used for navigation. As an initial demonstration, we\nillustrate the main technique on a publicly available traffic dataset in a\nneighborhood in New York City.\n', 'title': 'Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology'}]","I think I can help you with that!

Title: Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology

Abstract: This work presents a new method to quantify connectivity in transportation networks. Inspired by the field of topological data analysis, we propose a novel approach to explore the robustness of road network connectivity in the presence of congestion on the roadway. The robustness of the pattern is summarized in a congestion barcode, which can be constructed directly from traffic datasets commonly used for navigation. As an initial demonstration, we illustrate the main technique on a publicly available traffic dataset in a neighborhood in New York City.

Let me know if this is what you were looking for!"
"Are generalized derivations, quasi-derivations, and quasi-centroid concepts discussed in relation to BiHom-Lie colour algebras?",283bbf47,"[{'abstract': '  BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with\ntwo commuting multiplicative linear maps. The main purpose of this paper is to\ndefine representations and a cohomology of BiHom-Lie colour algebras and to\nstudy some key constructions and properties.\nMoreover, we discuss $\\alpha^{k}\\beta^l$-generalized derivations,\n$\\alpha^{k}\\beta^l$-quasi-derivations and $\\alpha^{k}\\beta^l$-quasi-centroid.\nWe provide some properties and their relationships with BiHom-Jordan colour\nalgebra.\n', 'title': 'BiHom-Lie colour algebras structures'}]","Title: BiHom-Lie Colour algebra structures

Abstract: BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with two commuting multiplicative linear maps. The main purpose of this paper is to define representations and a cohomology of BiHom-Lie colour algebras and to study some key constructions and properties. Moreover, we discuss $\alpha^{k}\beta^l$-generalized derivations, $\alpha^{k}\beta^l$-quasi-derivations and $\alpha^{k}\beta^l$-quasi-centroid. We provide some properties and their relationships with BiHom-Jordan colour algebra."
What modification was proposed to the functional to improve accuracy at saturation density?,770970c4,"[{'abstract': '  We further progress along the line of Ref. [Phys. Rev. {\\bf A 94}, 043614\n(2016)] where a functional for Fermi systems with anomalously large $s$-wave\nscattering length $a_s$ was proposed that has no free parameters. The\nfunctional is designed to correctly reproduce the unitary limit in Fermi gases\ntogether with the leading-order contributions in the s- and p-wave channels at\nlow density. The functional is shown to be predictive up to densities\n$\\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang\nfunctional, valid for $\\rho < 10^{-6}$ fm$^{-3}$. The form of the functional\nretained in this work is further motivated. It is shown that the new functional\ncorresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all\norders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One\nconclusion from the present work is that, except in the extremely low--density\nregime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with\nrespect to the unitary limit. Starting from the functional, we introduce\ndensity--dependent scales and show that scales associated to the bare\ninteraction are strongly renormalized by medium effects. As a consequence, some\nof the scales at play around saturation are dominated by the unitary gas\nproperties and not directly to low-energy constants. For instance, we show that\nthe scale in the s-wave channel around saturation is proportional to the\nso-called Bertsch parameter $\\xi_0$ and becomes independent of $a_s$. We also\npoint out that these scales are of the same order of magnitude than those\nempirically obtained in the Skyrme energy density functional. We finally\npropose a slight modification of the functional such that it becomes accurate\nup to the saturation density $\\rho\\simeq 0.16$ fm$^{-3}$.\n', 'title': 'From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter'}]","Title: From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter

Abstract:   We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One
conclusion from the present work is that, except in the extremely low--density
regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with
respect to the unitary limit. Starting from the functional, we introduce
density--dependent scales and show that scales associated to the bare
interaction are strongly renormalized by medium effects. As a consequence, some
of the scales at play around saturation are dominated by the unitary gas
properties and not directly to low-energy constants. For instance, we show that
the scale in the s-wave channel around saturation is proportional to the
so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also
point out that these scales are of the same order of magnitude than those
empirically obtained in the Skyrme energy density functional. We finally
propose a slight modification of the functional such that it becomes accurate
up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$.

In this paper, the authors propose a slight modification to the functional to make it accurate up to the saturation density ρ≃ 0.16 fm⁻³."
What theoretical concepts are used to analyze the performance of MPS,85b24256,"[{'abstract': '  We design a new myopic strategy for a wide class of sequential design of\nexperiment (DOE) problems, where the goal is to collect data in order to to\nfulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling\n(MPS), is inspired by the classical posterior (Thompson) sampling algorithm for\nmulti-armed bandits and leverages the flexibility of probabilistic programming\nand approximate Bayesian inference to address a broad set of problems.\nEmpirically, this general-purpose strategy is competitive with more specialised\nmethods in a wide array of DOE tasks, and more importantly, enables addressing\ncomplex DOE goals where no existing method seems applicable. On the theoretical\nside, we leverage ideas from adaptive submodularity and reinforcement learning\nto derive conditions under which MPS achieves sublinear regret against natural\nbenchmark policies.\n', 'title': 'Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming'}]","Title: Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming

Abstract:   We design a new myopic strategy for a wide class of sequential design of
experiment (DOE) problems, where the goal is to collect data in order to to
fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling
(MPS), is inspired by the classical posterior (Thompson) sampling algorithm for
multi-armed bandits and leverages the flexibility of probabilistic programming
and approximate Bayesian inference to address a broad set of problems.
Empirically, this general-purpose strategy is competitive with more specialised
methods in a wide array of DOE tasks, and more importantly, enables addressing
complex DOE goals where no existing method seems applicable. On the theoretical
side, we leverage ideas from adaptive submodularity and reinforcement learning
to derive conditions under which MPS achieves sublinear regret against natural
benchmark policies.

Theoretical concepts used to analyze the performance of MPS are:

* Adaptive submodularity
* Reinforcement learning"
What are new interpretations of tidy subgroups and scale function in relation to endomorphisms of totally disconnected locally compact groups?,fc941b5c,"[{'abstract': '  We study the scale and tidy subgroups of an endomorphism of a totally\ndisconnected locally compact group using a geometric framework. This leads to\nnew interpretations of tidy subgroups and the scale function. Foremost, we\nobtain a geometric tidying procedure which applies to endomorphisms as well as\na geometric proof of the fact that tidiness is equivalent to being minimizing\nfor a given endomorphism. Our framework also yields an endomorphism version of\nthe Baumgartner-Willis tree representation theorem. We conclude with a\nconstruction of new endomorphisms of totally disconnected locally compact\ngroups from old via HNN-extensions.\n', 'title': 'Willis Theory via Graphs'}]","The researcher is looking for the paper that discusses new interpretations of tidy subgroups and scale functions in relation to endomorphisms of totally disconnected locally compact groups.

Title: Willis Theory via Graphs

Abstract: We study the scale and tidy subgroups of an endomorphism of a totally disconnected locally compact group using a geometric framework. This leads to new interpretations of tidy subgroups and the scale function. Foremost, we obtain a geometric tidying procedure which applies to endomorphisms as well as a geometric proof of the fact that tidiness is equivalent to being minimizing for a given endomorphism. Our framework also yields an endomorphism version of the Baumgartner-Willis tree representation theorem. We conclude with a construction of new endomorphisms of totally disconnected locally compact groups from old via HNN-extensions."
How do air drag panels contribute to disturbance rejection in a satellite attitude control system?,6a470a56,"[{'abstract': ""  This paper describes the development of a magnetic attitude control subsystem\nfor a 2U cubesat. Due to the presence of gravity gradient torques, the\nsatellite dynamics are open-loop unstable near the desired pointing\nconfiguration. Nevertheless the linearized time-varying system is completely\ncontrollable, under easily verifiable conditions, and the system's disturbance\nrejection capabilities can be enhanced by adding air drag panels exemplifying a\nbeneficial interplay between hardware design and control. In the paper,\nconditions for the complete controllability for the case of a magnetically\ncontrolled satellite with passive air drag panels are developed, and simulation\ncase studies with the LQR and MPC control designs applied in combination with a\nnonlinear time-varying input transformation are presented to demonstrate the\nability of the closed-loop system to satisfy mission objectives despite\ndisturbance torques.\n"", 'title': 'Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques'}]","I found the relevant research paper for you!

Title: Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques

Abstract: This paper describes the development of a magnetic attitude control subsystem for a 2U cubesat. Due to the presence of gravity gradient torques, the satellite dynamics are open-loop unstable near the desired pointing configuration. Nevertheless the linearized time-varying system is completely controllable, under easily verifiable conditions, and the system's disturbance rejection capabilities can be enhanced by adding air drag panels exemplifying a beneficial interplay between hardware design and control. In the paper, conditions for the complete controllability for the case of a magnetically controlled satellite with passive air drag panels are developed, and simulation case studies with the LQR and MPC control designs applied in combination with a nonlinear time-varying input transformation are presented to demonstrate the ability of the closed-loop system to satisfy mission objectives despite disturbance torques."
How can tidal streams of dwarf galaxies help us understand the shape of a host galaxy's gravitational potential?,eafdfaf6,"[{'abstract': '  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy\noffer a unique way to constrain the shape of galactic gravitational potentials.\nSuch streams can be used as leaning tower gravitational experiments on galactic\nscales. The most well motivated modification of gravity proposed as an\nalternative to dark matter on galactic scales is Milgromian dynamics (MOND),\nand we present here the first ever N-body simulations of the dynamical\nevolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a\nrealistic baryonic mass model for the Milky Way, we attempt to reproduce the\npresent-day spatial and kinematic structure of the Sagittarius dwarf and its\nimmense tidal stream that wraps around the Milky Way. With very little freedom\non the original structure of the progenitor, constrained by the total\nluminosity of the Sagittarius structure and by the observed stellar mass-size\nrelation for isolated dwarf galaxies, we find reasonable agreement between our\nsimulations and observations of this system. The observed stellar velocities in\nthe leading arm can be reproduced if we include a massive hot gas corona around\nthe Milky Way that is flattened in the direction of the principal plane of its\nsatellites. This is the first time that tidal dissolution in MOND has been\ntested rigorously at these mass and acceleration scales.\n', 'title': 'Stellar streams as gravitational experiments I. The case of Sagittarius'}]","Title: Stellar streams as gravitational experiments I. The case of Sagittarius

Abstract:  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy offer a unique way to constrain the shape of galactic gravitational potentials. Such streams can be used as leaning tower gravitational experiments on galactic scales. The most well motivated modification of gravity proposed as an alternative to dark matter on galactic scales is Milgromian dynamics (MOND), and we present here the first ever N-body simulations of the dynamical evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a realistic baryonic mass model for the Milky Way, we attempt to reproduce the present-day spatial and kinematic structure of the Sagittarius dwarf and its immense tidal stream that wraps around the Milky Way. With very little freedom on the original structure of the progenitor, constrained by the total luminosity of the Sagittarius structure and by the observed stellar mass-size relation for isolated dwarf galaxies, we find reasonable agreement between our simulations and observations of this system. The observed stellar velocities in the leading arm can be reproduced if we include a massive hot gas corona around the Milky Way that is flattened in the direction of the principal plane of its satellites. This is the first time that tidal dissolution in MOND has been tested rigorously at these mass and acceleration scales."
What are the properties of pro-$p$ Poincaré duality pairs in two dimensions?,2ec884d4,"[{'abstract': '  We classify pro-$p$ Poincaré duality pairs in dimension two. We then use\nthis classification to build a pro-$p$ analogue of the curve complex and\nestablish its basic properties. We conclude with some statements concerning\nseparability properties of the mapping class group.\n', 'title': 'Classification of pro-$p$ PD$^2$ pairs and the pro-$p$ curve complex'}]","I think I can help you with that!

Title: Classification of pro-$p$ PD$^2$ pairs and the pro-$p$ curve complex

Abstract: We classify pro-$p$ Poincaré duality pairs in dimension two. We then use this classification to build a pro-$p$ analogue of the curve complex and establish its basic properties. We conclude with some statements concerning separability properties of the mapping class group.

Unfortunately, the abstract doesn't provide explicit details about the properties of pro-$p$ Poincaré duality pairs in two dimensions. However, this paper seems to be the most relevant resource for your question. Would you like me to help you explore further or clarify any specific aspects of this paper?"
What is the rotational state of the object 1I/2017 U1 ('Oumuamua')?,3e10061f,"[{'abstract': ""  The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a\nplanetesimal born in another planetary system. This interloper exhibits a\nvariable colour within a range that is broadly consistent with local small\nbodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited\nKuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an\naxial ratio exceeding 5:1. Rotation period estimates are inconsistent and\nvaried, with reported values between 6.9 and 8.3 hours. Here we analyse all\navailable optical photometry reported to date. No single rotation period can\nexplain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be\nin an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or\ntumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135\nand 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the\navailable data are insufficient to uniquely constrain the true frequencies and\nshape. Assuming a body that responds to NPA rotation in a similar manner to\nSolar System asteroids and comets, the timescale to damp 1I/'Oumuamua's\ntumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling\nwithin its parent planetary system, and will remain tumbling well after it has\nleft ours.\n"", 'title': 'The tumbling rotational state of 1I/`Oumuamua'}]","I think I have found the answer you're looking for!

Title: The tumbling rotational state of 1I/`Oumuamua

Abstract: The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a planetesimal born in another planetary system. This interloper exhibits a variable colour within a range that is broadly consistent with local small bodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited Kuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an axial ratio exceeding 5:1. Rotation period estimates are inconsistent and varied, with reported values between 6.9 and 8.3 hours. Here we analyse all available optical photometry reported to date. No single rotation period can explain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be in an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or tumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135 and 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the available data are insufficient to uniquely constrain the true frequencies and shape. Assuming a body that responds to NPA rotation in a similar manner to Solar System asteroids and comets, the timescale to damp 1I/'Oumuamua's tumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling within its parent planetary system, and will remain tumbling well after it has left ours."
"What is the impact of the shape of a difference of Gaussian functions kernel, or Mexican Hat operator, on patterns in a neural field model?",74ec15ab,"[{'abstract': '  The formation of pattern in biological systems may be modeled by a set of\nreaction-diffusion equations. A diffusion-type coupling operator biologically\nsignificant in neuroscience is a difference of Gaussian functions (Mexican Hat\noperator) used as a spatial-convolution kernel. We are interested in the\ndifference among behaviors of \\emph{stochastic} neural field equations, namely\nspace-time stochastic differential-integral equations, and similar\ndeterministic ones. We explore, quantitatively, how the parameters of our model\nthat measure the shape of the coupling kernel, coupling strength, and aspects\nof the spatially-smoothed space-time noise, control the pattern in the\nresulting evolving random field. We find that a spatial pattern that is damped\nin time in a deterministic system may be sustained and amplified by\nstochasticity, most strikingly at an optimal spatio-temporal noise level. In\naddition, we find that spatially-smoothed noise alone causes pattern formation\neven without spatial coupling.\n', 'title': 'Smoothed Noise and Mexican Hat Coupling Produce Pattern in a Stochastic Neural Field'}]","Title: Smoothed Noise and Mexican Hat Coupling Produce Pattern in a Stochastic Neural Field

Abstract: The formation of pattern in biological systems may be modeled by a set of reaction-diffusion equations. A diffusion-type coupling operator biologically significant in neuroscience is a difference of Gaussian functions (Mexican Hat operator) used as a spatial-convolution kernel. We are interested in the difference among behaviors of stochastic neural field equations, namely space-time stochastic differential-integral equations, and similar deterministic ones. We explore, quantitatively, how the parameters of our model that measure the shape of the coupling kernel, coupling strength, and aspects of the spatially-smoothed space-time noise, control the pattern in the resulting evolving random field. We find that a spatial pattern that is damped in time in a deterministic system may be sustained and amplified by stochasticity, most strikingly at an optimal spatio-temporal noise level. In addition, we find that spatially-smoothed noise alone causes pattern formation even without spatial coupling."
What are epsilon-machines and how are they used to analyze brain activity?,665eefa0,"[{'abstract': '  We introduce new techniques to the analysis of neural spatiotemporal dynamics\nvia applying $\\epsilon$-machine reconstruction to electroencephalography (EEG)\nmicrostate sequences. Microstates are short duration quasi-stable states of the\ndynamically changing electrical field topographies recorded via an array of\nelectrodes from the human scalp, and cluster into four canonical classes. The\nsequence of microstates observed under particular conditions can be considered\nan information source with unknown underlying structure. $\\epsilon$-machines\nare discrete dynamical system automata with state-dependent probabilities on\ndifferent future observations (in this case the next measured EEG microstate).\nThey artificially reproduce underlying structure in an optimally predictive\nmanner as generative models exhibiting dynamics emulating the behaviour of the\nsource. Here we present experiments using both simulations and empirical data\nsupporting the value of associating these discrete dynamical systems with\nmental states (e.g. mind-wandering, focused attention, etc.) and with clinical\npopulations. The neurodynamics of mental states and clinical populations can\nthen be further characterized by properties of these dynamical systems,\nincluding: i) statistical complexity (determined by the number of states of the\ncorresponding $\\epsilon$-automaton); ii) entropy rate; iii) characteristic\nsequence patterning (syntax, probabilistic grammars); iv) duration, persistence\nand stability of dynamical patterns; and v) algebraic measures such as\nKrohn-Rhodes complexity or holonomy length of the decompositions of these. The\npotential applications include the characterization of mental states in\nneurodynamic terms for mental health diagnostics, well-being interventions,\nhuman-machine interface, and others on both subject-specific and\ngroup/population-level.\n', 'title': 'Simulating and Reconstructing Neurodynamics with Epsilon-Automata Applied to Electroencephalography (EEG) Microstate Sequences'}]","Title: Simulating and Reconstructing Neurodynamics with Epsilon-Automata Applied to Electroencephalography (EEG) Microstate Sequences

Abstract: We introduce new techniques to the analysis of neural spatiotemporal dynamics via applying ε-machine reconstruction to electroencephalography (EEG) microstate sequences. Microstates are short duration quasi-stable states of the dynamically changing electrical field topographies recorded via an array of electrodes from the human scalp, and cluster into four canonical classes. The sequence of microstates observed under particular conditions can be considered an information source with unknown underlying structure. ε-machines are discrete dynamical system automata with state-dependent probabilities on different future observations (in this case the next measured EEG microstate). They artificially reproduce underlying structure in an optimally predictive manner as generative models exhibiting dynamics emulating the behaviour of the source."
How does a self-consistent bath configuration with low entanglement enhance the calculation of dynamical response functions using DMRG?,bcf17fb7,"[{'abstract': ""  We implement an efficient numerical method to calculate response functions of\ncomplex impurities based on the Density Matrix Renormalization Group (DMRG) and\nuse it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This\nmethod uses the correction vector to obtain precise Green's functions on the\nreal frequency axis at zero temperature. By using a self-consistent bath\nconfiguration with very low entanglement, we take full advantage of the DMRG to\ncalculate dynamical response functions paving the way to treat large effective\nimpurities such as those corresponding to multi-orbital interacting models and\nmulti-site or multi-momenta clusters. This method leads to reliable\ncalculations of non-local self energies at arbitrary dopings and interactions\nand at any energy scale.\n"", 'title': 'Solving the multi-site and multi-orbital Dynamical Mean Field Theory using Density Matrix Renormalization'}]","Title: Solving the multi-site and multi-orbital Dynamical Mean Field Theory using Density Matrix Renormalization

Abstract: We implement an efficient numerical method to calculate response functions of complex impurities based on the Density Matrix Renormalization Group (DMRG) and use it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This method uses the correction vector to obtain precise Green's functions on the real frequency axis at zero temperature. By using a self-consistent bath configuration with very low entanglement, we take full advantage of the DMRG to calculate dynamical response functions paving the way to treat large effective impurities such as those corresponding to multi-orbital interacting models and multi-site or multi-momenta clusters. This method leads to reliable calculations of non-local self energies at arbitrary dopings and interactions and at any energy scale."
What is a key difference in the error floor of SS codes for binary input channels compared to AWGN channels?,05af551c,"[{'abstract': '  Sparse superposition (SS) codes were originally proposed as a\ncapacity-achieving communication scheme over the additive white Gaussian noise\nchannel (AWGNC) [1]. Very recently, it was discovered that these codes are\nuniversal, in the sense that they achieve capacity over any memoryless channel\nunder generalized approximate message-passing (GAMP) decoding [2], although\nthis decoder has never been stated for SS codes. In this contribution we\nintroduce the GAMP decoder for SS codes, we confirm empirically the\nuniversality of this communication scheme through its study on various channels\nand we provide the main analysis tools: state evolution and potential. We also\ncompare the performance of GAMP with the Bayes-optimal MMSE decoder. We\nempirically illustrate that despite the presence of a phase transition\npreventing GAMP to reach the optimal performance, spatial coupling allows to\nboost the performance that eventually tends to capacity in a proper limit. We\nalso prove that, in contrast with the AWGNC case, SS codes for binary input\nchannels have a vanishing error floor in the limit of large codewords.\nMoreover, the performance of Hadamard-based encoders is assessed for practical\nimplementations.\n', 'title': 'Generalized Approximate Message-Passing Decoder for Universal Sparse Superposition Codes'}]","Here is the answer to your question:

Title: Generalized Approximate Message-Passing Decoder for Universal Sparse Superposition Codes

Abstract: Sparse superposition (SS) codes were originally proposed as a capacity-achieving communication scheme over the additive white Gaussian noise channel (AWGNC) [1]. Very recently, it was discovered that these codes are universal, in the sense that they achieve capacity over any memoryless channel under generalized approximate message-passing (GAMP) decoding [2], although this decoder has never been stated for SS codes. In this contribution we introduce the GAMP decoder for SS codes, we confirm empirically the universality of this communication scheme through its study on various channels and we provide the main analysis tools: state evolution and potential. We also compare the performance of GAMP with the Bayes-optimal MMSE decoder. We empirically illustrate that despite the presence of a phase transition preventing GAMP to reach the optimal performance, spatial coupling allows to boost the performance that eventually tends to capacity in a proper limit. We also prove that, in contrast with the AWGNC case, SS codes for binary input channels have a vanishing error floor in the limit of large codewords. Moreover, the performance of Hadamard-based encoders is assessed for practical implementations.

The key difference is that SS codes for binary input channels have a vanishing error floor in the limit of large codewords, whereas they do not have this property for AWGN channels."
"In the EAGLE simulation, how does varying the lens model and observational properties influence the bias on Hubble constant measurements?",9dc57430,"[{'abstract': '  Cosmological parameter constraints from observations of time-delay lenses are\nbecoming increasingly precise. However, there may be significant bias and\nscatter in these measurements due to, among other things, the so-called\nmass-sheet degeneracy. To estimate these uncertainties, we analyze strong\nlenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet\ntransformation to the radial density profiles of lenses, and by selecting\nlenses near isothermality, we find that the bias on H0 can be reduced to 5%\nwith an intrinsic scatter of 10%, confirming previous results performed on a\ndifferent simulation data set. We further investigate whether combining lensing\nobservables with kinematic constraints helps to minimize this bias. We do not\ndetect any significant dependence of the bias on lens model parameters or\nobservational properties of the galaxy, but depending on the source--lens\nconfiguration, a bias may still exist. Cross lenses provide an accurate\nestimate of the Hubble constant, while fold (double) lenses tend to be biased\nlow (high). With kinematic constraints, double lenses show bias and intrinsic\nscatter of 6% and 10%, respectively, while quad lenses show bias and intrinsic\nscatter of 0.5% and 10%, respectively. For lenses with a reduced $\\chi^2 > 1$,\na power-law dependence of the $\\chi^2$ on the lens environment (number of\nnearby galaxies) is seen. Lastly, we model, in greater detail, the cases of two\ndouble lenses that are significantly biased. We are able to remove the bias,\nsuggesting that the remaining biases could also be reduced by carefully taking\ninto account additional sources of systematic uncertainty.\n', 'title': 'Reducing biases on $H_0$ measurements using strong lensing and galaxy dynamics: results from the EAGLE simulation'}]","Title: Reducing biases on $H_0$ measurements using strong lensing and galaxy dynamics: results from the EAGLE simulation

Abstract: Cosmological parameter constraints from observations of time-delay lenses are becoming increasingly precise. However, there may be significant bias and scatter in these measurements due to, among other things, the mass-sheet degeneracy. To estimate these uncertainties, we analyze strong lenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet transformation to the radial density profiles of lenses, and by selecting lenses near isothermality, we find that the bias on H0 can be reduced to 5% with an intrinsic scatter of 10%, confirming previous results performed on a different simulation data set. We further investigate whether combining lensing observables with kinematic constraints helps to minimize this bias. We do not detect any significant dependence of the bias on lens model parameters or observational properties of the galaxy, but depending on the source--lens configuration, a bias may still exist."
How can the complexity of calculating Frobenius action representation matrices be estimated?,275af301,"[{'abstract': '  This paper is concerned with the computation of representation matrices for\nthe action of Frobenius to the cohomology groups of algebraic varieties.\nSpecifically we shall give an algorithm to compute the matrices for arbitrary\nalgebraic varieties with defining equations over perfect fields of positive\ncharacteristic, and estimate its complexity. Moreover, we propose a specific\nefficient method, which works for complete intersections.\n', 'title': 'Computing representation matrices for the action of Frobenius to cohomology groups'}]","Title: Computing representation matrices for the action of Frobenius to cohomology groups

Abstract:   This paper is concerned with the computation of representation matrices for
the action of Frobenius to the cohomology groups of algebraic varieties.
Specifically we shall give an algorithm to compute the matrices for arbitrary
algebraic varieties with defining equations over perfect fields of positive
characteristic, and estimate its complexity. Moreover, we propose a specific
efficient method, which works for complete intersections."
What is the term for a specialized tool in hearing aids that steers sound in a desired direction?,f312bd54,"[{'abstract': '  In this work we apply Amplitude Modulation Spectrum (AMS) features to the\nsource localization problem. Our approach computes 36 bilateral features for 2s\nlong signal segments and estimates the azimuthal directions of a sound source\nthrough a binaurally trained classifier. This directional information of a\nsound source could be e.g. used to steer the beamformer in a hearing aid to the\nsource of interest in order to increase the SNR. We evaluated our approach on\nthe development set of the IEEE-AASP Challenge on sound source localization and\ntracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline\napproach. Additionally, our approach is computationally less complex.\n', 'title': 'Binaural Source Localization based on Modulation-Domain Features and Decision Pooling'}]","Title: Binaural Source Localization based on Modulation-Domain Features and Decision Pooling

Abstract:   In this work we apply Amplitude Modulation Spectrum (AMS) features to the source localization problem. Our approach computes 36 bilateral features for 2s long signal segments and estimates the azimuthal directions of a sound source through a binaurally trained classifier. This directional information of a sound source could be e.g. used to steer the beamformer in a hearing aid to the source of interest in order to increase the SNR. We evaluated our approach on the development set of the IEEE-AASP Challenge on sound source localization and tracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline approach. Additionally, our approach is computationally less complex.

The term being referred to is a ""beamformer""."
What is the flaw in the common interpretation of variance in measurement theory?,de234f2e,"[{'abstract': '  The existing measurement theory interprets the variance as the dispersion of\nmeasured value, which is actually contrary to a general mathematical knowledge\nthat the variance of a constant is 0. This paper will fully demonstrate that\nthe variance in measurement theory is actually the evaluation of probability\ninterval of an error instead of the dispersion of a measured value, point out\nthe key point of mistake in the existing interpretation, and fully interpret a\nseries of changes in conceptual logic and processing method brought about by\nthis new concept.\n', 'title': 'The Correct Application of Variance Concept in Measurement Theory'}]","Based on the provided context, I found the answer to your question.

Title: The Correct Application of Variance Concept in Measurement Theory

Abstract: The existing measurement theory interprets the variance as the dispersion of measured value, which is actually contrary to a general mathematical knowledge that the variance of a constant is 0. This paper will fully demonstrate that the variance in measurement theory is actually the evaluation of probability interval of an error instead of the dispersion of a measured value, point out the key point of mistake in the existing interpretation, and fully interpret a series of changes in conceptual logic and processing method brought about by this new concept.

The flaw in the common interpretation of variance in measurement theory is that it interprets variance as the dispersion of measured value, which is incorrect."
How can integral quadratic constraints (IQCs) be used to verify the exponential stability of systems?,c6f40b9c,"[{'abstract': '  The theory of integral quadratic constraints (IQCs) allows verification of\nstability and gain-bound properties of systems containing nonlinear or\nuncertain elements. Gain bounds often imply exponential stability, but it can\nbe challenging to compute useful numerical bounds on the exponential decay\nrate. This work presents a generalization of the classical IQC results of\nMegretski and Rantzer that leads to a tractable computational procedure for\nfinding exponential rate certificates that are far less conservative than ones\ncomputed from $L_2$ gain bounds alone. An expanded library of IQCs for\ncertifying exponential stability is also provided and the effectiveness of the\ntechnique is demonstrated via numerical examples.\n', 'title': 'Exponential Stability Analysis via Integral Quadratic Constraints'}]","Title: Exponential Stability Analysis via Integral Quadratic Constraints

Abstract: The theory of integral quadratic constraints (IQCs) allows verification of stability and gain-bound properties of systems containing nonlinear or uncertain elements. Gain bounds often imply exponential stability, but it can be challenging to compute useful numerical bounds on the exponential decay rate. This work presents a generalization of the classical IQC results of Megretski and Rantzer that leads to a tractable computational procedure for finding exponential rate certificates that are far less conservative than ones computed from $L_2$ gain bounds alone. An expanded library of IQCs for certifying exponential stability is also provided and the effectiveness of the technique is demonstrated via numerical examples."
How do Recurrent Neural Networks (RNNs) perform in session-based recommendation systems?,99e6bdf2,"[{'abstract': ""  In recent years, research has been done on applying Recurrent Neural Networks\n(RNNs) as recommender systems. Results have been promising, especially in the\nsession-based setting where RNNs have been shown to outperform state-of-the-art\nmodels. In many of these experiments, the RNN could potentially improve the\nrecommendations by utilizing information about the user's past sessions, in\naddition to its own interactions in the current session. A problem for\nsession-based recommendation, is how to produce accurate recommendations at the\nstart of a session, before the system has learned much about the user's current\ninterests. We propose a novel approach that extends a RNN recommender to be\nable to process the user's recent sessions, in order to improve\nrecommendations. This is done by using a second RNN to learn from recent\nsessions, and predict the user's interest in the current session. By feeding\nthis information to the original RNN, it is able to improve its\nrecommendations. Our experiments on two different datasets show that the\nproposed approach can significantly improve recommendations throughout the\nsessions, compared to a single RNN working only on the current session. The\nproposed model especially improves recommendations at the start of sessions,\nand is therefore able to deal with the cold start problem within sessions.\n"", 'title': 'Inter-Session Modeling for Session-Based Recommendation'}]","Here's the answer to your question:

Title: Inter-Session Modeling for Session-Based Recommendation

Abstract: In recent years, research has been done on applying Recurrent Neural Networks (RNNs) as recommender systems. Results have been promising, especially in the session-based setting where RNNs have been shown to outperform state-of-the-art models. In many of these experiments, the RNN could potentially improve the recommendations by utilizing information about the user's past sessions, in addition to its own interactions in the current session. A problem for session-based recommendation, is how to produce accurate recommendations at the start of a session, before the system has learned much about the user's current interests. We propose a novel approach that extends a RNN recommender to be able to process the user's recent sessions, in order to improve recommendations. This is done by using a second RNN to learn from recent sessions, and predict the user's interest in the current session. By feeding this information to the original RNN, it is able to improve its recommendations. Our experiments on two different datasets show that the proposed approach can significantly improve recommendations throughout the sessions, compared to a single RNN working only on the current session. The proposed model especially improves recommendations at the start of sessions, and is therefore able to deal with the cold start problem within sessions.

This research paper shows that RNNs perform well in session-based recommendation systems, especially when paired with additional techniques to leverage information from past sessions."
"Can bisymmetry replace associativity in the definition of quasitrivial symmetric nondecreasing operations?
",97224ad3,"[{'abstract': '  In this paper we are interested in the class of n-ary operations on an\narbitrary chain that are quasitrivial, symmetric, nondecreasing, and\nassociative. We first provide a description of these operations. We then prove\nthat associativity can be replaced with bisymmetry in the definition of this\nclass. Finally we investigate the special situation where the chain is finite.\n', 'title': 'Characterizations of quasitrivial symmetric nondecreasing associative operations'}]","I think I can help you with that!

Title: Characterizations of quasitrivial symmetric nondecreasing associative operations

Abstract:   In this paper we are interested in the class of n-ary operations on an
arbitrary chain that are quasitrivial, symmetric, nondecreasing, and
associative. We first provide a description of these operations. We then prove
that associativity can be replaced with bisymmetry in the definition of this
class. Finally we investigate the special situation where the chain is finite.

The answer to your question is yes, bisymmetry can replace associativity in the definition of quasitrivial symmetric nondecreasing operations, according to this paper."
How can the sine-Gordon equation model be used to describe magnetization waves in multilayer ferromagnets?,6dbbdbc7,"[{'abstract': '  The particular type of four-kink multi-solitons (or quadrons) adiabatic\ndynamics of the sine-Gordon equation in a model with two identical point\nattracting impurities has been studied. This model can be used for describing\nmagnetization localized waves in multilayer ferromagnet. The quadrons structure\nand properties has been numerically investigated. The cases of both large and\nsmall distances between impurities has been viewed. The dependence of the\nlocalized in impurity region nonlinear high-amplitude waves frequencies on the\ndistance between the impurities has been found. For an analytical description\nof two bound localized on impurities nonlinear waves dynamics, using\nperturbation theory, the system of differential equations for harmonic\noscillators with elastic link has been found. The analytical model\nqualitatively describes the results of the sine-Gordon equation numerical\nsimulation.\n', 'title': 'Dynamics of the multi-soliton waves in the sine-Gordon model with two identical point impurities'}]","Title: Dynamics of the multi-soliton waves in the sine-Gordon model with two identical point impurities

Abstract: The particular type of four-kink multi-solitons (or quadrons) adiabatic dynamics of the sine-Gordon equation in a model with two identical point attracting impurities has been studied. This model can be used for describing magnetization localized waves in multilayer ferromagnet. The quadrons structure and properties has been numerically investigated. The cases of both large and small distances between impurities has been viewed. The dependence of the localized in impurity region nonlinear high-amplitude waves frequencies on the distance between the impurities has been found. For an analytical description of two bound localized on impurities nonlinear waves dynamics, using perturbation theory, the system of differential equations for harmonic oscillators with elastic link has been found. The analytical model qualitatively describes the results of the sine-Gordon equation numerical simulation."
What technique was used to study the change in electronic structure of V2O5 nanorods with increasing temperature?,7ffef689,"[{'abstract': '  Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,\nexhibits interesting semiconductor to metal transition in the temperature range\nof 530-560 K. The metallic behavior originates because of the reduction of V2O5\nthrough oxygen vacancies. In the present report, V2O5 nanorods in the\northorhombic phase with crystal orientation of (001) are grown using vapor\ntransport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal\nformula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the\nformation of metallic phase above the transition temperature is established\nfrom the temperature-dependent Raman spectroscopic studies. The origin of the\nmetallic behavior of V2O5 is also understood due to the breakdown of pdpi bond\nbetween OI and nearest V atom instigated by the formation of vanadyl OI\nvacancy, confirmed from the downward shift of the bottom most split-off\nconduction bands in the material with increasing temperature.\n', 'title': 'Role of Vanadyl Oxygen in Understanding Metallic Behavior of V2O5(001) Nanorods'}]","Title: Role of Vanadyl Oxygen in Understanding Metallic Behavior of V2O5(001) Nanorods

Abstract: Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,
exhibits interesting semiconductor to metal transition in the temperature range
of 530-560 K. The metallic behavior originates because of the reduction of V2O5
through oxygen vacancies. In the present report, V2O5 nanorods in the
orthorhombic phase with crystal orientation of (001) are grown using vapor
transport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal
formula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the
formation of metallic phase above the transition temperature is established
from the temperature-dependent Raman spectroscopic studies. The origin of the
metallic behavior of V2O5 is also understood due to the breakdown of pdpi bond
between OI and nearest V atom instigated by the formation of vanadyl OI
vacancy, confirmed from the downward shift of the bottom most split-off
conduction bands in the material with increasing temperature.

The answer is: Raman spectroscopic studies."
The authors analyze Twitter CPU usage data using a Bayesian trend filtering model. What is unique about this model compared to competing methods? ,961f1a3b,"[{'abstract': '  We propose a novel class of dynamic shrinkage processes for Bayesian time\nseries and regression analysis. Building upon a global-local framework of prior\nconstruction, in which continuous scale mixtures of Gaussian distributions are\nemployed for both desirable shrinkage properties and computational\ntractability, we model dependence among the local scale parameters. The\nresulting processes inherit the desirable shrinkage behavior of popular\nglobal-local priors, such as the horseshoe prior, but provide additional\nlocalized adaptivity, which is important for modeling time series data or\nregression functions with local features. We construct a computationally\nefficient Gibbs sampling algorithm based on a Pólya-Gamma scale mixture\nrepresentation of the proposed process. Using dynamic shrinkage processes, we\ndevelop a Bayesian trend filtering model that produces more accurate estimates\nand tighter posterior credible intervals than competing methods, and apply the\nmodel for irregular curve-fitting of minute-by-minute Twitter CPU usage data.\nIn addition, we develop an adaptive time-varying parameter regression model to\nassess the efficacy of the Fama-French five-factor asset pricing model with\nmomentum added as a sixth factor. Our dynamic analysis of manufacturing and\nhealthcare industry data shows that with the exception of the market risk, no\nother risk factors are significant except for brief periods.\n', 'title': 'Dynamic Shrinkage Processes'}]","Title: Dynamic Shrinkage Processes

Abstract:  We propose a novel class of dynamic shrinkage processes for Bayesian time series and regression analysis. Building upon a global-local framework of prior construction, in which continuous scale mixtures of Gaussian distributions are employed for both desirable shrinkage properties and computational tractability, we model dependence among the local scale parameters. The resulting processes inherit the desirable shrinkage behavior of popular global-local priors, such as the horseshoe prior, but provide additional localized adaptivity, which is important for modeling time series data or regression functions with local features. We construct a computationally efficient Gibbs sampling algorithm based on a Pólya-Gamma scale mixture representation of the proposed process. Using dynamic shrinkage processes, we develop a Bayesian trend filtering model that produces more accurate estimates and tighter posterior credible intervals than competing methods, and apply the model for irregular curve-fitting of minute-by-minute Twitter CPU usage data. In addition, we develop an adaptive time-varying parameter regression model to assess the efficacy of the Fama-French five-factor asset pricing model with momentum added as a sixth factor. Our dynamic analysis of manufacturing and healthcare industry data shows that with the exception of the market risk, no other risk factors are significant except for brief periods.

This model is unique because it provides more accurate estimates and tighter posterior credible intervals compared to competing methods."
How do people make decisions when reconfiguring a stack of three blocks?,a3a29e04,"[{'abstract': ""  In this paper, we present a new task that investigates how people interact\nwith and make judgments about towers of blocks. In Experiment~1, participants\nin the lab solved a series of problems in which they had to re-configure three\nblocks from an initial to a final configuration. We recorded whether they used\none hand or two hands to do so. In Experiment~2, we asked participants online\nto judge whether they think the person in the lab used one or two hands. The\nresults revealed a close correspondence between participants' actions in the\nlab, and the mental simulations of participants online. To explain\nparticipants' actions and mental simulations, we develop a model that plans\nover a symbolic representation of the situation, executes the plan using a\ngeometric solver, and checks the plan's feasibility by taking into account the\nphysical constraints of the scene. Our model explains participants' actions and\njudgments to a high degree of quantitative accuracy.\n"", 'title': 'Physical problem solving: Joint planning with symbolic, geometric, and dynamic constraints'}]","I think I can help you with that!

Title: Physical problem solving: Joint planning with symbolic, geometric, and dynamic constraints

Abstract: In this paper, we present a new task that investigates how people interact with and make judgments about towers of blocks. In Experiment~1, participants in the lab solved a series of problems in which they had to re-configure three blocks from an initial to a final configuration. We recorded whether they used one hand or two hands to do so. In Experiment~2, we asked participants online to judge whether they think the person in the lab used one or two hands. The results revealed a close correspondence between participants' actions in the lab, and the mental simulations of participants online. To explain participants' actions and mental simulations, we develop a model that plans over a symbolic representation of the situation, executes the plan using a geometric solver, and checks the plan's feasibility by taking into account the physical constraints of the scene. Our model explains participants' actions and judgments to a high degree of quantitative accuracy.

This paper seems to discuss how people make decisions when reconfiguring a stack of three blocks, which is what you were looking for!"
How does the proposed algorithm perform for bounded degree graphs in the minimax and Bayesian settings?,4849fb2f,"[{'abstract': '  We study the problem of causal structure learning over a set of random\nvariables when the experimenter is allowed to perform at most $M$ experiments\nin a non-adaptive manner. We consider the optimal learning strategy in terms of\nminimizing the portions of the structure that remains unknown given the limited\nnumber of experiments in both Bayesian and minimax setting. We characterize the\ntheoretical optimal solution and propose an algorithm, which designs the\nexperiments efficiently in terms of time complexity. We show that for bounded\ndegree graphs, in the minimax case and in the Bayesian case with uniform\npriors, our proposed algorithm is a $\\rho$-approximation algorithm, where\n$\\rho$ is independent of the order of the underlying graph. Simulations on both\nsynthetic and real data show that the performance of our algorithm is very\nclose to the optimal solution.\n', 'title': 'Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments'}]","Title: Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments

Abstract: We study the problem of causal structure learning over a set of random variables when the experimenter is allowed to perform at most $M$ experiments in a non-adaptive manner. We consider the optimal learning strategy in terms of minimizing the portions of the structure that remains unknown given the limited number of experiments in both Bayesian and minimax setting. We characterize the theoretical optimal solution and propose an algorithm, which designs the experiments efficiently in terms of time complexity. We show that for bounded degree graphs, in the minimax case and in the Bayesian case with uniform priors, our proposed algorithm is a $\rho$-approximation algorithm, where $\rho$ is independent of the order of the underlying graph. Simulations on both synthetic and real data show that the performance of our algorithm is very close to the optimal solution."
How can the cumulative cost be minimized in model predictive control (MPC) when dealing with stochastic multiplicative uncertainty?,2414c58d,"[{'abstract': '  In this paper we present a framework for risk-sensitive model predictive\ncontrol (MPC) of linear systems affected by stochastic multiplicative\nuncertainty. Our key innovation is to consider a time-consistent, dynamic risk\nevaluation of the cumulative cost as the objective function to be minimized.\nThis framework is axiomatically justified in terms of time-consistency of risk\nassessments, is amenable to dynamic optimization, and is unifying in the sense\nthat it captures a full range of risk preferences from risk-neutral (i.e.,\nexpectation) to worst case. Within this framework, we propose and analyze an\nonline risk-sensitive MPC algorithm that is provably stabilizing. Furthermore,\nby exploiting the dual representation of time-consistent, dynamic risk\nmeasures, we cast the computation of the MPC control law as a convex\noptimization problem amenable to real-time implementation. Simulation results\nare presented and discussed.\n', 'title': 'A Framework for Time-Consistent, Risk-Sensitive Model Predictive Control: Theory and Algorithms'}]","Title: A Framework for Time-Consistent, Risk-Sensitive Model Predictive Control: Theory and Algorithms

Abstract:   In this paper we present a framework for risk-sensitive model predictive
control (MPC) of linear systems affected by stochastic multiplicative
uncertainty. Our key innovation is to consider a time-consistent, dynamic risk
evaluation of the cumulative cost as the objective function to be minimized.
This framework is axiomatically justified in terms of time-consistency of risk
assessments, is amenable to dynamic optimization, and is unifying in the sense
that it captures a full range of risk preferences from risk-neutral (i.e.,
expectation) to worst case. Within this framework, we propose and analyze an
online risk-sensitive MPC algorithm that is provably stabilizing. Furthermore,
by exploiting the dual representation of time-consistent, dynamic risk
measures, we cast the computation of the MPC control law as a convex
optimization problem amenable to real-time implementation. Simulation results
are presented and discussed."
How does  Monte Carlo Tree Search (MCTS) assist in chemical synthesis planning?,88e873a5,"[{'abstract': '  Retrosynthesis is a technique to plan the chemical synthesis of organic\nmolecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a\nsearch tree is built by analysing molecules recursively and dissecting them\ninto simpler molecular building blocks until one obtains a set of known\nbuilding blocks. The search space is intractably large, and it is difficult to\ndetermine the value of retrosynthetic positions. Here, we propose to model\nretrosynthesis as a Markov Decision Process. In combination with a Deep Neural\nNetwork policy learned from essentially the complete published knowledge of\nchemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In\nexploratory studies, we demonstrate that MCTS with neural network policies\noutperforms the traditionally used best-first search with hand-coded\nheuristics.\n', 'title': 'Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies'}]","Based on the context, I found the relevant information:

Title: Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies

Abstract: Retrosynthesis is a technique to plan the chemical synthesis of organic molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a search tree is built by analysing molecules recursively and dissecting them into simpler molecular building blocks until one obtains a set of known building blocks. The search space is intractably large, and it is difficult to determine the value of retrosynthetic positions. Here, we propose to model retrosynthesis as a Markov Decision Process. In combination with a Deep Neural Network policy learned from essentially the complete published knowledge of chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In exploratory studies, we demonstrate that MCTS with neural network policies outperforms the traditionally used best-first search with hand-coded heuristics.

In this paper, Monte Carlo Tree Search (MCTS) assists in chemical synthesis planning by evaluating positions in retrosynthesis, a technique to plan the chemical synthesis of organic molecules."
How does the specific structure of RANLUX's modulus contribute to fast modular multiplication?,b11ac4a6,"[{'abstract': '  The most popular and widely used subtract-with-borrow generator, also known\nas RANLUX, is reimplemented as a linear congruential generator using large\ninteger arithmetic with the modulus size of 576 bits. Modern computers, as well\nas the specific structure of the modulus inferred from RANLUX, allow for the\ndevelopment of a fast modular multiplication -- the core of the procedure. This\nwas previously believed to be slow and have too high cost in terms of computing\nresources. Our tests show a significant gain in generation speed which is\ncomparable with other fast, high quality random number generators. An\nadditional feature is the fast skipping of generator states leading to a\nseeding scheme which guarantees the uniqueness of random number sequences.\n', 'title': 'A revision of the subtract-with-borrow random number generators'}]","Title: A revision of the subtract-with-borrow random number generators

Abstract:   The most popular and widely used subtract-with-borrow generator, also known
as RANLUX, is reimplemented as a linear congruential generator using large
integer arithmetic with the modulus size of 576 bits. Modern computers, as well
as the specific structure of the modulus inferred from RANLUX, allow for the
development of a fast modular multiplication -- the core of the procedure. This
was previously believed to be slow and have too high cost in terms of computing
resources. Our tests show a significant gain in generation speed which is
comparable with other fast, high quality random number generators. An
additional feature is the fast skipping of generator states leading to a
seeding scheme which guarantees the uniqueness of random number sequences."
What discriminative learning framework utilizes distribution dissimilarity to reference templates?,360f22b7,"[{'abstract': '  This paper presents a distance-based discriminative framework for learning\nwith probability distributions. Instead of using kernel mean embeddings or\ngeneralized radial basis kernels, we introduce embeddings based on\ndissimilarity of distributions to some reference distributions denoted as\ntemplates. Our framework extends the theory of similarity of Balcan et al.\n(2008) to the population distribution case and we show that, for some learning\nproblems, some dissimilarity on distribution achieves low-error linear decision\nfunctions with high probability. Our key result is to prove that the theory\nalso holds for empirical distributions. Algorithmically, the proposed approach\nconsists in computing a mapping based on pairwise dissimilarity where learning\na linear decision function is amenable. Our experimental results show that the\nWasserstein distance embedding performs better than kernel mean embeddings and\ncomputing Wasserstein distance is far more tractable than estimating pairwise\nKullback-Leibler divergence of empirical distributions.\n', 'title': 'Distance Measure Machines'}]","I think I can help you with that!

The answer to your question is:

Title: Distance Measure Machines

Abstract: This paper presents a distance-based discriminative framework for learning with probability distributions. Instead of using kernel mean embeddings or generalized radial basis kernels, we introduce embeddings based on dissimilarity of distributions to some reference distributions denoted as templates. Our framework extends the theory of similarity of Balcan et al. (2008) to the population distribution case and we show that, for some learning problems, some dissimilarity on distribution achieves low-error linear decision functions with high probability. Our key result is to prove that the theory also holds for empirical distributions. Algorithmically, the proposed approach consists in computing a mapping based on pairwise dissimilarity where learning a linear decision function is amenable. Our experimental results show that the Wasserstein distance embedding performs better than kernel mean embeddings and computing Wasserstein distance is far more tractable than estimating pairwise Kullback-Leibler divergence of empirical distributions."
How do researchers address the computational complexity of stochastic and chance-constrained MISOCP formulations?,98034c2d,"[{'abstract': '  Second order conic programming (SOCP) has been used to model various\napplications in power systems, such as operation and expansion planning. In\nthis paper, we present a two-stage stochastic mixed integer SOCP (MISOCP) model\nfor the distribution system expansion planning problem that considers\nuncertainty and also captures the nonlinear AC power flow. To avoid costly\ninvestment plans due to some extreme scenarios, we further present a\nchance-constrained variant that could lead to cost-effective solutions. To\naddress the computational challenge, we extend the basic Benders decomposition\nmethod and develop a bilinear variant to compute stochastic and\nchance-constrained MISOCP formulations. A set of numerical experiments is\nperformed to illustrate the performance of our models and computational\nmethods. In particular, results show that our Benders decomposition algorithms\ndrastically outperform a professional MISOCP solver in handling stochastic\nscenarios by orders of magnitude.\n', 'title': 'Stochastic and Chance-Constrained Conic Distribution System Expansion Planning Using Bilinear Benders Decomposition'}]","Title: Stochastic and Chance-Constrained Conic Distribution System Expansion Planning Using Bilinear Benders Decomposition

Abstract: Second order conic programming (SOCP) has been used to model various applications in power systems, such as operation and expansion planning. In this paper, we present a two-stage stochastic mixed integer SOCP (MISOCP) model for the distribution system expansion planning problem that considers uncertainty and also captures the nonlinear AC power flow. To avoid costly investment plans due to some extreme scenarios, we further present a chance-constrained variant that could lead to cost-effective solutions. To address the computational challenge, we extend the basic Benders decomposition method and develop a bilinear variant to compute stochastic and chance-constrained MISOCP formulations. A set of numerical experiments is performed to illustrate the performance of our models and computational methods. In particular, results show that our Benders decomposition algorithms drastically outperform a professional MISOCP solver in handling stochastic scenarios by orders of magnitude."
What is an alpha-shortfall and how can it be measured?,43db1701,"[{'abstract': '  The processes of the averaged regression quantiles and of their modifications\nprovide useful tools in the regression models when the covariates are not fully\nunder our control. As an application we mention the probabilistic risk\nassessment in the situation when the return depends on some exogenous\nvariables. The processes enable to evaluate the expected $\\alpha$-shortfall\n($0\\leq\\alpha\\leq 1$) and other measures of the risk, recently generally\naccepted in the financial literature, but also help to measure the risk in\nenvironment analysis and elsewhere.\n', 'title': 'Empirical regression quantile process with possible application to risk analysis'}]","I'm happy to help you with your question! However, after carefully reading the provided abstracts, I couldn't find any mention of ""alpha-shortfall"" or a direct answer to your question.

It's possible that the concept of ""alpha-shortfall"" is not related to any of the research papers listed, or it might require additional context or information to provide a meaningful response. If you could provide more context or clarify what you mean by ""alpha-shortfall,"" I'll do my best to help you find the answer."
In what way does dynamical mean field theory (DMFT) contribute to the understanding of the Kondo lattice electronic structure?,99ff5c0e,"[{'abstract': '  The temperature-dependent evolution of the Kondo lattice is a long-standing\ntopic of theoretical and experimental investigation and yet it lacks a truly\nmicroscopic description of the relation of the basic $f$-$d$ hybridization\nprocesses to the fundamental temperature scales of Kondo screening and\nFermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$\nhybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo\nlattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved\nphotoemission (ARPES) with sufficient detail to allow direct comparison to\nfirst principles dynamical mean field theory (DMFT) calculations containing\nfull realism of crystalline electric field states. The ARPES results, for two\northogonal (001) and (100) cleaved surfaces and three different $f$-$d$\nhybridization scenarios, with additional microscopic insight provided by DMFT,\nreveal $f$ participation in the Fermi surface at temperatures much higher than\nthe lattice coherence temperature, $T^*\\approx$ 45 K, commonly believed to be\nthe onset for such behavior. The identification of a $T$-dependent crystalline\nelectric field degeneracy crossover in the DMFT theory $below$ $T^*$ is\nspecifically highlighted.\n', 'title': 'Evolution of the Kondo lattice electronic structure above the transport coherence temperature'}]","Title: Evolution of the Kondo lattice electronic structure above the transport coherence temperature

Abstract: The temperature-dependent evolution of the Kondo lattice is a long-standing topic of theoretical and experimental investigation and yet it lacks a truly microscopic description of the relation of the basic $f$-$d$ hybridization processes to the fundamental temperature scales of Kondo screening and Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$ hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved photoemission (ARPES) with sufficient detail to allow direct comparison to first principles dynamical mean field theory (DMFT) calculations containing full realism of crystalline electric field states. The ARPES results, for two orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$ hybridization scenarios, with additional microscopic insight provided by DMFT, reveal $f$ participation in the Fermi surface at temperatures much higher than the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be the onset for such behavior. The identification of a $T$-dependent crystalline electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is specifically highlighted."
How does the performance of the proposed algorithm compare to existing methods for long-term pedestrian forecasting?,d43aaba1,"[{'abstract': '  The success of autonomous systems will depend upon their ability to safely\nnavigate human-centric environments. This motivates the need for a real-time,\nprobabilistic forecasting algorithm for pedestrians, cyclists, and other agents\nsince these predictions will form a necessary step in assessing the risk of any\naction. This paper presents a novel approach to probabilistic forecasting for\npedestrians based on weighted sums of ordinary differential equations that are\nlearned from historical trajectory information within a fixed scene. The\nresulting algorithm is embarrassingly parallel and is able to work at real-time\nspeeds using a naive Python implementation. The quality of predicted locations\nof agents generated by the proposed algorithm is validated on a variety of\nexamples and considerably higher than existing state of the art approaches over\nlong time horizons.\n', 'title': 'Technical Report for Real-Time Certified Probabilistic Pedestrian Forecasting'}]","Based on the provided context, the answer to your question is:

Title: Technical Report for Real-Time Certified Probabilistic Pedestrian Forecasting

Abstract: The success of autonomous systems will depend upon their ability to safely navigate human-centric environments. This motivates the need for a real-time, probabilistic forecasting algorithm for pedestrians, cyclists, and other agents since these predictions will form a necessary step in assessing the risk of any action. This paper presents a novel approach to probabilistic forecasting for pedestrians based on weighted sums of ordinary differential equations that are learned from historical trajectory information within a fixed scene. The resulting algorithm is embarrassingly parallel and is able to work at real-time speeds using a naive Python implementation. The quality of predicted locations of agents generated by the proposed algorithm is validated on a variety of examples and considerably higher than existing state of the art approaches over long time horizons."
Does the concept of extensionality affect the mutual interpretability of Adjunctive Set Theory and the other theories?,6a9443eb,"[{'abstract': ""  An elementary rheory of concatenation is introduced and used to establish\nmutual interpretability of Robinson arithmetic, Minimal Predicative Set Theory,\nthe quantifier-free part of Kirby's finitary set theory, and Adjunctive Set\nTheory, with or without extensionality.\n"", 'title': 'Mutual Interpretability of Robinson Arithmetic and Adjunctive Set Theory with Extensionality'}]","Title: Mutual Interpretability of Robinson Arithmetic and Adjunctive Set Theory with Extensionality

Abstract: An elementary rheory of concatenation is introduced and used to establish mutual interpretability of Robinson arithmetic, Minimal Predicative Set Theory, the quantifier-free part of Kirby's finitary set theory, and Adjunctive Set Theory, with or without extensionality.

This research paper suggests that the concept of extensionality does not affect the mutual interpretability of Adjunctive Set Theory and the other theories, as they are shown to be mutually interpretable both with and without extensionality."
What is the relationship between the distance standard deviation and the classical standard deviation?,f2435784,"[{'abstract': ""  The distance standard deviation, which arises in distance correlation\nanalysis of multivariate data, is studied as a measure of spread. New\nrepresentations for the distance standard deviation are obtained in terms of\nGini's mean difference and in terms of the moments of spacings of order\nstatistics. Inequalities for the distance variance are derived, proving that\nthe distance standard deviation is bounded above by the classical standard\ndeviation and by Gini's mean difference. Further, it is shown that the distance\nstandard deviation satisfies the axiomatic properties of a measure of spread.\nExplicit closed-form expressions for the distance variance are obtained for a\nbroad class of parametric distributions. The asymptotic distribution of the\nsample distance variance is derived.\n"", 'title': 'The Distance Standard Deviation'}]","I think I can help you with that!

Title: The Distance Standard Deviation

Abstract: The distance standard deviation, which arises in distance correlation analysis of multivariate data, is studied as a measure of spread. New representations for the distance standard deviation are obtained in terms of Gini's mean difference and in terms of the moments of spacings of order statistics. Inequalities for the distance variance are derived, proving that the distance standard deviation is bounded above by the classical standard deviation and by Gini's mean difference. Further, it is shown that the distance standard deviation satisfies the axiomatic properties of a measure of spread. Explicit closed-form expressions for the distance variance are obtained for a broad class of parametric distributions. The asymptotic distribution of the sample distance variance is derived.

According to the abstract, the distance standard deviation is bounded above by the classical standard deviation. This suggests that the distance standard deviation is always less than or equal to the classical standard deviation."
How does the Knaster-Tarski theorem relate to the existence of fixed points for monotone nonexpansive mappings?,09345cd0,"[{'abstract': ""  Let $X$ be a partially ordered set with the property that each family of\norder intervals of the form $[a,b],[a,\\rightarrow )$ with the finite\nintersection property has a nonempty intersection. We show that every directed\nsubset of $X$ has a supremum. Then we apply the above result to prove that if\n$X$ is a topological space with a partial order $\\preceq $ for which the order\nintervals are compact, $\\mathcal{F}$ a nonempty commutative family of monotone\nmaps from $X$ into $X$ and there exists $c\\in X$ such that $c\\preceq Tc$ for\nevery $T\\in \\mathcal{F}$, then the set of common fixed points of $\\mathcal{F}$\nis nonempty and has a maximal element. The result, specialized to the case of\nBanach spaces gives a general fixed point theorem that drops almost all\nassumptions from the recent results in this area. An application to the theory\nof integral equations of Urysohn's type is also given.\n"", 'title': 'The Knaster-Tarski theorem versus monotone nonexpansive mappings'}]","Title: The Knaster-Tarski theorem versus monotone nonexpansive mappings

Abstract: Let $X$ be a partially ordered set with the property that each family of order intervals of the form $[a,b],[a,\rightarrow )$ with the finite intersection property has a nonempty intersection. We show that every directed subset of $X$ has a supremum. Then we apply the above result to prove that if $X$ is a topological space with a partial order $\preceq $ for which the order intervals are compact, $\mathcal{F}$ a nonempty commutative family of monotone maps from $X$ into $X$ and there exists $c\in X$ such that $c\preceq Tc$ for every $T\in \mathcal{F}$, then the set of common fixed points of $\mathcal{F}$ is nonempty and has a maximal element. The result, specialized to the case of Banach spaces gives a general fixed point theorem that drops almost all assumptions from the recent results in this area. An application to the theory of integral equations of Urysohn's type is also given."
What are the computational advantages of the proposed Support Vector Machine training algorithm?,dc660c88,"[{'abstract': '  This work proposes a new algorithm for training a re-weighted L2 Support\nVector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès\net al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In\nparticular, the margin required for each training vector is set independently,\ndefining a new weighted SVM model. These weights are selected to be binary, and\nthey are automatically adapted during the training of the model, resulting in a\nvariation of the Frank-Wolfe optimization algorithm with essentially the same\ncomputational complexity as the original algorithm. As shown experimentally,\nthis algorithm is computationally cheaper to apply since it requires less\niterations to converge, and it produces models with a sparser representation in\nterms of support vectors and which are more stable with respect to the\nselection of the regularization hyper-parameter.\n', 'title': 'Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers'}]","Title: Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers

Abstract:   This work proposes a new algorithm for training a re-weighted L2 Support Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In particular, the margin required for each training vector is set independently, defining a new weighted SVM model. These weights are selected to be binary, and they are automatically adapted during the training of the model, resulting in a variation of the Frank-Wolfe optimization algorithm with essentially the same computational complexity as the original algorithm. As shown experimentally, this algorithm is computationally cheaper to apply since it requires less iterations to converge, and it produces models with a sparser representation in terms of support vectors and which are more stable with respect to the selection of the regularization hyper-parameter."
What is the name of the proposed method in this paper that aims to improve background estimation?,c2ae51e3,"[{'abstract': '  Classical principal component analysis (PCA) is not robust to the presence of\nsparse outliers in the data. The use of the $\\ell_1$ norm in the Robust PCA\n(RPCA) method successfully eliminates the weakness of PCA in separating the\nsparse outliers. In this paper, by sticking a simple weight to the Frobenius\nnorm, we propose a weighted low rank (WLR) method to avoid the often\ncomputationally expensive algorithms relying on the $\\ell_1$ norm. As a proof\nof concept, a background estimation model has been presented and compared with\ntwo $\\ell_1$ norm minimization algorithms. We illustrate that as long as a\nsimple weight matrix is inferred from the data, one can use the weighted\nFrobenius norm and achieve the same or better performance.\n', 'title': 'Weighted Low Rank Approximation for Background Estimation Problems'}]","I apologize, but I couldn't find a paper that proposes a method to improve background estimation. The papers provided focus on different topics, such as prescribed burn instrumentation, pedestrian motion prediction, model reduction in chemical reaction networks, joint power and admission control, and real-time certified probabilistic pedestrian forecasting. None of the abstracts mention improving background estimation."
What percentage of scatter plots in the test set did the described method successfully extract data from?,44e4ea99,"[{'abstract': '  Charts are an excellent way to convey patterns and trends in data, but they\ndo not facilitate further modeling of the data or close inspection of\nindividual data points. We present a fully automated system for extracting the\nnumerical values of data points from images of scatter plots. We use deep\nlearning techniques to identify the key components of the chart, and optical\ncharacter recognition together with robust regression to map from pixels to the\ncoordinate system of the chart. We focus on scatter plots with linear scales,\nwhich already have several interesting challenges. Previous work has done fully\nautomatic extraction for other types of charts, but to our knowledge this is\nthe first approach that is fully automatic for scatter plots. Our method\nperforms well, achieving successful data extraction on 89% of the plots in our\ntest set.\n', 'title': 'Scatteract: Automated extraction of data from scatter plots'}]","Title: Scatteract: Automated extraction of data from scatter plots

Abstract: Charts are an excellent way to convey patterns and trends in data, but they do not facilitate further modeling of the data or close inspection of individual data points. We present a fully automated system for extracting the numerical values of data points from images of scatter plots. We use deep learning techniques to identify the key components of the chart, and optical character recognition together with robust regression to map from pixels to the coordinate system of the chart. We focus on scatter plots with linear scales, which already have several interesting challenges. Previous work has done fully automatic extraction for other types of charts, but to our knowledge this is the first approach that is fully automatic for scatter plots. Our method performs well, achieving successful data extraction on 89% of the plots in our test set.

Answer: The method successfully extracted data from 89% of scatter plots in the test set."
How does the detectability of DESs compare when represented as deterministic finite automata?,c4153524,"[{'abstract': '  Detectability of discrete event systems (DESs) is a question whether the\ncurrent and subsequent states can be determined based on observations. Shu and\nLin designed a polynomial-time algorithm to check strong (periodic)\ndetectability and an exponential-time (polynomial-space) algorithm to check\nweak (periodic) detectability. Zhang showed that checking weak (periodic)\ndetectability is PSpace-complete. This intractable complexity opens a question\nwhether there are structurally simpler DESs for which the problem is tractable.\nIn this paper, we show that it is not the case by considering DESs represented\nas deterministic finite automata without non-trivial cycles, which are\nstructurally the simplest deadlock-free DESs. We show that even for such very\nsimple DESs, checking weak (periodic) detectability remains intractable. On the\ncontrary, we show that strong (periodic) detectability of DESs can be\nefficiently verified on a parallel computer.\n', 'title': 'Complexity of Deciding Detectability in Discrete Event Systems'}]","Title: Complexity of Deciding Detectability in Discrete Event Systems

Abstract: Detectability of discrete event systems (DESs) is a question whether the current and subsequent states can be determined based on observations. Shu and Lin designed a polynomial-time algorithm to check strong (periodic) detectability and an exponential-time (polynomial-space) algorithm to check weak (periodic) detectability. Zhang showed that checking weak (periodic) detectability is PSpace-complete. This intractable complexity opens a question whether there are structurally simpler DESs for which the problem is tractable. In this paper, we show that it is not the case by considering DESs represented as deterministic finite automata without non-trivial cycles, which are structurally the simplest deadlock-free DESs. We show that even for such very simple DESs, checking weak (periodic) detectability remains intractable. On the contrary, we show that strong (periodic) detectability of DESs can be efficiently verified on a parallel computer."
Can you describe the methodology for forming information networks and subject domain models using Wikipedia data?,a1f042a0,"[{'abstract': '  The new index of the author\'s popularity estimation is represented in the\npaper. The index is calculated on the basis of Wikipedia encyclopedia analysis\n(Wikipedia Index - WI). Unlike the conventional existed citation indices, the\nsuggested mark allows to evaluate not only the popularity of the author, as it\ncan be done by means of calculating the general citation number or by the\nHirsch index, which is often used to measure the author\'s research rate. The\nindex gives an opportunity to estimate the author\'s popularity, his/her\ninfluence within the sought-after area ""knowledge area"" in the Internet - in\nthe Wikipedia. The suggested index is supposed to be calculated in frames of\nthe subject domain, and it, on the one hand, avoids the mistaken computation of\nthe homonyms, and on the other hand - provides the entirety of the subject\narea. There are proposed algorithms and the technique of the Wikipedia Index\ncalculation through the network encyclopedia sounding, the exemplified\ncalculations of the index for the prominent researchers, and also the methods\nof the information networks formation - models of the subject domains by the\nautomatic monitoring and networks information reference resources analysis. The\nconsidered in the paper notion network corresponds the terms-heads of the\nWikipedia articles.\n', 'title': 'Wiki-index of authors popularity'}]","Title: Wiki-index of authors popularity

Abstract:   The new index of the author's popularity estimation is represented in the
paper. The index is calculated on the basis of Wikipedia encyclopedia analysis
(Wikipedia Index - WI). Unlike the conventional existed citation indices, the
suggested mark allows to evaluate not only the popularity of the author, as it
can be done by means of calculating the general citation number or by the
Hirsch index, which is often used to measure the author's research rate. The
index gives an opportunity to estimate the author's popularity, his/her
influence within the sought-after area ""knowledge area"" in the Internet - in
the Wikipedia. The suggested index is supposed to be calculated in frames of
the subject domain, and it, on the one hand, avoids the mistaken computation of
the homonyms, and on the other hand - provides the entirety of the subject
area. There are proposed algorithms and the technique of the Wikipedia Index
calculation through the network encyclopedia sounding, the exemplified
calculations of the index for the prominent researchers, and also the methods
of the information networks formation - models of the subject domains by the
automatic monitoring and networks information reference resources analysis. The
considered in the paper notion network corresponds the terms-heads of the
Wikipedia articles."
What is the connection between sparse feature selection and sparse mean estimation?,e433db31,"[{'abstract': ""  Sparse feature selection is necessary when we fit statistical models, we have\naccess to a large group of features, don't know which are relevant, but assume\nthat most are not. Alternatively, when the number of features is larger than\nthe available data the model becomes over parametrized and the sparse feature\nselection task involves selecting the most informative variables for the model.\nWhen the model is a simple location model and the number of relevant features\ndoes not grow with the total number of features, sparse feature selection\ncorresponds to sparse mean estimation. We deal with a simplified mean\nestimation problem consisting of an additive model with gaussian noise and mean\nthat is in a restricted, finite hypothesis space. This restriction simplifies\nthe mean estimation problem into a selection problem of combinatorial nature.\nAlthough the hypothesis space is finite, its size is exponential in the\ndimension of the mean. In limited data settings and when the size of the\nhypothesis space depends on the amount of data or on the dimension of the data,\nchoosing an approximation set of hypotheses is a desirable approach. Choosing a\nset of hypotheses instead of a single one implies replacing the bias-variance\ntrade off with a resolution-stability trade off. Generalization capacity\nprovides a resolution selection criterion based on allowing the learning\nalgorithm to communicate the largest amount of information in the data to the\nlearner without error. In this work the theory of approximation set coding and\ngeneralization capacity is explored in order to understand this approach. We\nthen apply the generalization capacity criterion to the simplified sparse mean\nestimation problem and detail an importance sampling algorithm which at once\nsolves the difficulty posed by large hypothesis spaces and the slow convergence\nof uniform sampling algorithms.\n"", 'title': 'Sparse mean localization by information theory'}]","Title: Sparse mean localization by information theory

Abstract:  Sparse feature selection is necessary when we fit statistical models, we have access to a large group of features, don't know which are relevant, but assume that most are not. Alternatively, when the number of features is larger than the available data the model becomes over parametrized and the sparse feature selection task involves selecting the most informative variables for the model. When the model is a simple location model and the number of relevant features does not grow with the total number of features, sparse feature selection corresponds to sparse mean estimation."
What are Atkin-style results and how can they be applied to point-counting algorithms for genus-two curves?,cc0b7020,"[{'abstract': ""  Schoof's classic algorithm allows point-counting for elliptic curves over\nfinite fields in polynomial time. This algorithm was subsequently improved by\nAtkin, using factorizations of modular polynomials, and by Elkies, using a\ntheory of explicit isogenies. Moving to Jacobians of genus-2 curves, the\ncurrent state of the art for point counting is a generalization of Schoof's\nalgorithm. While we are currently missing the tools we need to generalize\nElkies' methods to genus 2, recently Martindale and Milio have computed\nanalogues of modular polynomials for genus-2 curves whose Jacobians have real\nmultiplication by maximal orders of small discriminant. In this article, we\nprove Atkin-style results for genus-2 Jacobians with real multiplication by\nmaximal orders, with a view to using these new modular polynomials to improve\nthe practicality of point-counting algorithms for these curves.\n"", 'title': 'Isogenies for point counting on genus two hyperelliptic curves with maximal real multiplication'}]","I think I can help you with that!

Title: Isogenies for point counting on genus two hyperelliptic curves with maximal real multiplication

Abstract: Schoof's classic algorithm allows point-counting for elliptic curves over finite fields in polynomial time. This algorithm was subsequently improved by Atkin, using factorizations of modular polynomials, and by Elkies, using a theory of explicit isogenies. Moving to Jacobians of genus-2 curves, the current state of the art for point counting is a generalization of Schoof's algorithm. While we are currently missing the tools we need to generalize Elkies' methods to genus 2, recently Martindale and Milio have computed analogues of modular polynomials for genus-2 curves whose Jacobians have real multiplication by maximal orders of small discriminant. In this article, we prove Atkin-style results for genus-2 Jacobians with real multiplication by maximal orders, with a view to using these new modular polynomials to improve the practicality of point-counting algorithms for these curves.

Atkin-style results refer to the improvement of Schoof's algorithm by Atkin, using factorizations of modular polynomials. In this paper, the authors generalize Atkin's method to genus-2 Jacobians with real multiplication by maximal orders, which can be applied to improve the practicality of point-counting algorithms for these curves."
How are bounds based on the trace of the inverse and the inverse squared of a matrix useful for  singular value computations?,6fcfed81,"[{'abstract': ""  Lower bounds on the smallest eigenvalue of a symmetric positive definite\nmatrices $A\\in\\mathbb{R}^{m\\times m}$ play an important role in condition\nnumber estimation and in iterative methods for singular value computation. In\nparticular, the bounds based on ${\\rm Tr}(A^{-1})$ and ${\\rm Tr}(A^{-2})$\nattract attention recently because they can be computed in $O(m)$ work when $A$\nis tridiagonal. In this paper, we focus on these bounds and investigate their\nproperties in detail. First, we consider the problem of finding the optimal\nbound that can be computed solely from ${\\rm Tr}(A^{-1})$ and ${\\rm\nTr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one\nin terms of sharpness. Next, we study the gap between the Laguerre bound and\nthe smallest eigenvalue. We characterize the situation in which the gap becomes\nlargest in terms of the eigenvalue distribution of $A$ and show that the gap\nbecomes smallest when ${\\rm Tr}(A^{-2})/\\{{\\rm Tr}(A^{-1})\\}^2$ approaches 1 or\n$\\frac{1}{m}$. These results will be useful, for example, in designing\nefficient shift strategies for singular value computation algorithms.\n"", 'title': ""On the optimality and sharpness of Laguerre's lower bound on the smallest eigenvalue of a symmetric positive definite matrix""}]","Title: On the optimality and sharpness of Laguerre's lower bound on the smallest eigenvalue of a symmetric positive definite matrix

Abstract: Lower bounds on the smallest eigenvalue of a symmetric positive definite matrices $A\in\mathbb{R}^{m\times m}$ play an important role in condition number estimation and in iterative methods for singular value computation. In particular, the bounds based on ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$ attract attention recently because they can be computed in $O(m)$ work when $A$ is tridiagonal. In this paper, we focus on these bounds and investigate their properties in detail. First, we consider the problem of finding the optimal bound that can be computed solely from ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one in terms of sharpness. Next, we study the gap between the Laguerre bound and the smallest eigenvalue. We characterize the situation in which the gap becomes largest in terms of the eigenvalue distribution of $A$ and show that the gap becomes smallest when ${\rm Tr}(A^{-2})/\{{\rm Tr}(A^{-1})\}^2$ approaches 1 or $\frac{1}{m}$. These results will be useful, for example, in designing efficient shift strategies for singular value computation algorithms."
 In what asymptotic regime does the delay operator converge to a friction term when coupled with a spatial elliptic second order operator?,139a87ec,"[{'abstract': '  In the first part of this work we show the convergence with respect to an\nasymptotic parameter {\\epsilon} of a delayed heat equation. It represents a\nmathematical extension of works considered previously by the authors [Milisic\net al. 2011, Milisic et al. 2016]. Namely, this is the first result involving\ndelay operators approximating protein linkages coupled with a spatial elliptic\nsecond order operator. For the sake of simplicity we choose the Laplace\noperator, although more general results could be derived. The main arguments\nare (i) new energy estimates and (ii) a stability result extended from the\nprevious work to this more involved context. They allow to prove convergence of\nthe delay operator to a friction term together with the Laplace operator in the\nsame asymptotic regime considered without the space dependence in [Milisic et\nal, 2011]. In a second part we extend fixed-point results for the fully\nnon-linear model introduced in [Milisic et al, 2016] and prove global existence\nin time. This shows that the blow-up scenario observed previously does not\noccur. Since the latter result was interpreted as a rupture of adhesion forces,\nwe discuss the possibility of bond breaking both from the analytic and\nnumerical point of view.\n', 'title': 'Space dependent adhesion forces mediated by transient elastic linkages : new convergence and global existence results'}]","The answer can be found in the first abstract.

Title: Space dependent adhesion forces mediated by transient elastic linkages : new convergence and global existence results

Abstract:   In the first part of this work we show the convergence with respect to an
asymptotic parameter {\epsilon} of a delayed heat equation. It represents a
mathematical extension of works considered previously by the authors [Milisic
et al. 2011, Milisic et al. 2016]. Namely, this is the first result involving
delay operators approximating protein linkages coupled with a spatial elliptic
second order operator. For the sake of simplicity we choose the Laplace
operator, although more general results could be derived. The main arguments
are (i) new energy estimates and (ii) a stability result extended from the
previous work to this more involved context. They allow to prove convergence of
the delay operator to a friction term together with the Laplace operator in the
same asymptotic regime considered without the space dependence in [Milisic et
al, 2011]. In a second part we extend fixed-point results for the fully
non-linear model introduced in [Milisic et al, 2016] and prove global existence
in time. This shows that the blow-up scenario observed previously does not
occur. Since the latter result was interpreted as a rupture of adhesion forces,
we discuss the possibility of bond breaking both from the analytic and
numerical point of view.

The asymptotic regime is with respect to the asymptotic parameter ε."
Can this method be used for simultaneous readout of quantum information stored in both atomic state and position?,1a459385,"[{'abstract': ""  We demonstrate the parallel and non-destructive readout of the hyperfine\nstate for optically trapped $^{87}$Rb atoms. The scheme is based on\nstate-selective fluorescence imaging and achieves detection fidelities $>$98%\nwithin 10$\\,$ms, while keeping 99% of the atoms trapped. For the read-out of\ndense arrays of neutral atoms in optical lattices, where the fluorescence\nimages of neighboring atoms overlap, we apply a novel image analysis technique\nusing Bayesian inference to determine the internal state of multiple atoms. Our\nmethod is scalable to large neutral atom registers relevant for future quantum\ninformation processing tasks requiring fast and non-destructive readout and can\nalso be used for the simultaneous read-out of quantum information stored in\ninternal qubit states and in the atoms' positions.\n"", 'title': 'Fast non-destructive parallel readout of neutral atom registers in optical potentials'}]","Title: Fast non-destructive parallel readout of neutral atom registers in optical potentials

Abstract: We demonstrate the parallel and non-destructive readout of the hyperfine state for optically trapped $^{87}$Rb atoms. The scheme is based on state-selective fluorescence imaging and achieves detection fidelities >98% within 10$\,$ms, while keeping 99% of the atoms trapped. For the read-out of dense arrays of neutral atoms in optical lattices, where the fluorescence images of neighboring atoms overlap, we apply a novel image analysis technique using Bayesian inference to determine the internal state of multiple atoms. Our method is scalable to large neutral atom registers relevant for future quantum information processing tasks requiring fast and non-destructive readout and can also be used for the simultaneous read-out of quantum information stored in internal qubit states and in the atoms' positions."
What new formalism is proposed to study the interaction between large and small scale structures?,b79a4d54,"[{'abstract': '  The next generation of cosmological surveys will operate over unprecedented\nscales, and will therefore provide exciting new opportunities for testing\ngeneral relativity. The standard method for modelling the structures that these\nsurveys will observe is to use cosmological perturbation theory for linear\nstructures on horizon-sized scales, and Newtonian gravity for non-linear\nstructures on much smaller scales. We propose a two-parameter formalism that\ngeneralizes this approach, thereby allowing interactions between large and\nsmall scales to be studied in a self-consistent and well-defined way. This uses\nboth post-Newtonian gravity and cosmological perturbation theory, and can be\nused to model realistic cosmological scenarios including matter, radiation and\na cosmological constant. We find that the resulting field equations can be\nwritten as a hierarchical set of perturbation equations. At leading-order,\nthese equations allow us to recover a standard set of Friedmann equations, as\nwell as a Newton-Poisson equation for the inhomogeneous part of the Newtonian\nenergy density in an expanding background. For the perturbations in the\nlarge-scale cosmology, however, we find that the field equations are sourced by\nboth non-linear and mode-mixing terms, due to the existence of small-scale\nstructures. These extra terms should be expected to give rise to new\ngravitational effects, through the mixing of gravitational modes on small and\nlarge scales - effects that are beyond the scope of standard linear\ncosmological perturbation theory. We expect our formalism to be useful for\naccurately modelling gravitational physics in universes that contain non-linear\nstructures, and for investigating the effects of non-linear gravity in the era\nof ultra-large-scale surveys.\n', 'title': 'Perturbation theory for cosmologies with non-linear structure'}]","Title: Perturbation theory for cosmologies with non-linear structure

Abstract: The next generation of cosmological surveys will operate over unprecedented scales, and will therefore provide exciting new opportunities for testing general relativity. The standard method for modelling the structures that these surveys will observe is to use cosmological perturbation theory for linear structures on horizon-sized scales, and Newtonian gravity for non-linear structures on much smaller scales. We propose a two-parameter formalism that generalizes this approach, thereby allowing interactions between large and small scales to be studied in a self-consistent and well-defined way. This uses both post-Newtonian gravity and cosmological perturbation theory, and can be used to model realistic cosmological scenarios including matter, radiation and a cosmological constant. We find that the resulting field equations can be written as a hierarchical set of perturbation equations. At leading-order, these equations allow us to recover a standard set of Friedmann equations, as well as a Newton-Poisson equation for the inhomogeneous part of the Newtonian energy density in an expanding background. For the perturbations in the large-scale cosmology, however, we find that the field equations are sourced by both non-linear and mode-mixing terms, due to the existence of small-scale structures. These extra terms should be expected to give rise to new gravitational effects, through the mixing of gravitational modes on small and large scales - effects that are beyond the scope of standard linear cosmological perturbation theory. We expect our formalism to be useful for accurately modelling gravitational physics in universes that contain non-linear structures, and for investigating the effects of non-linear gravity in the era of ultra-large-scale surveys."
What are the challenges of implementing runtime enforcement strategies in software applications?,38cdcdcd,"[{'abstract': '  Runtime enforcement can be effectively used to improve the reliability of\nsoftware applications. However, it often requires the definition of ad hoc\npolicies and enforcement strategies, which might be expensive to identify and\nimplement. This paper discusses how to exploit lifecycle events to obtain\nuseful enforcement strategies that can be easily reused across applications,\nthus reducing the cost of adoption of the runtime enforcement technology. The\npaper finally sketches how this idea can be used to define libraries that can\nautomatically overcome problems related to applications misusing them.\n', 'title': 'Increasing the Reusability of Enforcers with Lifecycle Events'}]","After reviewing the provided abstracts, I think I have found the relevant answer:

Title: Increasing the Reusability of Enforcers with Lifecycle Events

Abstract: Runtime enforcement can be effectively used to improve the reliability of software applications. However, it often requires the definition of ad hoc policies and enforcement strategies, which might be expensive to identify and implement. This paper discusses how to exploit lifecycle events to obtain useful enforcement strategies that can be easily reused across applications, thus reducing the cost of adoption of the runtime enforcement technology. The paper finally sketches how this idea can be used to define libraries that can automatically overcome problems related to applications misusing them.

The challenge mentioned in this abstract is that runtime enforcement strategies require the definition of ad hoc policies and enforcement strategies, which might be expensive to identify and implement."
What is the impact of the dimension and size of the data on the spectrum of random feature maps?,7fbd7879,"[{'abstract': '  Random feature maps are ubiquitous in modern statistical machine learning,\nwhere they generalize random projections by means of powerful, yet often\ndifficult to analyze nonlinear operators. In this paper, we leverage the\n""concentration"" phenomenon induced by random matrix theory to perform a\nspectral analysis on the Gram matrix of these random feature maps, here for\nGaussian mixture models of simultaneously large dimension and size. Our results\nare instrumental to a deeper understanding on the interplay of the nonlinearity\nand the statistics of the data, thereby allowing for a better tuning of random\nfeature-based techniques.\n', 'title': 'On the Spectrum of Random Features Maps of High Dimensional Data'}]","Based on the provided context, I found a relevant paper that addresses the question about the impact of the dimension and size of the data on the spectrum of random feature maps.

Title: On the Spectrum of Random Features Maps of High Dimensional Data

Abstract: Random feature maps are ubiquitous in modern statistical machine learning, where they generalize random projections by means of powerful, yet often difficult to analyze nonlinear operators. In this paper, we leverage the ""concentration"" phenomenon induced by random matrix theory to perform a spectral analysis on the Gram matrix of these random feature maps, here for Gaussian mixture models of simultaneously large dimension and size. Our results are instrumental to a deeper understanding on the interplay of the nonlinearity and the statistics of the data, thereby allowing for a better tuning of random feature-based techniques."
How can the method of Total Conductance be implemented efficiently using standard gradient operators?,e9b9395e,"[{'abstract': '  The challenge of assigning importance to individual neurons in a network is\nof interest when interpreting deep learning models. In recent work, Dhamdhere\net al. proposed Total Conductance, a ""natural refinement of Integrated\nGradients"" for attributing importance to internal neurons. Unfortunately, the\nauthors found that calculating conductance in tensorflow required the addition\nof several custom gradient operators and did not scale well. In this work, we\nshow that the formula for Total Conductance is mathematically equivalent to\nPath Integrated Gradients computed on a hidden layer in the network. We provide\na scalable implementation of Total Conductance using standard tensorflow\ngradient operators that we call Neuron Integrated Gradients. We compare Neuron\nIntegrated Gradients to DeepLIFT, a pre-existing computationally efficient\napproach that is applicable to calculating internal neuron importance. We find\nthat DeepLIFT produces strong empirical results and is faster to compute, but\nbecause it lacks the theoretical properties of Neuron Integrated Gradients, it\nmay not always be preferred in practice. Colab notebook reproducing results:\nthis http URL\n', 'title': 'Computationally Efficient Measures of Internal Neuron Importance'}]","Here is the answer to your question:

Title: Computationally Efficient Measures of Internal Neuron Importance

Abstract: The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a ""natural refinement of Integrated Gradients"" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: this http URL"
Which algorithm for splittable multicommodity flow routing improves existing solutions in terms of runtime and performance?,d5423c14,"[{'abstract': ""  We present novel oblivious routing algorithms for both splittable and\nunsplittable multicommodity flow. Our algorithm for minimizing congestion for\n\\emph{unsplittable} multicommodity flow is the first oblivious routing\nalgorithm for this setting. As an intermediate step towards this algorithm, we\npresent a novel generalization of Valiant's classical load balancing scheme for\npacket-switched networks to arbitrary graphs, which is of independent interest.\nOur algorithm for minimizing congestion for \\emph{splittable} multicommodity\nflow improves upon the state-of-the-art, in terms of both running time and\nperformance, for graphs that exhibit good expansion guarantees. Our algorithms\nrely on diffusing traffic via iterative applications of the random walk\noperator. Consequently, the performance guarantees of our algorithms are\nderived from the convergence of the random walk operator to the stationary\ndistribution and are expressed in terms of the spectral gap of the graph (which\ndominates the mixing time).\n"", 'title': 'Oblivious Routing via Random Walks'}]","Title: Oblivious Routing via Random Walks

Abstract:  We present novel oblivious routing algorithms for both splittable and
unsplittable multicommodity flow. Our algorithm for minimizing congestion for
\emph{unsplittable} multicommodity flow is the first oblivious routing
algorithm for this setting. As an intermediate step towards this algorithm, we
present a novel generalization of Valiant's classical load balancing scheme for
packet-switched networks to arbitrary graphs, which is of independent interest.
Our algorithm for minimizing congestion for \emph{splittable} multicommodity
flow improves upon the state-of-the-art, in terms of both running time and
performance, for graphs that exhibit good expansion guarantees. Our algorithms
rely on diffusing traffic via iterative applications of the random walk
operator. Consequently, the performance guarantees of our algorithms are
derived from the convergence of the random walk operator to the stationary
distribution and are expressed in terms of the spectral gap of the graph (which
dominates the mixing time)."
What mathematical techniques are used to demonstrate the connection between compressible Navier-Stokes equations with a growth term and a two-phase free boundary fluid system? ,e4a10022,"[{'abstract': '  Starting from isentropic compressible Navier-Stokes equations with growth\nterm in the continuity equation, we rigorously justify that performing an\nincompressible limit one arrives to the two-phase free boundary fluid system.\n', 'title': 'Incompressible limit of the Navier-Stokes model with a growth term'}]","Based on the context, I think I can help you find the answer.

Title: Incompressible limit of the Navier-Stokes model with a growth term

Abstract: Starting from isentropic compressible Navier-Stokes equations with growth term in the continuity equation, we rigorously justify that performing an incompressible limit one arrives to the two-phase free boundary fluid system.

Unfortunately, the abstract does not explicitly mention the mathematical techniques used to demonstrate the connection between compressible Navier-Stokes equations with a growth term and a two-phase free boundary fluid system. The abstract only states that the connection is rigorously justified, but it does not provide further details."
why are one-dimensional models sufficient for FTE CDI cell modeling,47f6fb50,"[{'abstract': '  Capacitive deionization (CDI) is a fast-emerging water desalination\ntechnology in which a small cell voltage of ~1 V across porous carbon\nelectrodes removes salt from feedwaters via electrosorption. In flow-through\nelectrode (FTE) CDI cell architecture, feedwater is pumped through macropores\nor laser perforated channels in porous electrodes, enabling highly compact\ncells with parallel flow and electric field, as well as rapid salt removal. We\nhere present a one-dimensional model describing water desalination by FTE CDI,\nand a comparison to data from a custom-built experimental cell. The model\nemploys simple cell boundary conditions derived via scaling arguments. We show\ngood model-to-data fits with reasonable values for fitting parameters such as\nthe Stern layer capacitance, micropore volume, and attraction energy. Thus, we\ndemonstrate that from an engineering modeling perspective, an FTE CDI cell may\nbe described with simpler one-dimensional models, unlike more typical\nflow-between electrodes architecture where 2D models are required.\n', 'title': 'A one-dimensional model for water desalination by flow-through electrode capacitive deionization'}]","Title: A one-dimensional model for water desalination by flow-through electrode capacitive deionization

Abstract: Capacitive deionization (CDI) is a fast-emerging water desalination technology in which a small cell voltage of ~1 V across porous carbon electrodes removes salt from feedwaters via electrosorption. In flow-through electrode (FTE) CDI cell architecture, feedwater is pumped through macropores or laser perforated channels in porous electrodes, enabling highly compact cells with parallel flow and electric field, as well as rapid salt removal. We here present a one-dimensional model describing water desalination by FTE CDI, and a comparison to data from a custom-built experimental cell. The model employs simple cell boundary conditions derived via scaling arguments. We show good model-to-data fits with reasonable values for fitting parameters such as the Stern layer capacitance, micropore volume, and attraction energy. Thus, we demonstrate that from an engineering modeling perspective, an FTE CDI cell may be described with simpler one-dimensional models, unlike more typical flow-between electrodes architecture where 2D models are required."
What are some models of concurrency that have been encoded in the CHR formalism? ,31e01f2d,"[{'abstract': '  Constraint Handling Rules is an effective concurrent declarative programming\nlanguage and a versatile computational logic formalism. CHR programs consist of\nguarded reactive rules that transform multisets of constraints. One of the main\nfeatures of CHR is its inherent concurrency. Intuitively, rules can be applied\nto parts of a multiset in parallel. In this comprehensive survey, we give an\noverview of concurrent and parallel as well as distributed CHR semantics,\nstandard and more exotic, that have been proposed over the years at various\nlevels of refinement. These semantics range from the abstract to the concrete.\nThey are related by formal soundness results. Their correctness is established\nas correspondence between parallel and sequential computations. We present\ncommon concise sample CHR programs that have been widely used in experiments\nand benchmarks. We review parallel CHR implementations in software and\nhardware. The experimental results obtained show a consistent parallel speedup.\nMost implementations are available online. The CHR formalism can also be used\nto implement and reason with models for concurrency. To this end, the Software\nTransaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus\nhave been faithfully encoded in CHR. Under consideration in Theory and Practice\nof Logic Programming (TPLP).\n', 'title': 'Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey'}]","Based on the context, the answer to the question is:

Title: Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey

Abstract: Constraint Handling Rules is an effective concurrent declarative programming language and a versatile computational logic formalism. CHR programs consist of guarded reactive rules that transform multisets of constraints. One of the main features of CHR is its inherent concurrency. Intuitively, rules can be applied to parts of a multiset in parallel. In this comprehensive survey, we give an overview of concurrent and parallel as well as distributed CHR semantics, standard and more exotic, that have been proposed over the years at various levels of refinement. These semantics range from the abstract to the concrete. They are related by formal soundness results. Their correctness is established as correspondence between parallel and sequential computations. We present common concise sample CHR programs that have been widely used in experiments and benchmarks. We review parallel CHR implementations in software and hardware. The experimental results obtained show a consistent parallel speedup. Most implementations are available online. The CHR formalism can also be used to implement and reason with models for concurrency. To this end, the Software Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus have been faithfully encoded in CHR. Under consideration in Theory and Practice of Logic Programming (TPLP)."
What are the drawbacks of using the backpropagation algorithm to train deep neural networks?,9e6a51dd,"[{'abstract': '  Training a neural network using backpropagation algorithm requires passing\nerror gradients sequentially through the network. The backward locking prevents\nus from updating network layers in parallel and fully leveraging the computing\nresources. Recently, there are several works trying to decouple and parallelize\nthe backpropagation algorithm. However, all of them suffer from severe accuracy\nloss or memory explosion when the neural network is deep. To address these\nchallenging issues, we propose a novel parallel-objective formulation for the\nobjective function of the neural network. After that, we introduce features\nreplay algorithm and prove that it is guaranteed to converge to critical points\nfor the non-convex problem under certain conditions. Finally, we apply our\nmethod to training deep convolutional neural networks, and the experimental\nresults show that the proposed method achieves {faster} convergence, {lower}\nmemory consumption, and {better} generalization error than compared methods.\n', 'title': 'Training Neural Networks Using Features Replay'}]","The drawback of using the backpropagation algorithm to train deep neural networks is that it prevents us from updating network layers in parallel and fully leveraging the computing resources, leading to severe accuracy loss or memory explosion when the neural network is deep.

Title: Training Neural Networks Using Features Replay

Abstract: Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network. The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources. Recently, there are several works trying to decouple and parallelize the backpropagation algorithm. However, all of them suffer from severe accuracy loss or memory explosion when the neural network is deep. To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network. After that, we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally, we apply our method to training deep convolutional neural networks, and the experimental results show that the proposed method achieves faster convergence, lower memory consumption, and better generalization error than compared methods."
What two novel techniques are proposed in this paper to enhance the performance of kernel acoustic models?,db01d270,"[{'abstract': '  We study large-scale kernel methods for acoustic modeling in speech\nrecognition and compare their performance to deep neural networks (DNNs). We\nperform experiments on four speech recognition datasets, including the TIMIT\nand Broadcast News benchmark tasks, and compare these two types of models on\nframe-level performance metrics (accuracy, cross-entropy), as well as on\nrecognition metrics (word/character error rate). In order to scale kernel\nmethods to these large datasets, we use the random Fourier feature method of\nRahimi and Recht (2007). We propose two novel techniques for improving the\nperformance of kernel acoustic models. First, in order to reduce the number of\nrandom features required by kernel models, we propose a simple but effective\nmethod for feature selection. The method is able to explore a large number of\nnon-linear features while maintaining a compact model more efficiently than\nexisting approaches. Second, we present a number of frame-level metrics which\ncorrelate very strongly with recognition performance when computed on the\nheldout set; we take advantage of these correlations by monitoring these\nmetrics during training in order to decide when to stop learning. This\ntechnique can noticeably improve the recognition performance of both DNN and\nkernel models, while narrowing the gap between them. Additionally, we show that\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\nof our kernel models significantly, in addition to speeding up training and\nmaking the models more compact. Together, these three methods dramatically\nimprove the performance of kernel acoustic models, making their performance\ncomparable to DNNs on the tasks we explored.\n', 'title': 'Kernel Approximation Methods for Speech Recognition'}]","Title: Kernel Approximation Methods for Speech Recognition

Abstract: We study large-scale kernel methods for acoustic modeling in speech recognition and compare their performance to deep neural networks (DNNs). We perform experiments on four speech recognition datasets, including the TIMIT and Broadcast News benchmark tasks, and compare these two types of models on frame-level performance metrics (accuracy, cross-entropy), as well as on recognition metrics (word/character error rate). In order to scale kernel methods to these large datasets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, in order to reduce the number of random features required by kernel models, we propose a simple but effective method for feature selection. The method is able to explore a large number of non-linear features while maintaining a compact model more efficiently than existing approaches. Second, we present a number of frame-level metrics which correlate very strongly with recognition performance when computed on the heldout set; we take advantage of these correlations by monitoring these metrics during training in order to decide when to stop learning.

The two novel techniques proposed in this paper to enhance the performance of kernel acoustic models are:

1. A simple but effective method for feature selection to reduce the number of random features required by kernel models.
2. A method that uses frame-level metrics to correlate strongly with recognition performance when computed on the heldout set, and monitoring these metrics during training to decide when to stop learning."
What algorithm is proposed in this paper that addresses a publicly acknowledged challenge in online multiclass prediction?,936c2387,"[{'abstract': '  We present an efficient second-order algorithm with\n$\\tilde{O}(\\frac{1}{\\eta}\\sqrt{T})$ regret for the bandit online multiclass\nproblem. The regret bound holds simultaneously with respect to a family of loss\nfunctions parameterized by $\\eta$, for a range of $\\eta$ restricted by the norm\nof the competitor. The family of loss functions ranges from hinge loss\n($\\eta=0$) to squared hinge loss ($\\eta=1$). This provides a solution to the\nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for\n$\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our\nalgorithm experimentally, showing that it also performs favorably against\nearlier algorithms.\n', 'title': 'Efficient Online Bandit Multiclass Learning with $\\tilde{O}(\\sqrt{T})$ Regret'}]","I couldn't find a paper that directly proposes an algorithm for online multiclass prediction. However, I found a paper that proposes a novel approach to probabilistic forecasting for pedestrians, which could be related to multiclass prediction.

Title: Technical Report for Real-Time Certified Probabilistic Pedestrian Forecasting

Abstract: The success of autonomous systems will depend upon their ability to safely navigate human-centric environments. This motivates the need for a real-time, probabilistic forecasting algorithm for pedestrians, cyclists, and other agents since these predictions will form a necessary step in assessing the risk of any action. This paper presents a novel approach to probabilistic forecasting for pedestrians based on weighted sums of ordinary differential equations that are learned from historical trajectory information within a fixed scene. The resulting algorithm is embarrassingly parallel and is able to work at real-time speeds using a naive Python implementation. The quality of predicted locations of agents generated by the proposed algorithm is validated on a variety of examples and considerably higher than existing state of the art approaches over long time horizons."
What are the system-wide regret guarantees for algorithms designed for multi-user spectrum access in stochastic environments?,9e452d96,"[{'abstract': '  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms\nfor uncoordinated spectrum access. The number of users is assumed to be unknown\nto each user. A stochastic setting is first considered, where the rewards on a\nchannel are the same for each user. In contrast to prior work, it is assumed\nthat the number of users can possibly exceed the number of channels, and that\nrewards can be non-zero even under collisions. The proposed algorithm consists\nof an estimation phase and an allocation phase. It is shown that if every user\nadopts the algorithm, the system wide regret is constant with time with high\nprobability. The regret guarantees hold for any number of users and channels,\nin particular, even when the number of users is less than the number of\nchannels. Next, an adversarial multi-user MAB framework is considered, where\nthe rewards on the channels are user-dependent. It is assumed that the number\nof users is less than the number of channels, and that the users receive zero\nreward on collision. The proposed algorithm combines the Exp3.P algorithm\ndeveloped in prior work for single user adversarial bandits with a collision\nresolution mechanism to achieve sub-linear regret. It is shown that if every\nuser employs the proposed algorithm, the system wide regret is of the order\n$O(T^\\frac{3}{4})$ over a horizon of time $T$. The algorithms in both\nstochastic and adversarial scenarios are extended to the dynamic case where the\nnumber of users in the system evolves over time and are shown to lead to\nsub-linear regret.\n', 'title': 'Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access'}]","Title: Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access

Abstract: A multi-user multi-armed bandit (MAB) framework is used to develop algorithms for uncoordinated spectrum access. The number of users is assumed to be unknown to each user. A stochastic setting is first considered, where the rewards on a channel are the same for each user. In contrast to prior work, it is assumed that the number of users can possibly exceed the number of channels, and that rewards can be non-zero even under collisions. The proposed algorithm consists of an estimation phase and an allocation phase. It is shown that if every user adopts the algorithm, the system wide regret is constant with time with high probability. The regret guarantees hold for any number of users and channels, in particular, even when the number of users is less than the number of channels. Next, an adversarial multi-user MAB framework is considered, where the rewards on the channels are user-dependent. It is assumed that the number of users is less than the number of channels, and that the users receive zero reward on collision. The proposed algorithm combines the Exp3.P algorithm developed in prior work for single user adversarial bandits with a collision resolution mechanism to achieve sub-linear regret. It is shown that if every user employs the proposed algorithm, the system wide regret is of the order $O(T^\frac{3}{4})$ over a horizon of time $T$. The algorithms in both stochastic and adversarial scenarios are extended to the dynamic case where the number of users in the system evolves over time and are shown to lead to sub-linear regret."
Which cellular process was found to be regulated by the miR-199/miR-214 microRNA cluster in relation to triple-negative breast cancer?,1fcbaa77,"[{'abstract': '  MicroRNAs play important roles in many biological processes. Their aberrant\nexpression can have oncogenic or tumor suppressor function directly\nparticipating to carcinogenesis, malignant transformation, invasiveness and\nmetastasis. Indeed, miRNA profiles can distinguish not only between normal and\ncancerous tissue but they can also successfully classify different subtypes of\na particular cancer. Here, we focus on a particular class of transcripts\nencoding polycistronic miRNA genes that yields multiple miRNA components. We\ndescribe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully\nredesigned release of the MMRA computational pipeline (MiRNA Master Regulator\nAnalysis), developed to search for clustered miRNAs potentially driving cancer\nmolecular subtyping. Genomically clustered miRNAs are frequently co-expressed\nto target different components of pro-tumorigenic signalling pathways. By\napplying ClustMMRA to breast cancer patient data, we identified key miRNA\nclusters driving the phenotype of different tumor subgroups. The pipeline was\napplied to two independent breast cancer datasets, providing statistically\nconcordant results between the two analysis. We validated in cell lines the\nmiR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative\nsubtype phenotype through its control of proliferation and EMT.\n', 'title': 'Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer'}]","Title: Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer

Abstract: MicroRNAs play important roles in many biological processes. Their aberrant expression can have oncogenic or tumor suppressor function directly participating to carcinogenesis, malignant transformation, invasiveness and metastasis. Indeed, miRNA profiles can distinguish not only between normal and cancerous tissue but they can also successfully classify different subtypes of a particular cancer. Here, we focus on a particular class of transcripts encoding polycistronic miRNA genes that yields multiple miRNA components. We describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully redesigned release of the MMRA computational pipeline (MiRNA Master Regulator Analysis), developed to search for clustered miRNAs potentially driving cancer molecular subtyping. Genomically clustered miRNAs are frequently co-expressed to target different components of pro-tumorigenic signalling pathways. By applying ClustMMRA to breast cancer patient data, we identified key miRNA clusters driving the phenotype of different tumor subgroups. The pipeline was applied to two independent breast cancer datasets, providing statistically concordant results between the two analysis. We validated in cell lines the miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative subtype phenotype through its control of proliferation and EMT.

The cellular process found to be regulated by the miR-199/miR-214 microRNA cluster in relation to triple-negative breast cancer is Epithelial to Mesenchymal Transition (EMT)."
What is the dependence of the Calabi flow's existence time on the initial data and background metric?,a2c4eaac,"[{'abstract': '  In this paper, we prove that there exists a dimensional constant $\\delta > 0$\nsuch that given any background Kähler metric $\\omega$, the Calabi flow with\ninitial data $u_0$ satisfying \\begin{equation*} \\partial \\bar \\partial u_0 \\in\nL^\\infty (M) \\text{ and } (1- \\delta )\\omega < \\omega_{u_0} < (1+\\delta\n)\\omega, \\end{equation*} admits a unique short time solution and it becomes\nsmooth immediately, where $\\omega_{u_0} : = \\omega +\\sqrt{-1}\\partial\n\\bar\\partial u_0$. The existence time depends on initial data $u_0$ and the\nmetric $\\omega$. As a corollary, we get that Calabi flow has short time\nexistence for any initial data satisfying \\begin{equation*} \\partial \\bar\n\\partial u_0 \\in C^0(M) \\text{ and } \\omega_{u_0} > 0, \\end{equation*} which\nshould be interpreted as a ""continuous Kähler metric"". A main technical\ningredient is Schauder-type estimates for biharmonic heat equation on\nRiemannian manifolds with time weighted Hölder norms.\n', 'title': 'The Calabi flow with rough initial data'}]","Title: The Calabi flow with rough initial data

Abstract:   In this paper, we prove that there exists a dimensional constant $\delta > 0$
such that given any background Kähler metric $\omega$, the Calabi flow with
initial data $u_0$ satisfying \begin{equation*} \partial \bar \partial u_0 \in
L^\infty (M) \text{ and } (1- \delta )\omega < \omega_{u_0} < (1+\delta
)\omega, \end{equation*} admits a unique short time solution and it becomes
smooth immediately, where $\omega_{u_0} : = \omega +\sqrt{-1}\partial
\bar\partial u_0$. The existence time depends on initial data $u_0$ and the
metric $\omega$. As a corollary, we get that Calabi flow has short time
existence for any initial data satisfying \begin{equation*} \partial \bar
\partial u_0 \in C^0(M) \text{ and } \omega_{u_0} > 0, \end{equation*} which
should be interpreted as a ""continuous Kähler metric"". A main technical
ingredient is Schauder-type estimates for biharmonic heat equation on
Riemannian manifolds with time weighted Hölder norms."
What is the impact of different feature selection methods on the performance of a XGBoost model for business risk classification?,b14f69d3,"[{'abstract': '  This paper aims to explore models based on the extreme gradient boosting\n(XGBoost) approach for business risk classification. Feature selection (FS)\nalgorithms and hyper-parameter optimizations are simultaneously considered\nduring model training. The five most commonly used FS methods including weight\nby Gini, weight by Chi-square, hierarchical variable clustering, weight by\ncorrelation, and weight by information are applied to alleviate the effect of\nredundant features. Two hyper-parameter optimization approaches, random search\n(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in\nXGBoost. The effect of different FS and hyper-parameter optimization methods on\nthe model performance are investigated by the Wilcoxon Signed Rank Test. The\nperformance of XGBoost is compared to the traditionally utilized logistic\nregression (LR) model in terms of classification accuracy, area under the curve\n(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results\nshow that hierarchical clustering is the optimal FS method for LR while weight\nby Chi-square achieves the best performance in XG-Boost. Both TPE and RS\noptimization in XGBoost outperform LR significantly. TPE optimization shows a\nsuperiority over RS since it results in a significantly higher accuracy and a\nmarginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE\ntuning shows a lower variability than the RS method. Finally, the ranking of\nfeature importance based on XGBoost enhances the model interpretation.\nTherefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an\noperative while powerful approach for business risk modeling.\n', 'title': 'A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization'}]","Title: A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization

Abstract:   This paper aims to explore models based on the extreme gradient boosting
(XGBoost) approach for business risk classification. Feature selection (FS)
algorithms and hyper-parameter optimizations are simultaneously considered
during model training. The five most commonly used FS methods including weight
by Gini, weight by Chi-square, hierarchical variable clustering, weight by
correlation, and weight by information are applied to alleviate the effect of
redundant features. Two hyper-parameter optimization approaches, random search
(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
XGBoost. The effect of different FS and hyper-parameter optimization methods on
the model performance are investigated by the Wilcoxon Signed Rank Test. The
performance of XGBoost is compared to the traditionally utilized logistic
regression (LR) model in terms of classification accuracy, area under the curve
(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
show that hierarchical clustering is the optimal FS method for LR while weight
by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
optimization in XGBoost outperform LR significantly. TPE optimization shows a
superiority over RS since it results in a significantly higher accuracy and a
marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
tuning shows a lower variability than the RS method. Finally, the ranking of
feature importance based on XGBoost enhances the model interpretation.
Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
operative while powerful approach for business risk modeling."
What type of asymmetry is observed in the mixing layer during the evolution of a rotating Rayleigh-Taylor system?,15cc0c0d,"[{'abstract': '  The turbulent Rayleigh--Taylor system in a rotating reference frame is\ninvestigated by direct numerical simulations within the Oberbeck-Boussinesq\napproximation. On the basis of theoretical arguments, supported by our\nsimulations, we show that the Rossby number decreases in time, and therefore\nthe Coriolis force becomes more important as the system evolves and produces\nmany effects on Rayleigh--Taylor turbulence. We find that rotation reduces the\nintensity of turbulent velocity fluctuations and therefore the growth rate of\nthe temperature mixing layer. Moreover, in presence of rotation the conversion\nof potential energy into turbulent kinetic energy is found to be less effective\nand the efficiency of the heat transfer is reduced. Finally, during the\nevolution of the mixing layer we observe the development of a\ncyclone-anticyclone asymmetry.\n', 'title': 'Rotating Rayleigh-Taylor turbulence'}]","Title: Rotating Rayleigh-Taylor turbulence

Abstract: The turbulent Rayleigh--Taylor system in a rotating reference frame is investigated by direct numerical simulations within the Oberbeck-Boussinesq approximation. On the basis of theoretical arguments, supported by our simulations, we show that the Rossby number decreases in time, and therefore the Coriolis force becomes more important as the system evolves and produces many effects on Rayleigh--Taylor turbulence. We find that rotation reduces the intensity of turbulent velocity fluctuations and therefore the growth rate of the temperature mixing layer. Moreover, in presence of rotation the conversion of potential energy into turbulent kinetic energy is found to be less effective and the efficiency of the heat transfer is reduced. Finally, during the evolution of the mixing layer we observe the development of a cyclone-anticyclone asymmetry."
What are the implications of identifying multiple core-periphery pairs in an interbank market?,4f903d8a,"[{'abstract': '  Interbank markets are often characterised in terms of a core-periphery\nnetwork structure, with a highly interconnected core of banks holding the\nmarket together, and a periphery of banks connected mostly to the core but not\ninternally. This paradigm has recently been challenged for short time scales,\nwhere interbank markets seem better characterised by a bipartite structure with\nmore core-periphery connections than inside the core. Using a novel\ncore-periphery detection method on the eMID interbank market, we enrich this\npicture by showing that the network is actually characterised by multiple\ncore-periphery pairs. Moreover, a transition from core-periphery to bipartite\nstructures occurs by shortening the temporal scale of data aggregation. We\nfurther show how the global financial crisis transformed the market, in terms\nof composition, multiplicity and internal organisation of core-periphery pairs.\nBy unveiling such a fine-grained organisation and transformation of the\ninterbank market, our method can find important applications in the\nunderstanding of how distress can propagate over financial networks.\n', 'title': 'Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis'}]","Title: Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis

Abstract: Interbank markets are often characterised in terms of a core-periphery network structure, with a highly interconnected core of banks holding the market together, and a periphery of banks connected mostly to the core but not internally. This paradigm has recently been challenged for short time scales, where interbank markets seem better characterised by a bipartite structure with more core-periphery connections than inside the core. Using a novel core-periphery detection method on the eMID interbank market, we enrich this picture by showing that the network is actually characterised by multiple core-periphery pairs. Moreover, a transition from core-periphery to bipartite structures occurs by shortening the temporal scale of data aggregation. We further show how the global financial crisis transformed the market, in terms of composition, multiplicity and internal organisation of core-periphery pairs. By unveiling such a fine-grained organisation and transformation of the interbank market, our method can find important applications in the understanding of how distress can propagate over financial networks."
Is there evidence that money plays a significant role in determining election results in Brazil? ,892c1468,"[{'abstract': ""  The principle of democracy is that the people govern through elected\nrepresentatives. Therefore, a democracy is healthy as long as the elected\npoliticians do represent the people. We have analyzed data from the Brazilian\nelectoral court (Tribunal Superior Eleitoral, TSE) concerning money donations\nfor the electoral campaigns and the election results. Our work points to two\ndisturbing conclusions: money is a determining factor on whether a candidate is\nelected or not (as opposed to representativeness); secondly, the use of\nBenford's Law to analyze the declared donations received by the parties and\nelectoral campaigns shows evidence of fraud in the declarations. A better term\nto define Brazil's government system is what we define as chrimatocracy (govern\nby money).\n"", 'title': ""Evidence of Fraud in Brazil's Electoral Campaigns Via the Benford's Law""}]","Title: Evidence of Fraud in Brazil's Electoral Campaigns Via the Benford's Law

Abstract: The principle of democracy is that the people govern through elected representatives. Therefore, a democracy is healthy as long as the elected politicians do represent the people. We have analyzed data from the Brazilian electoral court (Tribunal Superior Eleitoral, TSE) concerning money donations for the electoral campaigns and the election results. Our work points to two disturbing conclusions: money is a determining factor on whether a candidate is elected or not (as opposed to representativeness); secondly, the use of Benford's Law to analyze the declared donations received by the parties and electoral campaigns shows evidence of fraud in the declarations. A better term to define Brazil's government system is what we define as chrimatocracy (govern by money)."
What is the role of mean square distance calculations in metadynamics?,ae89eb39,"[{'abstract': '  Molecular dynamics simulates the~movements of atoms. Due to its high cost,\nmany methods have been developed to ""push the~simulation forward"". One of them,\nmetadynamics, can hasten the~molecular dynamics with the~help of variables\ndescribing the~simulated process. However, the~evaluation of these variables\ncan include numerous mean square distance calculations that introduce\nsubstantial computational demands, thus jeopardize the~benefit of the~approach.\nRecently, we proposed an~approximative method that significantly reduces\nthe~number of these distance calculations. Here we evaluate the~performance and\nthe~scalability on two molecular systems. We assess the~maximal theoretical\nspeed-up based on the reduction of distance computations and Ahmdal\'s law and\ncompare it to the~practical speed-up achieved with our implementation.\n', 'title': 'Acceleration of Mean Square Distance Calculations with Floating Close Structure in Metadynamics Simulations'}]","I think I can help you with that!

Title: Acceleration of Mean Square Distance Calculations with Floating Close Structure in Metadynamics Simulations

Abstract: Molecular dynamics simulates the movements of atoms. Due to its high cost, many methods have been developed to ""push the simulation forward"". One of them, metadynamics, can hasten the molecular dynamics with the help of variables describing the simulated process. However, the evaluation of these variables can include numerous mean square distance calculations that introduce substantial computational demands, thus jeopardize the benefit of the approach. Recently, we proposed an approximative method that significantly reduces the number of these distance calculations. Here we evaluate the performance and the scalability on two molecular systems. We assess the maximal theoretical speed-up based on the reduction of distance computations and Ahmdal's law and compare it to the practical speed-up achieved with our implementation.

According to this paper, mean square distance calculations play a crucial role in metadynamics simulations, as they are a necessary step in evaluating variables that describe the simulated process. However, these calculations introduce substantial computational demands, which can slow down the simulation."
What are the challenges of applying Pareto Local Search to problems with more than two objectives?,6dac50d5,"[{'abstract': '  We propose a new Pareto Local Search Algorithm for the many-objective\ncombinatorial optimization. Pareto Local Search proved to be a very effective\ntool in the case of the bi-objective combinatorial optimization and it was used\nin a number of the state-of-the-art algorithms for problems of this kind. On\nthe other hand, the standard Pareto Local Search algorithm becomes very\ninefficient for problems with more than two objectives. We build an effective\nMany-Objective Pareto Local Search algorithm using three new mechanisms: the\nefficient update of large Pareto archives with ND-Tree data structure, a new\nmechanism for the selection of the promising solutions for the neighborhood\nexploration, and a partial exploration of the neighborhoods. We apply the\nproposed algorithm to the instances of two different problems, i.e. the\ntraveling salesperson problem and the traveling salesperson problem with\nprofits with up to 5 objectives showing high effectiveness of the proposed\nalgorithm.\n', 'title': 'Many-Objective Pareto Local Search'}]","Based on the provided context, I found an answer to your question.

Title: Many-Objective Pareto Local Search

Abstract:  We propose a new Pareto Local Search Algorithm for the many-objective
combinatorial optimization. Pareto Local Search proved to be a very effective
tool in the case of the bi-objective combinatorial optimization and it was used
in a number of the state-of-the-art algorithms for problems of this kind. On
the other hand, the standard Pareto Local Search algorithm becomes very
inefficient for problems with more than two objectives. We build an effective
Many-Objective Pareto Local Search algorithm using three new mechanisms: the
efficient update of large Pareto archives with ND-Tree data structure, a new
mechanism for the selection of the promising solutions for the neighborhood
exploration, and a partial exploration of the neighborhoods. We apply the
proposed algorithm to the instances of two different problems, i.e. the
traveling salesperson problem and the traveling salesperson problem with
profits with up to 5 objectives showing high effectiveness of the proposed
algorithm.

The challenge of applying Pareto Local Search to problems with more than two objectives is that the standard Pareto Local Search algorithm becomes very inefficient."
What is a hybrid algorithm that combines the Ensemble Kalman Particle Filter (EnKF) and the Ensemble Transform Kalman Filter (ETKF)?,f1255665,"[{'abstract': '  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)\nare a key component of probabilistic weather forecasting. They represent the\nuncertainty in the initial conditions by an ensemble which incorporates\ninformation coming from the physical model with the latest observations.\nHigh-resolution numerical weather prediction models ran at operational centers\nare able to resolve non-linear and non-Gaussian physical phenomena such as\nconvection. There is therefore a growing need to develop ensemble assimilation\nalgorithms able to deal with non-Gaussianity while staying computationally\nfeasible. In the present paper we address some of these needs by proposing a\nnew hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully\nformulated in ensemble space and uses a deterministic scheme such that it has\nthe ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a\nlimiting case. A new criterion for choosing the proportion of particle filter\nand ETKF update is also proposed. The new algorithm is implemented in the COSMO\nframework and numerical experiments in a quasi-operational convective-scale\nsetup are conducted. The results show the feasibility of the new algorithm in\npractice and indicate a strong potential for such local hybrid methods, in\nparticular for forecasting non-Gaussian variables such as wind and hourly\nprecipitation.\n', 'title': 'A local ensemble transform Kalman particle filter for convective scale data assimilation'}]","I think I can help you find the answer!

Title: A local ensemble transform Kalman particle filter for convective scale data assimilation

Abstract: Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF) are a key component of probabilistic weather forecasting. They represent the uncertainty in the initial conditions by an ensemble which incorporates information coming from the physical model with the latest observations. High-resolution numerical weather prediction models ran at operational centers are able to resolve non-linear and non-Gaussian physical phenomena such as convection. There is therefore a growing need to develop ensemble assimilation algorithms able to deal with non-Gaussianity while staying computationally feasible. In the present paper we address some of these needs by proposing a new hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully formulated in ensemble space and uses a deterministic scheme such that it has the ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a limiting case. A new criterion for choosing the proportion of particle filter and ETKF update is also proposed. The new algorithm is implemented in the COSMO framework and numerical experiments in a quasi-operational convective-scale setup are conducted. The results show the feasibility of the new algorithm in practice and indicate a strong potential for such local hybrid methods, in particular for forecasting non-Gaussian variables such as wind and hourly precipitation."
How do Wavelet Tensor Train (WTT) and Discrete Wavelet Transforms (DWT) perform as feature extraction techniques for FTIR data of medicinal plants?,b6b8595c,"[{'abstract': '  We describe a variant construction of the unstable Adams spectral the\nsequence for a space $Y$, associated to any free simplicial resolution of\n$H^*(Y;R)$ for $R=\\mathbb{F}_p$ or $\\mathbb{Q}$. We use this construction to\ndescribe the differentials and filtration in the spectral sequence in terms of\nappropriate systems of higher cohomology operations.\n', 'title': 'Higher structure in the unstable Adams spectral sequence'}]","Title: Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants

Abstract: Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species were used to explore the influence of preprocessing and feature extraction on efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and Discrete Wavelet Transforms (DWT) were compared as feature extraction techniques for FTIR data of medicinal plants. Various combinations of signal processing steps showed different behavior when applied to classification and clustering tasks. Best results for WTT and DWT found through grid search were similar, significantly improving quality of clustering as well as classification accuracy for tuned logistic regression in comparison to original spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a more versatile and easier to use as a data processing tool in various signal processing applications."
What is a simple method to improve the accuracy of a deep learning classifier after initial training?,ebe4826c,"[{'abstract': '  We found an easy and quick post-learning method named ""Icing on the Cake"" to\nenhance a classification performance in deep learning. The method is that we\ntrain only the final classifier again after an ordinary training is done.\n', 'title': 'Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning'}]","I think I have just the thing for you!

Title: Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning

Abstract: We found an easy and quick post-learning method named ""Icing on the Cake"" to enhance a classification performance in deep learning. The method is that we train only the final classifier again after an ordinary training is done."
What techniques are proposed to address the scalability issue in distributed trust systems for computations?,73ab7d0c,"[{'abstract': '  Large-scale computational experiments, often running over weeks and over\nlarge datasets, are used extensively in fields such as epidemiology,\nmeteorology, computational biology, and healthcare to understand phenomena, and\ndesign high-stakes policies affecting everyday health and economy. For\ninstance, the OpenMalaria framework is a computationally-intensive simulation\nused by various non-governmental and governmental agencies to understand\nmalarial disease spread and effectiveness of intervention strategies, and\nsubsequently design healthcare policies. Given that such shared results form\nthe basis of inferences drawn, technological solutions designed, and day-to-day\npolicies drafted, it is essential that the computations are validated and\ntrusted. In particular, in a multi-agent environment involving several\nindependent computing agents, a notion of trust in results generated by peers\nis critical in facilitating transparency, accountability, and collaboration.\nUsing a novel combination of distributed validation of atomic computation\nblocks and a blockchain-based immutable audits mechanism, this work proposes a\nuniversal framework for distributed trust in computations. In particular we\naddress the scalaibility problem by reducing the storage and communication\ncosts using a lossy compression scheme. This framework guarantees not only\nverifiability of final results, but also the validity of local computations,\nand its cost-benefit tradeoffs are studied using a synthetic example of\ntraining a neural network.\n', 'title': 'Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach'}]","Title: Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach

Abstract:   Large-scale computational experiments, often running over weeks and over
large datasets, are used extensively in fields such as epidemiology,
meteorology, computational biology, and healthcare to understand phenomena, and
design high-stakes policies affecting everyday health and economy. For
instance, the OpenMalaria framework is a computationally-intensive simulation
used by various non-governmental and governmental agencies to understand
malarial disease spread and effectiveness of intervention strategies, and
subsequently design healthcare policies. Given that such shared results form
the basis of inferences drawn, technological solutions designed, and day-to-day
policies drafted, it is essential that the computations are validated and
trusted. In particular, in a multi-agent environment involving several
independent computing agents, a notion of trust in results generated by peers
is critical in facilitating transparency, accountability, and collaboration.
Using a novel combination of distributed validation of atomic computation
blocks and a blockchain-based immutable audits mechanism, this work proposes a
universal framework for distributed trust in computations. In particular we
address the scalaibility problem by reducing the storage and communication
costs using a lossy compression scheme. This framework guarantees not only
verifiability of final results, but also the validity of local computations,
and its cost-benefit tradeoffs are studied using a synthetic example of
training a neural network."
What other evidence suggests the presence of additional planets in the Proxima Centauri system? ,7f3ed9d9,"[{'abstract': '  Proxima Centauri is known as the closest star from the Sun. Recently, radial\nvelocity observations revealed the existence of an Earth-mass planet around it.\nWith an orbital period of ~11 days, the surface of Proxima Centauri b is\ntemperate and might be habitable. We took a photometric monitoring campaign to\nsearch for its transit, using the Bright Star Survey Telescope at the Zhongshan\nStation in Antarctica. A transit-like signal appearing on 2016 September 8th,\nis identified tentatively. Its midtime, $T_{C}=2,457,640.1990\\pm0.0017$ HJD, is\nconsistent with the predicted ephemeris based on RV orbit in a 1$\\sigma$\nconfidence interval. Time-correlated noise is pronounced in the light curve of\nProxima Centauri, affecting detection of transits. We develop a technique, in a\nGaussian process framework, to gauge the statistical significance of potential\ntransit detection. The tentative transit signal reported here, has a confidence\nlevel of $2.5\\sigma$. Further detection of its periodic signals is necessary to\nconfirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima\nCentauri in next Polar night at Dome A in Antarctica, taking the advantage of\ncontinuous darkness. \\citet{Kipping17} reported two tentative transit-like\nsignals of Proxima Centauri b, observed by the Microvariability and Oscillation\nof Stars space Telescope in 2014 and 2015, respectively. The midtransit time of\nour detection is 138 minutes later than that predicted by their transit\nephemeris. If all the signals are real transits, the misalignment of the epochs\nplausibly suggests transit timing variations of Proxima Centauri b induced by\nan outer planet in this system.\n', 'title': 'Searching for the Transit of the Earth--mass exoplanet Proxima~Centauri~b in Antarctica: Preliminary Result'}]","Title: Searching for the Transit of the Earth--mass exoplanet Proxima~Centauri~b in Antarctica: Preliminary Result

Abstract: ... The midtransit time of our detection is 138 minutes later than that predicted by their transit ephemeris. If all the signals are real transits, the misalignment of the epochs plausibly suggests transit timing variations of Proxima Centauri b induced by an outer planet in this system.

This paper suggests the presence of additional planets in the Proxima Centauri system due to the observed transit timing variations of Proxima Centauri b, which could be caused by an outer planet in the system."
What is the name of the new CNN architecture proposed in a paper focusing on rotation invariance?,692d6bcc,"[{'abstract': '  Rotation invariance and translation invariance have great values in image\nrecognition tasks. In this paper, we bring a new architecture in convolutional\nneural network (CNN) named cyclic convolutional layer to achieve rotation\ninvariance in 2-D symbol recognition. We can also get the position and\norientation of the 2-D symbol by the network to achieve detection purpose for\nmultiple non-overlap target. Last but not least, this architecture can achieve\none-shot learning in some cases using those invariance.\n', 'title': 'Rotation Invariance Neural Network'}]","Title: Rotation Invariance Neural Network

Abstract: Rotation invariance and translation invariance have great values in image recognition tasks. In this paper, we bring a new architecture in convolutional neural network (CNN) named cyclic convolutional layer to achieve rotation invariance in 2-D symbol recognition. We can also get the position and orientation of the 2-D symbol by the network to achieve detection purpose for multiple non-overlap target. Last but not least, this architecture can achieve one-shot learning in some cases using those invariance."
Describe a processing core that is compatible with both RISC-V software and the Pulpino hardware platform.,8cb826d0,"[{'abstract': '  Internet-of-Things end-nodes demand low power processing platforms\ncharacterized by heterogeneous dedicated units, controlled by a processor core\nrunning concurrent control threads. Such architecture scheme fits one of the\nmain target application domain of the RISC-V instruction set. We present an\nopen-source processing core compliant with RISC-V on the software side and with\nthe popular Pulpino processor platform on the hardware side, while supporting\ninterleaved multi-threading for IoT applications. The latter feature is a novel\ncontribution in this application domain. We report details about the\nmicroarchitecture design along with performance data.\n', 'title': 'The microarchitecture of a multi-threaded RISC-V compliant processing core family for IoT end-nodes'}]","I think I can help you with that!

Title: The microarchitecture of a multi-threaded RISC-V compliant processing core family for IoT end-nodes

Abstract: Internet-of-Things end-nodes demand low power processing platforms characterized by heterogeneous dedicated units, controlled by a processor core running concurrent control threads. Such architecture scheme fits one of the main target application domain of the RISC-V instruction set. We present an open-source processing core compliant with RISC-V on the software side and with the popular Pulpino processor platform on the hardware side, while supporting interleaved multi-threading for IoT applications. The latter feature is a novel contribution in this application domain. We report details about the microarchitecture design along with performance data."
What condition on the evolution of the qubit observables allows for factorization of an N-time correlator?,99d0743c,"[{'abstract': '  We consider multi-time correlators for output signals from linear detectors,\ncontinuously measuring several qubit observables at the same time. Using the\nquantum Bayesian formalism, we show that for unital (symmetric) evolution in\nthe absence of phase backaction, an $N$-time correlator can be expressed as a\nproduct of two-time correlators when $N$ is even. For odd $N$, there is a\nsimilar factorization, which also includes a single-time average. Theoretical\npredictions agree well with experimental results for two detectors, which\nsimultaneously measure non-commuting qubit observables.\n', 'title': 'Multi-time correlators in continuous measurement of qubit observables'}]","Title: Multi-time correlators in continuous measurement of qubit observables

Abstract: We consider multi-time correlators for output signals from linear detectors, continuously measuring several qubit observables at the same time. Using the quantum Bayesian formalism, we show that for unital (symmetric) evolution in the absence of phase backaction, an $N$-time correlator can be expressed as a product of two-time correlators when $N$ is even. For odd $N$, there is a similar factorization, which also includes a single-time average. Theoretical predictions agree well with experimental results for two detectors, which simultaneously measure non-commuting qubit observables.

The condition on the evolution of the qubit observables that allows for factorization of an $N$-time correlator is unital (symmetric) evolution in the absence of phase backaction."
Describe the numerical technique used to track the moving solid-liquid interface in this lattice Boltzmann method,cc4e1a59,"[{'abstract': '  In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice\nBoltzmann (LB) method is developed for solid-liquid phase change heat transfer\nin metal foams under local thermal non-equilibrium (LTNE) condition. The\nenthalpy-based MRT-LB method consists of three different MRT-LB models: one for\nflow field based on the generalized non-Darcy model, and the other two for\nphase change material (PCM) and metal foam temperature fields described by the\nLTNE model. The moving solid-liquid phase interface is implicitly tracked\nthrough the liquid fraction, which is simultaneously obtained when the energy\nequations of PCM and metal foam are solved. The present method has several\ndistinctive features. First, as compared with previous studies, the present\nmethod avoids the iteration procedure, thus it retains the inherent merits of\nthe standard LB method and is superior over the iteration method in terms of\naccuracy and computational efficiency. Second, a volumetric LB scheme instead\nof the bounce-back scheme is employed to realize the no-slip velocity condition\nin the interface and solid phase regions, which is consistent with the actual\nsituation. Last but not least, the MRT collision model is employed, and with\nadditional degrees of freedom, it has the ability to reduce the numerical\ndiffusion across phase interface induced by solid-liquid phase change.\nNumerical tests demonstrate that the present method can be served as an\naccurate and efficient numerical tool for studying metal foam enhanced\nsolid-liquid phase change heat transfer in latent heat storage. Finally,\ncomparisons and discussions are made to offer useful information for practical\napplications of the present method.\n', 'title': 'An enthalpy-based multiple-relaxation-time lattice Boltzmann method for solid-liquid phase change heat transfer in metal foams'}]","Title: An enthalpy-based multiple-relaxation-time lattice Boltzmann method for solid-liquid phase change heat transfer in metal foams

Abstract: In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice Boltzmann (LB) method is developed for solid-liquid phase change heat transfer in metal foams under local thermal non-equilibrium (LTNE) condition. The enthalpy-based MRT-LB method consists of three different MRT-LB models: one for flow field based on the generalized non-Darcy model, and the other two for phase change material (PCM) and metal foam temperature fields described by the LTNE model. The moving solid-liquid phase interface is implicitly tracked through the liquid fraction, which is simultaneously obtained when the energy equations of PCM and metal foam are solved.

The numerical technique used to track the moving solid-liquid interface in this lattice Boltzmann method is by implicitly tracking the interface through the liquid fraction, which is simultaneously obtained when the energy equations of Phase Change Material (PCM) and metal foam are solved."
What are the potential drawbacks of using a highly expressive inference model in a variational autoencoder?,db99fa73,"[{'abstract': '  The variational autoencoder (VAE) is a popular model for density estimation\nand representation learning. Canonically, the variational principle suggests to\nprefer an expressive inference model so that the variational approximation is\naccurate. However, it is often overlooked that an overly-expressive inference\nmodel can be detrimental to the test set performance of both the amortized\nposterior approximator and, more importantly, the generative density estimator.\nIn this paper, we leverage the fact that VAEs rely on amortized inference and\npropose techniques for amortized inference regularization (AIR) that control\nthe smoothness of the inference model. We demonstrate that, by applying AIR, it\nis possible to improve VAE generalization on both inference and generative\nperformance. Our paper challenges the belief that amortized inference is simply\na mechanism for approximating maximum likelihood training and illustrates that\nregularization of the amortization family provides a new direction for\nunderstanding and improving generalization in VAEs.\n', 'title': 'Amortized Inference Regularization'}]","Title: Amortized Inference Regularization

Abstract: The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator.

The potential drawback of using a highly expressive inference model in a variational autoencoder is that it can be detrimental to the test set performance of both the amortized posterior approximator and the generative density estimator."
How do renormalization group equations help in determining these logarithmic corrections?,b670a3df,"[{'abstract': '  We explicitly compute the critical exponents associated with logarithmic\ncorrections (the so-called hatted exponents) starting from the renormalization\ngroup equations and the mean field behavior for a wide class of models at the\nupper critical behavior (for short and long range $\\phi^n$-theories) and below\nit. This allows us to check the scaling relations among these critical\nexponents obtained by analysing the complex singularities (Lee-Yang and Fisher\nzeroes) of these models. Moreover, we have obtained an explicit method to\ncompute the $\\hat{\\coppa}$ exponent [defined by $\\xi\\sim L (\\log\nL)^{\\hat{\\coppa}}$] and, finally, we have found a new derivation of the scaling\nlaw associated with it.\n', 'title': 'Revisiting (logarithmic) scaling relations using renormalization group'}]","Title: Revisiting (logarithmic) scaling relations using renormalization group

Abstract: We explicitly compute the critical exponents associated with logarithmic corrections (the so-called hatted exponents) starting from the renormalization group equations and the mean field behavior for a wide class of models at the upper critical behavior (for short and long range $\phi^n$-theories) and below it. This allows us to check the scaling relations among these critical exponents obtained by analysing the complex singularities (Lee-Yang and Fisher zeroes) of these models. Moreover, we have obtained an explicit method to compute the $\hat{\coppa}$ exponent [defined by $\xi\sim L (\log L)^{\hat{\coppa}}$] and, finally, we have found a new derivation of the scaling law associated with it.

According to this paper, the renormalization group equations help in determining these logarithmic corrections by allowing the explicit computation of critical exponents associated with logarithmic corrections (hatted exponents) and checking scaling relations among these critical exponents."
Does Voyager 1 provide evidence for enriched diffusion in the heliosheath?,7976e680,"[{'abstract': '  We have derived background corrected intensities of 3-50 MeV galactic\nelectrons observed by Voyager 1 as it passes through the heliosheath from 95 to\n122 AU. The overall intensity change of the background corrected data from the\ninner to the outer boundary of the heliosheath is a maximum of a factor ~100 at\n15 MeV. At lower energies this fractional change becomes less and the corrected\nelectron spectra in the heliosheath becomes progressively steeper, reaching\nvalues ~ -2.5 for the spectral index just outside of the termination shock. At\nhigher energies the spectra of electrons has an exponent changing from the\nnegative LIS spectral index of -1.3 to values approaching zero in the\nheliosheath as a result of the solar modulation of the galactic electron\ncomponent. The large modulation effects observed below ~100 MV are possible\nevidence for enhanced diffusion as part of the modulation process for electrons\nin the heliosheath.\n', 'title': 'A Galactic Cosmic Ray Electron Intensity Increase of a factor of up to 100 At Energies between 3 and 50 MeV in the Heliosheath between the Termination Shock and the Heliopause Due to Solar Modulation As Measured by Voyager 1'}]","Title: A Galactic Cosmic Ray Electron Intensity Increase of a factor of up to 100 At Energies between 3 and 50 MeV in the Heliosheath between the Termination Shock and the Heliopause Due to Solar Modulation As Measured by Voyager 1

Abstract: We have derived background corrected intensities of 3-50 MeV galactic electrons observed by Voyager 1 as it passes through the heliosheath from 95 to 122 AU. The overall intensity change of the background corrected data from the inner to the outer boundary of the heliosheath is a maximum of a factor ~100 at 15 MeV. At lower energies this fractional change becomes less and the corrected electron spectra in the heliosheath becomes progressively steeper, reaching values ~ -2.5 for the spectral index just outside of the termination shock. At higher energies the spectra of electrons has an exponent changing from the negative LIS spectral index of -1.3 to values approaching zero in the heliosheath as a result of the solar modulation of the galactic electron component. The large modulation effects observed below ~100 MV are possible evidence for enhanced diffusion as part of the modulation process for electrons in the heliosheath.

This research paper suggests that Voyager 1 provides evidence for enhanced diffusion in the heliosheath."
What new element was included in the model to better reproduce the observed stellar velocities in the leading arm of the Sagittarius stream?,eafdfaf6,"[{'abstract': '  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy\noffer a unique way to constrain the shape of galactic gravitational potentials.\nSuch streams can be used as leaning tower gravitational experiments on galactic\nscales. The most well motivated modification of gravity proposed as an\nalternative to dark matter on galactic scales is Milgromian dynamics (MOND),\nand we present here the first ever N-body simulations of the dynamical\nevolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a\nrealistic baryonic mass model for the Milky Way, we attempt to reproduce the\npresent-day spatial and kinematic structure of the Sagittarius dwarf and its\nimmense tidal stream that wraps around the Milky Way. With very little freedom\non the original structure of the progenitor, constrained by the total\nluminosity of the Sagittarius structure and by the observed stellar mass-size\nrelation for isolated dwarf galaxies, we find reasonable agreement between our\nsimulations and observations of this system. The observed stellar velocities in\nthe leading arm can be reproduced if we include a massive hot gas corona around\nthe Milky Way that is flattened in the direction of the principal plane of its\nsatellites. This is the first time that tidal dissolution in MOND has been\ntested rigorously at these mass and acceleration scales.\n', 'title': 'Stellar streams as gravitational experiments I. The case of Sagittarius'}]","Title: Stellar streams as gravitational experiments I. The case of Sagittarius

Abstract: Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy offer a unique way to constrain the shape of galactic gravitational potentials. Such streams can be used as leaning tower gravitational experiments on galactic scales. The most well motivated modification of gravity proposed as an alternative to dark matter on galactic scales is Milgromian dynamics (MOND), and we present here the first ever N-body simulations of the dynamical evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a realistic baryonic mass model for the Milky Way, we attempt to reproduce the present-day spatial and kinematic structure of the Sagittarius dwarf and its immense tidal stream that wraps around the Milky Way. With very little freedom on the original structure of the progenitor, constrained by the total luminosity of the Sagittarius structure and by the observed stellar mass-size relation for isolated dwarf galaxies, we find reasonable agreement between our simulations and observations of this system. The observed stellar velocities in the leading arm can be reproduced if we include a massive hot gas corona around the Milky Way that is flattened in the direction of the principal plane of its satellites. This is the first time that tidal dissolution in MOND has been tested rigorously at these mass and acceleration scales.

The new element included in the model to better reproduce the observed stellar velocities in the leading arm of the Sagittarius stream is a massive hot gas corona around the Milky Way that is flattened in the direction of the principal plane of its satellites."
What is the computational complexity of retrieving a fixed portion of the backbone in weighted MAX-SAT?,08862526,"[{'abstract': '  The weighted Maximum Satisfiability problem (weighted MAX-SAT) is a NP-hard\nproblem with numerous applications arising in artificial intelligence. As an\nefficient tool for heuristic design, the backbone has been applied to\nheuristics design for many NP-hard problems. In this paper, we investigated the\ncomputational complexity for retrieving the backbone in weighted MAX-SAT and\ndeveloped a new algorithm for solving this problem. We showed that it is\nintractable to retrieve the full backbone under the assumption that . Moreover,\nit is intractable to retrieve a fixed fraction of the backbone as well. And\nthen we presented a backbone guided local search (BGLS) with Walksat operator\nfor weighted MAX-SAT. BGLS consists of two phases: the first phase samples the\nbackbone information from local optima and the backbone phase conducts local\nsearch under the guideline of backbone. Extensive experimental results on the\nbenchmark showed that BGLS outperforms the existing heuristics in both solution\nquality and runtime.\n', 'title': 'Approximating the Backbone in the Weighted Maximum Satisfiability Problem'}]","Title: Approximating the Backbone in the Weighted Maximum Satisfiability Problem

Abstract:  The weighted Maximum Satisfiability problem (weighted MAX-SAT) is a NP-hard problem with numerous applications arising in artificial intelligence. As an efficient tool for heuristic design, the backbone has been applied to heuristics design for many NP-hard problems. In this paper, we investigated the computational complexity for retrieving the backbone in weighted MAX-SAT and developed a new algorithm for solving this problem. We showed that it is intractable to retrieve the full backbone under the assumption that . Moreover, it is intractable to retrieve a fixed fraction of the backbone as well."
What is the relationship between this research and the tensor product theorem of Krop and Sullivan?,bfe68414,"[{'abstract': '  We determine the composition factors of the tensor product $S(E)\\otimes S(E)$\nof two copies of the symmetric algebra of the natural module $E$ of a general\nlinear group over an algebraically closed field of positive characteristic. Our\nmain result may be regarded as a substantial generalisation of the tensor\nproduct theorem of Krop and Sullivan, on composition factors of $S(E)$. We\nearlier answered the question of which polynomially injective modules are\ninfinitesimally injective in terms of the ""divisibility index"". We are now able\nto give an explicit description of the divisibility index for polynomial\nmodules for general linear groups of degree at most $3$.\n', 'title': 'Composition Factors of Tensor Products of Symmetric Powers'}]","Title: Composition Factors of Tensor Products of Symmetric Powers

Abstract:   We determine the composition factors of the tensor product $S(E)\otimes S(E)$
of two copies of the symmetric algebra of the natural module $E$ of a general
linear group over an algebraically closed field of positive characteristic. Our
main result may be regarded as a substantial generalisation of the tensor
product theorem of Krop and Sullivan, on composition factors of $S(E)$. We
earlier answered the question of which polynomially injective modules are
infinitesimally injective in terms of the ""divisibility index"". We are now able
to give an explicit description of the divisibility index for polynomial
modules for general linear groups of degree at most $3$."
Mathematics for post-quantum cryptography summer school in Thiès,3027d5ca,"[{'abstract': ""  These lectures notes were written for a summer school on Mathematics for\npost-quantum cryptography in Thiès, Senegal. They try to provide a guide for\nMasters' students to get through the vast literature on elliptic curves,\nwithout getting lost on their way to learning isogeny based cryptography. They\nare by no means a reference text on the theory of elliptic curves, nor on\ncryptography; students are encouraged to complement these notes with some of\nthe books recommended in the bibliography.\nThe presentation is divided in three parts, roughly corresponding to the\nthree lectures given. In an effort to keep the reader interested, each part\nalternates between the fundamental theory of elliptic curves, and applications\nin cryptography. We often prefer to have the main ideas flow smoothly, rather\nthan having a rigorous presentation as one would have in a more classical book.\nThe reader will excuse us for the inaccuracies and the omissions.\n"", 'title': 'Mathematics of Isogeny Based Cryptography'}]","I think I can help you with that!

Title: Mathematics of Isogeny Based Cryptography

Abstract: These lectures notes were written for a summer school on Mathematics for post-quantum cryptography in Thiès, Senegal. They try to provide a guide for Masters' students to get through the vast literature on elliptic curves, without getting lost on their way to learning isogeny based cryptography. They are by no means a reference text on the theory of elliptic curves, nor on cryptography; students are encouraged to complement these notes with some of the books recommended in the bibliography. The presentation is divided in three parts, roughly corresponding to the three lectures given. In an effort to keep the reader interested, each part alternates between the fundamental theory of elliptic curves, and applications in cryptography. We often prefer to have the main ideas flow smoothly, rather than having a rigorous presentation as one would have in a more classical book. The reader will excuse us for the inaccuracies and the omissions."
How do changes in air-water surface tension affect the movement of camphoric acid in a gel tablet?,1656ac4b,"[{'abstract': '  We report experiments on an agarose gel tablet loaded with camphoric acid\n(c-boat) set into self-motion by interfacial tension gradients at the air-water\ninterface. We observe three distinct modes of c-boat motion: harmonic mode\nwhere the c-boat speed oscillates sinusoidally in time, a steady mode where the\nc-boat maintains constant speed, and a relaxation oscillation mode where the\nc-boat maintains near-zero speed between sudden jumps in speed and position at\nregular time intervals. Whereas all three modes have been separately reported\nbefore in different systems, we show they belong to a common description.\nThrough control of the air-water surface tension with Sodium Dodecyl Sulfate\n(SDS), we experimentally deduce the three self-propulsive modes result from\nsurface tension difference between Camphoric Acid (CA) and the ambient\nsurroundings.\n', 'title': 'Dynamics of a Camphoric Acid boat at the air-water interface'}]","I think I can help you with that!

Title: Dynamics of a Camphoric Acid boat at the air-water interface

Abstract:   We report experiments on an agarose gel tablet loaded with camphoric acid
(c-boat) set into self-motion by interfacial tension gradients at the air-water
interface. We observe three distinct modes of c-boat motion: harmonic mode
where the c-boat speed oscillates sinusoidally in time, a steady mode where the
c-boat maintains constant speed, and a relaxation oscillation mode where the
c-boat maintains near-zero speed between sudden jumps in speed and position at
regular time intervals. Whereas all three modes have been separately reported
before in different systems, we show they belong to a common description.
Through control of the air-water surface tension with Sodium Dodecyl Sulfate
(SDS), we experimentally deduce the three self-propulsive modes result from
surface tension difference between Camphoric Acid (CA) and the ambient
surroundings.

According to this research paper, changes in air-water surface tension affect the movement of camphoric acid in a gel tablet, resulting in three distinct modes of motion: harmonic, steady, and relaxation oscillation modes. The surface tension difference between Camphoric Acid and the ambient surroundings is the driving force behind these self-propulsive modes."
"In the case of  K-theory, is there a closed form for the structure constants of the affine Grassmannian associated to  SL2?",11bef1ff,"[{'abstract': '  Let $G:=\\widehat{SL_2}$ denote the affine Kac-Moody group associated to\n$SL_2$ and $\\bar{\\mathcal{X}}$ the associated affine Grassmannian. We determine\nan inductive formula for the Schubert basis structure constants in the\ntorus-equivariant Grothendieck group of $\\bar{\\mathcal{X}}$. In the case of\nordinary (non-equivariant) $K$-theory we find an explicit closed form for the\nstructure constants. We also determine an inductive formula for the structure\nconstants in the torus-equivariant cohomology ring, and use this formula to\nfind closed forms for some of the structure constants.\n', 'title': 'An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$'}]","Title: An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$

Abstract: Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to $SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine an inductive formula for the Schubert basis structure constants in the torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of ordinary (non-equivariant) $K$-theory we find an explicit closed form for the structure constants. We also determine an inductive formula for the structure constants in the torus-equivariant cohomology ring, and use this formula to find closed forms for some of the structure constants."
What are the limitations of least-squares based algorithms in mHealth interventions?,06420b80,"[{'abstract': '  This paper considers the actor-critic contextual bandit for the mobile health\n(mHealth) intervention. The state-of-the-art decision-making methods in mHealth\ngenerally assume that the noise in the dynamic system follows the Gaussian\ndistribution. Those methods use the least-square-based algorithm to estimate\nthe expected reward, which is prone to the existence of outliers. To deal with\nthe issue of outliers, we propose a novel robust actor-critic contextual bandit\nmethod for the mHealth intervention. In the critic updating, the\ncapped-$\\ell_{2}$ norm is used to measure the approximation error, which\nprevents outliers from dominating our objective. A set of weights could be\nachieved from the critic updating. Considering them gives a weighted objective\nfor the actor updating. It provides the badly noised sample in the critic\nupdating with zero weights for the actor updating. As a result, the robustness\nof both actor-critic updating is enhanced. There is a key parameter in the\ncapped-$\\ell_{2}$ norm. We provide a reliable method to properly set it by\nmaking use of one of the most fundamental definitions of outliers in\nstatistics. Extensive experiment results demonstrate that our method can\nachieve almost identical results compared with the state-of-the-art methods on\nthe dataset without outliers and dramatically outperform them on the datasets\nnoised by outliers.\n', 'title': 'Robust Contextual Bandit via the Capped-$\\ell_{2}$ norm'}]",I couldn't find any information related to the limitations of least-squares based algorithms in mHealth interventions in the provided abstracts. None of the titles or abstracts mention mHealth interventions or discuss the limitations of least-squares based algorithms in that context.
"Can you describe the bounds on N(n,k,m,t;r) and provide examples of their constructions?",8f5842b2,"[{'abstract': '  Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai,\nmimic a distributed storage of a set of $n$ data items on $m$ servers, in such\na way that any batch of $k$ data items can be retrieved by reading at most some\n$t$ symbols from each server. Combinatorial batch codes, are replication-based\nbatch codes in which each server stores a subset of the data items.\nIn this paper, we propose a generalization of combinatorial batch codes,\ncalled multiset combinatorial batch codes (MCBC), in which $n$ data items are\nstored in $m$ servers, such that any multiset request of $k$ items, where any\nitem is requested at most $r$ times, can be retrieved by reading at most $t$\nitems from each server. The setup of this new family of codes is motivated by\nrecent work on codes which enable high availability and parallel reads in\ndistributed storage systems. The main problem under this paradigm is to\nminimize the number of items stored in the servers, given the values of\n$n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and\nsufficient condition for the existence of MCBCs. Then, we present several\nbounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we\ndetermine the value of $N(n,k,m,1;r)$ for any $n\\geq\n\\left\\lfloor\\frac{k-1}{r}\\right\\rfloor{m\\choose k-1}-(m-k+1)A(m,4,k-2)$, where\n$A(m,4,k-2)$ is the maximum size of a binary constant weight code of length\n$m$, distance four and weight $k-2$. We also determine the exact value of\n$N(n,k,m,1;r)$ when $r\\in\\{k,k-1\\}$ or $k=m$.\n', 'title': 'Multiset Combinatorial Batch Codes'}]","Title: Multiset Combinatorial Batch Codes

Abstract: Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai, mimic a distributed storage of a set of $n$ data items on $m$ servers, in such a way that any batch of $k$ data items can be retrieved by reading at most some $t$ symbols from each server. Combinatorial batch codes, are replication-based batch codes in which each server stores a subset of the data items. In this paper, we propose a generalization of combinatorial batch codes, called multiset combinatorial batch codes (MCBC), in which $n$ data items are stored in $m$ servers, such that any multiset request of $k$ items, where any item is requested at most $r$ times, can be retrieved by reading at most $t$ items from each server. The setup of this new family of codes is motivated by recent work on codes which enable high availability and parallel reads in distributed storage systems. The main problem under this paradigm is to minimize the number of items stored in the servers, given the values of $n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and sufficient condition for the existence of MCBCs. Then, we present several bounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we determine the value of $N(n,k,m,1;r)$ for any $n\geq \left\lfloor\frac{k-1}{r}\right\rfloor{m\choose k-1}-(m-k+1)A(m,4,k-2)$, where $A(m,4,k-2)$ is the maximum size of a binary constant weight code of length $m$, distance four and weight $k-2$. We also determine the exact value of $N(n,k,m,1;r)$ when $r\in\{k,k-1\}$ or $k=m$."
Can speed limits similar to the quantum speed limit be found in the context of  imaginary-time Schrödinger equations?,2e9d6419,"[{'abstract': '  The quantum speed limit (QSL), or the energy-time uncertainty relation,\ndescribes the fundamental maximum rate for quantum time evolution and has been\nregarded as being unique in quantum mechanics. In this study, we obtain a\nclassical speed limit corresponding to the QSL using the Hilbert space for the\nclassical Liouville equation. Thus, classical mechanics has a fundamental speed\nlimit, and QSL is not a purely quantum phenomenon but a universal dynamical\nproperty of the Hilbert space. Furthermore, we obtain similar speed limits for\nthe imaginary-time Schroedinger equations such as the master equation.\n', 'title': 'Quantum Speed Limit is Not Quantum'}]","Title: Quantum Speed Limit is Not Quantum

Abstract: The quantum speed limit (QSL), or the energy-time uncertainty relation, describes the fundamental maximum rate for quantum time evolution and has been regarded as being unique in quantum mechanics. In this study, we obtain a classical speed limit corresponding to the QSL using the Hilbert space for the classical Liouville equation. Thus, classical mechanics has a fundamental speed limit, and QSL is not a purely quantum phenomenon but a universal dynamical property of the Hilbert space. Furthermore, we obtain similar speed limits for the imaginary-time Schroedinger equations such as the master equation."
What is the size of the vocabulary of classes used in the YouTube-8M benchmark dataset?,f6fee8d2,"[{'abstract': ""  Large-scale datasets have played a significant role in progress of neural\nnetwork and deep learning areas. YouTube-8M is such a benchmark dataset for\ngeneral multi-label video classification. It was created from over 7 million\nYouTube videos (450,000 hours of video) and includes video labels from a\nvocabulary of 4716 classes (3.4 labels/video on average). It also comes with\npre-extracted audio & visual features from every second of video (3.2 billion\nfeature vectors in total). Google cloud recently released the datasets and\norganized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.\nCompetitors are challenged to develop classification algorithms that assign\nvideo-level labels using the new and improved Youtube-8M V2 dataset. Inspired\nby the competition, we started exploration of audio understanding and\nclassification using deep learning algorithms and ensemble methods. We built\nseveral baseline predictions according to the benchmark paper and public github\ntensorflow code. Furthermore, we improved global prediction accuracy (GAP) from\nbase level 77% to 80.7% through approaches of ensemble.\n"", 'title': 'An Effective Way to Improve YouTube-8M Classification Accuracy in Google Cloud Platform'}]","Title: An Effective Way to Improve YouTube-8M Classification Accuracy in Google Cloud Platform

Abstract:   Large-scale datasets have played a significant role in progress of neural 
network and deep learning areas. YouTube-8M is such a benchmark dataset for 
general multi-label video classification. It was created from over 7 million 
YouTube videos (450,000 hours of video) and includes video labels from a 
vocabulary of 4716 classes (3.4 labels/video on average). It also comes with 
pre-extracted audio & visual features from every second of video (3.2 billion 
feature vectors in total). Google cloud recently released the datasets and 
organized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle. 
Competitors are challenged to develop classification algorithms that assign 
video-level labels using the new and improved Youtube-8M V2 dataset. Inspired 
by the competition, we started exploration of audio understanding and 
classification using deep learning algorithms and ensemble methods. We built 
several baseline predictions according to the benchmark paper and public github 
tensorflow code. Furthermore, we improved global prediction accuracy (GAP) from 
base level 77% to 80.7% through approaches of ensemble.

The answer is 4716 classes."
What is the impact on throughput when coded chunks are formed in the signal domain compared to the data domain?,ed653419,"[{'abstract': '  In this paper we consider a single-cell downlink scenario where a\nmultiple-antenna base station delivers contents to multiple cache-enabled user\nterminals. Based on the multicasting opportunities provided by the so-called\nCoded Caching technique, we investigate three delivery approaches. Our baseline\nscheme employs the coded caching technique on top of max-min fair multicasting.\nThe second one consists of a joint design of Zero-Forcing (ZF) and coded\ncaching, where the coded chunks are formed in the signal domain (complex\nfield). The third scheme is similar to the second one with the difference that\nthe coded chunks are formed in the data domain (finite field). We derive\nclosed-form rate expressions where our results suggest that the latter two\nschemes surpass the first one in terms of Degrees of Freedom (DoF). However, at\nthe intermediate SNR regime forming coded chunks in the signal domain results\nin power loss, and will deteriorate throughput of the second scheme. The main\nmessage of our paper is that the schemes performing well in terms of DoF may\nnot be directly appropriate for intermediate SNR regimes, and modified schemes\nshould be employed.\n', 'title': 'Multi-Antenna Coded Caching'}]","Title: Multi-Antenna Coded Caching

Abstract: In this paper we consider a single-cell downlink scenario where a multiple-antenna base station delivers contents to multiple cache-enabled user terminals. Based on the multicasting opportunities provided by the so-called Coded Caching technique, we investigate three delivery approaches. Our baseline scheme employs the coded caching technique on top of max-min fair multicasting. The second one consists of a joint design of Zero-Forcing (ZF) and coded caching, where the coded chunks are formed in the signal domain (complex field). The third scheme is similar to the second one with the difference that the coded chunks are formed in the data domain (finite field). We derive closed-form rate expressions where our results suggest that the latter two schemes surpass the first one in terms of Degrees of Freedom (DoF). However, at the intermediate SNR regime forming coded chunks in the signal domain results in power loss, and will deteriorate throughput of the second scheme. The main message of our paper is that the schemes performing well in terms of DoF may not be directly appropriate for intermediate SNR regimes, and modified schemes should be employed.

The impact on throughput when coded chunks are formed in the signal domain compared to the data domain is that it results in power loss and will deteriorate throughput at the intermediate SNR regime."
What are the potential benefits of using machine learning models to predict wind pressure compared to traditional methods?,8b6823e9,"[{'abstract': '  Numerous studies have been carried out to measure wind pressures around\ncircular cylinders since the early 20th century due to its engineering\nsignificance. Consequently, a large amount of wind pressure data sets have\naccumulated, which presents an excellent opportunity for using machine learning\n(ML) techniques to train models to predict wind pressures around circular\ncylinders. Wind pressures around smooth circular cylinders are a function of\nmainly the Reynolds number (Re), turbulence intensity (Ti) of the incident\nwind, and circumferential angle of the cylinder. Considering these three\nparameters as the inputs, this study trained two ML models to predict mean and\nfluctuating pressures respectively. Three machine learning algorithms including\ndecision tree regressor, random forest, and gradient boosting regression trees\n(GBRT) were tested. The GBRT models exhibited the best performance for\npredicting both mean and fluctuating pressures, and they are capable of making\naccurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to\n15%. It is believed that the GBRT models provide very efficient and economical\nalternative to traditional wind tunnel tests and computational fluid dynamic\nsimulations for determining wind pressures around smooth circular cylinders\nwithin the studied Re and Ti range.\n', 'title': 'Predicting wind pressures around circular cylinders using machine learning techniques'}]","The potential benefits of using machine learning models to predict wind pressure compared to traditional methods are mentioned in the abstract of the paper titled ""Predicting wind pressures around circular cylinders using machine learning techniques.""

Title: Predicting wind pressures around circular cylinders using machine learning techniques

Abstract: ... It is believed that the GBRT models provide very efficient and economical alternative to traditional wind tunnel tests and computational fluid dynamic simulations for determining wind pressures around smooth circular cylinders within the studied Re and Ti range.

In this context, the benefits of using machine learning models include efficiency and cost-effectiveness compared to traditional methods such as wind tunnel tests and computational fluid dynamic simulations."
How do medium effects influence the scales associated with the bare interaction?,770970c4,"[{'abstract': '  We further progress along the line of Ref. [Phys. Rev. {\\bf A 94}, 043614\n(2016)] where a functional for Fermi systems with anomalously large $s$-wave\nscattering length $a_s$ was proposed that has no free parameters. The\nfunctional is designed to correctly reproduce the unitary limit in Fermi gases\ntogether with the leading-order contributions in the s- and p-wave channels at\nlow density. The functional is shown to be predictive up to densities\n$\\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang\nfunctional, valid for $\\rho < 10^{-6}$ fm$^{-3}$. The form of the functional\nretained in this work is further motivated. It is shown that the new functional\ncorresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all\norders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One\nconclusion from the present work is that, except in the extremely low--density\nregime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with\nrespect to the unitary limit. Starting from the functional, we introduce\ndensity--dependent scales and show that scales associated to the bare\ninteraction are strongly renormalized by medium effects. As a consequence, some\nof the scales at play around saturation are dominated by the unitary gas\nproperties and not directly to low-energy constants. For instance, we show that\nthe scale in the s-wave channel around saturation is proportional to the\nso-called Bertsch parameter $\\xi_0$ and becomes independent of $a_s$. We also\npoint out that these scales are of the same order of magnitude than those\nempirically obtained in the Skyrme energy density functional. We finally\npropose a slight modification of the functional such that it becomes accurate\nup to the saturation density $\\rho\\simeq 0.16$ fm$^{-3}$.\n', 'title': 'From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter'}]","Title: From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter

Abstract:   We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One
conclusion from the present work is that, except in the extremely low--density
regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with
respect to the unitary limit. Starting from the functional, we introduce
density--dependent scales and show that scales associated to the bare
interaction are strongly renormalized by medium effects. As a consequence, some
of the scales at play around saturation are dominated by the unitary gas
properties and not directly to low-energy constants. For instance, we show that
the scale in the s-wave channel around saturation is proportional to the
so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also
point out that these scales are of the same order of magnitude than those
empirically obtained in the Skyrme energy density functional. We finally
propose a slight modification of the functional such that it becomes accurate
up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$."
What is the role of EHR data in precision medicine and how can phenotyping algorithms contribute?,c6909af1,"[{'abstract': '  Electronic Health Records (EHR) are data generated during routine clinical\ncare. EHR offer researchers unprecedented phenotypic breadth and depth and have\nthe potential to accelerate the pace of precision medicine at scale. A main EHR\nuse-case is creating phenotyping algorithms to define disease status, onset and\nseverity. Currently, no common machine-readable standard exists for defining\nphenotyping algorithms which often are stored in human-readable formats. As a\nresult, the translation of algorithms to implementation code is challenging and\nsharing across the scientific community is problematic. In this paper, we\nevaluate openEHR, a formal EHR data specification, for computable\nrepresentations of EHR phenotyping algorithms.\n', 'title': 'Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms'}]","Title: Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms

Abstract: Electronic Health Records (EHR) are data generated during routine clinical care. EHR offer researchers unprecedented phenotypic breadth and depth and have the potential to accelerate the pace of precision medicine at scale. A main EHR use-case is creating phenotyping algorithms to define disease status, onset and severity. Currently, no common machine-readable standard exists for defining phenotyping algorithms which often are stored in human-readable formats. As a result, the translation of algorithms to implementation code is challenging and sharing across the scientific community is problematic. In this paper, we evaluate openEHR, a formal EHR data specification, for computable representations of EHR phenotyping algorithms."
"Under what circumstances does the Calabi flow have short time existence, assuming a continuous Kähler metric? ",a2c4eaac,"[{'abstract': '  In this paper, we prove that there exists a dimensional constant $\\delta > 0$\nsuch that given any background Kähler metric $\\omega$, the Calabi flow with\ninitial data $u_0$ satisfying \\begin{equation*} \\partial \\bar \\partial u_0 \\in\nL^\\infty (M) \\text{ and } (1- \\delta )\\omega < \\omega_{u_0} < (1+\\delta\n)\\omega, \\end{equation*} admits a unique short time solution and it becomes\nsmooth immediately, where $\\omega_{u_0} : = \\omega +\\sqrt{-1}\\partial\n\\bar\\partial u_0$. The existence time depends on initial data $u_0$ and the\nmetric $\\omega$. As a corollary, we get that Calabi flow has short time\nexistence for any initial data satisfying \\begin{equation*} \\partial \\bar\n\\partial u_0 \\in C^0(M) \\text{ and } \\omega_{u_0} > 0, \\end{equation*} which\nshould be interpreted as a ""continuous Kähler metric"". A main technical\ningredient is Schauder-type estimates for biharmonic heat equation on\nRiemannian manifolds with time weighted Hölder norms.\n', 'title': 'The Calabi flow with rough initial data'}]","Title: The Calabi flow with rough initial data

Abstract: In this paper, we prove that there exists a dimensional constant $\delta > 0$ such that given any background Kähler metric $\omega$, the Calabi flow with initial data $u_0$ satisfying 
\begin{equation*} \partial \bar \partial u_0 \in L^\infty (M) \text{ and } (1- \delta )\omega < \omega_{u_0} < (1+\delta )\omega, \end{equation*} 
admits a unique short time solution and it becomes smooth immediately, where $\omega_{u_0} : = \omega +\sqrt{-1}\partial \bar\partial u_0$. The existence time depends on initial data $u_0$ and the metric $\omega$. As a corollary, we get that Calabi flow has short time existence for any initial data satisfying 
\begin{equation*} \partial \bar \partial u_0 \in C^0(M) \text{ and } \omega_{u_0} > 0, \end{equation*} 
which should be interpreted as a ""continuous Kähler metric"". A main technical ingredient is Schauder-type estimates for biharmonic heat equation on Riemannian manifolds with time weighted Hölder norms."
What is the relationship between the correlation parameter of mated-CRT maps and the gamma value in gamma-Liouville quantum gravity?,208a81fb,"[{'abstract': ""  We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain\nrandom planar maps converge to $\\gamma$-Liouville quantum gravity\n($\\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized\nmatings of correlated continuum random trees, and $\\gamma$ ranges from $0$ to\n$2$ as one varies the correlation parameter. We also show that the associated\nspace-filling path on the embedded map converges to space-filling\nSLE$_{\\kappa}$ for $\\kappa =16/\\gamma^2$ (in the annealed sense) and that\nsimple random walk on the embedded map converges to Brownian motion (in the\nquenched sense). Our arguments also yield analogous statements for the Smith\n(square tiling) embedding of the mated-CRT map.\nThis work constitutes the first proof that a discrete conformal embedding of\na random planar map converges to LQG. Many more such statements have been\nconjectured. Since the mated-CRT map can be viewed as a coarse-grained\napproximation to other random planar maps (the UIPT, tree-weighted maps,\nbipolar-oriented maps, etc.), our results indicate a potential approach for\nproving that embeddings of these maps converge to LQG as well.\nTo prove the main result, we establish several (independently interesting)\ntheorems about LQG surfaces decorated by space-filling SLE. There is a natural\nway to use the SLE curve to divide the plane into `cells' corresponding to\nvertices of the mated-CRT map. We study the law of the shape of the\norigin-containing cell, in particular proving moments for the ratio of its\nsquared diameter to its area. We also give bounds on the degree of the\norigin-containing cell and establish a form of ergodicity for the entire\nconfiguration. Ultimately, we use these properties to show (using a general\ntheorem proved in a separate paper) that random walk on these cells converges\nto a time change of Brownian motion, which in turn leads to the Tutte embedding\nresult.\n"", 'title': 'The Tutte embedding of the mated-CRT map converges to Liouville quantum gravity'}]","I think I can help you with that!

Title: The Tutte embedding of the mated-CRT map converges to Liouville quantum gravity

Abstract: We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain
random planar maps converge to $\gamma$-Liouville quantum gravity
($\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized
matings of correlated continuum random trees, and $\gamma$ ranges from $0$ to
$2$ as one varies the correlation parameter. ...

According to this abstract, the correlation parameter of mated-CRT maps is related to the gamma value in gamma-Liouville quantum gravity, specifically $\gamma$ ranges from 0 to 2 as one varies the correlation parameter."
What are proceedings volume for the Fifth International Workshop on Verification and Program Transformation?,fb9c9cbf,"[{'abstract': '  This volume contains the proceedings of the Fifth International Workshop on\nVerification and Program Transformation (VPT 2017). The workshop took place in\nUppsala, Sweden, on April 29th, 2017, affiliated with the European Joint\nConferences on Theory and Practice of Software (ETAPS). The aim of the VPT\nworkshop series is to provide a forum where people from the areas of program\ntransformation and program verification can fruitfully exchange ideas and gain\na deeper understanding of the interactions between those two fields. Seven\npapers were presented at the workshop. Additionally, three invited talks were\ngiven by Javier Esparza (Technische Universität München, Germany), Manuel\nHermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov\n(Linux Verification Center, ISPRAS, Moscow, Russia).\n', 'title': 'Proceedings Fifth International Workshop on Verification and Program Transformation'}]","Based on the context, the proceedings volume for the Fifth International Workshop on Verification and Program Transformation is:

Title: Proceedings Fifth International Workshop on Verification and Program Transformation

Abstract: This volume contains the proceedings of the Fifth International Workshop on Verification and Program Transformation (VPT 2017). The workshop took place in Uppsala, Sweden, on April 29th, 2017, affiliated with the European Joint Conferences on Theory and Practice of Software (ETAPS). The aim of the VPT workshop series is to provide a forum where people from the areas of program transformation and program verification can fruitfully exchange ideas and gain a deeper understanding of the interactions between those two fields. Seven papers were presented at the workshop. Additionally, three invited talks were given by Javier Esparza (Technische Universität München, Germany), Manuel Hermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov (Linux Verification Center, ISPRAS, Moscow, Russia)."
What are the conditions required for complete controllability of a magnetically controlled satellite?,6a470a56,"[{'abstract': ""  This paper describes the development of a magnetic attitude control subsystem\nfor a 2U cubesat. Due to the presence of gravity gradient torques, the\nsatellite dynamics are open-loop unstable near the desired pointing\nconfiguration. Nevertheless the linearized time-varying system is completely\ncontrollable, under easily verifiable conditions, and the system's disturbance\nrejection capabilities can be enhanced by adding air drag panels exemplifying a\nbeneficial interplay between hardware design and control. In the paper,\nconditions for the complete controllability for the case of a magnetically\ncontrolled satellite with passive air drag panels are developed, and simulation\ncase studies with the LQR and MPC control designs applied in combination with a\nnonlinear time-varying input transformation are presented to demonstrate the\nability of the closed-loop system to satisfy mission objectives despite\ndisturbance torques.\n"", 'title': 'Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques'}]","Title: Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques

Abstract: This paper describes the development of a magnetic attitude control subsystem for a 2U cubesat. Due to the presence of gravity gradient torques, the satellite dynamics are open-loop unstable near the desired pointing configuration. Nevertheless the linearized time-varying system is completely controllable, under easily verifiable conditions, and the system's disturbance rejection capabilities can be enhanced by adding air drag panels exemplifying a beneficial interplay between hardware design and control. In the paper, conditions for the complete controllability for the case of a magnetically controlled satellite with passive air drag panels are developed, and simulation case studies with the LQR and MPC control designs applied in combination with a nonlinear time-varying input transformation are presented to demonstrate the ability of the closed-loop system to satisfy mission objectives despite disturbance torques."
How can cerebral blood flow (CBF) measurements obtained through rs-fASL be used as a clinical biomarker?,de31f884,"[{'abstract': '  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily\npractice and academic research stay discreet compared to resting-state BOLD.\nHowever, by giving direct access to cerebral blood flow maps, rs-fASL leads to\nsignificant clinical subject scaled application as CBF can be considered as a\nbiomarker in common neuropathology. Our work here focuses on the link between\noverall quality of rs-fASL and duration of acquisition. To this end, we\nconsider subject self-Default Mode Network (DMN), and assess DMN quality\ndepletion compared to a gold standard DMN depending on the duration of\nacquisition.\n', 'title': 'Resting-state ASL : Toward an optimal sequence duration'}]","Title: Resting-state ASL: Toward an optimal sequence duration

Abstract:   Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily
practice and academic research stay discreet compared to resting-state BOLD.
However, by giving direct access to cerebral blood flow maps, rs-fASL leads to
significant clinical subject scaled application as CBF can be considered as a
biomarker in common neuropathology. Our work here focuses on the link between
overall quality of rs-fASL and duration of acquisition. To this end, we
consider subject self-Default Mode Network (DMN), and assess DMN quality
depletion compared to a gold standard DMN depending on the duration of
acquisition."
What are the remaining challenges in developing fully integrated artificial camouflage solutions?,4c3f6dd1,"[{'abstract': '  We identify the components of bio-inspired artificial camouflage systems\nincluding actuation, sensing, and distributed computation. After summarizing\nrecent results in understanding the physiology and system-level performance of\na variety of biological systems, we describe computational algorithms that can\ngenerate similar patterns and have the potential for distributed\nimplementation. We find that the existing body of work predominately treats\ncomponent technology in an isolated manner that precludes a material-like\nimplementation that is scale-free and robust. We conclude with open research\nchallenges towards the realization of integrated camouflage solutions.\n', 'title': 'From Natural to Artificial Camouflage: Components and Systems'}]","The researcher is looking for papers related to artificial camouflage solutions. After reviewing the provided abstracts, I found that the paper ""From Natural to Artificial Camouflage: Components and Systems"" is relevant to this topic.

Title: From Natural to Artificial Camouflage: Components and Systems

Abstract:   We identify the components of bio-inspired artificial camouflage systems
including actuation, sensing, and distributed computation. After summarizing
recent results in understanding the physiology and system-level performance of
a variety of biological systems, we describe computational algorithms that can
generate similar patterns and have the potential for distributed
implementation. We find that the existing body of work predominately treats
component technology in an isolated manner that precludes a material-like
implementation that is scale-free and robust. We conclude with open research
challenges towards the realization of integrated camouflage solutions.

According to this paper, the remaining challenges in developing fully integrated artificial camouflage solutions include:

* Integrating components in a material-like implementation that is scale-free and robust
* Overcoming the current isolated treatment of component technology
* Addressing the lack of a unified approach to artificial camouflage systems

These challenges are mentioned in the abstract as open research directions towards the realization of integrated camouflage solutions."
How can cross-technology communication improve spectrum efficiency in a crowded ISM band?,024bba55,"[{'abstract': '  In wireless communication, heterogeneous technologies such as WiFi, ZigBee\nand BlueTooth operate in the same ISM band.With the exponential growth in the\nnumber of wireless devices, the ISM band becomes more and more crowded. These\nheterogeneous devices have to compete with each other to access spectrum\nresources, generating cross-technology interference (CTI). Since CTI may\ndestroy wireless communication, this field is facing an urgent and challenging\nneed to investigate spectrum efficiency under CTI. In this paper, we introduce\na novel framework to address this problem from two aspects. On the one hand,\nfrom the perspective of each communication technology itself, we propose novel\nchannel/link models to capture the channel/link status under CTI. On the other\nhand, we investigate spectrum efficiency from the perspective by taking all\nheterogeneous technologies as a whole and building crosstechnology\ncommunication among them. The capability of direct communication among\nheterogeneous devices brings great opportunities to harmoniously sharing the\nspectrum with collaboration rather than competition.\n', 'title': 'Achieving Spectrum Efficient Communication Under Cross-Technology Interference'}]","Title: Achieving Spectrum Efficient Communication Under Cross-Technology Interference

Abstract: In wireless communication, heterogeneous technologies such as WiFi, ZigBee and BlueTooth operate in the same ISM band.With the exponential growth in the number of wireless devices, the ISM band becomes more and more crowded. These heterogeneous devices have to compete with each other to access spectrum resources, generating cross-technology interference (CTI). Since CTI may destroy wireless communication, this field is facing an urgent and challenging need to investigate spectrum efficiency under CTI. In this paper, we introduce a novel framework to address this problem from two aspects. On the one hand, from the perspective of each communication technology itself, we propose novel channel/link models to capture the channel/link status under CTI. On the other hand, we investigate spectrum efficiency from the perspective by taking all heterogeneous technologies as a whole and building cross-technology communication among them. The capability of direct communication among heterogeneous devices brings great opportunities to harmoniously sharing the spectrum with collaboration rather than competition."
What is the role of the Weyl tensor at the boundary of a Riemannian manifold in determining the supremum of the isoperimetric quotient?,c0050835,"[{'abstract': '  Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with\nsmooth boundary $\\partial M$. Suppose that $(M,g)$ admits a scalar-flat\nconformal metric. We prove that the supremum of the isoperimetric quotient over\nthe scalar-flat conformal class is strictly larger than the best constant of\nthe isoperimetric inequality in the Euclidean space, and consequently is\nachieved, if either (i) $n\\ge 12$ and $\\partial M$ has a nonumbilic point; or\n(ii) $n\\ge 10$, $\\partial M$ is umbilic and the Weyl tensor does not vanish at\nsome boundary point.\n', 'title': 'On the isoperimetric quotient over scalar-flat conformal classes'}]","Title: On the isoperimetric quotient over scalar-flat conformal classes

Abstract: Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with smooth boundary $\partial M$. Suppose that $(M,g)$ admits a scalar-flat conformal metric. We prove that the supremum of the isoperimetric quotient over the scalar-flat conformal class is strictly larger than the best constant of the isoperimetric inequality in the Euclidean space, and consequently is achieved, if either (i) $n\ge 12$ and $\partial M$ has a nonumbilic point; or (ii) $n\ge 10$, $\partial M$ is umbilic and the Weyl tensor does not vanish at some boundary point.

The Weyl tensor plays a crucial role in determining the supremum of the isoperimetric quotient at the boundary of a Riemannian manifold, specifically when the boundary is umbilic and the dimension is at least 10."
What type of breast cancer subtype did the researchers find to be associated with the miR-199/miR-214 microRNA cluster?,1fcbaa77,"[{'abstract': '  MicroRNAs play important roles in many biological processes. Their aberrant\nexpression can have oncogenic or tumor suppressor function directly\nparticipating to carcinogenesis, malignant transformation, invasiveness and\nmetastasis. Indeed, miRNA profiles can distinguish not only between normal and\ncancerous tissue but they can also successfully classify different subtypes of\na particular cancer. Here, we focus on a particular class of transcripts\nencoding polycistronic miRNA genes that yields multiple miRNA components. We\ndescribe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully\nredesigned release of the MMRA computational pipeline (MiRNA Master Regulator\nAnalysis), developed to search for clustered miRNAs potentially driving cancer\nmolecular subtyping. Genomically clustered miRNAs are frequently co-expressed\nto target different components of pro-tumorigenic signalling pathways. By\napplying ClustMMRA to breast cancer patient data, we identified key miRNA\nclusters driving the phenotype of different tumor subgroups. The pipeline was\napplied to two independent breast cancer datasets, providing statistically\nconcordant results between the two analysis. We validated in cell lines the\nmiR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative\nsubtype phenotype through its control of proliferation and EMT.\n', 'title': 'Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer'}]","Title: Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer

Abstract: MicroRNAs play important roles in many biological processes. Their aberrant expression can have oncogenic or tumor suppressor function directly participating to carcinogenesis, malignant transformation, invasiveness and metastasis. Indeed, miRNA profiles can distinguish not only between normal and cancerous tissue but they can also successfully classify different subtypes of a particular cancer. Here, we focus on a particular class of transcripts encoding polycistronic miRNA genes that yields multiple miRNA components. We describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully redesigned release of the MMRA computational pipeline (MiRNA Master Regulator Analysis), developed to search for clustered miRNAs potentially driving cancer molecular subtyping. Genomically clustered miRNAs are frequently co-expressed to target different components of pro-tumorigenic signalling pathways. By applying ClustMMRA to breast cancer patient data, we identified key miRNA clusters driving the phenotype of different tumor subgroups. The pipeline was applied to two independent breast cancer datasets, providing statistically concordant results between the two analysis. We validated in cell lines the miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative subtype phenotype through its control of proliferation and EMT."
What are the technical challenges associated with stripe formation at small oblique angles in a traveling-wave problem?,fd96f484,"[{'abstract': '  We study the effect of domain growth on the orientation of striped phases in\na Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter\ndependence that allows stripe formation in a half plane, and suppresses\npatterns in the complement, while the boundary of the pattern-forming region is\npropagating with fixed normal velocity. We construct front solutions that leave\nbehind stripes in the pattern-forming region that are parallel to or at a small\noblique angle to the boundary.\nTechnically, the construction of stripe formation parallel to the boundary\nrelies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at\na small oblique angle are constructed using a functional-analytic, perturbative\napproach. Here, the main difficulties are the presence of continuous spectrum\nand the fact that small oblique angles appear as a singular perturbation in a\ntraveling-wave problem. We resolve the former difficulty using a farfield-core\ndecomposition and Fredholm theory in weighted spaces. The singular perturbation\nproblem is resolved using preconditioners and boot-strapping.\n', 'title': 'Pattern-forming fronts in a Swift-Hohenberg equation with directional quenching - parallel and oblique stripes'}]","Title: Pattern-forming fronts in a Swift-Hohenberg equation with directional quenching - parallel and oblique stripes

Abstract: We study the effect of domain growth on the orientation of striped phases in a Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter dependence that allows stripe formation in a half plane, and suppresses patterns in the complement, while the boundary of the pattern-forming region is propagating with fixed normal velocity. We construct front solutions that leave behind stripes in the pattern-forming region that are parallel to or at a small oblique angle to the boundary. 
Technically, the construction of stripe formation parallel to the boundary relies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at a small oblique angle are constructed using a functional-analytic, perturbative approach. Here, the main difficulties are the presence of continuous spectrum and the fact that small oblique angles appear as a singular perturbation in a traveling-wave problem. We resolve the former difficulty using a farfield-core decomposition and Fredholm theory in weighted spaces. The singular perturbation problem is resolved using preconditioners and boot-strapping."
What factors contribute to the  development of star formation in spiral arm spurs within galaxies like M51?,d298210e,"[{'abstract': '  The process that leads to the formation of the bright star forming sites\nobserved along prominent spiral arms remains elusive. We present results of a\nmulti-wavelength study of a spiral arm segment in the nearby grand-design\nspiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas\nspurs. The combined observations of the(ionized, atomic, molecular, dusty)\ninterstellar medium (ISM) with star formation tracers (HII regions, young\n<10Myr stellar clusters) suggest (1) no variation in giant molecular cloud\n(GMC) properties between arm and gas spurs, (2) gas spurs and extinction\nfeathers arising from the same structure with a close spatial relation between\ngas spurs and ongoing/recent star formation (despite higher gas surface\ndensities in the spiral arm), (3) no trend in star formation age either along\nthe arm or along a spur, (4) evidence for strong star formation feedback in gas\nspurs: (5) tentative evidence for star formation triggered by stellar feedback\nfor one spur, and (6) GMC associations (GMAs) being no special entities but the\nresult of blending of gas arm/spur cross-sections in lower resolution\nobservations. We conclude that there is no evidence for a coherent star\nformation onset mechanism that can be solely associated to the presence of the\nspiral density wave. This suggests that other (more localized) mechanisms are\nimportant to delay star formation such that it occurs in spurs. The evidence of\nstar formation proceeding over several million years within individual spurs\nimplies that the mechanism that leads to star formation acts or is sustained\nover a longer time-scale.\n', 'title': 'The PdBI Arcsecond Whirlpool Survey (PAWS). The Role of Spiral Arms in Cloud and Star Formation'}]","Based on the provided context, I found a relevant research paper that may answer your question:

Title: The PdBI Arcsecond Whirlpool Survey (PAWS). The Role of Spiral Arms in Cloud and Star Formation

Abstract: The process that leads to the formation of the bright star forming sites observed along prominent spiral arms remains elusive. We present results of a multi-wavelength study of a spiral arm segment in the nearby grand-design spiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas spurs. The combined observations of the (ionized, atomic, molecular, dusty) interstellar medium (ISM) with star formation tracers (HII regions, young <10Myr stellar clusters) suggest (1) no variation in giant molecular cloud (GMC) properties between arm and gas spurs, (2) gas spurs and extinction feathers arising from the same structure with a close spatial relation between gas spurs and ongoing/recent star formation (despite higher gas surface densities in the spiral arm), (3) no trend in star formation age either along the arm or along a spur, (4) evidence for strong star formation feedback in gas spurs: (5) tentative evidence for star formation triggered by stellar feedback for one spur, and (6) GMC associations (GMAs) being no special entities but the result of blending of gas arm/spur cross-sections in lower resolution observations. We conclude that there is no evidence for a coherent star formation onset mechanism that can be solely associated to the presence of the spiral density wave. This suggests that other (more localized) mechanisms are important to delay star formation such that it occurs in spurs. The evidence of star formation proceeding over several million years within individual spurs implies that the mechanism that leads to star formation acts or is sustained over a longer time-scale.

The factors that contribute to the development of star formation in spiral arm spurs within galaxies like M51, according to this study, include:

* Localized mechanisms that delay star formation, allowing it to occur in spurs
* Strong star formation feedback in gas spurs
* Tentative evidence for star formation triggered by stellar feedback
* The presence of gas spurs and extinction feathers arising from the same structure
* Close spatial relation between gas spurs and ongoing/recent star formation despite higher gas surface densities in the spiral arm

Please let me know if you would like me to search for more information or clarify any of these points."
What are some alternative methods for fitting support vector machines besides quadratic programming?,f563d740,"[{'abstract': '  Support vector machines (SVMs) are an important tool in modern data analysis.\nTraditionally, support vector machines have been fitted via quadratic\nprogramming, either using purpose-built or off-the-shelf algorithms. We present\nan alternative approach to SVM fitting via the majorization--minimization (MM)\nparadigm. Algorithms that are derived via MM algorithm constructions can be\nshown to monotonically decrease their objectives at each iteration, as well as\nbe globally convergent to stationary points. We demonstrate the construction of\niteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,\nfor SVM risk minimization problems involving the hinge, least-square,\nsquared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net\npenalizations. Successful implementations of our algorithms are presented via\nsome numerical examples.\n', 'title': 'Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines: A Majorization--Minimization Algorithm Approach'}]","Title: Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines: A Majorization--Minimization Algorithm Approach

Abstract: Support vector machines (SVMs) are an important tool in modern data analysis.
Traditionally, support vector machines have been fitted via quadratic
programming, either using purpose-built or off-the-shelf algorithms. We present
an alternative approach to SVM fitting via the majorization--minimization (MM)
paradigm. Algorithms that are derived via MM algorithm constructions can be
shown to monotonically decrease their objectives at each iteration, as well as
be globally convergent to stationary points. We demonstrate the construction of
iteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,
for SVM risk minimization problems involving the hinge, least-square,
squared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net
penalizations. Successful implementations of our algorithms are presented via
some numerical examples."
How many symmetries does the Waring decomposition  of sM_langle 3 angle have?,b92da3a4,"[{'abstract': '  The recent discovery that the exponent of matrix multiplication is determined\nby the rank of the symmetrized matrix multiplication tensor has invigorated\ninterest in better understanding symmetrized matrix multiplication. I present\nan explicit rank 18 Waring decomposition of $sM_{\\langle 3\\rangle}$ and\ndescribe its symmetry group.\n', 'title': 'A rank 18 Waring decomposition of $sM_{\\langle 3\\rangle}$ with 432 symmetries'}]","Title: A rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ with 432 symmetries

Abstract: The recent discovery that the exponent of matrix multiplication is determined by the rank of the symmetrized matrix multiplication tensor has invigorated interest in better understanding symmetrized matrix multiplication. I present an explicit rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ and describe its symmetry group.

Answer: The Waring decomposition of $sM_{\langle 3\rangle}$ has 432 symmetries."
What are the Navier-Stokes equations and were they solved in two dimensions?,f590328c,"[{'abstract': ""  The existence of weak solutions to the stationary Navier-Stokes equations in\nthe whole plane $\\mathbb{R}^2$ is proven. This particular geometry was the only\ncase left open since the work of Leray in 1933. The reason is that due to the\nabsence of boundaries the local behavior of the solutions cannot be controlled\nby the enstrophy in two dimensions. We overcome this difficulty by constructing\napproximate weak solutions having a prescribed mean velocity on some given\nbounded set. As a corollary, we obtain infinitely many weak solutions in\n$\\mathbb{R}^2$ parameterized by this mean velocity, which is reminiscent of the\nexpected convergence of the velocity field at large distances to any prescribed\nconstant vector field. This explicit parameterization of the weak solutions\nallows us to prove a weak-strong uniqueness theorem for small data. The\nquestion of the asymptotic behavior of the weak solutions remains however open,\nwhen the uniqueness theorem doesn't apply.\n"", 'title': 'Existence and uniqueness of steady weak solutions to the Navier-Stokes equations in $\\mathbb{R}^2$'}]","I can help you with that!

The Navier-Stokes equations were discussed in several papers, but I think you might be referring to the first paper:

Title: Existence and uniqueness of steady weak solutions to the Navier-Stokes equations in $\mathbb{R}^2$

Abstract: The existence of weak solutions to the stationary Navier-Stokes equations in the whole plane $\mathbb{R}^2$ is proven. This particular geometry was the only case left open since the work of Leray in 1933. The reason is that due to the absence of boundaries the local behavior of the solutions cannot be controlled by the enstrophy in two dimensions. We overcome this difficulty by constructing approximate weak solutions having a prescribed mean velocity on some given bounded set. As a corollary, we obtain infinitely many weak solutions in $\mathbb{R}^2$ parameterized by this mean velocity, which is reminiscent of the expected convergence of the velocity field at large distances to any prescribed constant vector field. This explicit parameterization of the weak solutions allows us to prove a weak-strong uniqueness theorem for small data. The question of the asymptotic behavior of the weak solutions remains however open, when the uniqueness theorem doesn't apply.

This paper does indeed solve the Navier-Stokes equations in two dimensions, specifically in the whole plane $\mathbb{R}^2$."
Can ceRNAs influence biological processes that involve regulatory oscillations?,c558d2ec,"[{'abstract': ""  Non-conding RNAs play a key role in the post-transcriptional regulation of\nmRNA translation and turnover in eukaryotes. miRNAs, in particular, interact\nwith their target RNAs through protein-mediated, sequence-specific binding,\ngiving rise to extended and highly heterogeneous miRNA-RNA interaction\nnetworks. Within such networks, competition to bind miRNAs can generate an\neffective positive coupling between their targets. Competing endogenous RNAs\n(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.\nAlbeit potentially weak, ceRNA interactions can occur both dynamically,\naffecting e.g. the regulatory clock, and at stationarity, in which case ceRNA\nnetworks as a whole can be implicated in the composition of the cell's\nproteome. Many features of ceRNA interactions, including the conditions under\nwhich they become significant, can be unraveled by mathematical and in silico\nmodels. We review the understanding of the ceRNA effect obtained within such\nframeworks, focusing on the methods employed to quantify it, its role in the\nprocessing of gene expression noise, and how network topology can determine its\nreach.\n"", 'title': 'Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs'}]","Title: Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs

Abstract:   Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.

This paper explicitly mentions that ceRNA interactions can affect the regulatory clock, which implies that they can influence biological processes that involve regulatory oscillations."
What is an efficient method for approximating a large matrix when individual row or column sampling is costly?,1476108a,"[{'abstract': ""  A common problem in large-scale data analysis is to approximate a matrix\nusing a combination of specifically sampled rows and columns, known as CUR\ndecomposition. Unfortunately, in many real-world environments, the ability to\nsample specific individual rows or columns of the matrix is limited by either\nsystem constraints or cost. In this paper, we consider matrix approximation by\nsampling predefined \\emph{blocks} of columns (or rows) from the matrix. We\npresent an algorithm for sampling useful column blocks and provide novel\nguarantees for the quality of the approximation. This algorithm has application\nin problems as diverse as biometric data analysis to distributed computing. We\ndemonstrate the effectiveness of the proposed algorithms for computing the\nBlock CUR decomposition of large matrices in a distributed setting with\nmultiple nodes in a compute cluster, where such blocks correspond to columns\n(or rows) of the matrix stored on the same node, which can be retrieved with\nmuch less overhead than retrieving individual columns stored across different\nnodes. In the biometric setting, the rows correspond to different users and\ncolumns correspond to users' biometric reaction to external stimuli, {\\em\ne.g.,}~watching video content, at a particular time instant. There is\nsignificant cost in acquiring each user's reaction to lengthy content so we\nsample a few important scenes to approximate the biometric response. An\nindividual time sample in this use case cannot be queried in isolation due to\nthe lack of context that caused that biometric reaction. Instead, collections\nof time segments ({\\em i.e.,} blocks) must be presented to the user. The\npractical application of these algorithms is shown via experimental results\nusing real-world user biometric data from a content testing environment.\n"", 'title': 'Block CUR: Decomposing Matrices using Groups of Columns'}]","I think I can help you with that!

Title: Block CUR: Decomposing Matrices using Groups of Columns

Abstract: A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. We present an algorithm for sampling useful column blocks and provide novel guarantees for the quality of the approximation. This algorithm has application in problems as diverse as biometric data analysis to distributed computing. We demonstrate the effectiveness of the proposed algorithms for computing the Block CUR decomposition of large matrices in a distributed setting with multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. In the biometric setting, the rows correspond to different users and columns correspond to users' biometric reaction to external stimuli, {\em e.g.,}~watching video content, at a particular time instant. There is significant cost in acquiring each user's reaction to lengthy content so we sample a few important scenes to approximate the biometric response. An individual time sample in this use case cannot be queried in isolation due to the lack of context that caused that biometric reaction. Instead, collections of time segments ({\em i.e.,} blocks) must be presented to the user. The practical application of these algorithms is shown via experimental results using real-world user biometric data from a content testing environment."
What type of baseline representation is compared to the learned sparse representations in the paper?,1fb8bf53,"[{'abstract': '  A variety of representation learning approaches have been investigated for\nreinforcement learning; much less attention, however, has been given to\ninvestigating the utility of sparse coding. Outside of reinforcement learning,\nsparse coding representations have been widely used, with non-convex objectives\nthat result in discriminative representations. In this work, we develop a\nsupervised sparse coding objective for policy evaluation. Despite the\nnon-convexity of this objective, we prove that all local minima are global\nminima, making the approach amenable to simple optimization strategies. We\nempirically show that it is key to use a supervised objective, rather than the\nmore straightforward unsupervised sparse coding approach. We compare the\nlearned representations to a canonical fixed sparse representation, called\ntile-coding, demonstrating that the sparse coding representation outperforms a\nwide variety of tilecoding representations.\n', 'title': 'Learning Sparse Representations in Reinforcement Learning with Sparse Coding'}]","Title: Learning Sparse Representations in Reinforcement Learning with Sparse Coding

Abstract:   A variety of representation learning approaches have been investigated for
reinforcement learning; much less attention, however, has been given to
investigating the utility of sparse coding. Outside of reinforcement learning,
sparse coding representations have been widely used, with non-convex objectives
that result in discriminative representations. In this work, we develop a
supervised sparse coding objective for policy evaluation. Despite the
non-convexity of this objective, we prove that all local minima are global
minima, making the approach amenable to simple optimization strategies. We
empirically show that it is key to use a supervised objective, rather than the
more straightforward unsupervised sparse coding approach. We compare the
learned representations to a canonical fixed sparse representation, called
tile-coding, demonstrating that the sparse coding representation outperforms a
wide variety of tilecoding representations.

The baseline representation compared to the learned sparse representations in this paper is ""tile-coding""."
What are the challenges faced by replenishment managers when predicting sales of products sensitive to weather events?,40c5cbdb,"[{'abstract': '  One key requirement for effective supply chain management is the quality of\nits inventory management. Various inventory management methods are typically\nemployed for different types of products based on their demand patterns,\nproduct attributes, and supply network. In this paper, our goal is to develop\nrobust demand prediction methods for weather sensitive products at retail\nstores. We employ historical datasets from Walmart, whose customers and markets\nare often exposed to extreme weather events which can have a huge impact on\nsales regarding the affected stores and products. We want to accurately predict\nthe sales of 111 potentially weather-sensitive products around the time of\nmajor weather events at 45 of Walmart retails locations in the U.S.\nIntuitively, we may expect an uptick in the sales of umbrellas before a big\nthunderstorm, but it is difficult for replenishment managers to predict the\nlevel of inventory needed to avoid being out-of-stock or overstock during and\nafter that storm. While they rely on a variety of vendor tools to predict sales\naround extreme weather events, they mostly employ a time-consuming process that\nlacks a systematic measure of effectiveness. We employ all the methods critical\nto any analytics project and start with data exploration. Critical features are\nextracted from the raw historical dataset for demand forecasting accuracy and\nrobustness. In particular, we employ Artificial Neural Network for forecasting\ndemand for each product sold around the time of major weather events. Finally,\nwe evaluate our model to evaluate their accuracy and robustness.\n', 'title': 'Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores'}]","Title: Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores

Abstract: One key requirement for effective supply chain management is the quality of its inventory management. Various inventory management methods are typically employed for different types of products based on their demand patterns, product attributes, and supply network. In this paper, our goal is to develop robust demand prediction methods for weather sensitive products at retail stores. We employ historical datasets from Walmart, whose customers and markets are often exposed to extreme weather events which can have a huge impact on sales regarding the affected stores and products. We want to accurately predict the sales of 111 potentially weather-sensitive products around the time of major weather events at 45 of Walmart retails locations in the U.S. Intuitively, we may expect an uptick in the sales of umbrellas before a big thunderstorm, but it is difficult for replenishment managers to predict the level of inventory needed to avoid being out-of-stock or overstock during and after that storm. While they rely on a variety of vendor tools to predict sales around extreme weather events, they mostly employ a time-consuming process that lacks a systematic measure of effectiveness."
Can relying on p-values to assess research robustness lead to an inaccurate understanding of replicability?,8df2e984,"[{'abstract': ""  We show that publishing results using the statistical significance\nfilter---publishing only when the p-value is less than 0.05---leads to a\nvicious cycle of overoptimistic expectation of the replicability of results.\nFirst, we show analytically that when true statistical power is relatively low,\ncomputing power based on statistically significant results will lead to\noverestimates of power. Then, we present a case study using 10 experimental\ncomparisons drawn from a recently published meta-analysis in psycholinguistics\n(Jäger et al., 2017). We show that the statistically significant results\nyield an illusion of replicability. This illusion holds even if the researcher\ndoesn't conduct any formal power analysis but just uses statistical\nsignificance to informally assess robustness (i.e., replicability) of results.\n"", 'title': 'The statistical significance filter leads to overconfident expectations of replicability'}]","Here is the answer:

Title: The statistical significance filter leads to overconfident expectations of replicability

Abstract: We show that publishing results using the statistical significance filter---publishing only when the p-value is less than 0.05---leads to a vicious cycle of overoptimistic expectation of the replicability of results. First, we show analytically that when true statistical power is relatively low, computing power based on statistically significant results will lead to overestimates of power. Then, we present a case study using 10 experimental comparisons drawn from a recently published meta-analysis in psycholinguistics (Jäger et al., 2017). We show that the statistically significant results yield an illusion of replicability. This illusion holds even if the researcher doesn't conduct any formal power analysis but just uses statistical significance to informally assess robustness (i.e., replicability) of results."
What is the range of label set sizes used in experiments evaluating Subset LLDA?,543dbf40,"[{'abstract': '  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard\nunsupervised Latent Dirichlet Allocation (LDA) algorithm, to address\nmulti-label learning tasks. Previous work has shown it to perform in par with\nother state-of-the-art multi-label methods. Nonetheless, with increasing label\nsets sizes LLDA encounters scalability issues. In this work, we introduce\nSubset LLDA, a simple variant of the standard LLDA algorithm, that not only can\neffectively scale up to problems with hundreds of thousands of labels but also\nimproves over the LLDA state-of-the-art. We conduct extensive experiments on\neight data sets, with label sets sizes ranging from hundreds to hundreds of\nthousands, comparing our proposed algorithm with the previously proposed LLDA\nalgorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme\nmulti-label classification. The results show a steady advantage of our method\nover the other LLDA algorithms and competitive results compared to the extreme\nmulti-label classification algorithms.\n', 'title': 'Subset Labeled LDA for Large-Scale Multi-Label Classification'}]","Title: Subset Labeled LDA for Large-Scale Multi-Label Classification

Abstract: Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address multi-label learning tasks. Previous work has shown it to perform in par with other state-of-the-art multi-label methods. Nonetheless, with increasing label sets sizes LLDA encounters scalability issues. In this work, we introduce Subset LLDA, a simple variant of the standard LLDA algorithm, that not only can effectively scale up to problems with hundreds of thousands of labels but also improves over the LLDA state-of-the-art. We conduct extensive experiments on eight data sets, with label sets sizes ranging from hundreds to hundreds of thousands, comparing our proposed algorithm with the previously proposed LLDA algorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme multi-label classification. The results show a steady advantage of our method over the other LLDA algorithms and competitive results compared to the extreme multi-label classification algorithms.

Answer: The range of label set sizes used in experiments evaluating Subset LLDA is from hundreds to hundreds of thousands."
What experimental technique was used to study the temperature dependence of f-electron spectral weight in CeCoIn5?,99ff5c0e,"[{'abstract': '  The temperature-dependent evolution of the Kondo lattice is a long-standing\ntopic of theoretical and experimental investigation and yet it lacks a truly\nmicroscopic description of the relation of the basic $f$-$d$ hybridization\nprocesses to the fundamental temperature scales of Kondo screening and\nFermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$\nhybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo\nlattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved\nphotoemission (ARPES) with sufficient detail to allow direct comparison to\nfirst principles dynamical mean field theory (DMFT) calculations containing\nfull realism of crystalline electric field states. The ARPES results, for two\northogonal (001) and (100) cleaved surfaces and three different $f$-$d$\nhybridization scenarios, with additional microscopic insight provided by DMFT,\nreveal $f$ participation in the Fermi surface at temperatures much higher than\nthe lattice coherence temperature, $T^*\\approx$ 45 K, commonly believed to be\nthe onset for such behavior. The identification of a $T$-dependent crystalline\nelectric field degeneracy crossover in the DMFT theory $below$ $T^*$ is\nspecifically highlighted.\n', 'title': 'Evolution of the Kondo lattice electronic structure above the transport coherence temperature'}]","Here's the answer:

Title: Evolution of the Kondo lattice electronic structure above the transport coherence temperature

Abstract: The temperature-dependent evolution of the Kondo lattice is a long-standing topic of theoretical and experimental investigation and yet it lacks a truly microscopic description of the relation of the basic $f$-$d$ hybridization processes to the fundamental temperature scales of Kondo screening and Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$ hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved photoemission (ARPES) with sufficient detail to allow direct comparison to first principles dynamical mean field theory (DMFT) calculations containing full realism of crystalline electric field states. The ARPES results, for two orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$ hybridization scenarios, with additional microscopic insight provided by DMFT, reveal $f$ participation in the Fermi surface at temperatures much higher than the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be the onset for such behavior. The identification of a $T$-dependent crystalline electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is specifically highlighted.

The experimental technique used to study the temperature dependence of f-electron spectral weight in CeCoIn5 is f-resonant angle-resolved photoemission (ARPES)."
How do tropical recurrent sequences behave when the Newton polygon of the associated vector has a single bounded edge?,e77a2ef1,"[{'abstract': '  Tropical recurrent sequences are introduced satisfying a given vector (being\na tropical counterpart of classical linear recurrent sequences). We consider\nthe case when Newton polygon of the vector has a single (bounded) edge. In this\ncase there are periodic tropical recurrent sequences which are similar to\nclassical linear recurrent sequences. A question is studied when there exists a\nnon-periodic tropical recurrent sequence satisfying a given vector, and partial\nanswers are provided to this question. Also an algorithm is designed which\ntests existence of non-periodic tropical recurrent sequences satisfying a given\nvector with integer coordinates. Finally, we introduce a tropical entropy of a\nvector and provide some bounds on it.\n', 'title': 'Tropical recurrent sequences'}]","Title: Tropical recurrent sequences

Abstract: Tropical recurrent sequences are introduced satisfying a given vector (being
a tropical counterpart of classical linear recurrent sequences). We consider
the case when Newton polygon of the vector has a single (bounded) edge. In this
case there are periodic tropical recurrent sequences which are similar to
classical linear recurrent sequences. A question is studied when there exists a
non-periodic tropical recurrent sequence satisfying a given vector, and partial
answers are provided to this question. Also an algorithm is designed which
tests existence of non-periodic tropical recurrent sequences satisfying a given
vector with integer coordinates. Finally, we introduce a tropical entropy of a
vector and provide some bounds on it."
What simulator was created to assist in the development and testing of autonomous vehicles algorithms?,ac8fced9,"[{'abstract': '  Developing and testing algorithms for autonomous vehicles in real world is an\nexpensive and time consuming process. Also, in order to utilize recent advances\nin machine intelligence and deep learning we need to collect a large amount of\nannotated training data in a variety of conditions and environments. We present\na new simulator built on Unreal Engine that offers physically and visually\nrealistic simulations for both of these goals. Our simulator includes a physics\nengine that can operate at a high frequency for real-time hardware-in-the-loop\n(HITL) simulations with support for popular protocols (e.g. MavLink). The\nsimulator is designed from the ground up to be extensible to accommodate new\ntypes of vehicles, hardware platforms and software protocols. In addition, the\nmodular design enables various components to be easily usable independently in\nother projects. We demonstrate the simulator by first implementing a quadrotor\nas an autonomous vehicle and then experimentally comparing the software\ncomponents with real-world flights.\n', 'title': 'AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles'}]","Title: AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles

Abstract: Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights."
What experimental comparisons have been conducted to evaluate the proposed method for concept similarity measurement?,808a67c9,"[{'abstract': '  With the use of ontologies in several domains such as semantic web,\ninformation retrieval, artificial intelligence, the concept of similarity\nmeasuring has become a very important domain of research. Therefore, in the\ncurrent paper, we propose our method of similarity measuring which uses the\nDijkstra algorithm to define and compute the shortest path. Then, we use this\none to compute the semantic distance between two concepts defined in the same\nhierarchy of ontology. Afterward, we base on this result to compute the\nsemantic similarity. Finally, we present an experimental comparison between our\nmethod and other methods of similarity measuring.\n', 'title': 'An enhanced method to compute the similarity between concepts of ontology'}]","I think I have found the answer to your question!

Title: An enhanced method to compute the similarity between concepts of ontology

Abstract: With the use of ontologies in several domains such as semantic web, information retrieval, artificial intelligence, the concept of similarity measuring has become a very important domain of research. Therefore, in the current paper, we propose our method of similarity measuring which uses the Dijkstra algorithm to define and compute the shortest path. Then, we use this one to compute the semantic distance between two concepts defined in the same hierarchy of ontology. Afterward, we base on this result to compute the semantic similarity. Finally, we present an experimental comparison between our method and other methods of similarity measuring."
How can simulated robot experiments be used to improve instance grasping in cluttered scenes?,05e4d60b,"[{'abstract': '  Learning-based approaches to robotic manipulation are limited by the\nscalability of data collection and accessibility of labels. In this paper, we\npresent a multi-task domain adaptation framework for instance grasping in\ncluttered scenes by utilizing simulated robot experiments. Our neural network\ntakes monocular RGB images and the instance segmentation mask of a specified\ntarget object as inputs, and predicts the probability of successfully grasping\nthe specified object for each candidate motor command. The proposed transfer\nlearning framework trains a model for instance grasping in simulation and uses\na domain-adversarial loss to transfer the trained model to real robots using\nindiscriminate grasping data, which is available both in simulation and the\nreal world. We evaluate our model in real-world robot experiments, comparing it\nwith alternative model architectures as well as an indiscriminate grasping\nbaseline.\n', 'title': 'Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation'}]","The researcher is looking for information on how simulated robot experiments can be used to improve instance grasping in cluttered scenes.

Title: Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation

Abstract: Learning-based approaches to robotic manipulation are limited by the scalability of data collection and accessibility of labels. In this paper, we present a multi-task domain adaptation framework for instance grasping in cluttered scenes by utilizing simulated robot experiments. Our neural network takes monocular RGB images and the instance segmentation mask of a specified target object as inputs, and predicts the probability of successfully grasping the specified object for each candidate motor command. The proposed transfer learning framework trains a model for instance grasping in simulation and uses a domain-adversarial loss to transfer the trained model to real robots using indiscriminate grasping data, which is available both in simulation and the real world. We evaluate our model in real-world robot experiments, comparing it with alternative model architectures as well as an indiscriminate grasping baseline."
What are the time and memory complexities of the  UCPI algorithm for spectral gap estimation?,1dcd9f7d,"[{'abstract': '  We consider the problem of estimating from sample paths the absolute spectral\ngap $\\gamma_*$ of a reversible, irreducible and aperiodic Markov chain\n$(X_t)_{t \\in \\mathbb{N}}$ over a finite state $\\Omega$. We propose the ${\\tt\nUCPI}$ (Upper Confidence Power Iteration) algorithm for this problem, a\nlow-complexity algorithm which estimates the spectral gap in time ${\\cal O}(n)$\nand memory space ${\\cal O}((\\ln n)^2)$ given $n$ samples. This is in stark\ncontrast with most known methods which require at least memory space ${\\cal\nO}(|\\Omega|)$, so that they cannot be applied to large state spaces.\nFurthermore, ${\\tt UCPI}$ is amenable to parallel implementation.\n', 'title': 'Computationally Efficient Estimation of the Spectral Gap of a Markov Chain'}]","Title: Computationally Efficient Estimation of the Spectral Gap of a Markov Chain

Abstract: We consider the problem of estimating from sample paths the absolute spectral gap $\gamma_*$ of a reversible, irreducible and aperiodic Markov chain $(X_t)_{t \in \mathbb{N}}$ over a finite state $\Omega$. We propose the ${\tt UCPI}$ (Upper Confidence Power Iteration) algorithm for this problem, a low-complexity algorithm which estimates the spectral gap in time ${\cal O}(n)$ and memory space ${\cal O}((\ln n)^2)$ given $n$ samples. This is in stark contrast with most known methods which require at least memory space ${\cal O}(|\Omega|)$, so that they cannot be applied to large state spaces. Furthermore, ${\tt UCPI}$ is amenable to parallel implementation.

The time complexity is O(n) and the memory complexity is O((ln n)^2)."
In what ways do entropy-based features compare to traditional methods in surgical skills assessment?,13dc61bf,"[{'abstract': '  Purpose: Basic surgical skills of suturing and knot tying are an essential\npart of medical training. Having an automated system for surgical skills\nassessment could help save experts time and improve training efficiency. There\nhave been some recent attempts at automated surgical skills assessment using\neither video analysis or acceleration data. In this paper, we present a novel\napproach for automated assessment of OSATS based surgical skills and provide an\nanalysis of different features on multi-modal data (video and accelerometer\ndata). Methods: We conduct the largest study, to the best of our knowledge, for\nbasic surgical skills assessment on a dataset that contained video and\naccelerometer data for suturing and knot-tying tasks. We introduce ""entropy\nbased"" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy\n(XApEn), which quantify the amount of predictability and regularity of\nfluctuations in time-series data. The proposed features are compared to\nexisting methods of Sequential Motion Texture (SMT), Discrete Cosine Transform\n(DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment.\nResults: We report average performance of different features across all\napplicable OSATS criteria for suturing and knot tying tasks. Our analysis shows\nthat the proposed entropy based features out-perform previous state-of-the-art\nmethods using video data. For accelerometer data, our method performs better\nfor suturing only. We also show that fusion of video and acceleration features\ncan improve overall performance with the proposed entropy features achieving\nhighest accuracy. Conclusions: Automated surgical skills assessment can be\nachieved with high accuracy using the proposed entropy features. Such a system\ncan significantly improve the efficiency of surgical training in medical\nschools and teaching hospitals.\n', 'title': 'Video and Accelerometer-Based Motion Analysis for Automated Surgical Skills Assessment'}]","I'd be happy to help!

Title: Video and Accelerometer-Based Motion Analysis for Automated Surgical Skills Assessment

Abstract: 
Purpose: Basic surgical skills of suturing and knot tying are an essential part of medical training. Having an automated system for surgical skills assessment could help save experts time and improve training efficiency. There have been some recent attempts at automated surgical skills assessment using either video analysis or acceleration data. In this paper, we present a novel approach for automated assessment of OSATS based surgical skills and provide an analysis of different features on multi-modal data (video and accelerometer data). Methods: We conduct the largest study, to the best of our knowledge, for basic surgical skills assessment on a dataset that contained video and accelerometer data for suturing and knot-tying tasks. We introduce ""entropy-based"" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy (XApEn), which quantify the amount of predictability and regularity of fluctuations in time-series data. The proposed features are compared to existing methods of Sequential Motion Texture (SMT), Discrete Cosine Transform (DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment. Results: We report average performance of different features across all applicable OSATS criteria for suturing and knot tying tasks. Our analysis shows that the proposed entropy-based features out-perform previous state-of-the-art methods using video data. For accelerometer data, our method performs better for suturing only. We also show that fusion of video and acceleration features can improve overall performance with the proposed entropy features achieving highest accuracy. Conclusions: Automated surgical skills assessment can be achieved with high accuracy using the proposed entropy features. Such a system can significantly improve the efficiency of surgical training in medical schools and teaching hospitals.

In this paper, the authors compare their proposed entropy-based features (ApEn and XApEn) with traditional methods (SMT, DCT, and DFT) in surgical skills assessment. The results show that the entropy-based features outperform the traditional methods using video data, and perform better for suturing tasks using accelerometer data."
Why does the author consider the 2017 FQXi essay contest theme premature?,2ad242b7,"[{'abstract': '  Deconstruction of the theme of the 2017 FQXi essay contest is already an\ninteresting exercise in its own right: Teleology is rarely useful in physics\n--- the only known mainstream physics example (black hole event horizons) has a\nvery mixed score-card --- so the ""goals"" and ""aims and intentions"" alluded to\nin the theme of the 2017 FQXi essay contest are already somewhat pushing the\nlimits. Furthermore, ""aims and intentions"" certainly carries the implication of\nconsciousness, and opens up a whole can of worms related to the mind-body\nproblem. As for ""mindless mathematical laws"", that allusion is certainly in\ntension with at least some versions of the ""mathematical universe hypothesis"".\nFinally ""wandering towards a goal"" again carries the implication of\nconsciousness, with all its attendant problems.\nIn this essay I will argue, simply because we do not yet have any really good\nmathematical or physical theory of consciousness, that the theme of this essay\ncontest is premature, and unlikely to lead to any resolution that would be\nwidely accepted in the mathematics or physics communities.\n', 'title': 'From mindless mathematics to thinking meat?'}]","Title: From mindless mathematics to thinking meat?

Abstract: Deconstruction of the theme of the 2017 FQXi essay contest is already an interesting exercise in its own right: Teleology is rarely useful in physics --- the only known mainstream physics example (black hole event horizons) has a very mixed score-card --- so the ""goals"" and ""aims and intentions"" alluded to in the theme of the 2017 FQXi essay contest are already somewhat pushing the limits. Furthermore, ""aims and intentions"" certainly carries the implication of consciousness, and opens up a whole can of worms related to the mind-body problem. As for ""mindless mathematical laws"", that allusion is certainly in tension with at least some versions of the ""mathematical universe hypothesis"". Finally ""wandering towards a goal"" again carries the implication of consciousness, with all its attendant problems. In this essay I will argue, simply because we do not yet have any really good mathematical or physical theory of consciousness, that the theme of this essay contest is premature, and unlikely to lead to any resolution that would be widely accepted in the mathematics or physics communities.

The author considers the 2017 FQXi essay contest theme premature because we do not yet have a good mathematical or physical theory of consciousness, which is implied in the theme's reference to ""goals"", ""aims and intentions"", and ""wandering towards a goal""."
What are the limitations of the AdaBoost algorithm in classification tasks?,7c5f6206,"[{'abstract': '  Well-known for its simplicity and effectiveness in classification, AdaBoost,\nhowever, suffers from overfitting when class-conditional distributions have\nsignificant overlap. Moreover, it is very sensitive to noise that appears in\nthe labels. This article tackles the above limitations simultaneously via\noptimizing a modified loss function (i.e., the conditional risk). The proposed\napproach has the following two advantages. (1) It is able to directly take into\naccount label uncertainty with an associated label confidence. (2) It\nintroduces a ""trustworthiness"" measure on training samples via the Bayesian\nrisk rule, and hence the resulting classifier tends to have finite sample\nperformance that is superior to that of the original AdaBoost when there is a\nlarge overlap between class conditional distributions. Theoretical properties\nof the proposed method are investigated. Extensive experimental results using\nsynthetic data and real-world data sets from UCI machine learning repository\nare provided. The empirical study shows the high competitiveness of the\nproposed method in predication accuracy and robustness when compared with the\noriginal AdaBoost and several existing robust AdaBoost algorithms.\n', 'title': 'Robust and Efficient Boosting Method using the Conditional Risk'}]","I found the answer in the first abstract.

Title: Robust and Efficient Boosting Method using the Conditional Risk

Abstract: Well-known for its simplicity and effectiveness in classification, AdaBoost, however, suffers from overfitting when class-conditional distributions have significant overlap. Moreover, it is very sensitive to noise that appears in the labels.

So, the limitations of the AdaBoost algorithm in classification tasks are:

* Overfitting when class-conditional distributions have significant overlap
* Sensitivity to noise in the labels."
How do sparse ternary projections compare to traditional Gaussian projections in terms of hardware implementation?,a22b9d6e,"[{'abstract': '  Compressed sensing (CS) is a sampling theory that allows reconstruction of\nsparse (or compressible) signals from an incomplete number of measurements,\nusing of a sensing mechanism implemented by an appropriate projection matrix.\nThe CS theory is based on random Gaussian projection matrices, which satisfy\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\nprojections are more suitable for hardware implementation. In this paper, we\npresent a deep learning approach to obtain very sparse ternary projections for\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\nexperimental results on real images demonstrate the effectiveness of the\nproposed approach compared to state-of-the-art methods, with significant\nadvantage in terms of complexity.\n', 'title': 'Deep Learning Sparse Ternary Projections for Compressed Sensing of Images'}]","Title: Deep Learning Sparse Ternary Projections for Compressed Sensing of Images

Abstract: Compressed sensing (CS) is a sampling theory that allows reconstruction of sparse (or compressible) signals from an incomplete number of measurements, using of a sensing mechanism implemented by an appropriate projection matrix. The CS theory is based on random Gaussian projection matrices, which satisfy recovery guarantees with high probability; however, sparse ternary {0, -1, +1} projections are more suitable for hardware implementation. In this paper, we present a deep learning approach to obtain very sparse ternary projections for compressed sensing. Our deep learning architecture jointly learns a pair of a projection matrix and a reconstruction operator in an end-to-end fashion. The experimental results on real images demonstrate the effectiveness of the proposed approach compared to state-of-the-art methods, with significant advantage in terms of complexity.

According to the abstract, sparse ternary projections are more suitable for hardware implementation compared to traditional Gaussian projections."
Can deep learning models be used to estimate the amount of difference a human observer would perceive between two images?,052f7042,"[{'abstract': '  Computer vision has made remarkable progress in recent years. Deep neural\nnetwork (DNN) models optimized to identify objects in images exhibit\nunprecedented task-trained accuracy and, remarkably, some generalization\nability: new visual problems can now be solved more easily based on previous\nlearning. Biological vision (learned in life and through evolution) is also\naccurate and general-purpose. Is it possible that these different learning\nregimes converge to similar problem-dependent optimal computations? We\ntherefore asked whether the human system-level computation of visual perception\nhas DNN correlates and considered several anecdotal test cases. We found that\nperceptual sensitivity to image changes has DNN mid-computation correlates,\nwhile sensitivity to segmentation, crowding and shape has DNN end-computation\ncorrelates. Our results quantify the applicability of using DNN computation to\nestimate perceptual loss, and are consistent with the fascinating theoretical\nview that properties of human perception are a consequence of\narchitecture-independent visual learning.\n', 'title': 'Human perception in computer vision'}]","After reviewing the abstracts, I found a relevant paper that might answer your question.

Title: Human perception in computer vision

Abstract: Computer vision has made remarkable progress in recent years. Deep neural network (DNN) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and, remarkably, some generalization ability: new visual problems can now be solved more easily based on previous learning. Biological vision (learned in life and through evolution) is also accurate and general-purpose. Is it possible that these different learning regimes converge to similar problem-dependent optimal computations? We therefore asked whether the human system-level computation of visual perception has DNN correlates and considered several anecdotal test cases. We found that perceptual sensitivity to image changes has DNN mid-computation correlates, while sensitivity to segmentation, crowding, and shape has DNN end-computation correlates. Our results quantify the applicability of using DNN computation to estimate perceptual loss, and are consistent with the fascinating theoretical view that properties of human perception are a consequence of architecture-independent visual learning.

This paper explores the relationship between human perception and deep neural networks in computer vision. The authors investigate whether DNN models can estimate human perceptual sensitivity to image changes, and they found that DNN computation can be used to estimate perceptual loss. This suggests that deep learning models can be used to estimate the amount of difference a human observer would perceive between two images."
What are the advantages of Self-organizing Networks (SON) for 4G and 5G networks?,fcf8573a,"[{'abstract': '  In this paper, we provide an analysis of self-organized network management,\nwith an end-to-end perspective of the network. Self-organization as applied to\ncellular networks is usually referred to Self-organizing Networks (SONs), and\nit is a key driver for improving Operations, Administration, and Maintenance\n(OAM) activities. SON aims at reducing the cost of installation and management\nof 4G and future 5G networks, by simplifying operational tasks through the\ncapability to configure, optimize and heal itself. To satisfy 5G network\nmanagement requirements, this autonomous management vision has to be extended\nto the end to end network. In literature and also in some instances of products\navailable in the market, Machine Learning (ML) has been identified as the key\ntool to implement autonomous adaptability and take advantage of experience when\nmaking decisions. In this paper, we survey how network management can\nsignificantly benefit from ML solutions. We review and provide the basic\nconcepts and taxonomy for SON, network management and ML. We analyse the\navailable state of the art in the literature, standardization, and in the\nmarket. We pay special attention to 3rd Generation Partnership Project (3GPP)\nevolution in the area of network management and to the data that can be\nextracted from 3GPP networks, in order to gain knowledge and experience in how\nthe network is working, and improve network performance in a proactive way.\nFinally, we go through the main challenges associated with this line of\nresearch, in both 4G and in what 5G is getting designed, while identifying new\ndirections for research.\n', 'title': 'From 4G to 5G: Self-organized Network Management meets Machine Learning'}]","Title: From 4G to 5G: Self-organized Network Management meets Machine Learning

Abstract:   In this paper, we provide an analysis of self-organized network management,
with an end-to-end perspective of the network. Self-organization as applied to
cellular networks is usually referred to Self-organizing Networks (SONs), and
it is a key driver for improving Operations, Administration, and Maintenance
(OAM) activities. SON aims at reducing the cost of installation and management
of 4G and future 5G networks, by simplifying operational tasks through the
capability to configure, optimize and heal itself.

According to this abstract, the advantages of Self-organizing Networks (SON) for 4G and 5G networks are:

1. Reducing the cost of installation and management of networks.
2. Simplifying operational tasks through the capability to configure, optimize, and heal itself.
3. Improving Operations, Administration, and Maintenance (OAM) activities."
What are the strengths and weaknesses of different strategies for handling problematic constraints in ASP?,9d0a7a2a,"[{'abstract': '  Answer Set Programming (ASP) is a well-established declarative paradigm. One\nof the successes of ASP is the availability of efficient systems.\nState-of-the-art systems are based on the ground+solve approach. In some\napplications this approach is infeasible because the grounding of one or few\nconstraints is expensive. In this paper, we systematically compare alternative\nstrategies to avoid the instantiation of problematic constraints, that are\nbased on custom extensions of the solver. Results on real and synthetic\nbenchmarks highlight some strengths and weaknesses of the different strategies.\n(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)\n', 'title': 'Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis'}]","I think I can help you with that!

The answer to your question can be found in the first research paper:

Title: Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis

Abstract: Answer Set Programming (ASP) is a well-established declarative paradigm. One of the successes of ASP is the availability of efficient systems. State-of-the-art systems are based on the ground+solve approach. In some applications this approach is infeasible because the grounding of one or few constraints is expensive. In this paper, we systematically compare alternative strategies to avoid the instantiation of problematic constraints, that are based on custom extensions of the solver. Results on real and synthetic benchmarks highlight some strengths and weaknesses of the different strategies."
What are some of the most common applications of Genetic Programming (GP) in Image Processing (IP) ?,20be1095,"[{'abstract': '  During the last two decades, Genetic Programming (GP) has been largely used\nto tackle optimization, classification, and automatic features selection\nrelated tasks. The widespread use of GP is mainly due to its flexible and\ncomprehensible tree-type structure. Similarly, research is also gaining\nmomentum in the field of Image Processing (IP) because of its promising results\nover wide areas of applications ranging from medical IP to multispectral\nimaging. IP is mainly involved in applications such as computer vision, pattern\nrecognition, image compression, storage and transmission, and medical\ndiagnostics. This prevailing nature of images and their associated algorithm\ni.e complexities gave an impetus to the exploration of GP. GP has thus been\nused in different ways for IP since its inception. Many interesting GP\ntechniques have been developed and employed in the field of IP. To give the\nresearch community an extensive view of these techniques, this paper presents\nthe diverse applications of GP in IP and provides useful resources for further\nresearch. Also, comparison of different parameters used in ten different\napplications of IP are summarized in tabular form. Moreover, analysis of\ndifferent parameters used in IP related tasks is carried-out to save the time\nneeded in future for evaluating the parameters of GP. As more advancement is\nmade in GP methodologies, its success in solving complex tasks not only related\nto IP but also in other fields will increase. Additionally, guidelines are\nprovided for applying GP in IP related tasks, pros and cons of GP techniques\nare discussed, and some future directions are also set.\n', 'title': 'A Recent Survey on the Applications of Genetic Programming in Image Processing'}]","A research question!

After carefully reading the provided abstracts, I found that the first title matches your question:

Title: A Recent Survey on the Applications of Genetic Programming in Image Processing

Abstract: During the last two decades, Genetic Programming (GP) has been largely used to tackle optimization, classification, and automatic features selection related tasks. The widespread use of GP is mainly due to its flexible and comprehensible tree-type structure. Similarly, research is also gaining momentum in the field of Image Processing (IP) because of its promising results over wide areas of applications ranging from medical IP to multispectral imaging. IP is mainly involved in applications such as computer vision, pattern recognition, image compression, storage and transmission, and medical diagnostics. This prevailing nature of images and their associated algorithm i.e complexities gave an impetus to the exploration of GP. GP has thus been used in different ways for IP since its inception. Many interesting GP techniques have been developed and employed in the field of IP. To give the research community an extensive view of these techniques, this paper presents the diverse applications of GP in IP and provides useful resources for further research. Also, comparison of different parameters used in ten different applications of IP are summarized in tabular form. Moreover, analysis of different parameters used in IP related tasks is carried-out to save the time needed in future for evaluating the parameters of GP. As more advancement is made in GP methodologies, its success in solving complex tasks not only related to IP but also in other fields will increase. Additionally, guidelines are provided for applying GP in IP related tasks, pros and cons of GP techniques are discussed, and some future directions are also set."
What are the applications of diffusion maps in analyzing dynamical systems?,b20583d4,"[{'abstract': '  Diffusion maps are an emerging data-driven technique for non-linear\ndimensionality reduction, which are especially useful for the analysis of\ncoherent structures and nonlinear embeddings of dynamical systems. However, the\ncomputational complexity of the diffusion maps algorithm scales with the number\nof observations. Thus, long time-series data presents a significant challenge\nfor fast and efficient embedding. We propose integrating the Nyström method\nwith diffusion maps in order to ease the computational demand. We achieve a\nspeedup of roughly two to four times when approximating the dominant diffusion\nmap components.\n', 'title': 'Diffusion Maps meet Nyström'}]","Title: Diffusion Maps meet Nyström

Abstract: Diffusion maps are an emerging data-driven technique for non-linear dimensionality reduction, which are especially useful for the analysis of coherent structures and nonlinear embeddings of dynamical systems. However, the computational complexity of the diffusion maps algorithm scales with the number of observations. Thus, long time-series data presents a significant challenge for fast and efficient embedding. We propose integrating the Nyström method with diffusion maps in order to ease the computational demand. We achieve a speedup of roughly two to four times when approximating the dominant diffusion map components."
What is the role of  J_eff=1/2 isospins in determining the spin-wave gap of Sr₂IrO₄?,69d987f5,"[{'abstract': '  The collective magnetic excitations in the spin-orbit Mott insulator\n(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\\,0.01,\\,0.04,\\, 0.1$) were investigated by\nmeans of resonant inelastic x-ray scattering. We report significant magnon\nenergy gaps at both the crystallographic and antiferromagnetic zone centers at\nall doping levels, along with a remarkably pronounced momentum-dependent\nlifetime broadening. The spin-wave gap is accounted for by a significant\nanisotropy in the interactions between $J_\\text{eff}=1/2$ isospins, thus\nmarking the departure of Sr$_2$IrO$_4$ from the essentially isotropic\nHeisenberg model appropriate for the superconducting cuprates.\n', 'title': 'Anisotropic exchange and spin-wave damping in pure and electron-doped Sr$_2$IrO$_4$'}]","Title: Anisotropic exchange and spin-wave damping in pure and electron-doped Sr₂IrO₄

Abstract:   The collective magnetic excitations in the spin-orbit Mott insulator
(Sr₁₋ₓLaₓ)₂IrO₄ (x=0, 0.01, 0.04, 0.1) were investigated by means of resonant inelastic x-ray scattering. We report significant magnon energy gaps at both the crystallographic and antiferromagnetic zone centers at all doping levels, along with a remarkably pronounced momentum-dependent lifetime broadening. The spin-wave gap is accounted for by a significant anisotropy in the interactions between Jₑff=1/2 isospins, thus marking the departure of Sr₂IrO₄ from the essentially isotropic Heisenberg model appropriate for the superconducting cuprates."
How do the stellar population ages of sub-galactic regions in early-type galaxies and star-forming regions of spiral galaxies compare?,57ddba54,"[{'abstract': '  Galaxies in the local Universe are known to follow bimodal distributions in\nthe global stellar populations properties. We analyze the distribution of the\nlocal average stellar-population ages of 654,053 sub-galactic regions resolved\non ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the\nCALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.\nWe find a bimodal local-age distribution, with an old and a young peak\nprimarily due to regions in early-type galaxies and star-forming regions of\nspirals, respectively. Within spiral galaxies, the older ages of bulges and\ninter-arm regions relative to spiral arms support an internal age bimodality.\nAlthough regions of higher stellar-mass surface-density, mu*, are typically\nolder, mu* alone does not determine the stellar population age and a bimodal\ndistribution is found at any fixed mu*. We identify an ""old ridge"" of regions\nof age ~9 Gyr, independent of mu*, and a ""young sequence"" of regions with age\nincreasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as\nregions containing only old stars, and the latter as regions where the relative\ncontamination of old stellar populations by young stars decreases as mu*\nincreases. The reason why this bimodal age distribution is not inconsistent\nwith the unimodal shape of the cosmic-averaged star-formation history is that\ni) the dominating contribution by young stars biases the age low with respect\nto the average epoch of star formation, and ii) the use of a single average age\nper region is unable to represent the full time-extent of the star-formation\nhistory of ""young-sequence"" regions.\n', 'title': 'Resolving the age bimodality of galaxy stellar populations on kpc scales'}]","Title: Resolving the age bimodality of galaxy stellar populations on kpc scales

Abstract: Galaxies in the local Universe are known to follow bimodal distributions in the global stellar populations properties. We analyze the distribution of the local average stellar-population ages of 654,053 sub-galactic regions resolved on ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the CALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging. We find a bimodal local-age distribution, with an old and a young peak primarily due to regions in early-type galaxies and star-forming regions of spirals, respectively. 

The stellar population ages of sub-galactic regions in early-type galaxies are typically older, while those in star-forming regions of spiral galaxies are typically younger."
Can topological phases emerge from spin-orbital interactions in inhomogeneous quantum systems?,4afdd7bf,"[{'abstract': '  We study the evolution of spin-orbital correlations in an inhomogeneous\nquantum system with an impurity replacing a doublon by a holon orbital degree\nof freedom. Spin-orbital entanglement is large when spin correlations are\nantiferromagnetic, while for a ferromagnetic host we obtain a pure orbital\ndescription. In this regime the orbital model can be mapped on spinless\nfermions and we uncover topological phases with zero energy modes at the edge\nor at the domain between magnetically inequivalent regions.\n', 'title': 'Topological Phases emerging from Spin-Orbital Physics'}]","Title: Topological Phases emerging from Spin-Orbital Physics

Abstract:   We study the evolution of spin-orbital correlations in an inhomogeneous
quantum system with an impurity replacing a doublon by a holon orbital degree
of freedom. Spin-orbital entanglement is large when spin correlations are
antiferromagnetic, while for a ferromagnetic host we obtain a pure orbital
description. In this regime the orbital model can be mapped on spinless
fermions and we uncover topological phases with zero energy modes at the edge
or at the domain between magnetically inequivalent regions."
How is a bilateral teleoperation platform used in conjunction with deep learning for robot motion generation?,70416061,"[{'abstract': '  We present bilateral teleoperation system for task learning and robot motion\ngeneration. Our system includes a bilateral teleoperation platform and a deep\nlearning software. The deep learning software refers to human demonstration\nusing the bilateral teleoperation platform to collect visual images and robotic\nencoder values. It leverages the datasets of images and robotic encoder\ninformation to learn about the inter-modal correspondence between visual images\nand robot motion. In detail, the deep learning software uses a combination of\nDeep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent\nNeural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor\nangles, to learn motion taught be human teleoperation. The learnt models are\nused to predict new motion trajectories for similar tasks. Experimental results\nshow that our system has the adaptivity to generate motion for similar scooping\ntasks. Detailed analysis is performed based on failure cases of the\nexperimental results. Some insights about the cans and cannots of the system\nare summarized.\n', 'title': 'Deep Learning Scooping Motion using Bilateral Teleoperations'}]","Title: Deep Learning Scooping Motion using Bilateral Teleoperations

Abstract: We present bilateral teleoperation system for task learning and robot motion generation. Our system includes a bilateral teleoperation platform and a deep learning software. The deep learning software refers to human demonstration using the bilateral teleoperation platform to collect visual images and robotic encoder values. It leverages the datasets of images and robotic encoder information to learn about the inter-modal correspondence between visual images and robot motion. In detail, the deep learning software uses a combination of Deep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent Neural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor angles, to learn motion taught be human teleoperation. The learnt models are used to predict new motion trajectories for similar tasks. Experimental results show that our system has the adaptivity to generate motion for similar scooping tasks. Detailed analysis is performed based on failure cases of the experimental results. Some insights about the cans and cannots of the system are summarized."
What is the primary goal of the optimization problem presented in this paper?,1919ce1a,"[{'abstract': '  With the tremendous increase of the Internet traffic, achieving the best\nperformance with limited resources is becoming an extremely urgent problem. In\norder to address this concern, in this paper, we build an optimization problem\nwhich aims to maximize the total utility of traffic flows with the capacity\nconstraint of nodes and links in the network. Based on Duality Theory, we\npropose an iterative algorithm which adjusts the rates of traffic flows and\ncapacity of nodes and links simultaneously to maximize the total utility.\nSimulation results show that our algorithm performs better than the NUP\nalgorithm on BA and ER network models, which has shown to get the best\nperformance so far. Since our research combines the topology information with\ncapacity constraint, it may give some insights for resource allocation in real\ncommunication networks.\n', 'title': 'Optimal Resource Allocation with Node and Link Capacity Constraints in Complex Networks'}]","Based on the context, I found a paper that matches the question.

Title: Optimal Resource Allocation with Node and Link Capacity Constraints in Complex Networks

Abstract: With the tremendous increase of the Internet traffic, achieving the best performance with limited resources is becoming an extremely urgent problem. In order to address this concern, in this paper, we build an optimization problem which aims to maximize the total utility of traffic flows with the capacity constraint of nodes and links in the network. Based on Duality Theory, we propose an iterative algorithm which adjusts the rates of traffic flows and capacity of nodes and links simultaneously to maximize the total utility. Simulation results show that our algorithm performs better than the NUP algorithm on BA and ER network models, which has shown to get the best performance so far. Since our research combines the topology information with capacity constraint, it may give some insights for resource allocation in real communication networks."
What are some methods for reducing the complexity of chemical reaction networks?,331b62b5,"[{'abstract': '  In this paper, we propose an optimization-based sparse learning approach to\nidentify the set of most influential reactions in a chemical reaction network.\nThis reduced set of reactions is then employed to construct a reduced chemical\nreaction mechanism, which is relevant to chemical interaction network modeling.\nThe problem of identifying influential reactions is first formulated as a\nmixed-integer quadratic program, and then a relaxation method is leveraged to\nreduce the computational complexity of our approach. Qualitative and\nquantitative validation of the sparse encoding approach demonstrates that the\nmodel captures important network structural properties with moderate\ncomputational load.\n', 'title': 'A Data-Driven Sparse-Learning Approach to Model Reduction in Chemical Reaction Networks'}]","A researcher looking for methods to reduce the complexity of chemical reaction networks!

I think I have found something that might be of interest to you.

Title: A Data-Driven Sparse-Learning Approach to Model Reduction in Chemical Reaction Networks

Abstract: In this paper, we propose an optimization-based sparse learning approach to identify the set of most influential reactions in a chemical reaction network. This reduced set of reactions is then employed to construct a reduced chemical reaction mechanism, which is relevant to chemical interaction network modeling. The problem of identifying influential reactions is first formulated as a mixed-integer quadratic program, and then a relaxation method is leveraged to reduce the computational complexity of our approach. Qualitative and quantitative validation of the sparse encoding approach demonstrates that the model captures important network structural properties with moderate computational load.

This paper presents a method for reducing the complexity of chemical reaction networks by identifying the most influential reactions and constructing a reduced chemical reaction mechanism."
How are force fields with both intramolecular and intermolecular terms applied to calculate thermodynamic properties in molecular simulations?,a36f168e,"[{'abstract': '  The calculation of caloric properties such as heat capacity, Joule-Thomson\ncoefficients and the speed of sound by classical force-field-based molecular\nsimulation methodology has received scant attention in the literature,\nparticularly for systems composed of complex molecules whose force fields (FFs)\nare characterized by a combination of intramolecular and intermolecular terms\n(referred to herein as ""flexible FFs""). The calculation of a thermodynamic\nproperty for a system whose molecules are described by such a FF involves the\ncalculation of the residual property prior to its addition to the corresponding\nideal-gas (IG) property, the latter of which is separately calculated, either\nusing thermochemical compilations or nowadays accurate quantum mechanical\ncalculations. Although the simulation of a volumetric residual property\nproceeds by simply replacing the intermolecular FF in the rigid molecule case\nby the total (intramolecular plus intermolecular) FF, this is not the case for\na caloric property. We discuss the methodology required in performing such\ncalculations, and focus on the example of the molar heat capacity at constant\npressure, $c_P$, one of the most important caloric properties. We also consider\nthree approximations for the calculation procedure, and illustrate their\nconsequences for the examples of the relatively simple molecule 2-propanol,\n${\\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\\rm HO(CH_2)_2NH_2}$, an\nimportant fluid used in carbon capture.\n', 'title': 'Molecular Simulation of Caloric Properties of Fluids Modelled by Force Fields with Intramolecular Contributions: Application to Heat Capacities'}]","Title: Molecular Simulation of Caloric Properties of Fluids Modelled by Force Fields with Intramolecular Contributions: Application to Heat Capacities

Abstract: The calculation of caloric properties such as heat capacity, Joule-Thomson coefficients and the speed of sound by classical force-field-based molecular simulation methodology has received scant attention in the literature, particularly for systems composed of complex molecules whose force fields (FFs) are characterized by a combination of intramolecular and intermolecular terms (referred to herein as ""flexible FFs""). The calculation of a thermodynamic property for a system whose molecules are described by such a FF involves the calculation of the residual property prior to its addition to the corresponding ideal-gas (IG) property, the latter of which is separately calculated, either using thermochemical compilations or nowadays accurate quantum mechanical calculations. Although the simulation of a volumetric residual property proceeds by simply replacing the intermolecular FF in the rigid molecule case by the total (intramolecular plus intermolecular) FF, this is not the case for a caloric property. We discuss the methodology required in performing such calculations, and focus on the example of the molar heat capacity at constant pressure, $c_P$, one of the most important caloric properties. We also consider three approximations for the calculation procedure, and illustrate their consequences for the examples of the relatively simple molecule 2-propanol, ${\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\rm HO(CH_2)_2NH_2}$, an important fluid used in carbon capture."
Can the analytical derivation of gravitational wave amplitude dependence on q be applied to scenarios with spectator fields?,d42d4d86,"[{'abstract': '  Parametric resonance is among the most efficient phenomena generating\ngravitational waves (GWs) in the early Universe. The dynamics of parametric\nresonance, and hence of the GWs, depend exclusively on the resonance parameter\n$q$. The latter is determined by the properties of each scenario: the initial\namplitude and potential curvature of the oscillating field, and its coupling to\nother species. Previous works have only studied the GW production for fixed\nvalue(s) of $q$. We present an analytical derivation of the GW amplitude\ndependence on $q$, valid for any scenario, which we confront against numerical\nresults. By running lattice simulations in an expanding grid, we study for a\nwide range of $q$ values, the production of GWs in post-inflationary preheating\nscenarios driven by parametric resonance. We present simple fits for the final\namplitude and position of the local maxima in the GW spectrum. Our\nparametrization allows to predict the location and amplitude of the GW\nbackground today, for an arbitrary $q$. The GW signal can be rather large, as\n$h^2\\Omega_{\\rm GW}(f_p) \\lesssim 10^{-11}$, but it is always peaked at high\nfrequencies $f_p \\gtrsim 10^{7}$ Hz. We also discuss the case of\nspectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,\nor the Standard Model Higgs.\n', 'title': 'Gravitational wave production from preheating: parameter dependence'}]","Title: Gravitational wave production from preheating: parameter dependence

Abstract: Parametric resonance is among the most efficient phenomena generating gravitational waves (GWs) in the early Universe. The dynamics of parametric resonance, and hence of the GWs, depend exclusively on the resonance parameter $q$. The latter is determined by the properties of each scenario: the initial amplitude and potential curvature of the oscillating field, and its coupling to other species. Previous works have only studied the GW production for fixed value(s) of $q$. We present an analytical derivation of the GW amplitude dependence on $q$, valid for any scenario, which we confront against numerical results. By running lattice simulations in an expanding grid, we study for a wide range of $q$ values, the production of GWs in post-inflationary preheating scenarios driven by parametric resonance. We present simple fits for the final amplitude and position of the local maxima in the GW spectrum. Our parametrization allows to predict the location and amplitude of the GW background today, for an arbitrary $q$. The GW signal can be rather large, as $h^2\Omega_{GW}(f_p) \lesssim 10^{-11}$, but it is always peaked at high frequencies $f_p \gtrsim 10^{7}$ Hz. We also discuss the case of spectator-field scenarios, where the oscillatory field can be e.g. a curvaton, or the Standard Model Higgs.

Yes, the analytical derivation of gravitational wave amplitude dependence on q can be applied to scenarios with spectator fields."
"What are the Segal and operadic equivariant infinite loop space machines, and how do they relate to each other?",94508f30,"[{'abstract': '  We rework and generalize equivariant infinite loop space theory, which shows\nhow to construct G-spectra from G-spaces with suitable structure. There is a\nnaive version which gives naive G-spectra for any topological group G, but our\nfocus is on the construction of genuine G-spectra when G is finite.\nWe give new information about the Segal and operadic equivariant infinite\nloop space machines, supplying many details that are missing from the\nliterature, and we prove by direct comparison that the two machines give\nequivalent output when fed equivalent input. The proof of the corresponding\nnonequivariant uniqueness theorem, due to May and Thomason, works for naive\nG-spectra for general G but fails hopelessly for genuine G-spectra when G is\nfinite. Even in the nonequivariant case, our comparison theorem is considerably\nmore precise, giving a direct point-set level comparison.\nWe have taken the opportunity to update this general area, equivariant and\nnonequivariant, giving many new proofs, filling in some gaps, and giving some\ncorrections to results in the literature.\n', 'title': 'Equivariant infinite loop space theory, I. The space level story'}]","Based on the provided context, I found the relevant information in the first abstract.

Title: Equivariant infinite loop space theory, I. The space level story

Abstract: We rework and generalize equivariant infinite loop space theory, which shows how to construct G-spectra from G-spaces with suitable structure. There is a naive version which gives naive G-spectra for any topological group G, but our focus is on the construction of genuine G-spectra when G is finite. We give new information about the Segal and operadic equivariant infinite loop space machines, supplying many details that are missing from the literature, and we prove by direct comparison that the two machines give equivalent output when fed equivalent input.

According to this abstract, the Segal and operadic equivariant infinite loop space machines are two machines that construct G-spectra from G-spaces with suitable structure. They are related to each other in that they give equivalent output when fed equivalent input, as proved by direct comparison."
What are some examples of random sets that are stochastically self-similar?,a96d0ccc,"[{'abstract': '  The class of stochastically self-similar sets contains many famous examples\nof random sets, e.g. Mandelbrot percolation and general fractal percolation.\nUnder the assumption of the uniform open set condition and some mild\nassumptions on the iterated function systems used, we show that the\nquasi-Assouad dimension of self-similar random recursive sets is almost surely\nequal to the almost sure Hausdorff dimension of the set. We further comment on\nrandom homogeneous and $V$-variable sets and the removal of overlap conditions.\n', 'title': 'The quasi-Assouad dimension for stochastically self-similar sets'}]","I think I can help you with that!

Based on the context, I found an answer to your question. Here it is:

Title: The quasi-Assouad dimension for stochastically self-similar sets

Abstract: The class of stochastically self-similar sets contains many famous examples of random sets, e.g. Mandelbrot percolation and general fractal percolation. Under the assumption of the uniform open set condition and some mild assumptions on the iterated function systems used, we show that the quasi-Assouad dimension of self-similar random recursive sets is almost surely equal to the almost sure Hausdorff dimension of the set. We further comment on random homogeneous and $V$-variable sets and the removal of overlap conditions.

The examples of random sets that are stochastically self-similar mentioned in this abstract are Mandelbrot percolation and general fractal percolation."
Are analytical and simulation results consistent in demonstrating the robustness of the proposed transmission strategy against phase noise? ,d9bf8683,"[{'abstract': '  This work encompasses Rate-Splitting (RS), providing significant benefits in\nmulti-user settings in the context of huge degrees of freedom promised by\nmassive Multiple-Input Multiple-Output (MIMO). However, the requirement of\nmassive MIMO for cost-efficient implementation makes them more prone to\nhardware imperfections such as phase noise (PN). As a result, we focus on a\nrealistic broadcast channel with a large number of antennas and hampered by the\nunavoidable PN. Moreover, we employ the RS transmission strategy, and we show\nits robustness against PN, since the sum-rate does not saturate at high\nsignal-to-noise ratio (SNR). Although, the analytical results are obtained by\nmeans of the deterministic equivalent analysis, they coincide with simulation\nresults even for finite system dimensions.\n', 'title': 'Mitigation of Phase Noise in Massive MIMO Systems: A Rate-Splitting Approach'}]","Title: Mitigation of Phase Noise in Massive MIMO Systems: A Rate-Splitting Approach

Abstract: This work encompasses Rate-Splitting (RS), providing significant benefits in multi-user settings in the context of huge degrees of freedom promised by massive Multiple-Input Multiple-Output (MIMO). However, the requirement of massive MIMO for cost-efficient implementation makes them more prone to hardware imperfections such as phase noise (PN). As a result, we focus on a realistic broadcast channel with a large number of antennas and hampered by the unavoidable PN. Moreover, we employ the RS transmission strategy, and we show its robustness against PN, since the sum-rate does not saturate at high signal-to-noise ratio (SNR). Although, the analytical results are obtained by means of the deterministic equivalent analysis, they coincide with simulation results even for finite system dimensions."
Can Gorenstein projective modules of the tensor ring be characterized by properties of the original ring?,7d3e25a7,"[{'abstract': '  Let $R$ be a two-sided noetherian ring and $M$ be a nilpotent $R$-bimodule,\nwhich is finitely generated on both sides. We study Gorenstein homological\nproperties of the tensor ring $T_R(M)$. Under certain conditions, the ring $R$\nis Gorenstein if and only if so is $T_R(M)$. We characterize Gorenstein\nprojective $T_R(M)$-modules in terms of $R$-modules.\n', 'title': 'Gorenstein homological properties of tensor rings'}]","Here's a relevant research paper that might answer your question:

Title: Gorenstein homological properties of tensor rings

Abstract: Let $R$ be a two-sided noetherian ring and $M$ be a nilpotent $R$-bimodule, which is finitely generated on both sides. We study Gorenstein homological properties of the tensor ring $T_R(M)$. Under certain conditions, the ring $R$ is Gorenstein if and only if so is $T_R(M)$. We characterize Gorenstein projective $T_R(M)$-modules in terms of $R$-modules."
What kind of problems does the numerical method presented in this paper solve efficiently?,e5fe31fd,"[{'abstract': '  This work discusses the numerical approximation of a nonlinear\nreaction-advection-diffusion equation, which is a dimensionless form of the\nWeertman equation. This equation models steadily-moving dislocations in\nmaterials science. It reduces to the celebrated Peierls-Nabarro equation when\nits advection term is set to zero. The approach rests on considering a\ntime-dependent formulation, which admits the equation under study as its\nlong-time limit. Introducing a Preconditioned Collocation Scheme based on\nFourier transforms, the iterative numerical method presented solves the\ntime-dependent problem, delivering at convergence the desired numerical\nsolution to the Weertman equation. Although it rests on an explicit\ntime-evolution scheme, the method allows for large time steps, and captures the\nsolution in a robust manner. Numerical results illustrate the efficiency of the\napproach for several types of nonlinearities.\n', 'title': 'Fourier-based numerical approximation of the Weertman equation for moving dislocations'}]","Title: A Fast Interior Point Method for Atomic Norm Soft Thresholding

Abstract:   The atomic norm provides a generalization of the $\ell_1$-norm to continuous
parameter spaces. When applied as a sparse regularizer for line spectral
estimation the solution can be obtained by solving a convex optimization
problem. This problem is known as atomic norm soft thresholding (AST). It can
be cast as a semidefinite program and solved by standard methods. In the
semidefinite formulation there are $O(N^2)$ dual variables and a standard
primal-dual interior point method requires at least $O(N^6)$ flops per
iteration. That has lead researcher to consider alternating direction method of
multipliers (ADMM) for the solution of AST, but this method is still somewhat
slow for large problem sizes. To obtain a faster algorithm we reformulate AST
as a non-symmetric conic program. That has two properties of key importance to
its numerical solution: the conic formulation has only $O(N)$ dual variables
and the Toeplitz structure inherent to AST is preserved. Based on it we derive
FastAST which is a primal-dual interior point method for solving AST. Two
variants are considered with the fastest one requiring only $O(N^2)$ flops per
iteration. Extensive numerical experiments demonstrate that FastAST solves AST
significantly faster than a state-of-the-art solver based on ADMM."
What theoretical methods were used in exploring the excited state properties of donor clusters in silicon?,1d0bb4aa,"[{'abstract': '  Excited states of a single donor in bulk silicon have previously been studied\nextensively based on effective mass theory. However, a proper theoretical\ndescription of the excited states of a donor cluster is still scarce. Here we\nstudy the excitations of lines of defects within a single-valley spherical band\napproximation, thus mapping the problem to a scaled hydrogen atom array. A\nseries of detailed full configuration-interaction and time-dependent hybrid\ndensity-functional theory calculations have been performed to understand linear\nclusters of up to 10 donors. Our studies illustrate the generic features of\ntheir excited states, addressing the competition between formation of\ninter-donor ionic states and intra-donor atomic excited states. At short\ninter-donor distances, excited states of donor molecules are dominant, at\nintermediate distances ionic states play an important role, and at long\ndistances the intra-donor excitations are predominant as expected. The\ncalculations presented here emphasise the importance of correlations between\ndonor electrons, and are thus complementary to other recent approaches that\ninclude effective mass anisotropy and multi-valley effects. The exchange\nsplittings between relevant excited states have also been estimated for a donor\npair and for a three-donor arrays; the splittings are much larger than those in\nthe ground state in the range of donor separations between 10 and 20 nm. This\nestablishes a solid theoretical basis for the use of excited-state exchange\ninteractions for controllable quantum gate operations in silicon.\n', 'title': 'Excited states of defect lines in silicon: A first-principles study based on hydrogen cluster analogues'}]","Title: Excited states of defect lines in silicon: A first-principles study based on hydrogen cluster analogues

Abstract: Excited states of a single donor in bulk silicon have previously been studied extensively based on effective mass theory. However, a proper theoretical description of the excited states of a donor cluster is still scarce. Here we study the excitations of lines of defects within a single-valley spherical band approximation, thus mapping the problem to a scaled hydrogen atom array. A series of detailed full configuration-interaction and time-dependent hybrid density-functional theory calculations have been performed to understand linear clusters of up to 10 donors. Our studies illustrate the generic features of their excited states, addressing the competition between formation of inter-donor ionic states and intra-donor atomic excited states. At short inter-donor distances, excited states of donor molecules are dominant, at intermediate distances ionic states play an important role, and at long distances the intra-donor excitations are predominant as expected. The calculations presented here emphasise the importance of correlations between donor electrons, and are thus complementary to other recent approaches that include effective mass anisotropy and multi-valley effects. The exchange splittings between relevant excited states have also been estimated for a donor pair and for a three-donor arrays; the splittings are much larger than those in the ground state in the range of donor separations between 10 and 20 nm. This establishes a solid theoretical basis for the use of excited-state exchange interactions for controllable quantum gate operations in silicon.

Theoretical methods used: Full configuration-interaction and time-dependent hybrid density-functional theory calculations."
What are the limitations of traditional hybrid robot architectures?,91b60d6b,"[{'abstract': '  When developing general purpose robots, the overarching software architecture\ncan greatly affect the ease of accomplishing various tasks. Initial efforts to\ncreate unified robot systems in the 1990s led to hybrid architectures,\nemphasizing a hierarchy in which deliberative plans direct the use of reactive\nskills. However, since that time there has been significant progress in the\nlow-level skills available to robots, including manipulation and perception,\nmaking it newly feasible to accomplish many more tasks in real-world domains.\nThere is thus renewed optimism that robots will be able to perform a wide array\nof tasks while maintaining responsiveness to human operators. However, the top\nlayer in traditional hybrid architectures, designed to achieve long-term goals,\ncan make it difficult to react quickly to human interactions during goal-driven\nexecution. To mitigate this difficulty, we propose a novel architecture that\nsupports such transitions by adding a top-level reactive module which has\nflexible access to both reactive skills and a deliberative control module. To\nvalidate this architecture, we present a case study of its application on a\ndomestic service robot platform.\n', 'title': 'LAAIR: A Layered Architecture for Autonomous Interactive Robots'}]","Title: LAAIR: A Layered Architecture for Autonomous Interactive Robots

Abstract:  When developing general purpose robots, the overarching software architecture
can greatly affect the ease of accomplishing various tasks. Initial efforts to
create unified robot systems in the 1990s led to hybrid architectures,
emphasizing a hierarchy in which deliberative plans direct the use of reactive
skills. However, since that time there has been significant progress in the
low-level skills available to robots, including manipulation and perception,
making it newly feasible to accomplish many more tasks in real-world domains.
There is thus renewed optimism that robots will be able to perform a wide array
of tasks while maintaining responsiveness to human operators. However, the top
layer in traditional hybrid architectures, designed to achieve long-term goals,
can make it difficult to react quickly to human interactions during goal-driven
execution. To mitigate this difficulty, we propose a novel architecture that
supports such transitions by adding a top-level reactive module which has
flexible access to both reactive skills and a deliberative control module. To
validate this architecture, we present a case study of its application on a
domestic service robot platform.

The limitation of traditional hybrid robot architectures is that the top layer, designed to achieve long-term goals, can make it difficult to react quickly to human interactions during goal-driven execution."
Can local Ricci curvature bounds in non-smooth spaces be used to prove isoperimetric inequalities?,e605021c,"[{'abstract': ""  Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove\nthat if a Riemannian manifold has Ricci curvature bounded below in a metric\nball which moreover has almost maximal volume, then in a smaller ball (in a\nquantified sense) it holds an almost-euclidean isoperimetric inequality. The\nresult is actually established in the more general framework of non-smooth\nspaces satisfying local Ricci curvature lower bounds in a synthetic sense via\noptimal transportation.\n"", 'title': 'Almost euclidean Isoperimetric Inequalities in spaces satisfying local Ricci curvature lower bounds'}]","After a quick search, I think I have found the answer you're looking for:

Title: Almost euclidean Isoperimetric Inequalities in spaces satisfying local Ricci curvature lower bounds

Abstract: Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove that if a Riemannian manifold has Ricci curvature bounded below in a metric ball which moreover has almost maximal volume, then in a smaller ball (in a quantified sense) it holds an almost-euclidean isoperimetric inequality. The result is actually established in the more general framework of non-smooth spaces satisfying local Ricci curvature lower bounds in a synthetic sense via optimal transportation."
Are there classical ensembles of Young diagrams that can be deformed with a one-parameter approach?,94c6217d,"[{'abstract': '  We introduce a large class of random Young diagrams which can be regarded as\na natural one-parameter deformation of some classical Young diagram ensembles;\na deformation which is related to Jack polynomials and Jack characters. We show\nthat each such a random Young diagram converges asymptotically to some limit\nshape and that the fluctuations around the limit are asymptotically Gaussian.\n', 'title': 'Gaussian fluctuations of Jack-deformed random Young diagrams'}]","I think I've found a match!

Title: Gaussian fluctuations of Jack-deformed random Young diagrams

Abstract: We introduce a large class of random Young diagrams which can be regarded as a natural one-parameter deformation of some classical Young diagram ensembles; a deformation which is related to Jack polynomials and Jack characters. We show that each such a random Young diagram converges asymptotically to some limit shape and that the fluctuations around the limit are asymptotically Gaussian."
What mathematical guarantees are provided for the accuracy of  parameter estimation in this gradient-based method?,97eca714,"[{'abstract': '  We develop a new approach to learn the parameters of regression models with\nhidden variables. In a nutshell, we estimate the gradient of the regression\nfunction at a set of random points, and cluster the estimated gradients. The\ncenters of the clusters are used as estimates for the parameters of hidden\nunits. We justify this approach by studying a toy model, whereby the regression\nfunction is a linear combination of sigmoids. We prove that indeed the\nestimated gradients concentrate around the parameter vectors of the hidden\nunits, and provide non-asymptotic bounds on the number of required samples. To\nthe best of our knowledge, no comparable guarantees have been proven for linear\ncombinations of sigmoids.\n', 'title': 'Learning Combinations of Sigmoids Through Gradient Estimation'}]","Title: Learning Combinations of Sigmoids Through Gradient Estimation

Abstract: We develop a new approach to learn the parameters of regression models with hidden variables. In a nutshell, we estimate the gradient of the regression function at a set of random points, and cluster the estimated gradients. The centers of the clusters are used as estimates for the parameters of hidden units. We justify this approach by studying a toy model, whereby the regression function is a linear combination of sigmoids. We prove that indeed the estimated gradients concentrate around the parameter vectors of the hidden units, and provide non-asymptotic bounds on the number of required samples. To the best of our knowledge, no comparable guarantees have been proven for linear combinations of sigmoids."
What are the typical time lags between an IMF turning and the observed response in ionospheric currents?,85077dd8,"[{'abstract': '  We characterize the response of the quiet time (no substorms or storms)\nlarge-scale ionospheric transient equivalent currents to north-south and\nsouth-north IMF turnings by using a dynamical network of ground-based\nmagnetometers. Canonical correlation between all pairs of SuperMAG magnetometer\nstations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\\circ}$)\nis used to establish the extent of near-simultaneous magnetic response between\nregions of magnetic local time-MLAT. Parameters and maps that describe\nspatial-temporal correlation are used to characterize the system and its\nresponse to the turnings aggregated over several hundred events. We find that\nregions that experience large increases in correlation post turning coincide\nwith typical locations of a two-cell convection system and are influenced by\nthe interplanetary magnetic field $\\mathit{B}_{y}$. The time between the\nturnings reaching the magnetopause and a network response is found to be\n$\\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the\nnightside.\n', 'title': 'Characterizing the ionospheric current pattern response to southward and northward IMF turnings with dynamical SuperMAG correlation networks'}]","Here is the answer:

Title: Characterizing the ionospheric current pattern response to southward and northward IMF turnings with dynamical SuperMAG correlation networks

Abstract:   We characterize the response of the quiet time (no substorms or storms)
large-scale ionospheric transient equivalent currents to north-south and
south-north IMF turnings by using a dynamical network of ground-based
magnetometers. Canonical correlation between all pairs of SuperMAG magnetometer
stations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\circ}$)
is used to establish the extent of near-simultaneous magnetic response between
regions of magnetic local time-MLAT. Parameters and maps that describe
spatial-temporal correlation are used to characterize the system and its
response to the turnings aggregated over several hundred events. We find that
regions that experience large increases in correlation post turning coincide
with typical locations of a two-cell convection system and are influenced by
the interplanetary magnetic field $\mathit{B}_{y}$. The time between the
turnings reaching the magnetopause and a network response is found to be
$\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the
nightside.

The typical time lags between an IMF turning and the observed response in ionospheric currents are around 8-10 minutes."
"Can you describe the methodology used in Primal-Dual MCTS to  determine optimal actions, considering the concept of sampled information relaxation?",463c4df3,"[{'abstract': '  Monte Carlo Tree Search (MCTS), most famously used in game-play artificial\nintelligence (e.g., the game of Go), is a well-known strategy for constructing\napproximate solutions to sequential decision problems. Its primary innovation\nis the use of a heuristic, known as a default policy, to obtain Monte Carlo\nestimates of downstream values for states in a decision tree. This information\nis used to iteratively expand the tree towards regions of states and actions\nthat an optimal policy might visit. However, to guarantee convergence to the\noptimal action, MCTS requires the entire tree to be expanded asymptotically. In\nthis paper, we propose a new technique called Primal-Dual MCTS that utilizes\nsampled information relaxation upper bounds on potential actions, creating the\npossibility of ""ignoring"" parts of the tree that stem from highly suboptimal\nchoices. This allows us to prove that despite converging to a partial decision\ntree in the limit, the recommended action from Primal-Dual MCTS is optimal. The\nnew approach shows significant promise when used to optimize the behavior of a\nsingle driver navigating a graph while operating on a ride-sharing platform.\nNumerical experiments on a real dataset of 7,000 trips in New Jersey suggest\nthat Primal-Dual MCTS improves upon standard MCTS by producing deeper decision\ntrees and exhibits a reduced sensitivity to the size of the action space.\n', 'title': 'Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds'}]","Title: Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds

Abstract: Monte Carlo Tree Search (MCTS), most famously used in game-play artificial
intelligence (e.g., the game of Go), is a well-known strategy for constructing
approximate solutions to sequential decision problems. Its primary innovation
is the use of a heuristic, known as a default policy, to obtain Monte Carlo
estimates of downstream values for states in a decision tree. This information
is used to iteratively expand the tree towards regions of states and actions
that an optimal policy might visit. However, to guarantee convergence to the
optimal action, MCTS requires the entire tree to be expanded asymptotically. In
this paper, we propose a new technique called Primal-Dual MCTS that utilizes
sampled information relaxation upper bounds on potential actions, creating the
possibility of ""ignoring"" parts of the tree that stem from highly suboptimal
choices. This allows us to prove that despite converging to a partial decision
tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The
new approach shows significant promise when used to optimize the behavior of a
single driver navigating a graph while operating on a ride-sharing platform.
Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest
that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision
trees and exhibits a reduced sensitivity to the size of the action space.

In this paper, the methodology used in Primal-Dual MCTS to determine optimal actions is based on the concept of sampled information relaxation. Specifically, Primal-Dual MCTS utilizes sampled information relaxation upper bounds on potential actions, which allows for the possibility of ""ignoring"" parts of the tree that stem from highly suboptimal choices. This enables the algorithm to converge to a partial decision tree in the limit while still recommending an optimal action."
How does the Minimal Effort Back Propagation technique for CNNs impact model performance?,ed0d5921,"[{'abstract': '  As traditional neural network consumes a significant amount of computing\nresources during back propagation, \\citet{Sun2017mePropSB} propose a simple yet\neffective technique to alleviate this problem. In this technique, only a small\nsubset of the full gradients are computed to update the model parameters. In\nthis paper we extend this technique into the Convolutional Neural Network(CNN)\nto reduce calculation in back propagation, and the surprising results verify\nits validity in CNN: only 5\\% of the gradients are passed back but the model\nstill achieves the same effect as the traditional CNN, or even better. We also\nshow that the top-$k$ selection of gradients leads to a sparse calculation in\nback propagation, which may bring significant computational benefits for high\ncomputational complexity of convolution operation in CNN.\n', 'title': 'Minimal Effort Back Propagation for Convolutional Neural Networks'}]","Title: Minimal Effort Back Propagation for Convolutional Neural Networks

Abstract: As traditional neural network consumes a significant amount of computing resources during back propagation, \citet{Sun2017mePropSB} propose a simple yet effective technique to alleviate this problem. In this technique, only a small subset of the full gradients are computed to update the model parameters. In this paper we extend this technique into the Convolutional Neural Network(CNN) to reduce calculation in back propagation, and the surprising results verify its validity in CNN: only 5\% of the gradients are passed back but the model still achieves the same effect as the traditional CNN, or even better. We also show that the top-$k$ selection of gradients leads to a sparse calculation in back propagation, which may bring significant computational benefits for high computational complexity of convolution operation in CNN.

According to the abstract, the Minimal Effort Back Propagation technique has a minimal impact on model performance, as the model still achieves the same effect as the traditional CNN, or even better, with only 5% of the gradients being passed back."
What mathematical functions are used to express the closed form solution for the option price?,84f29a2d,"[{'abstract': ""  The paper proposes an expanded version of the Local Variance Gamma model of\nCarr and Nadtochiy by adding drift to the governing underlying process. Still\nin this new model it is possible to derive an ordinary differential equation\nfor the option price which plays a role of Dupire's equation for the standard\nlocal volatility model. It is shown how calibration of multiple smiles (the\nwhole local volatility surface) can be done in such a case. Further, assuming\nthe local variance to be a piecewise linear function of strike and piecewise\nconstant function of time this ODE is solved in closed form in terms of\nConfluent hypergeometric functions. Calibration of the model to market smiles\ndoes not require solving any optimization problem and, in contrast, can be done\nterm-by-term by solving a system of non-linear algebraic equations for each\nmaturity, which is fast.\n"", 'title': 'An Expanded Local Variance Gamma model'}]","I think I can help you with that!

The answer to your question is:

Title: An Expanded Local Variance Gamma model

Abstract: The paper proposes an expanded version of the Local Variance Gamma model of 
Carr and Nadtochiy by adding drift to the governing underlying process. Still 
in this new model it is possible to derive an ordinary differential equation 
for the option price which plays a role of Dupire's equation for the standard 
local volatility model. It is shown how calibration of multiple smiles (the 
whole local volatility surface) can be done in such a case. Further, assuming 
the local variance to be a piecewise linear function of strike and piecewise 
constant function of time this ODE is solved in closed form in terms of 
Confluent hypergeometric functions. Calibration of the model to market smiles 
does not require solving any optimization problem and, in contrast, can be done 
term-by-term by solving a system of non-linear algebraic equations for each 
maturity, which is fast.

The mathematical function used to express the closed-form solution for the option price is the Confluent hypergeometric function."
How does this new technique decompose a semigroup T?,1aca6203,"[{'abstract': ""  This paper provides short proofs of two fundamental theorems of finite\nsemigroup theory whose previous proofs were significantly longer, namely the\ntwo-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike\ntheorem, using a new algebraic technique that we call the merge decomposition.\nA prototypical application of this technique decomposes a semigroup $T$ into a\ntwo-sided semidirect product whose components are built from two subsemigroups\n$T_1,T_2$, which together generate $T$, and the subsemigroup generated by their\nsetwise product $T_1T_2$. In this sense we decompose $T$ by merging the\nsubsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup\nhomomorphisms from free semigroups.\n"", 'title': 'Merge decompositions, two-sided Krohn-Rhodes, and aperiodic pointlikes'}]","I think I can help you with that!

Title: Merge decompositions, two-sided Krohn-Rhodes, and aperiodic pointlikes

Abstract: This paper provides short proofs of two fundamental theorems of finite semigroup theory whose previous proofs were significantly longer, namely the two-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike theorem, using a new algebraic technique that we call the merge decomposition. A prototypical application of this technique decomposes a semigroup $T$ into a two-sided semidirect product whose components are built from two subsemigroups $T_1,T_2$, which together generate $T$, and the subsemigroup generated by their setwise product $T_1T_2$. In this sense we decompose $T$ by merging the subsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup homomorphisms from free semigroups."
What function does a complete hybrid formula serve in the paper?,063757ba,"[{'abstract': ""  In this paper we introduce the notion of $\\zeta$-crossbreeding in a set of\n$\\zeta$-factorization formulas and also the notion of complete hybrid formula\nas the final result of that crossbreeding. The last formula is used as a\ncriterion for selection of families of $\\zeta$-kindred elements in class of\nreal continuous functions.\nDedicated to recalling of Gregory Mendel's pea-crossbreeding.\n"", 'title': ""Jacob's ladders, crossbreeding in the set of $ζ$-factorization formulas and selection of families of $ζ$-kindred real continuous functions""}]","Title: Jacob's ladders, crossbreeding in the set of ζ-factorization formulas and selection of families of ζ-kindred real continuous functions

Abstract: In this paper we introduce the notion of ζ-crossbreeding in a set of ζ-factorization formulas and also the notion of complete hybrid formula as the final result of that crossbreeding. The last formula is used as a criterion for selection of families of ζ-kindred elements in class of real continuous functions. 

(The answer is that a complete hybrid formula serves as a criterion for selection of families of ζ-kindred elements in the class of real continuous functions.)"
What experimental technique was used to investigate the electron response in graphene?,f16cfec9,"[{'abstract': '  The response of an electron system to electromagnetic fields with sharp\nspatial variations is strongly dependent on quantum electronic properties, even\nin ambient conditions, but difficult to access experimentally. We use\npropagating graphene plasmons, together with an engineered dielectric-metallic\nenvironment, to probe the graphene electron liquid and unveil its detailed\nelectronic response at short wavelengths.The near-field imaging experiments\nreveal a parameter-free match with the full theoretical quantum description of\nthe massless Dirac electron gas, in which we identify three types of quantum\neffects as keys to understanding the experimental response of graphene to\nshort-ranged terahertz electric fields. The first type is of single-particle\nnature and is related to shape deformations of the Fermi surface during a\nplasmon oscillations. The second and third types are a many-body effect\ncontrolled by the inertia and compressibility of the interacting electron\nliquid in graphene. We demonstrate how, in principle, our experimental approach\ncan determine the full spatiotemporal response of an electron system.\n', 'title': 'Tuning quantum non-local effects in graphene plasmonics'}]","Title: Tuning quantum non-local effects in graphene plasmonics

Abstract:  The response of an electron system to electromagnetic fields with sharp
spatial variations is strongly dependent on quantum electronic properties, even
in ambient conditions, but difficult to access experimentally. We use
propagating graphene plasmons, together with an engineered dielectric-metallic
environment, to probe the graphene electron liquid and unveil its detailed
electronic response at short wavelengths.The near-field imaging experiments
reveal a parameter-free match with the full theoretical quantum description of
the massless Dirac electron gas, in which we identify three types of quantum
effects as keys to understanding the experimental response of graphene to
short-ranged terahertz electric fields. The first type is of single-particle
nature and is related to shape deformations of the Fermi surface during a
plasmon oscillations. The second and third types are a many-body effect
controlled by the inertia and compressibility of the interacting electron
liquid in graphene. We demonstrate how, in principle, our experimental approach
can determine the full spatiotemporal response of an electron system."
What experimental platforms have been proposed to study the reduction of topological classification in correlated systems?,7c1718e2,"[{'abstract': '  One of the most challenging problems in correlated topological systems is a\nrealization of the reduction of topological classification, but very few\nexperimental platforms have been proposed so far. We here demonstrate that\nultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded\nin an optical lattice of two-leg ladder geometry can be the first promising\ntestbed for the reduction $\\mathbb{Z}\\to\\mathbb{Z}_4$, where solid evidence for\nthe reduction is available thanks to their high controllability. We further\ngive a detailed account of how to experimentally access this phenomenon; around\nthe edges, the destruction of one-particle gapless excitations can be observed\nby the local radio frequency spectroscopy, while that of gapless spin\nexcitations can be observed by a time-dependent spin expectation value of a\nsuperposed state of the ground state and the first excited state. We clarify\nthat even when the reduction occurs, a gapless edge mode is recovered around a\ndislocation, which can be another piece of evidence for the reduction.\n', 'title': 'Reduction of topological $\\mathbb{Z}$ classification in cold atomic systems'}]","Title: Reduction of topological $\mathbb{Z}$ classification in cold atomic systems

Abstract:   One of the most challenging problems in correlated topological systems is a
realization of the reduction of topological classification, but very few
experimental platforms have been proposed so far. We here demonstrate that
ultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded
in an optical lattice of two-leg ladder geometry can be the first promising
testbed for the reduction $\mathbb{Z}\to\mathbb{Z}_4$, where solid evidence for
the reduction is available thanks to their high controllability. We further
give a detailed account of how to experimentally access this phenomenon; around
the edges, the destruction of one-particle gapless excitations can be observed
by the local radio frequency spectroscopy, while that of gapless spin
excitations can be observed by a time-dependent spin expectation value of a
superposed state of the ground state and the first excited state. We clarify
that even when the reduction occurs, a gapless edge mode is recovered around a
dislocation, which can be another piece of evidence for the reduction."
What is Lehmer's question and how does it connect to graph complexity?,ed3926dc,"[{'abstract': ""  The (torsion) complexity of a finite edge-weighted graph is defined to be the\norder of the torsion subgroup of the abelian group presented by its Laplacian\nmatrix. When G is d-periodic (i.e., G has a free action of the rank-d free\nabelian group by graph automorphisms, with finite quotient) the Mahler measure\nof its Laplacian determinant polynomial is the growth rate of the complexity of\nfinite quotients of G. Lehmer's question, an open question about the roots of\nmonic integral polynomials, is equivalent to a question about the complexity\ngrowth of edge-weighted 1-periodic graphs.\n"", 'title': 'Graph complexity and Mahler measure'}]","Title: Graph complexity and Mahler measure

Abstract: The (torsion) complexity of a finite edge-weighted graph is defined to be the
order of the torsion subgroup of the abelian group presented by its Laplacian
matrix. When G is d-periodic (i.e., G has a free action of the rank-d free
abelian group by graph automorphisms, with finite quotient) the Mahler measure
of its Laplacian determinant polynomial is the growth rate of the complexity of
finite quotients of G. Lehmer's question, an open question about the roots of
monic integral polynomials, is equivalent to a question about the complexity
growth of edge-weighted 1-periodic graphs.

According to the abstract, Lehmer's question is an open question about the roots of monic integral polynomials, which is equivalent to a question about the complexity growth of edge-weighted 1-periodic graphs. In other words, Lehmer's question is connected to graph complexity through the growth rate of the complexity of finite quotients of d-periodic graphs."
Where can I find a Finnish language resource discussing the mathematical properties of self-affine sets?,de510dd0,"[{'abstract': '  We survey the dimension theory of self-affine sets for general mathematical\naudience. The article is in Finnish.\n', 'title': 'On self-affine sets'}]","I think I can help you with that!

Title: On self-affine sets

Abstract: We survey the dimension theory of self-affine sets for general mathematical audience. The article is in Finnish.

This paper appears to be the one you're looking for, as it's written in Finnish and discusses the mathematical properties of self-affine sets."
What theoretical framework is used for solving multi-agent cooperative search problems?,51f39a09,"[{'abstract': ""  This paper considers the problem of autonomous multi-agent cooperative target\nsearch in an unknown environment using a decentralized framework under a\nno-communication scenario. The targets are considered as static targets and the\nagents are considered to be homogeneous. The no-communication scenario\ntranslates as the agents do not exchange either the information about the\nenvironment or their actions among themselves. We propose an integrated\ndecision and control theoretic solution for a search problem which generates\nfeasible agent trajectories. In particular, a perception based algorithm is\nproposed which allows an agent to estimate the probable strategies of other\nagents' and to choose a decision based on such estimation. The algorithm shows\nrobustness with respect to the estimation accuracy to a certain degree. The\nperformance of the algorithm is compared with random strategies and numerical\nsimulation shows considerable advantages.\n"", 'title': 'An Integrated Decision and Control Theoretic Solution to Multi-Agent Co-Operative Search Problems'}]","Title: An Integrated Decision and Control Theoretic Solution to Multi-Agent Co-Operative Search Problems

Abstract:   This paper considers the problem of autonomous multi-agent cooperative target
search in an unknown environment using a decentralized framework under a
no-communication scenario. The targets are considered as static targets and the
agents are considered to be homogeneous. The no-communication scenario
translates as the agents do not exchange either the information about the
environment or their actions among themselves. We propose an integrated
decision and control theoretic solution for a search problem which generates
feasible agent trajectories. In particular, a perception based algorithm is
proposed which allows an agent to estimate the probable strategies of other
agents' and to choose a decision based on such estimation. The algorithm shows
robustness with respect to the estimation accuracy to a certain degree. The
performance of the algorithm is compared with random strategies and numerical
simulation shows considerable advantages."
What specific instructions were used to optimize the vectorization of the Landau collision integral in the new solver?,6402788f,"[{'abstract': '  The Landau collision integral is an accurate model for the small-angle\ndominated Coulomb collisions in fusion plasmas. We investigate a high order\naccurate, fully conservative, finite element discretization of the nonlinear\nmulti-species Landau integral with adaptive mesh refinement using the PETSc\nlibrary (www.mcs.anl.gov/petsc). We develop algorithms and techniques to\nefficiently utilize emerging architectures with an approach that minimizes\nmemory usage and movement and is suitable for vector processing. The Landau\ncollision integral is vectorized with Intel AVX-512 intrinsics and the solver\nsustains as much as 22% of the theoretical peak flop rate of the Second\nGeneration Intel Xeon Phi, Knights Landing, processor.\n', 'title': 'Landau Collision Integral Solver with Adaptive Mesh Refinement on Emerging Architectures'}]","Title: Landau Collision Integral Solver with Adaptive Mesh Refinement on Emerging Architectures

Abstract: The Landau collision integral is an accurate model for the small-angle dominated Coulomb collisions in fusion plasmas. We investigate a high order accurate, fully conservative, finite element discretization of the nonlinear multi-species Landau integral with adaptive mesh refinement using the PETSc library (www.mcs.anl.gov/petsc). We develop algorithms and techniques to efficiently utilize emerging architectures with an approach that minimizes memory usage and movement and is suitable for vector processing. The Landau collision integral is vectorized with Intel AVX-512 intrinsics and the solver sustains as much as 22% of the theoretical peak flop rate of the Second Generation Intel Xeon Phi, Knights Landing, processor.

According to the abstract, the specific instruction used to optimize the vectorization of the Landau collision integral is the Intel AVX-512 intrinsics."
What are the potential problems associated with decision tree methods using categorical predictors?,d874dd13,"[{'abstract': '  One advantage of decision tree based methods like random forests is their\nability to natively handle categorical predictors without having to first\ntransform them (e.g., by using feature engineering techniques). However, in\nthis paper, we show how this capability can lead to an inherent ""absent levels""\nproblem for decision tree based methods that has never been thoroughly\ndiscussed, and whose consequences have never been carefully explored. This\nproblem occurs whenever there is an indeterminacy over how to handle an\nobservation that has reached a categorical split which was determined when the\nobservation in question\'s level was absent during training. Although these\nincidents may appear to be innocuous, by using Leo Breiman and Adele Cutler\'s\nrandom forests FORTRAN code and the randomForest R package (Liaw and Wiener,\n2002) as motivating case studies, we examine how overlooking the absent levels\nproblem can systematically bias a model. Furthermore, by using three real data\nexamples, we illustrate how absent levels can dramatically alter a model\'s\nperformance in practice, and we empirically demonstrate how some simple\nheuristics can be used to help mitigate the effects of the absent levels\nproblem until a more robust theoretical solution is found.\n', 'title': 'Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem'}]","Title: Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem

Abstract: One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent ""absent levels"" problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found."
"Besides inference, what other tasks can a neural network trained on a generative model accomplish? ",d1a7a20a,"[{'abstract': '  We consider the problem of inference in a causal generative model where the\nset of available observations differs between data instances. We show how\ncombining samples drawn from the graphical model with an appropriate masking\nfunction makes it possible to train a single neural network to approximate all\nthe corresponding conditional marginal distributions and thus amortize the cost\nof inference. We further demonstrate that the efficiency of importance sampling\nmay be improved by basing proposals on the output of the neural network. We\nalso outline how the same network can be used to generate samples from an\napproximate joint posterior via a chain decomposition of the graph.\n', 'title': 'A Universal Marginalizer for Amortized Inference in Generative Models'}]","Based on the provided abstracts, I found a relevant paper that discusses the tasks a neural network trained on a generative model can accomplish besides inference.

Title: Deep Fluids: A Generative Network for Parameterized Fluid Simulations

Abstract: This paper presents a novel generative model to synthesize fluid simulations from a set of reduced parameters. A convolutional neural network is trained on a collection of discrete, parameterizable fluid simulation velocity fields. Due to the capability of deep learning architectures to learn representative features of the data, our generative model is able to accurately approximate the training data set, while providing plausible interpolated in-betweens. The proposed generative model is optimized for fluids by a novel loss function that guarantees divergence-free velocity fields at all times. In addition, we demonstrate that we can handle complex parameterizations in reduced spaces, and advance simulations in time by integrating in the latent space with a second network. Our method models a wide variety of fluid behaviors, thus enabling applications such as fast construction of simulations, interpolation of fluids with different parameters, time re-sampling, latent space simulations, and compression of fluid simulation data. Reconstructed velocity fields are generated up to 700x faster than traditional CPU solvers, while achieving compression rates of over 1300x.

In this paper, the authors demonstrate that a neural network trained on a generative model can be used for tasks such as:

* Fast construction of simulations
* Interpolation of fluids with different parameters
* Time re-sampling
* Latent space simulations
* Compression of fluid simulation data

These tasks go beyond inference and showcase the capabilities of generative models in various applications."
"What is a challenge faced by session-based recommendation systems, particularly at the beginning of a user session?",99e6bdf2,"[{'abstract': ""  In recent years, research has been done on applying Recurrent Neural Networks\n(RNNs) as recommender systems. Results have been promising, especially in the\nsession-based setting where RNNs have been shown to outperform state-of-the-art\nmodels. In many of these experiments, the RNN could potentially improve the\nrecommendations by utilizing information about the user's past sessions, in\naddition to its own interactions in the current session. A problem for\nsession-based recommendation, is how to produce accurate recommendations at the\nstart of a session, before the system has learned much about the user's current\ninterests. We propose a novel approach that extends a RNN recommender to be\nable to process the user's recent sessions, in order to improve\nrecommendations. This is done by using a second RNN to learn from recent\nsessions, and predict the user's interest in the current session. By feeding\nthis information to the original RNN, it is able to improve its\nrecommendations. Our experiments on two different datasets show that the\nproposed approach can significantly improve recommendations throughout the\nsessions, compared to a single RNN working only on the current session. The\nproposed model especially improves recommendations at the start of sessions,\nand is therefore able to deal with the cold start problem within sessions.\n"", 'title': 'Inter-Session Modeling for Session-Based Recommendation'}]","Title: Inter-Session Modeling for Session-Based Recommendation

Abstract:  In recent years, research has been done on applying Recurrent Neural Networks
(RNNs) as recommender systems. Results have been promising, especially in the
session-based setting where RNNs have been shown to outperform state-of-the-art
models. In many of these experiments, the RNN could potentially improve the
recommendations by utilizing information about the user's past sessions, in
addition to its own interactions in the current session. A problem for
session-based recommendation, is how to produce accurate recommendations at the
start of a session, before the system has learned much about the user's current
interests."
What experimental facility at Mainz Microtron (MAMI) houses a new Short-Orbit Spectrometer (SOS) designed for low-energy pion detection?,ca1a7410,"[{'abstract': '  A new Short-Orbit Spectrometer (SOS) has been constructed and installed\nwithin the experimental facility of the A1 collaboration at Mainz Microtron\n(MAMI), with the goal to detect low-energy pions. It is equipped with a\nBrowne-Buechner magnet and a detector system consisting of two helium-ethane\nbased drift chambers and a scintillator telescope made of five layers. The\ndetector system allows detection of pions in the momentum range of 50 - 147\nMeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can\nbe placed at a distance range of 54 - 66 cm from the target center. Two\ncollimators are available for the measurements, one having 1.8 msr aperture and\nthe other having 7 msr aperture. The Short-Orbit Spectrometer has been\nsuccessfully calibrated and used in coincidence measurements together with the\nstandard magnetic spectrometers of the A1 collaboration.\n', 'title': 'A short-orbit spectrometer for low-energy pion detection in electroproduction experiments at MAMI'}]","Here is the answer:

Title: A short-orbit spectrometer for low-energy pion detection in electroproduction experiments at MAMI

Abstract: A new Short-Orbit Spectrometer (SOS) has been constructed and installed within the experimental facility of the A1 collaboration at Mainz Microtron (MAMI), with the goal to detect low-energy pions. It is equipped with a Browne-Buechner magnet and a detector system consisting of two helium-ethane based drift chambers and a scintillator telescope made of five layers. The detector system allows detection of pions in the momentum range of 50 - 147 MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can be placed at a distance range of 54 - 66 cm from the target center. Two collimators are available for the measurements, one having 1.8 msr aperture and the other having 7 msr aperture. The Short-Orbit Spectrometer has been successfully calibrated and used in coincidence measurements together with the standard magnetic spectrometers of the A1 collaboration."
How was the effectiveness of the proposed CNN approach evaluated in a clinical setting?,2f35e33f,"[{'abstract': '  Mammography screening for early detection of breast lesions currently suffers\nfrom high amounts of false positive findings, which result in unnecessary\ninvasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many\nof these false-positive findings prior to biopsy. Current approaches estimate\ntissue properties by means of quantitative parameters taken from generative,\nbiophysical models fit to the q-space encoded signal under certain assumptions\nregarding noise and spatial homogeneity. This process is prone to fitting\ninstability and partial information loss due to model simplicity. We reveal\nunexplored potentials of the signal by integrating all data processing\ncomponents into a convolutional neural network (CNN) architecture that is\ndesigned to propagate clinical target information down to the raw input images.\nThis approach enables simultaneous and target-specific optimization of image\nnormalization, signal exploitation, global representation learning and\nclassification. Using a multicentric data set of 222 patients, we demonstrate\nthat our approach significantly improves clinical decision making with respect\nto the current state of the art.\n', 'title': 'Revealing Hidden Potentials of the q-Space Signal in Breast Cancer'}]","Title: Revealing Hidden Potentials of the q-Space Signal in Breast Cancer

Abstract:   Mammography screening for early detection of breast lesions currently suffers
from high amounts of false positive findings, which result in unnecessary
invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many
of these false-positive findings prior to biopsy. Current approaches estimate
tissue properties by means of quantitative parameters taken from generative,
biophysical models fit to the q-space encoded signal under certain assumptions
regarding noise and spatial homogeneity. This process is prone to fitting
instability and partial information loss due to model simplicity. We reveal
unexplored potentials of the signal by integrating all data processing
components into a convolutional neural network (CNN) architecture that is
designed to propagate clinical target information down to the raw input images.
This approach enables simultaneous and target-specific optimization of image
normalization, signal exploitation, global representation learning and
classification. Using a multicentric data set of 222 patients, we demonstrate
that our approach significantly improves clinical decision making with respect
to the current state of the art.

The effectiveness of the proposed CNN approach was evaluated in a clinical setting using a multicentric data set of 222 patients, and it was demonstrated that the approach significantly improves clinical decision making with respect to the current state of the art."
"When does strong approximation hold in quasi-split, simply connected linear algebraic groups over number fields?",ead7896d,"[{'abstract': '  We prove that any open subset $U$ of a semi-simple simply connected\nquasi-split linear algebraic group $G$ with ${codim} (G\\setminus U, G)\\geq 2$\nover a number field satisfies strong approximation by establishing a fibration\nof $G$ over a toric variety. We also prove a similar result of strong\napproximation with Brauer-Manin obstruction for a partial equivariant smooth\ncompactification of a homogeneous space where all invertible functions are\nconstant and the semi-simple part of the linear algebraic group is quasi-split.\nSome semi-abelian varieties of any given dimension where the complements of a\nrational point do not satisfy strong approximation with Brauer-Manin\nobstruction are given.\n', 'title': 'Arithmetic purity of strong approximation for homogeneous spaces'}]","I think I can help you with that!

Title: Arithmetic purity of strong approximation for homogeneous spaces

Abstract:   We prove that any open subset $U$ of a semi-simple simply connected quasi-split linear algebraic group $G$ with ${codim} (G\setminus U, G)\geq 2$ over a number field satisfies strong approximation by establishing a fibration of $G$ over a toric variety. We also prove a similar result of strong approximation with Brauer-Manin obstruction for a partial equivariant smooth compactification of a homogeneous space where all invertible functions are constant and the semi-simple part of the linear algebraic group is quasi-split. Some semi-abelian varieties of any given dimension where the complements of a rational point do not satisfy strong approximation with Brauer-Manin obstruction are given."
what tasks does the VideoGCRF method improve upon using multiple temporal connectivity patterns? ,90aa61dd,"[{'abstract': '  In this work we introduce a time- and memory-efficient method for structured\nprediction that couples neuron decisions across both space at time. We show\nthat we are able to perform exact and efficient inference on a densely\nconnected spatio-temporal graph by capitalizing on recent advances on deep\nGaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)\nefficient, (b) has a unique global minimum, and (c) can be trained end-to-end\nalongside contemporary deep networks for video understanding. We experiment\nwith multiple connectivity patterns in the temporal domain, and present\nempirical improvements over strong baselines on the tasks of both semantic and\ninstance segmentation of videos.\n', 'title': 'Deep Spatio-Temporal Random Fields for Efficient Video Segmentation'}]","Title: Deep Spatio-Temporal Random Fields for Efficient Video Segmentation

Abstract: In this work we introduce a time- and memory-efficient method for structured
prediction that couples neuron decisions across both space at time. We show
that we are able to perform exact and efficient inference on a densely
connected spatio-temporal graph by capitalizing on recent advances on deep
Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)
efficient, (b) has a unique global minimum, and (c) can be trained end-to-end
alongside contemporary deep networks for video understanding. We experiment
with multiple connectivity patterns in the temporal domain, and present
empirical improvements over strong baselines on the tasks of both semantic and
instance segmentation of videos."
Are there nonparametric tests for detecting a signal in noise that are minimax?,24165338,"[{'abstract': '  For the problem of nonparametric detection of signal in Gaussian white noise\nwe point out strong asymptotically minimax tests. The sets of alternatives are\na ball in Besov space $B^r_{2\\infty}$ with ""small"" balls in $L_2$ removed.\n', 'title': 'On asymptotically minimax nonparametric detection of signal in Gaussian white noise'}]","Title: On asymptotically minimax nonparametric detection of signal in Gaussian white noise

Abstract: For the problem of nonparametric detection of signal in Gaussian white noise we point out strong asymptotically minimax tests. The sets of alternatives are a ball in Besov space $B^r_{2\infty}$ with ""small"" balls in $L_2$ removed."
What is the effect of varying fiber diameter on photoelectron yields in extruded scintillation counters?,79e86979,"[{'abstract': '  Photoelectron yields of extruded scintillation counters with titanium dioxide\ncoating and embedded wavelength shifting fibers read out by silicon\nphotomultipliers have been measured at the Fermilab Test Beam Facility using\n120\\,GeV protons. The yields were measured as a function of transverse,\nlongitudinal, and angular positions for a variety of scintillator compositions\nand reflective coating mixtures, fiber diameters, and photosensor sizes. Timing\nperformance was also studied. These studies were carried out by the Cosmic Ray\nVeto Group of the Mu2e collaboration as part of their R\\&D program.\n', 'title': 'Photoelectron Yields of Scintillation Counters with Embedded Wavelength-Shifting Fibers Read Out With Silicon Photomultipliers'}]","I've found the answer to your question!

Title: Photoelectron Yields of Scintillation Counters with Embedded Wavelength-Shifting Fibers Read Out With Silicon Photomultipliers

Abstract:   Photoelectron yields of extruded scintillation counters with titanium dioxide
coating and embedded wavelength shifting fibers read out by silicon
photomultipliers have been measured at the Fermilab Test Beam Facility using
120\,GeV protons. The yields were measured as a function of transverse,
longitudinal, and angular positions for a variety of scintillator compositions
and reflective coating mixtures, fiber diameters, and photosensor sizes. Timing
performance was also studied. These studies were carried out by the Cosmic Ray
Veto Group of the Mu2e collaboration as part of their R\&D program.

The abstract mentions that the yields were measured as a function of fiber diameters, but it doesn't provide a specific answer to the question of what the effect of varying fiber diameter is on photoelectron yields."
How do coded caching schemes employing Zero-Forcing (ZF) in the signal domain compare with a baseline scheme using max-min fair multicasting?,ed653419,"[{'abstract': '  In this paper we consider a single-cell downlink scenario where a\nmultiple-antenna base station delivers contents to multiple cache-enabled user\nterminals. Based on the multicasting opportunities provided by the so-called\nCoded Caching technique, we investigate three delivery approaches. Our baseline\nscheme employs the coded caching technique on top of max-min fair multicasting.\nThe second one consists of a joint design of Zero-Forcing (ZF) and coded\ncaching, where the coded chunks are formed in the signal domain (complex\nfield). The third scheme is similar to the second one with the difference that\nthe coded chunks are formed in the data domain (finite field). We derive\nclosed-form rate expressions where our results suggest that the latter two\nschemes surpass the first one in terms of Degrees of Freedom (DoF). However, at\nthe intermediate SNR regime forming coded chunks in the signal domain results\nin power loss, and will deteriorate throughput of the second scheme. The main\nmessage of our paper is that the schemes performing well in terms of DoF may\nnot be directly appropriate for intermediate SNR regimes, and modified schemes\nshould be employed.\n', 'title': 'Multi-Antenna Coded Caching'}]","Title: Multi-Antenna Coded Caching

Abstract: In this paper we consider a single-cell downlink scenario where a multiple-antenna base station delivers contents to multiple cache-enabled user terminals. Based on the multicasting opportunities provided by the so-called Coded Caching technique, we investigate three delivery approaches. Our baseline scheme employs the coded caching technique on top of max-min fair multicasting. The second one consists of a joint design of Zero-Forcing (ZF) and coded caching, where the coded chunks are formed in the signal domain (complex field). The third scheme is similar to the second one with the difference that the coded chunks are formed in the data domain (finite field). We derive closed-form rate expressions where our results suggest that the latter two schemes surpass the first one in terms of Degrees of Freedom (DoF). However, at the intermediate SNR regime forming coded chunks in the signal domain results in power loss, and will deteriorate throughput of the second scheme. The main message of our paper is that the schemes performing well in terms of DoF may not be directly appropriate for intermediate SNR regimes, and modified schemes should be employed."
How might cosmic ray levels change near Earth following a supernova?,e758332a,"[{'abstract': '  Recent 60Fe results have suggested that the estimated distances of supernovae\nin the last few million years should be reduced from 100 pc to 50 pc. Two\nevents or series of events are suggested, one about 2.7 million years to 1.7\nmillion years ago, and another may at 6.5 to 8.7 million years ago. We ask what\neffects such supernovae are expected to have on the terrestrial atmosphere and\nbiota. Assuming that the Local Bubble was formed before the event being\nconsidered, and that the supernova and the Earth were both inside a weak,\ndisordered magnetic field at that time, TeV-PeV cosmic rays at Earth will\nincrease by a factor of a few hundred. Tropospheric ionization will increase\nproportionately, and the overall muon radiation load on terrestrial organisms\nwill increase by a factor of 150. All return to pre-burst levels within 10kyr.\nIn the case of an ordered magnetic field, effects depend strongly on the field\norientation. The upper bound in this case is with a largely coherent field\naligned along the line of sight to the supernova, in which case TeV-PeV cosmic\nray flux increases are 10^4; in the case of a transverse field they are below\ncurrent levels. We suggest a substantial increase in the extended effects of\nsupernovae on Earth and in the lethal distance estimate; more work is\nneeded.This paper is an explicit followup to Thomas et al. (2016). We also here\nprovide more detail on the computational procedures used in both works.\n', 'title': ""A supernova at 50 pc: Effects on the Earth's atmosphere and biota""}]","Title: A supernova at 50 pc: Effects on the Earth's atmosphere and biota

Abstract:   Recent 60Fe results have suggested that the estimated distances of supernovae
in the last few million years should be reduced from 100 pc to 50 pc. Two
events or series of events are suggested, one about 2.7 million years to 1.7
million years ago, and another may at 6.5 to 8.7 million years ago. We ask what
effects such supernovae are expected to have on the terrestrial atmosphere and
biota. Assuming that the Local Bubble was formed before the event being
considered, and that the supernova and the Earth were both inside a weak,
disordered magnetic field at that time, TeV-PeV cosmic rays at Earth will
increase by a factor of a few hundred. Tropospheric ionization will increase
proportionately, and the overall muon radiation load on terrestrial organisms
will increase by a factor of 150. All return to pre-burst levels within 10kyr.
In the case of an ordered magnetic field, effects depend strongly on the field
orientation. The upper bound in this case is with a largely coherent field
aligned along the line of sight to the supernova, in which case TeV-PeV cosmic
ray flux increases are 10^4; in the case of a transverse field they are below
current levels. We suggest a substantial increase in the extended effects of
supernovae on Earth and in the lethal distance estimate; more work is
needed.This paper is an explicit followup to Thomas et al. (2016). We also here
provide more detail on the computational procedures used in both works."
How might creating plug-and-play robot components benefit the robotics industry?,3252d33f,"[{'abstract': ""  Today's landscape of robotics is dominated by vertical integration where\nsingle vendors develop the final product leading to slow progress, expensive\nproducts and customer lock-in. Opposite to this, an horizontal integration\nwould result in a rapid development of cost-effective mass-market products with\nan additional consumer empowerment. The transition of an industry from vertical\nintegration to horizontal integration is typically catalysed by de facto\nindustry standards that enable a simplified and seamless integration of\nproducts. However, in robotics there is currently no leading candidate for a\nglobal plug-and-play standard.\nThis paper tackles the problem of incompatibility between robot components\nthat hinder the reconfigurability and flexibility demanded by the robotics\nindustry. Particularly, it presents a model to create plug-and-play robot\nhardware components. Rather than iteratively evolving previous ontologies, our\nproposed model answers the needs identified by the industry while facilitating\ninteroperability, measurability and comparability of robotics technology. Our\napproach differs significantly with the ones presented before as it is\nhardware-oriented and establishes a clear set of actions towards the\nintegration of this model in real environments and with real manufacturers.\n"", 'title': 'An information model for modular robots: the Hardware Robot Information Model (HRIM)'}]","Title: An information model for modular robots: the Hardware Robot Information Model (HRIM)

Abstract: Today's landscape of robotics is dominated by vertical integration where single vendors develop the final product leading to slow progress, expensive products and customer lock-in. Opposite to this, an horizontal integration would result in a rapid development of cost-effective mass-market products with an additional consumer empowerment. The transition of an industry from vertical integration to horizontal integration is typically catalysed by de facto industry standards that enable a simplified and seamless integration of products. However, in robotics there is currently no leading candidate for a global plug-and-play standard.

This paper tackles the problem of incompatibility between robot components that hinder the reconfigurability and flexibility demanded by the robotics industry. Particularly, it presents a model to create plug-and-play robot hardware components. Rather than iteratively evolving previous ontologies, our proposed model answers the needs identified by the industry while facilitating interoperability, measurability and comparability of robotics technology. Our approach differs significantly with the ones presented before as it is hardware-oriented and establishes a clear set of actions towards the integration of this model in real environments and with real manufacturers."
How do existing tuning parameters for CS algorithms perform at high SNR?,73dca852,"[{'abstract': '  High signal to noise ratio (SNR) consistency of model selection criteria in\nlinear regression models has attracted a lot of attention recently. However,\nmost of the existing literature on high SNR consistency deals with model order\nselection. Further, the limited literature available on the high SNR\nconsistency of subset selection procedures (SSPs) is applicable to linear\nregression with full rank measurement matrices only. Hence, the performance of\nSSPs used in underdetermined linear models (a.k.a compressive sensing (CS)\nalgorithms) at high SNR is largely unknown. This paper fills this gap by\nderiving necessary and sufficient conditions for the high SNR consistency of\npopular CS algorithms like $l_0$-minimization, basis pursuit de-noising or\nLASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions\nanalytically establish the high SNR inconsistency of CS algorithms when used\nwith the tuning parameters discussed in literature. Novel tuning parameters\nwith SNR adaptations are developed using the sufficient conditions and the\nchoice of SNR adaptations are discussed analytically using convergence rate\nanalysis. CS algorithms with the proposed tuning parameters are numerically\nshown to be high SNR consistent and outperform existing tuning parameters in\nthe moderate to high SNR regime.\n', 'title': 'High SNR Consistent Compressive Sensing'}]","Title: High SNR Consistent Compressive Sensing

Abstract: High signal to noise ratio (SNR) consistency of model selection criteria in
linear regression models has attracted a lot of attention recently. However,
most of the existing literature on high SNR consistency deals with model order
selection. Further, the limited literature available on the high SNR
consistency of subset selection procedures (SSPs) is applicable to linear
regression with full rank measurement matrices only. Hence, the performance of
SSPs used in underdetermined linear models (a.k.a compressive sensing (CS)
algorithms) at high SNR is largely unknown. This paper fills this gap by
deriving necessary and sufficient conditions for the high SNR consistency of
popular CS algorithms like $l_0$-minimization, basis pursuit de-noising or
LASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions
analytically establish the high SNR inconsistency of CS algorithms when used
with the tuning parameters discussed in literature. Novel tuning parameters
with SNR adaptations are developed using the sufficient conditions and the
choice of SNR adaptations are discussed analytically using convergence rate
analysis. CS algorithms with the proposed tuning parameters are numerically
shown to be high SNR consistent and outperform existing tuning parameters in
the moderate to high SNR regime.

The answer to the question is that existing tuning parameters for CS algorithms perform inconsistently at high SNR, as mentioned in the abstract: ""Necessary conditions analytically establish the high SNR inconsistency of CS algorithms when used with the tuning parameters discussed in literature."""
What are some tips for testers to design effective test charters during exploratory testing?,823c2894,"[{'abstract': '  During exploratory testing sessions the tester simultaneously learns, designs\nand executes tests. The activity is iterative and utilizes the skills of the\ntester and provides flexibility and creativity.Test charters are used as a\nvehicle to support the testers during the testing. The aim of this study is to\nsupport practitioners in the design of test charters through checklists. We\naimed to identify factors allowing practitioners to critically reflect on their\ndesigns and contents of test charters to support practitioners in making\ninformed decisions of what to include in test charters. The factors and\ncontents have been elicited through interviews. Overall, 30 factors and 35\ncontent elements have been elicited.\n', 'title': 'Checklists to Support Test Charter Design in Exploratory Testing'}]",Title: Checklists to Support Test Charter Design in Exploratory Testing
What is the name of the novel neural network architecture proposed to address  unsupervised generative modeling of sequences?,b8cbcc5d,"[{'abstract': '  We explore whether useful temporal neural generative models can be learned\nfrom sequential data without back-propagation through time. We investigate the\nviability of a more neurocognitively-grounded approach in the context of\nunsupervised generative modeling of sequences. Specifically, we build on the\nconcept of predictive coding, which has gained influence in cognitive science,\nin a neural framework. To do so we develop a novel architecture, the Temporal\nNeural Coding Network, and its learning algorithm, Discrepancy Reduction. The\nunderlying directed generative model is fully recurrent, meaning that it\nemploys structural feedback connections and temporal feedback connections,\nyielding information propagation cycles that create local learning signals.\nThis facilitates a unified bottom-up and top-down approach for information\ntransfer inside the architecture. Our proposed algorithm shows promise on the\nbouncing balls generative modeling problem. Further experiments could be\nconducted to explore the strengths and weaknesses of our approach.\n', 'title': 'Learning to Adapt by Minimizing Discrepancy'}]","Title: Learning to Adapt by Minimizing Discrepancy

Abstract: We explore whether useful temporal neural generative models can be learned from sequential data without back-propagation through time. We investigate the viability of a more neurocognitively-grounded approach in the context of unsupervised generative modeling of sequences. Specifically, we build on the concept of predictive coding, which has gained influence in cognitive science, in a neural framework. To do so we develop a novel architecture, the Temporal Neural Coding Network, and its learning algorithm, Discrepancy Reduction. The underlying directed generative model is fully recurrent, meaning that it employs structural feedback connections and temporal feedback connections, yielding information propagation cycles that create local learning signals. This facilitates a unified bottom-up and top-down approach for information transfer inside the architecture. Our proposed algorithm shows promise on the bouncing balls generative modeling problem. Further experiments could be conducted to explore the strengths and weaknesses of our approach."
How can a CNN architecture be modified to better account for limited experimental data in neuroscience?,5c004e5f,"[{'abstract': ""  Neuroscientists classify neurons into different types that perform similar\ncomputations at different locations in the visual field. Traditional methods\nfor neural system identification do not capitalize on this separation of 'what'\nand 'where'. Learning deep convolutional feature spaces that are shared among\nmany neurons provides an exciting path forward, but the architectural design\nneeds to account for data limitations: While new experimental techniques enable\nrecordings from thousands of neurons, experimental time is limited so that one\ncan sample only a small fraction of each neuron's response space. Here, we show\nthat a major bottleneck for fitting convolutional neural networks (CNNs) to\nneural data is the estimation of the individual receptive field locations, a\nproblem that has been scratched only at the surface thus far. We propose a CNN\narchitecture with a sparse readout layer factorizing the spatial (where) and\nfeature (what) dimensions. Our network scales well to thousands of neurons and\nshort recordings and can be trained end-to-end. We evaluate this architecture\non ground-truth data to explore the challenges and limitations of CNN-based\nsystem identification. Moreover, we show that our network model outperforms\ncurrent state-of-the art system identification models of mouse primary visual\ncortex.\n"", 'title': 'Neural system identification for large populations separating ""what"" and ""where""'}]","Title: Neural system identification for large populations separating ""what"" and ""where""

Abstract: Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of 'what' and 'where'. Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron's response space. Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations, a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the-art system identification models of mouse primary visual cortex."
What is a simple climate-vegetation model used to determine for the seven TRAPPIST-1 planets?,d9d2efd9,"[{'abstract': '  The recent discovery of the planetary system hosted by the ultracool dwarf\nstar TRAPPIST-1 could open new perspectives into the investigation of planetary\nclimates of Earth-sized exoplanets, their atmospheres and their possible\nhabitability. In this paper, we use a simple climate-vegetation energy-balance\nmodel to study the climate of the seven TRAPPIST-1 planets and the climate\ndependence on the global albedo, on the fraction of vegetation that could cover\ntheir surfaces and on the different greenhouse conditions. The model allows us\nto investigate whether liquid water could be maintained on the planetary\nsurfaces (i.e., by defining a ""surface water zone"") in different planetary\nconditions, with or without the presence of greenhouse effect.\nIt is shown that planet TRAPPIST-1d seems to be the most stable from an\nEarth-like perspective, since it resides in the surface water zone for a wide\nrange of reasonable values of the model parameters. Moreover, according to the\nmodel outer planets (f, g and h) cannot host liquid water on their surfaces,\neven for Earth-like conditions, entering a snowball state. Although very\nsimple, the model allows to extract the main features of the TRAPPIST-1\nplanetary climates.\n', 'title': 'Comparative Climates of TRAPPIST-1 planetary system: results from a simple climate-vegetation model'}]","Title: Comparative Climates of TRAPPIST-1 planetary system: results from a simple climate-vegetation model

Abstract: The recent discovery of the planetary system hosted by the ultracool dwarf star TRAPPIST-1 could open new perspectives into the investigation of planetary climates of Earth-sized exoplanets, their atmospheres and their possible habitability. In this paper, we use a simple climate-vegetation energy-balance model to study the climate of the seven TRAPPIST-1 planets and the climate dependence on the global albedo, on the fraction of vegetation that could cover their surfaces and on the different greenhouse conditions. The model allows us to investigate whether liquid water could be maintained on the planetary surfaces (i.e., by defining a ""surface water zone"") in different planetary conditions, with or without the presence of greenhouse effect.

According to this abstract, the simple climate-vegetation model is used to determine whether liquid water could be maintained on the planetary surfaces (i.e., by defining a ""surface water zone"") in different planetary conditions, with or without the presence of greenhouse effect."
What are the security challenges of the Internet of Things (IoT) that relate to user interaction?,fb571ac1,"[{'abstract': '  The Internet of Things (IoT) demands authentication systems which can provide\nboth security and usability. Recent research utilizes the rich sensing\ncapabilities of smart devices to build security schemes operating without human\ninteraction, such as zero-interaction pairing (ZIP) and zero-interaction\nauthentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and\nreported promising results. However, those schemes were often evaluated under\nconditions which do not reflect realistic IoT scenarios. In addition, drawing\nany comparison among the existing schemes is impossible due to the lack of a\ncommon public dataset and unavailability of scheme implementations.\nIn this paper, we address these challenges by conducting the first\nlarge-scale comparative study of ZIP and ZIA schemes, carried out under\nrealistic conditions. We collect and release the most comprehensive dataset in\nthe domain to date, containing over 4250 hours of audio recordings and 1\nbillion sensor readings from three different scenarios, and evaluate five\nstate-of-the-art schemes based on these data. Our study reveals that the\neffectiveness of the existing proposals is highly dependent on the scenario\nthey are used in. In particular, we show that these schemes are subject to\nerror rates between 0.6% and 52.8%.\n', 'title': 'Perils of Zero-Interaction Security in the Internet of Things'}]","Title: Perils of Zero-Interaction Security in the Internet of Things

Abstract:   The Internet of Things (IoT) demands authentication systems which can provide
both security and usability. Recent research utilizes the rich sensing
capabilities of smart devices to build security schemes operating without human
interaction, such as zero-interaction pairing (ZIP) and zero-interaction
authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and
reported promising results. However, those schemes were often evaluated under
conditions which do not reflect realistic IoT scenarios. In addition, drawing
any comparison among the existing schemes is impossible due to the lack of a
common public dataset and unavailability of scheme implementations.
In this paper, we address these challenges by conducting the first
large-scale comparative study of ZIP and ZIA schemes, carried out under
realistic conditions. We collect and release the most comprehensive dataset in
the domain to date, containing over 4250 hours of audio recordings and 1
billion sensor readings from three different scenarios, and evaluate five
state-of-the-art schemes based on these data. Our study reveals that the
effectiveness of the existing proposals is highly dependent on the scenario
they are used in. In particular, we show that these schemes are subject to
error rates between 0.6% and 52.8%."
What is the impact of  feature side-information on the structure of learned models?,83274a30,"[{'abstract': '  Very often features come with their own vectorial descriptions which provide\ndetailed information about their properties. We refer to these vectorial\ndescriptions as feature side-information. In the standard learning scenario,\ninput is represented as a vector of features and the feature side-information\nis most often ignored or used only for feature selection prior to model\nfitting. We believe that feature side-information which carries information\nabout features intrinsic property will help improve model prediction if used in\na proper way during learning process. In this paper, we propose a framework\nthat allows for the incorporation of the feature side-information during the\nlearning of very general model families to improve the prediction performance.\nWe control the structures of the learned models so that they reflect features\nsimilarities as these are defined on the basis of the side-information. We\nperform experiments on a number of benchmark datasets which show significant\npredictive performance gains, over a number of baselines, as a result of the\nexploitation of the side-information.\n', 'title': 'Regularising Non-linear Models Using Feature Side-information'}]","Title: Regularising Non-linear Models Using Feature Side-information

Abstract: Very often features come with their own vectorial descriptions which provide
detailed information about their properties. We refer to these vectorial
descriptions as feature side-information. In the standard learning scenario,
input is represented as a vector of features and the feature side-information
is most often ignored or used only for feature selection prior to model
fitting. We believe that feature side-information which carries information
about features intrinsic property will help improve model prediction if used in
a proper way during learning process. In this paper, we propose a framework
that allows for the incorporation of the feature side-information during the
learning of very general model families to improve the prediction performance.
We control the structures of the learned models so that they reflect features
similarities as these are defined on the basis of the side-information. We
perform experiments on a number of benchmark datasets which show significant
predictive performance gains, over a number of baselines, as a result of the
exploitation of the side-information.

(The impact of feature side-information on the structure of learned models is that it helps to control the structures of the learned models so that they reflect features similarities as defined on the basis of the side-information, leading to significant predictive performance gains.)"
What type of maps are studied in relation to polar factorization on the sphere S^n?,73dde11f,"[{'abstract': '  Let M be a compact Riemannian manifold and let $\\mu$,d be the associated\nmeasure and distance on M. Robert McCann obtained, generalizing results for the\nEuclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->\nM pushing forward $\\mu$ to a measure $\\nu$: each S factors uniquely a.e. into\nthe composition S = T \\circ U, where U : M -> M is volume preserving and T : M\n-> M is the optimal map transporting $\\mu$ to $\\nu$ with respect to the cost\nfunction d^2/2.\nIn this article we study the polar factorization of conformal and projective\nmaps of the sphere S^n. For conformal maps, which may be identified with\nelements of the identity component of O(1,n+1), we prove that the polar\nfactorization in the sense of optimal mass transport coincides with the\nalgebraic polar factorization (Cartan decomposition) of this Lie group. For the\nprojective case, where the group GL_+(n+1) is involved, we find necessary and\nsufficient conditions for these two factorizations to agree.\n', 'title': 'Polar factorization of conformal and projective maps of the sphere in the sense of optimal mass transport'}]","Title: Polar factorization of conformal and projective maps of the sphere in the sense of optimal mass transport

Abstract:   Let M be a compact Riemannian manifold and let $\mu$,d be the associated
measure and distance on M. Robert McCann obtained, generalizing results for the
Euclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->
M pushing forward $\mu$ to a measure $\nu$: each S factors uniquely a.e. into
the composition S = T \circ U, where U : M -> M is volume preserving and T : M
-> M is the optimal map transporting $\mu$ to $\nu$ with respect to the cost
function d^2/2.
In this article we study the polar factorization of conformal and projective
maps of the sphere S^n. For conformal maps, which may be identified with
elements of the identity component of O(1,n+1), we prove that the polar
factorization in the sense of optimal mass transport coincides with the
algebraic polar factorization (Cartan decomposition) of this Lie group. For the
projective case, where the group GL_+(n+1) is involved, we find necessary and
sufficient conditions for these two factorizations to agree."
Can you find a paper that explains the capacity of a binary channel using discrete measure-theoretic probability theory?,9925f427,"[{'abstract': '  Muroga [M52] showed how to express the Shannon channel capacity of a discrete\nchannel with noise [S49] as an explicit function of the transition\nprobabilities. His method accommodates channels with any finite number of input\nsymbols, any finite number of output symbols and any transition probability\nmatrix. Silverman [S55] carried out Muroga\'s method in the special case of a\nbinary channel (and went on to analyse ""cascades"" of several such binary\nchannels).\nThis article is a note on the resulting formula for the capacity C(a, c) of a\nsingle binary channel. We aim to clarify some of the arguments and correct a\nsmall error. In service of this aim, we first formulate several of Shannon\'s\ndefinitions and proofs in terms of discrete measure-theoretic probability\ntheory. We provide an alternate proof to Silverman\'s, of the feasibility of the\noptimal input distribution for a binary channel. For convenience, we also\nexpress C(a, c) in a single expression explicitly dependent on a and c only,\nwhich Silverman stopped short of doing.\n', 'title': 'On the Computation of the Shannon Capacity of a Discrete Channel with Noise'}]","I think I can help you with that!

Title: On the Computation of the Shannon Capacity of a Discrete Channel with Noise

Abstract: Muroga [M52] showed how to express the Shannon channel capacity of a discrete channel with noise [S49] as an explicit function of the transition probabilities. His method accommodates channels with any finite number of input symbols, any finite number of output symbols and any transition probability matrix. Silverman [S55] carried out Muroga's method in the special case of a binary channel (and went on to analyse ""cascades"" of several such binary channels). This article is a note on the resulting formula for the capacity C(a, c) of a single binary channel. We aim to clarify some of the arguments and correct a small error. In service of this aim, we first formulate several of Shannon's definitions and proofs in terms of discrete measure-theoretic probability theory. We provide an alternate proof to Silverman's, of the feasibility of the optimal input distribution for a binary channel. For convenience, we also express C(a, c) in a single expression explicitly dependent on a and c only, which Silverman stopped short of doing."
What are some of the properties of dynamical systems that can be used to characterize mental health?,665eefa0,"[{'abstract': '  We introduce new techniques to the analysis of neural spatiotemporal dynamics\nvia applying $\\epsilon$-machine reconstruction to electroencephalography (EEG)\nmicrostate sequences. Microstates are short duration quasi-stable states of the\ndynamically changing electrical field topographies recorded via an array of\nelectrodes from the human scalp, and cluster into four canonical classes. The\nsequence of microstates observed under particular conditions can be considered\nan information source with unknown underlying structure. $\\epsilon$-machines\nare discrete dynamical system automata with state-dependent probabilities on\ndifferent future observations (in this case the next measured EEG microstate).\nThey artificially reproduce underlying structure in an optimally predictive\nmanner as generative models exhibiting dynamics emulating the behaviour of the\nsource. Here we present experiments using both simulations and empirical data\nsupporting the value of associating these discrete dynamical systems with\nmental states (e.g. mind-wandering, focused attention, etc.) and with clinical\npopulations. The neurodynamics of mental states and clinical populations can\nthen be further characterized by properties of these dynamical systems,\nincluding: i) statistical complexity (determined by the number of states of the\ncorresponding $\\epsilon$-automaton); ii) entropy rate; iii) characteristic\nsequence patterning (syntax, probabilistic grammars); iv) duration, persistence\nand stability of dynamical patterns; and v) algebraic measures such as\nKrohn-Rhodes complexity or holonomy length of the decompositions of these. The\npotential applications include the characterization of mental states in\nneurodynamic terms for mental health diagnostics, well-being interventions,\nhuman-machine interface, and others on both subject-specific and\ngroup/population-level.\n', 'title': 'Simulating and Reconstructing Neurodynamics with Epsilon-Automata Applied to Electroencephalography (EEG) Microstate Sequences'}]","After reviewing the research papers, I found that the properties of dynamical systems that can be used to characterize mental health are mentioned in the first paper. Here is the relevant information:

Title: Simulating and Reconstructing Neurodynamics with Epsilon-Automata Applied to Electroencephalography (EEG) Microstate Sequences

Abstract: ...The neurodynamics of mental states and clinical populations can then be further characterized by properties of these dynamical systems, including: i) statistical complexity (determined by the number of states of the corresponding ε-automaton); ii) entropy rate; iii) characteristic sequence patterning (syntax, probabilistic grammars); iv) duration, persistence and stability of dynamical patterns; and v) algebraic measures such as Krohn-Rhodes complexity or holonomy length of the decompositions of these...

The properties mentioned are:

* Statistical complexity
* Entropy rate
* Characteristic sequence patterning (syntax, probabilistic grammars)
* Duration, persistence, and stability of dynamical patterns
* Algebraic measures such as Krohn-Rhodes complexity or holonomy length of the decompositions of these"
What techniques are used to initialize coupled fire-atmosphere simulations with realistic weather conditions?,17c88ff9,"[{'abstract': '  Observational data collected during experiments, such as the planned Fire and\nSmoke Model Evaluation Experiment (FASMEE), are critical for progressing and\ntransitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM\ninto operational use. Historical meteorological data, representing typical\nweather conditions for the anticipated burn locations and times, have been\nprocessed to initialize and run a set of simulations representing the planned\nexperimental burns. Based on an analysis of these numerical simulations, this\npaper provides recommendations on the experimental setup that include the\nignition procedures, size and duration of the burns, and optimal sensor\nplacement. New techniques are developed to initialize coupled fire-atmosphere\nsimulations with weather conditions typical of the planned burn locations and\ntime of the year. Analysis of variation and sensitivity analysis of simulation\ndesign to model parameters by repeated Latin Hypercube Sampling are used to\nassess the locations of the sensors. The simulations provide the locations of\nthe measurements that maximize the expected variation of the sensor outputs\nwith the model parameters.\n', 'title': 'Experimental Design of a Prescribed Burn Instrumentation'}]","Title: Experimental Design of a Prescribed Burn Instrumentation

Abstract: Observational data collected during experiments, such as the planned Fire and Smoke Model Evaluation Experiment (FASMEE), are critical for progressing and transitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM into operational use. Historical meteorological data, representing typical weather conditions for the anticipated burn locations and times, have been processed to initialize and run a set of simulations representing the planned experimental burns. Based on an analysis of these numerical simulations, this paper provides recommendations on the experimental setup that include the ignition procedures, size and duration of the burns, and optimal sensor placement. New techniques are developed to initialize coupled fire-atmosphere simulations with weather conditions typical of the planned burn locations and time of the year. Analysis of variation and sensitivity analysis of simulation design to model parameters by repeated Latin Hypercube Sampling are used to assess the locations of the sensors. The simulations provide the locations of the measurements that maximize the expected variation of the sensor outputs with the model parameters.

The technique used to initialize coupled fire-atmosphere simulations with realistic weather conditions is the processing of historical meteorological data, representing typical weather conditions for the anticipated burn locations and times."
"In the context of surface superconductivity, what order term describes the effect of boundary shape on superconducting electrons?",444e4081,"[{'abstract': ""  We analyze the response of a type II superconducting wire to an external\nmagnetic field parallel to it in the framework of Ginzburg-Landau theory. We\nfocus on the surface superconductivity regime of applied field between the\nsecond and third critical values, where the superconducting state survives only\nclose to the sample's boundary. Our first finding is that, in first\napproximation, the shape of the boundary plays no role in determining the\ndensity of superconducting electrons. A second order term is however isolated,\ndirectly proportional to the mean curvature of the boundary. This demonstrates\nthat points of higher boundary curvature (counted inwards) attract\nsuperconducting electrons.\n"", 'title': 'Universal and shape dependent features of surface superconductivity'}]","Title: Universal and shape dependent features of surface superconductivity

Abstract: We analyze the response of a type II superconducting wire to an external magnetic field parallel to it in the framework of Ginzburg-Landau theory. We focus on the surface superconductivity regime of applied field between the second and third critical values, where the superconducting state survives only close to the sample's boundary. Our first finding is that, in first approximation, the shape of the boundary plays no role in determining the density of superconducting electrons. A second order term is however isolated, directly proportional to the mean curvature of the boundary. This demonstrates that points of higher boundary curvature (counted inwards) attract superconducting electrons.

According to this abstract, the second-order term, which is directly proportional to the mean curvature of the boundary, describes the effect of boundary shape on superconducting electrons."
What two methods are used to model the interaction between a heavy metal and a ferromagnet?,7a8cf8a9,"[{'abstract': '  A three-dimensional spin current solver based on a generalised spin\ndrift-diffusion description, including the spin Hall effect, is integrated with\na magnetisation dynamics solver. The resulting model is shown to simultaneously\nreproduce the spin-orbit torques generated using the spin Hall effect, spin\npumping torques generated by magnetisation dynamics in multilayers, as well as\nthe spin transfer torques acting on magnetisation regions with spatial\ngradients, whilst field-like and spin-like torques are reproduced in a spin\nvalve geometry. Two approaches to modelling interfaces are analysed, one based\non the spin mixing conductance and the other based on continuity of spin\ncurrents where the spin dephasing length governs the absorption of transverse\nspin components. In both cases analytical formulas are derived for the\nspin-orbit torques in a heavy metal / ferromagnet bilayer geometry, showing in\ngeneral both field-like and damping-like torques are generated. The limitations\nof the analytical approach are discussed, showing that even in a simple bilayer\ngeometry, due to the non-uniformity of the spin currents, a full\nthree-dimensional treatment is required. Finally the model is applied to the\nquantitative analysis of the spin Hall angle in Pt by reproducing published\nexperimental data on the ferromagnetic resonance linewidth in the bilayer\ngeometry.\n', 'title': 'Unified Treatment of Spin Torques using a Coupled Magnetisation Dynamics and Three-Dimensional Spin Current Solver'}]","Title: Unified Treatment of Spin Torques using a Coupled Magnetisation Dynamics and Three-Dimensional Spin Current Solver

Abstract: A three-dimensional spin current solver based on a generalised spin drift-diffusion description, including the spin Hall effect, is integrated with a magnetisation dynamics solver. The resulting model is shown to simultaneously reproduce the spin-orbit torques generated using the spin Hall effect, spin pumping torques generated by magnetisation dynamics in multilayers, as well as the spin transfer torques acting on magnetisation regions with spatial gradients, whilst field-like and spin-like torques are reproduced in a spin valve geometry. Two approaches to modelling interfaces are analysed, one based on the spin mixing conductance and the other based on continuity of spin currents where the spin dephasing length governs the absorption of transverse spin components."
What caused a checkerboard pattern in the density-density correlation function of a sonic black hole condensate?,6c017591,"[{'abstract': '  Analog black/white hole pairs, consisting of a region of supersonic flow,\nhave been achieved in a recent experiment by J. Steinhauer using an elongated\nBose-Einstein condensate. A growing standing density wave, and a checkerboard\nfeature in the density-density correlation function, were observed in the\nsupersonic region. We model the density-density correlation function, taking\ninto account both quantum fluctuations and the shot-to-shot variation of atom\nnumber normally present in ultracold-atom experiments. We find that quantum\nfluctuations alone produce some, but not all, of the features of the\ncorrelation function, whereas atom-number fluctuation alone can produce all the\nobserved features, and agreement is best when both are included. In both cases,\nthe density-density correlation is not intrinsic to the fluctuations, but\nrather is induced by modulation of the standing wave caused by the\nfluctuations.\n', 'title': 'Induced density correlations in a sonic black hole condensate'}]","Based on the provided context, I found the answer to your question:

Title: Induced density correlations in a sonic black hole condensate

Abstract: Analog black/white hole pairs, consisting of a region of supersonic flow, have been achieved in a recent experiment by J. Steinhauer using an elongated Bose-Einstein condensate. A growing standing density wave, and a checkerboard feature in the density-density correlation function, were observed in the supersonic region. We model the density-density correlation function, taking into account both quantum fluctuations and the shot-to-shot variation of atom number normally present in ultracold-atom experiments. We find that quantum fluctuations alone produce some, but not all, of the features of the correlation function, whereas atom-number fluctuation alone can produce all the observed features, and agreement is best when both are included. In both cases, the density-density correlation is not intrinsic to the fluctuations, but rather is induced by modulation of the standing wave caused by the fluctuations.

According to this abstract, the checkerboard pattern in the density-density correlation function of a sonic black hole condensate is caused by either quantum fluctuations or atom-number fluctuations, or a combination of both."
How does the relative irregularity of a Kodaira fibration relate to the surface groups in its fundamental group?,f0510f37,"[{'abstract': '  The fundamental group $\\pi$ of a Kodaira fibration is, by definition, the\nextension of a surface group $\\Pi_b$ by another surface group $\\Pi_g$, i.e. \\[\n1 \\rightarrow \\Pi_g \\rightarrow \\pi \\rightarrow \\Pi_b \\rightarrow 1. \\]\nConversely, we can inquire about what conditions need to be satisfied by a\ngroup of that sort in order to be the fundamental group of a Kodaira fibration.\nIn this short note we collect some restriction on the image of the classifying\nmap $m \\colon \\Pi_b \\to \\Gamma_g$ in terms of the coinvariant homology of\n$\\Pi_g$. In particular, we observe that if $\\pi$ is the fundamental group of a\nKodaira fibration with relative irregularity $g-s$, then $g \\leq 1+ 6s$, and we\nshow that this effectively constrains the possible choices for $\\pi$, namely\nthat there are group extensions as above that fail to satisfy this bound, hence\ncannot be the fundamental group of a Kodaira fibration. In particular this\nprovides examples of symplectic $4$--manifolds that fail to admit a Kähler\nstructure for reasons that eschew the usual obstructions.\n', 'title': 'A note on the fundamental group of Kodaira fibrations'}]","Title: A note on the fundamental group of Kodaira fibrations

Abstract: The fundamental group $\pi$ of a Kodaira fibration is, by definition, the extension of a surface group $\Pi_b$ by another surface group $\Pi_g$, i.e. \[
1 \rightarrow \Pi_g \rightarrow \pi \rightarrow \Pi_b \rightarrow 1. \]
Conversely, we can inquire about what conditions need to be satisfied by a group of that sort in order to be the fundamental group of a Kodaira fibration. In this short note we collect some restriction on the image of the classifying map $m \colon \Pi_b \to \Gamma_g$ in terms of the coinvariant homology of $\Pi_g$. In particular, we observe that if $\pi$ is the fundamental group of a Kodaira fibration with relative irregularity $g-s$, then $g \leq 1+ 6s$, and we show that this effectively constrains the possible choices for $\pi$, namely that there are group extensions as above that fail to satisfy this bound, hence cannot be the fundamental group of a Kodaira fibration. In particular this provides examples of symplectic $4$--manifolds that fail to admit a Kähler structure for reasons that eschew the usual obstructions.

The relative irregularity of a Kodaira fibration, $g-s$, is related to the surface groups in its fundamental group by the equation $g \leq 1+ 6s$, where $g$ and $s$ are parameters related to the surface groups $\Pi_g$ and $\Pi_b$ respectively."
What are the challenges associated with high detector occupancy in RICH detectors?,a4527d87,"[{'abstract': '  The limitations in performance of the present RICH system in the LHCb\nexperiment are given by the natural chromatic dispersion of the gaseous\nCherenkov radiator, the aberrations of the optical system and the pixel size of\nthe photon detectors. Moreover, the overall PID performance can be affected by\nhigh detector occupancy as the pattern recognition becomes more difficult with\nhigh particle multiplicities. This paper shows a way to improve performance by\nsystematically addressing each of the previously mentioned limitations. These\nideas are applied in the present and future upgrade phases of the LHCb\nexperiment. Although applied to specific circumstances, they are used as a\nparadigm on what is achievable in the development and realisation of high\nprecision RICH detectors.\n', 'title': 'The Future of RICH Detectors through the Light of the LHCb RICH'}]","Title: The Future of RICH Detectors through the Light of the LHCb RICH

Abstract: The limitations in performance of the present RICH system in the LHCb experiment are given by the natural chromatic dispersion of the gaseous Cherenkov radiator, the aberrations of the optical system and the pixel size of the photon detectors. Moreover, the overall PID performance can be affected by high detector occupancy as the pattern recognition becomes more difficult with high particle multiplicities. This paper shows a way to improve performance by systematically addressing each of the previously mentioned limitations. These ideas are applied in the present and future upgrade phases of the LHCb experiment. Although applied to specific circumstances, they are used as a paradigm on what is achievable in the development and realisation of high precision RICH detectors."
Search for a draft chapter about neural machine translation published in 2017.,7ac18b32,"[{'abstract': '  Draft of textbook chapter on neural machine translation. a comprehensive\ntreatment of the topic, ranging from introduction to neural networks,\ncomputation graphs, description of the currently dominant attentional\nsequence-to-sequence model, recent refinements, alternative architectures and\nchallenges. Written as chapter for the textbook Statistical Machine\nTranslation. Used in the JHU Fall 2017 class on machine translation.\n', 'title': 'Neural Machine Translation'}]","Title: Neural Machine Translation

Abstract: Draft of textbook chapter on neural machine translation. a comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges. Written as chapter for the textbook Statistical Machine Translation. Used in the JHU Fall 2017 class on machine translation."
What is the impact of the end of Moore's Law on AI development?,980d0a2b,"[{'abstract': ""  With the increasing commoditization of computer vision, speech recognition\nand machine translation systems and the widespread deployment of learning-based\nback-end technologies such as digital advertising and intelligent\ninfrastructures, AI (Artificial Intelligence) has moved from research labs to\nproduction. These changes have been made possible by unprecedented levels of\ndata and computation, by methodological advances in machine learning, by\ninnovations in systems software and architectures, and by the broad\naccessibility of these technologies.\nThe next generation of AI systems promises to accelerate these developments\nand increasingly impact our lives via frequent interactions and making (often\nmission-critical) decisions on our behalf, often in highly personalized\ncontexts. Realizing this promise, however, raises daunting challenges. In\nparticular, we need AI systems that make timely and safe decisions in\nunpredictable environments, that are robust against sophisticated adversaries,\nand that can process ever increasing amounts of data across organizations and\nindividuals without compromising confidentiality. These challenges will be\nexacerbated by the end of the Moore's Law, which will constrain the amount of\ndata these technologies can store and process. In this paper, we propose\nseveral open research directions in systems, architectures, and security that\ncan address these challenges and help unlock AI's potential to improve lives\nand society.\n"", 'title': 'A Berkeley View of Systems Challenges for AI'}]","Title: A Berkeley View of Systems Challenges for AI

Abstract:  With the increasing commoditization of computer vision, speech recognition
and machine translation systems and the widespread deployment of learning-based
back-end technologies such as digital advertising and intelligent
infrastructures, AI (Artificial Intelligence) has moved from research labs to
production. These changes have been made possible by unprecedented levels of
data and computation, by methodological advances in machine learning, by
innovations in systems software and architectures, and by the broad
accessibility of these technologies.
The next generation of AI systems promises to accelerate these developments
and increasingly impact our lives via frequent interactions and making (often
mission-critical) decisions on our behalf, often in highly personalized
contexts. Realizing this promise, however, raises daunting challenges. In
particular, we need AI systems that make timely and safe decisions in
unpredictable environments, that are robust against sophisticated adversaries,
and that can process ever increasing amounts of data across organizations and
individuals without compromising confidentiality. These challenges will be
exacerbated by the end of the Moore's Law, which will constrain the amount of
data these technologies can store and process. In this paper, we propose
several open research directions in systems, architectures, and security that
can address these challenges and help unlock AI's potential to improve lives
and society.

According to this paper, the end of Moore's Law will constrain the amount of data that AI technologies can store and process, thereby exacerbating the challenges faced by AI systems."
How are singularities in the moduli space of unordered n points on the Riemann sphere characterized?,eb7cc396,"[{'abstract': '  For $n\\ge5$, it is well known that the moduli space $\\mathfrak{M_{0,\\:n}}$ of\nunordered $n$ points on the Riemann sphere is a quotient space of the Zariski\nopen set $K_n$ of $\\mathbb C^{n-3}$ by an $S_n$ action. The stabilizers of this\n$S_n$ action at certain points of this Zariski open set $K_n$ correspond to the\ngroups fixing the sets of $n$ points on the Riemann sphere. Let $\\alpha$ be a\nsubset of $n$ distinct points on the Riemann sphere. We call the group of all\nlinear fractional transformations leaving $\\alpha$ invariant the stabilizer of\n$\\alpha$, which is finite by observation. For each non-trivial finite subgroup\n$G$ of the group ${\\rm PSL}(2,{\\Bbb C})$ of linear fractional transformations,\nwe give the necessary and sufficient condition for finite subsets of the\nRiemann sphere under which the stabilizers of them are conjugate to $G$. We\nalso prove that there does exist some finite subset of the Riemann sphere whose\nstabilizer coincides with $G$. Next we obtain the irreducible decompositions of\nthe representations of the stabilizers on the tangent spaces at the\nsingularities of $\\mathfrak{M_{0,\\:n}}$. At last, on $\\mathfrak{M_{0,\\:5}}$ and\n$\\mathfrak{M_{0,\\:6}}$, we work out explicitly the singularities and the\nrepresentations of their stabilizers on the tangent spaces at them.\n', 'title': 'Moduli Spaces of Unordered $n\\ge5$ Points on the Riemann Sphere and Their Singularities'}]","Title: Moduli Spaces of Unordered $n\ge5$ Points on the Riemann Sphere and Their Singularities

Abstract: For $n\ge5$, it is well known that the moduli space $\mathfrak{M_{0,\:n}}$ of unordered $n$ points on the Riemann sphere is a quotient space of the Zariski open set $K_n$ of $\mathbb C^{n-3}$ by an $S_n$ action. ... ... Next we obtain the irreducible decompositions of the representations of the stabilizers on the tangent spaces at the singularities of $\mathfrak{M_{0,\:n}}$. At last, on $\mathfrak{M_{0,\:5}}$ and $\mathfrak{M_{0,\:6}}$, we work out explicitly the singularities and the representations of their stabilizers on the tangent spaces at them.

This research paper explains the characterization of singularities in the moduli space of unordered n points on the Riemann sphere, which is what you were looking for."
Are there infinitely many non-trivial models of the Euclidean plane?,a219708c,"[{'abstract': '  We introduce a new invariant, the real (logarithmic)-Kodaira dimension, that\nallows to distinguish smooth real algebraic surfaces up to birational\ndiffeomorphism. As an application, we construct infinite families of smooth\nrational real algebraic surfaces with trivial homology groups, whose real loci\nare diffeomorphic to $\\mathbb{R}^2$, but which are pairwise not birationally\ndiffeomorphic. There are thus infinitely many non-trivial models of the\neuclidean plane, contrary to the compact case.\n', 'title': 'Algebraic models of the Euclidean plane'}]","Title: Algebraic models of the Euclidean plane

Abstract: We introduce a new invariant, the real (logarithmic)-Kodaira dimension, that allows to distinguish smooth real algebraic surfaces up to birational diffeomorphism. As an application, we construct infinite families of smooth rational real algebraic surfaces with trivial homology groups, whose real loci are diffeomorphic to $\mathbb{R}^2$, but which are pairwise not birationally diffeomorphic. There are thus infinitely many non-trivial models of the euclidean plane, contrary to the compact case."
What are the differences in pore collapse behavior under weak and strong shock conditions?,b93fb0b4,"[{'abstract': '  Shock wave interactions with defects, such as pores, are known to play a key\nrole in the chemical initiation of energetic materials. The shock response of\nhexanitrostilbene is studied through a combination of large scale reactive\nmolecular dynamics and mesoscale hydrodynamic simulations. In order to extend\nour simulation capability at the mesoscale to include weak shock conditions (<\n6 GPa), atomistic simulations of pore collapse are used to define a strain rate\ndependent strength model. Comparing these simulation methods allows us to\nimpose physically-reasonable constraints on the mesoscale model parameters. In\ndoing so, we have been able to study shock waves interacting with pores as a\nfunction of this viscoplastic material response. We find that the pore collapse\nbehavior of weak shocks is characteristically different to that of strong\nshocks.\n', 'title': 'Multiscale Modeling of Shock Wave Localization in Porous Energetic Material'}]","Title: Multiscale Modeling of Shock Wave Localization in Porous Energetic Material

Abstract: Shock wave interactions with defects, such as pores, are known to play a key role in the chemical initiation of energetic materials. The shock response of hexanitrostilbene is studied through a combination of large scale reactive molecular dynamics and mesoscale hydrodynamic simulations. In order to extend our simulation capability at the mesoscale to include weak shock conditions (< 6 GPa), atomistic simulations of pore collapse are used to define a strain rate dependent strength model. Comparing these simulation methods allows us to impose physically-reasonable constraints on the mesoscale model parameters. In doing so, we have been able to study shock waves interacting with pores as a function of this viscoplastic material response. We find that the pore collapse behavior of weak shocks is characteristically different to that of strong shocks."
What is the impact of ceRNA networks on the cell's proteome?,c558d2ec,"[{'abstract': ""  Non-conding RNAs play a key role in the post-transcriptional regulation of\nmRNA translation and turnover in eukaryotes. miRNAs, in particular, interact\nwith their target RNAs through protein-mediated, sequence-specific binding,\ngiving rise to extended and highly heterogeneous miRNA-RNA interaction\nnetworks. Within such networks, competition to bind miRNAs can generate an\neffective positive coupling between their targets. Competing endogenous RNAs\n(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.\nAlbeit potentially weak, ceRNA interactions can occur both dynamically,\naffecting e.g. the regulatory clock, and at stationarity, in which case ceRNA\nnetworks as a whole can be implicated in the composition of the cell's\nproteome. Many features of ceRNA interactions, including the conditions under\nwhich they become significant, can be unraveled by mathematical and in silico\nmodels. We review the understanding of the ceRNA effect obtained within such\nframeworks, focusing on the methods employed to quantify it, its role in the\nprocessing of gene expression noise, and how network topology can determine its\nreach.\n"", 'title': 'Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs'}]","Title: Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs

Abstract:   Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.

The answer to the question is found in the last sentence of the abstract, which states that ceRNA networks as a whole can be implicated in the composition of the cell's proteome."
What are the challenges associated with determining the value of retrosynthetic positions?,88e873a5,"[{'abstract': '  Retrosynthesis is a technique to plan the chemical synthesis of organic\nmolecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a\nsearch tree is built by analysing molecules recursively and dissecting them\ninto simpler molecular building blocks until one obtains a set of known\nbuilding blocks. The search space is intractably large, and it is difficult to\ndetermine the value of retrosynthetic positions. Here, we propose to model\nretrosynthesis as a Markov Decision Process. In combination with a Deep Neural\nNetwork policy learned from essentially the complete published knowledge of\nchemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In\nexploratory studies, we demonstrate that MCTS with neural network policies\noutperforms the traditionally used best-first search with hand-coded\nheuristics.\n', 'title': 'Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies'}]","Title: Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies

Abstract: Retrosynthesis is a technique to plan the chemical synthesis of organic molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a search tree is built by analysing molecules recursively and dissecting them into simpler molecular building blocks until one obtains a set of known building blocks. The search space is intractably large, and it is difficult to determine the value of retrosynthetic positions. Here, we propose to model retrosynthesis as a Markov Decision Process. In combination with a Deep Neural Network policy learned from essentially the complete published knowledge of chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In exploratory studies, we demonstrate that MCTS with neural network policies outperforms the traditionally used best-first search with hand-coded heuristics."
What was the ranking of the SU-RUG system among the participating teams in the competition track? ,d00379bc,"[{'abstract': '  This paper describes the Stockholm University/University of Groningen\n(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological\ninflection. Our system is based on an attentional sequence-to-sequence neural\nnetwork model using Long Short-Term Memory (LSTM) cells, with joint training of\nmorphological inflection and the inverse transformation, i.e. lemmatization and\nmorphological analysis. Our system outperforms the baseline with a large\nmargin, and our submission ranks as the 4th best team for the track we\nparticipate in (task 1, high-resource).\n', 'title': 'SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models'}]","Title: SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models

Abstract: This paper describes the Stockholm University/University of Groningen (SU-RUG) system for the SIGMORPHON 2017 shared task on morphological inflection. Our system is based on an attentional sequence-to-sequence neural network model using Long Short-Term Memory (LSTM) cells, with joint training of morphological inflection and the inverse transformation, i.e. lemmatization and morphological analysis. Our system outperforms the baseline with a large margin, and our submission ranks as the 4th best team for the track we participate in (task 1, high-resource).

Answer: The SU-RUG system ranked 4th among the participating teams in the competition track."
What is the name of the general-purpose architecture for graph modeling that includes  k-th order convolution operator and adaptive filtering module?,ba44471b,"[{'abstract': '  In this paper, we presented a novel convolutional neural network framework\nfor graph modeling, with the introduction of two new modules specially designed\nfor graph-structured data: the $k$-th order convolution operator and the\nadaptive filtering module. Importantly, our framework of High-order and\nAdaptive Graph Convolutional Network (HA-GCN) is a general-purposed\narchitecture that fits various applications on both node and graph centrics, as\nwell as graph generative models. We conducted extensive experiments on\ndemonstrating the advantages of our framework. Particularly, our HA-GCN\noutperforms the state-of-the-art models on node classification and molecule\nproperty prediction tasks. It also generates 32% more real molecules on the\nmolecule generation task, both of which will significantly benefit real-world\napplications such as material design and drug screening.\n', 'title': 'Graph Convolution: A High-Order and Adaptive Approach'}]","Title: Graph Convolution: A High-Order and Adaptive Approach

Abstract: In this paper, we presented a novel convolutional neural network framework for graph modeling, with the introduction of two new modules specially designed for graph-structured data: the $k$-th order convolution operator and the adaptive filtering module. Importantly, our framework of High-order and Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed architecture that fits various applications on both node and graph centrics, as well as graph generative models. We conducted extensive experiments on demonstrating the advantages of our framework. Particularly, our HA-GCN outperforms the state-of-the-art models on node classification and molecule property prediction tasks. It also generates 32% more real molecules on the molecule generation task, both of which will significantly benefit real-world applications such as material design and drug screening."
What type of extended Kleinian groups contain a Schottky group as a normal subgroup where the quotient is a dihedral group?,13e2cd42,"[{'abstract': '  A Schottky structure on a handlebody $M$ of genus $g$ is provided by a\nSchottky group of rank $g$. A symmetry (an orientation-reversing involution) of\n$M$ is known to have at most $(g+1)$ connected components of fixed points. Each\nof these components is either a point or a compact bordered surface (either\norientable or not) whose boundary is contained in the border of $M$. In this\npaper, we derive sharp upper bounds for the total number of connected\ncomponents of the sets of fixed points of given two or three symmetries of $M$.\nIn order to obtain such an upper bound, we obtain a geometrical structure\ndescription of those extended Kleinian groups $K$ containing a Schottky group\n$\\Gamma$ as finite index normal subgroup so that $K/\\Gamma$ is a dihedral group\n(called dihedral Schottky groups). Our upper bounds turn out to be different to\nthe corresponding ones at the level of closed Riemann surfaces. In contrast to\nthe case of Riemann surfaces, we observe that $M$ cannot have two different\nmaximal symmetries.\n', 'title': 'Symmetries of handlebodies and their fixed points: Dihedral extended Schottky groups'}]","Title: Symmetries of handlebodies and their fixed points: Dihedral extended Schottky groups

Abstract: A Schottky structure on a handlebody $M$ of genus $g$ is provided by a 
Schottky group of rank $g$. A symmetry (an orientation-reversing involution) of 
$M$ is known to have at most $(g+1)$ connected components of fixed points. 
Each of these components is either a point or a compact bordered surface 
(either orientable or not) whose boundary is contained in the border of $M$. 
In this paper, we derive sharp upper bounds for the total number of connected 
components of the sets of fixed points of given two or three symmetries of $M$. 
In order to obtain such an upper bound, we obtain a geometrical structure 
description of those extended Kleinian groups $K$ containing a Schottky group 
$\Gamma$ as finite index normal subgroup so that $K/\Gamma$ is a dihedral group 
(called dihedral Schottky groups). Our upper bounds turn out to be different to 
the corresponding ones at the level of closed Riemann surfaces. In contrast to 
the case of Riemann surfaces, we observe that $M$ cannot have two different 
maximal symmetries."
"What are the implications of underreporting in public health studies, and how can improved methods address this issue?",6fad8f92,"[{'abstract': '  Measurement error in observational datasets can lead to systematic bias in\ninferences based on these datasets. As studies based on observational data are\nincreasingly used to inform decisions with real-world impact, it is critical\nthat we develop a robust set of techniques for analyzing and adjusting for\nthese biases. In this paper we present a method for estimating the distribution\nof an outcome given a binary exposure that is subject to underreporting. Our\nmethod is based on a missing data view of the measurement error problem, where\nthe true exposure is treated as a latent variable that is marginalized out of a\njoint model. We prove three different conditions under which the outcome\ndistribution can still be identified from data containing only error-prone\nobservations of the exposure. We demonstrate this method on synthetic data and\nanalyze its sensitivity to near violations of the identifiability conditions.\nFinally, we use this method to estimate the effects of maternal smoking and\nopioid use during pregnancy on childhood obesity, two import problems from\npublic health. Using the proposed method, we estimate these effects using only\nsubject-reported drug use data and substantially refine the range of estimates\ngenerated by a sensitivity analysis-based approach. Further, the estimates\nproduced by our method are consistent with existing literature on both the\neffects of maternal smoking and the rate at which subjects underreport smoking.\n', 'title': 'Learning Models from Data with Measurement Error: Tackling Underreporting'}]","Title: Learning Models from Data with Measurement Error: Tackling Underreporting

Abstract: Measurement error in observational datasets can lead to systematic bias in inferences based on these datasets. As studies based on observational data are increasingly used to inform decisions with real-world impact, it is critical that we develop a robust set of techniques for analyzing and adjusting for these biases. In this paper we present a method for estimating the distribution of an outcome given a binary exposure that is subject to underreporting. Our method is based on a missing data view of the measurement error problem, where the true exposure is treated as a latent variable that is marginalized out of a joint model. We prove three different conditions under which the outcome distribution can still be identified from data containing only error-prone observations of the exposure. We demonstrate this method on synthetic data and analyze its sensitivity to near violations of the identifiability conditions. Finally, we use this method to estimate the effects of maternal smoking and opioid use during pregnancy on childhood obesity, two import problems from public health. Using the proposed method, we estimate these effects using only subject-reported drug use data and substantially refine the range of estimates generated by a sensitivity analysis-based approach. Further, the estimates produced by our method are consistent with existing literature on both the effects of maternal smoking and the rate at which subjects underreport smoking."
What role does the 3rd Generation Partnership Project (3GPP) play in the evolution of network management?,fcf8573a,"[{'abstract': '  In this paper, we provide an analysis of self-organized network management,\nwith an end-to-end perspective of the network. Self-organization as applied to\ncellular networks is usually referred to Self-organizing Networks (SONs), and\nit is a key driver for improving Operations, Administration, and Maintenance\n(OAM) activities. SON aims at reducing the cost of installation and management\nof 4G and future 5G networks, by simplifying operational tasks through the\ncapability to configure, optimize and heal itself. To satisfy 5G network\nmanagement requirements, this autonomous management vision has to be extended\nto the end to end network. In literature and also in some instances of products\navailable in the market, Machine Learning (ML) has been identified as the key\ntool to implement autonomous adaptability and take advantage of experience when\nmaking decisions. In this paper, we survey how network management can\nsignificantly benefit from ML solutions. We review and provide the basic\nconcepts and taxonomy for SON, network management and ML. We analyse the\navailable state of the art in the literature, standardization, and in the\nmarket. We pay special attention to 3rd Generation Partnership Project (3GPP)\nevolution in the area of network management and to the data that can be\nextracted from 3GPP networks, in order to gain knowledge and experience in how\nthe network is working, and improve network performance in a proactive way.\nFinally, we go through the main challenges associated with this line of\nresearch, in both 4G and in what 5G is getting designed, while identifying new\ndirections for research.\n', 'title': 'From 4G to 5G: Self-organized Network Management meets Machine Learning'}]","Title: From 4G to 5G: Self-organized Network Management meets Machine Learning

Abstract: In this paper, we provide an analysis of self-organized network management, with an end-to-end perspective of the network. Self-organization as applied to cellular networks is usually referred to Self-organizing Networks (SONs), and it is a key driver for improving Operations, Administration, and Maintenance (OAM) activities. SON aims at reducing the cost of installation and management of 4G and future 5G networks, by simplifying operational tasks through the capability to configure, optimize and heal itself. To satisfy 5G network management requirements, this autonomous management vision has to be extended to the end to end network. In literature and also in some instances of products available in the market, Machine Learning (ML) has been identified as the key tool to implement autonomous adaptability and take advantage of experience when making decisions. In this paper, we survey how network management can significantly benefit from ML solutions. We review and provide the basic concepts and taxonomy for SON, network management and ML. We analyse the available state of the art in the literature, standardization, and in the market. We pay special attention to 3GPP evolution in the area of network management and to the data that can be extracted from 3GPP networks, in order to gain knowledge and experience in how the network is working, and improve network performance in a proactive way. Finally, we go through the main challenges associated with this line of research, in both 4G and in what 5G is getting designed, while identifying new directions for research."
What is a key advantage of the streaming algorithm described in the paper?,4dadc2f3,"[{'abstract': '  We present an efficient algorithm to compute Euler characteristic curves of\ngray scale images of arbitrary dimension. In various applications the Euler\ncharacteristic curve is used as a descriptor of an image.\nOur algorithm is the first streaming algorithm for Euler characteristic\ncurves. The usage of streaming removes the necessity to store the entire image\nin RAM. Experiments show that our implementation handles terabyte scale images\non commodity hardware. Due to lock-free parallelism, it scales well with the\nnumber of processor cores. Our software---CHUNKYEuler---is available as open\nsource on Bitbucket.\nAdditionally, we put the concept of the Euler characteristic curve in the\nwider context of computational topology. In particular, we explain the\nconnection with persistence diagrams.\n', 'title': 'Streaming Algorithm for Euler Characteristic Curves of Multidimensional Images'}]","Title: Streaming Algorithm for Euler Characteristic Curves of Multidimensional Images

Abstract: We present an efficient algorithm to compute Euler characteristic curves of gray scale images of arbitrary dimension. In various applications the Euler characteristic curve is used as a descriptor of an image. Our algorithm is the first streaming algorithm for Euler characteristic curves. The usage of streaming removes the necessity to store the entire image in RAM. Experiments show that our implementation handles terabyte scale images on commodity hardware. Due to lock-free parallelism, it scales well with the number of processor cores. Our software---CHUNKYEuler---is available as open source on Bitbucket. Additionally, we put the concept of the Euler characteristic curve in the wider context of computational topology. In particular, we explain the connection with persistence diagrams.

The key advantage of the streaming algorithm described in this paper is that it does not require the entire image to be stored in RAM, allowing it to handle large-scale images efficiently."
How is the novel architecture validated in the context of a domestic service robot?,91b60d6b,"[{'abstract': '  When developing general purpose robots, the overarching software architecture\ncan greatly affect the ease of accomplishing various tasks. Initial efforts to\ncreate unified robot systems in the 1990s led to hybrid architectures,\nemphasizing a hierarchy in which deliberative plans direct the use of reactive\nskills. However, since that time there has been significant progress in the\nlow-level skills available to robots, including manipulation and perception,\nmaking it newly feasible to accomplish many more tasks in real-world domains.\nThere is thus renewed optimism that robots will be able to perform a wide array\nof tasks while maintaining responsiveness to human operators. However, the top\nlayer in traditional hybrid architectures, designed to achieve long-term goals,\ncan make it difficult to react quickly to human interactions during goal-driven\nexecution. To mitigate this difficulty, we propose a novel architecture that\nsupports such transitions by adding a top-level reactive module which has\nflexible access to both reactive skills and a deliberative control module. To\nvalidate this architecture, we present a case study of its application on a\ndomestic service robot platform.\n', 'title': 'LAAIR: A Layered Architecture for Autonomous Interactive Robots'}]","Title: LAAIR: A Layered Architecture for Autonomous Interactive Robots

Abstract:   When developing general purpose robots, the overarching software architecture
can greatly affect the ease of accomplishing various tasks. Initial efforts to
create unified robot systems in the 1990s led to hybrid architectures,
emphasizing a hierarchy in which deliberative plans direct the use of reactive
skills. However, since that time there has been significant progress in the
low-level skills available to robots, including manipulation and perception,
making it newly feasible to accomplish many more tasks in real-world domains.
There is thus renewed optimism that robots will be able to perform a wide array
of tasks while maintaining responsiveness to human operators. However, the top
layer in traditional hybrid architectures, designed to achieve long-term goals,
can make it difficult to react quickly to human interactions during goal-driven
execution. To mitigate this difficulty, we propose a novel architecture that
supports such transitions by adding a top-level reactive module which has
flexible access to both reactive skills and a deliberative control module. To
validate this architecture, we present a case study of its application on a
domestic service robot platform."
How does changing the topology on the space of objects affect classifying spaces?,c8a268a2,"[{'abstract': ""  This is an exposition of homotopical results on the geometric realization of\nsemi-simplicial spaces. We then use these to derive basic foundational results\nabout classifying spaces of topological categories, possibly without units. The\ntopics considered include: fibrancy conditions on topological categories; the\neffect on classifying spaces of freely adjoining units; approximate notions of\nunits; Quillen's Theorems A and B for non-unital topological categories; the\neffect on classifying spaces of changing the topology on the space of objects;\nthe Group-Completion Theorem.\n"", 'title': 'Semi-simplicial spaces'}]","I think I can help you with that!

Title: Semi-simplicial spaces

Abstract: This is an exposition of homotopical results on the geometric realization of semi-simplicial spaces. We then use these to derive basic foundational results about classifying spaces of topological categories, possibly without units. The topics considered include: fibrancy conditions on topological categories; the effect on classifying spaces of freely adjoining units; approximate notions of units; Quillen's Theorems A and B for non-unital topological categories; the effect on classifying spaces of changing the topology on the space of objects; the Group-Completion Theorem.

According to the abstract, changing the topology on the space of objects has an effect on classifying spaces, but the exact nature of this effect is not specified in this abstract."
How are K-bi-invariant convolution semigroups connected to  right K-invariant convolution semigroups?,b5315717,"[{'abstract': '  Our goal is to find classes of convolution semigroups on Lie groups $G$ that\ngive rise to interesting processes in symmetric spaces $G/K$. The\n$K$-bi-invariant convolution semigroups are a well-studied example. An\nappealing direction for the next step is to generalise to right $K$-invariant\nconvolution semigroups, but recent work of Liao has shown that these are in\none-to-one correspondence with $K$-bi-invariant convolution semigroups. We\ninvestigate a weaker notion of right $K$-invariance, but show that this is, in\nfact, the same as the usual notion. Another possible approach is to use\ngeneralised notions of negative definite functions, but this also leads to\nnothing new. We finally find an interesting class of convolution semigroups\nthat are obtained by making use of the Cartan decomposition of a semisimple Lie\ngroup, and the solution of certain stochastic differential equations. Examples\nsuggest that these are well-suited for generating random motion along geodesics\nin symmetric spaces.\n', 'title': 'Convolution Semigroups of Probability Measures on Gelfand Pairs, Revisited'}]","Title: Convolution Semigroups of Probability Measures on Gelfand Pairs, Revisited

Abstract:   Our goal is to find classes of convolution semigroups on Lie groups $G$ that
give rise to interesting processes in symmetric spaces $G/K$. The
$K$-bi-invariant convolution semigroups are a well-studied example. An
appealing direction for the next step is to generalise to right $K$-invariant
convolution semigroups, but recent work of Liao has shown that these are in
one-to-one correspondence with $K$-bi-invariant convolution semigroups. We
investigate a weaker notion of right $K$-invariance, but show that this is, in
fact, the same as the usual notion."
What asymptotic value for surface tension was observed in a study of flowing soap films?,9cc404f5,"[{'abstract': '  The surface tension of flowing soap films is measured with respect to the\nfilm thickness and the concentration of soap solution. We perform this\nmeasurement by measuring the curvature of the nylon wires that bound the soap\nfilm channel and use the measured curvature to parametrize the relation between\nthe surface tension and the tension of the wire. We find the surface tension of\nour soap films increases when the film is relatively thin or made of soap\nsolution of low concentration, otherwise it approaches an asymptotic value 30\nmN/m. A simple adsorption model with only two parameters describes our\nobservations reasonably well. With our measurements, we are also able to\nmeasure Gibbs elasticity for our soap film.\n', 'title': 'Surface tension of flowing soap films'}]","Title: Surface tension of flowing soap films

Abstract:   The surface tension of flowing soap films is measured with respect to the
film thickness and the concentration of soap solution. We perform this
measurement by measuring the curvature of the nylon wires that bound the soap
film channel and use the measured curvature to parametrize the relation between
the surface tension and the tension of the wire. We find the surface tension of
our soap films increases when the film is relatively thin or made of soap
solution of low concentration, otherwise it approaches an asymptotic value 30
mN/m. A simple adsorption model with only two parameters describes our
observations reasonably well. With our measurements, we are also able to
measure Gibbs elasticity for our soap film."
Are there methods to apply Thompson Sampling with complex models like neural networks?,49981aba,"[{'abstract': '  Thompson sampling has emerged as an effective heuristic for a broad range of\nonline decision problems. In its basic form, the algorithm requires computing\nand sampling from a posterior distribution over models, which is tractable only\nfor simple special cases. This paper develops ensemble sampling, which aims to\napproximate Thompson sampling while maintaining tractability even in the face\nof complex models such as neural networks. Ensemble sampling dramatically\nexpands on the range of applications for which Thompson sampling is viable. We\nestablish a theoretical basis that supports the approach and present\ncomputational results that offer further insight.\n', 'title': 'Ensemble Sampling'}]","Title: Ensemble Sampling

Abstract: Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight."
How is the concept of successive omniscience (SO) applied to efficiently attain omniscience in a multi-user system?,32be339b,"[{'abstract': '  The system that we study in this paper contains a set of users that observe a\ndiscrete memoryless multiple source and communicate via noise-free channels\nwith the aim of attaining omniscience, the state that all users recover the\nentire multiple source. We adopt the concept of successive omniscience (SO),\ni.e., letting the local omniscience in some user subset be attained before the\nglobal omniscience in the entire system, and consider the problem of how to\nefficiently attain omniscience in a successive manner. Based on the existing\nresults on SO, we propose a CompSetSO algorithm for determining a complimentary\nset, a user subset in which the local omniscience can be attained first without\nincreasing the sum-rate, the total number of communications, for the global\nomniscience. We also derive a sufficient condition for a user subset to be\ncomplimentary so that running the CompSetSO algorithm only requires a lower\nbound, instead of the exact value, of the minimum sum-rate for attaining global\nomniscience. The CompSetSO algorithm returns a complimentary user subset in\npolynomial time. We show by example how to recursively apply the CompSetSO\nalgorithm so that the global omniscience can be attained by multi-stages of SO.\n', 'title': 'A Practical Approach for Successive Omniscience'}]","Title: A Practical Approach for Successive Omniscience

Abstract: The system that we study in this paper contains a set of users that observe a discrete memoryless multiple source and communicate via noise-free channels with the aim of attaining omniscience, the state that all users recover the entire multiple source. We adopt the concept of successive omniscience (SO), i.e., letting the local omniscience in some user subset be attained before the global omniscience in the entire system, and consider the problem of how to efficiently attain omniscience in a successive manner. Based on the existing results on SO, we propose a CompSetSO algorithm for determining a complimentary set, a user subset in which the local omniscience can be attained first without increasing the sum-rate, the total number of communications, for the global omniscience. We also derive a sufficient condition for a user subset to be complimentary so that running the CompSetSO algorithm only requires a lower bound, instead of the exact value, of the minimum sum-rate for attaining global omniscience. The CompSetSO algorithm returns a complimentary user subset in polynomial time. We show by example how to recursively apply the CompSetSO algorithm so that the global omniscience can be attained by multi-stages of SO."
How does the density of spatial content around an object  influence object recognition in deep  convolutional neural networks?,cf412edf,"[{'abstract': '  Our eyes sample a disproportionately large amount of information at the\ncentre of gaze with increasingly sparse sampling into the periphery. This\nsampling scheme is widely believed to be a wiring constraint whereby high\nresolution at the centre is achieved by sacrificing spatial acuity in the\nperiphery. Here we propose that this sampling scheme may be optimal for object\nrecognition because the relevant spatial content is dense near an object and\nsparse in the surrounding vicinity. We tested this hypothesis by training deep\nconvolutional neural networks on full-resolution and foveated images. Our main\nfinding is that networks trained on images with foveated sampling show better\nobject classification compared to networks trained on full resolution images.\nImportantly, blurring images according to the human blur function yielded the\nbest performance compared to images with shallower or steeper blurring. Taken\ntogether our results suggest that, peripheral blurring in our eyes may have\nevolved for optimal object recognition, rather than merely to satisfy wiring\nconstraints.\n', 'title': 'Human peripheral blur is optimal for object recognition'}]","Title: Human peripheral blur is optimal for object recognition

Abstract: Our eyes sample a disproportionately large amount of information at the centre of gaze with increasingly sparse sampling into the periphery. This sampling scheme is widely believed to be a wiring constraint whereby high resolution at the centre is achieved by sacrificing spatial acuity in the periphery. Here we propose that this sampling scheme may be optimal for object recognition because the relevant spatial content is dense near an object and sparse in the surrounding vicinity. We tested this hypothesis by training deep convolutional neural networks on full-resolution and foveated images. Our main finding is that networks trained on images with foveated sampling show better object classification compared to networks trained on full resolution images. Importantly, blurring images according to the human blur function yielded the best performance compared to images with shallower or steeper blurring. Taken together our results suggest that, peripheral blurring in our eyes may have evolved for optimal object recognition, rather than merely to satisfy wiring constraints."
"In the context of drug resistance, how can branching processes be applied to understand the emergence of resistant cell types?",8911121c,"[{'abstract': '  Investigating the emergence of a particular cell type is a recurring theme in\nmodels of growing cellular populations. The evolution of resistance to therapy\nis a classic example. Common questions are: when does the cell type first\noccur, and via which sequence of steps is it most likely to emerge? For growing\npopulations, these questions can be formulated in a general framework of\nbranching processes spreading through a graph from a root to a target vertex.\nCells have a particular fitness value on each vertex and can transition along\nedges at specific rates. Vertices represents cell states, say \\mic{genotypes\n}or physical locations, while possible transitions are acquiring a mutation or\ncell migration. We focus on the setting where cells at the root vertex have the\nhighest fitness and transition rates are small. Simple formulas are derived for\nthe time to reach the target vertex and for the probability that it is reached\nalong a given path in the graph. We demonstrate our results on \\mic{several\nscenarios relevant to the emergence of drug resistance}, including: the\norderings of resistance-conferring mutations in bacteria and the impact of\nimperfect drug penetration in cancer.\n', 'title': 'Competing evolutionary paths in growing populations with applications to multidrug resistance'}]","Based on the provided context, I found a relevant research paper that addresses the question of how branching processes can be applied to understand the emergence of resistant cell types in the context of drug resistance.

Title: Competing evolutionary paths in growing populations with applications to multidrug resistance

Abstract: Investigating the emergence of a particular cell type is a recurring theme in models of growing cellular populations. The evolution of resistance to therapy is a classic example. Common questions are: when does the cell type first occur, and via which sequence of steps is it most likely to emerge? For growing populations, these questions can be formulated in a general framework of branching processes spreading through a graph from a root to a target vertex. Cells have a particular fitness value on each vertex and can transition along edges at specific rates. Vertices represent cell states, say genotypes or physical locations, while possible transitions are acquiring a mutation or cell migration. We focus on the setting where cells at the root vertex have the highest fitness and transition rates are small. Simple formulas are derived for the time to reach the target vertex and for the probability that it is reached along a given path in the graph. We demonstrate our results on several scenarios relevant to the emergence of drug resistance, including: the orderings of resistance-conferring mutations in bacteria and the impact of imperfect drug penetration in cancer."
what are routing techniques for mitigating radio interference in wireless body area networks,398a5231,"[{'abstract': ""  In this paper, we study the performance of two cross-layer optimized dynamic\nrouting techniques for radio interference mitigation across multiple coexisting\nwireless body area networks (BANs), based on real-life measurements. At the\nnetwork layer, the best route is selected according to channel state\ninformation from the physical layer, associated with low duty cycle TDMA at the\nMAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel\ncooperative multi-path routing (CMR) incorporating 3-branch selection\ncombining) perform real-time and reliable data transfer across BANs operating\nnear the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'\nmixed-activities is used for analyzing the proposed cross-layer optimization.\nWe show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and\neven 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage\nprobability at a realistic signal-to-interference-plus-noise ratio (SINR).\nAcceptable packet delivery ratios (PDR) and spectral efficiencies are obtained\nfrom SPR and CMR with reasonably sensitive receivers across a range of TDMA low\nduty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The\ndistribution fits for received SINR through routing are also derived and\nvalidated with theoretical analysis.\n"", 'title': 'Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks'}]","Based on the provided context, I found a relevant research paper that discusses routing techniques for mitigating radio interference in wireless body area networks.

Title: Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks

Abstract: In this paper, we study the performance of two cross-layer optimized dynamic routing techniques for radio interference mitigation across multiple coexisting wireless body area networks (BANs), based on real-life measurements. At the network layer, the best route is selected according to channel state information from the physical layer, associated with low duty cycle TDMA at the MAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel cooperative multi-path routing (CMR) incorporating 3-branch selection combining) perform real-time and reliable data transfer across BANs operating near the 2.4 GHz ISM band. An open-access experimental data set of 'everyday' mixed-activities is used for analyzing the proposed cross-layer optimization. We show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and even 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage probability at a realistic signal-to-interference-plus-noise ratio (SINR). Acceptable packet delivery ratios (PDR) and spectral efficiencies are obtained from SPR and CMR with reasonably sensitive receivers across a range of TDMA low duty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The distribution fits for received SINR through routing are also derived and validated with theoretical analysis."
How does the space-filling path on the embedded mated-CRT map relate to space-filling SLE?,208a81fb,"[{'abstract': ""  We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain\nrandom planar maps converge to $\\gamma$-Liouville quantum gravity\n($\\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized\nmatings of correlated continuum random trees, and $\\gamma$ ranges from $0$ to\n$2$ as one varies the correlation parameter. We also show that the associated\nspace-filling path on the embedded map converges to space-filling\nSLE$_{\\kappa}$ for $\\kappa =16/\\gamma^2$ (in the annealed sense) and that\nsimple random walk on the embedded map converges to Brownian motion (in the\nquenched sense). Our arguments also yield analogous statements for the Smith\n(square tiling) embedding of the mated-CRT map.\nThis work constitutes the first proof that a discrete conformal embedding of\na random planar map converges to LQG. Many more such statements have been\nconjectured. Since the mated-CRT map can be viewed as a coarse-grained\napproximation to other random planar maps (the UIPT, tree-weighted maps,\nbipolar-oriented maps, etc.), our results indicate a potential approach for\nproving that embeddings of these maps converge to LQG as well.\nTo prove the main result, we establish several (independently interesting)\ntheorems about LQG surfaces decorated by space-filling SLE. There is a natural\nway to use the SLE curve to divide the plane into `cells' corresponding to\nvertices of the mated-CRT map. We study the law of the shape of the\norigin-containing cell, in particular proving moments for the ratio of its\nsquared diameter to its area. We also give bounds on the degree of the\norigin-containing cell and establish a form of ergodicity for the entire\nconfiguration. Ultimately, we use these properties to show (using a general\ntheorem proved in a separate paper) that random walk on these cells converges\nto a time change of Brownian motion, which in turn leads to the Tutte embedding\nresult.\n"", 'title': 'The Tutte embedding of the mated-CRT map converges to Liouville quantum gravity'}]","Title: The Tutte embedding of the mated-CRT map converges to Liouville quantum gravity

Abstract:  We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain
random planar maps converge to $\gamma$-Liouville quantum gravity
($\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized
matings of correlated continuum random trees, and $\gamma$ ranges from $0$ to
$2$ as one varies the correlation parameter. We also show that the associated
space-filling path on the embedded map converges to space-filling
SLE$_{\kappa}$ for $\kappa =16/\gamma^2$ (in the annealed sense) and that
simple random walk on the embedded map converges to Brownian motion (in the
quenched sense). Our arguments also yield analogous statements for the Smith
(square tiling) embedding of the mated-CRT map.
This work constitutes the first proof that a discrete conformal embedding of
a random planar map converges to LQG. Many more such statements have been
conjectured. Since the mated-CRT map can be viewed as a coarse-grained
approximation to other random planar maps (the UIPT, tree-weighted maps,
bipolar-oriented maps, etc.), our results indicate a potential approach for
proving that embeddings of these maps converge to LQG as well.
To prove the main result, we establish several (independently interesting)
theorems about LQG surfaces decorated by space-filling SLE. There is a natural
way to use the SLE curve to divide the plane into `cells' corresponding to
vertices of the mated-CRT map. We study the law of the shape of the
origin-containing cell, in particular proving moments for the ratio of its
squared diameter to its area. We also give bounds on the degree of the
origin-containing cell and establish a form of ergodicity for the entire
configuration. Ultimately, we use these properties to show (using a general
theorem proved in a separate paper) that random walk on these cells converges
to a time change of Brownian motion, which in turn leads to the Tutte embedding
result."
What term is used to describe algorithms inspired by game mechanics?,23c0f86a,"[{'abstract': '  Examining games from a fresh perspective we present the idea of game-inspired\nand game-based algorithms, dubbed ""gamorithms"".\n', 'title': 'Gamorithm'}]","Title: Gamorithm

Abstract: Examining games from a fresh perspective we present the idea of game-inspired and game-based algorithms, dubbed ""gamorithms""."
What are some recent advancements being used for automated road defect detection?,c7adb59b,"[{'abstract': '  Road networks in cities are massive and is a critical component of mobility.\nFast response to defects, that can occur not only due to regular wear and tear\nbut also because of extreme events like storms, is essential. Hence there is a\nneed for an automated system that is quick, scalable and cost-effective for\ngathering information about defects. We propose a system for city-scale road\naudit, using some of the most recent developments in deep learning and semantic\nsegmentation. For building and benchmarking the system, we curated a dataset\nwhich has annotations required for road defects. However, many of the labels\nrequired for road audit have high ambiguity which we overcome by proposing a\nlabel hierarchy. We also propose a multi-step deep learning model that segments\nthe road, subdivide the road further into defects, tags the frame for each\ndefect and finally localizes the defects on a map gathered using GPS. We\nanalyze and evaluate the models on image tagging as well as segmentation at\ndifferent levels of the label hierarchy.\n', 'title': 'City-Scale Road Audit System using Deep Learning'}]","Title: City-Scale Road Audit System using Deep Learning

Abstract: Road networks in cities are massive and is a critical component of mobility.
Fast response to defects, that can occur not only due to regular wear and tear
but also because of extreme events like storms, is essential. Hence there is a
need for an automated system that is quick, scalable and cost-effective for
gathering information about defects. We propose a system for city-scale road
audit, using some of the most recent developments in deep learning and semantic
segmentation. For building and benchmarking the system, we curated a dataset
which has annotations required for road defects. However, many of the labels
required for road audit have high ambiguity which we overcome by proposing a
label hierarchy. We also propose a multi-step deep learning model that segments
the road, subdivide the road further into defects, tags the frame for each
defect and finally localizes the defects on a map gathered using GPS. We
analyze and evaluate the models on image tagging as well as segmentation at
different levels of the label hierarchy."
How can analyzing student text responses help improve instruction?,b0875fd1,"[{'abstract': ""  An important, yet largely unstudied, problem in student data analysis is to\ndetect misconceptions from students' responses to open-response questions.\nMisconception detection enables instructors to deliver more targeted feedback\non the misconceptions exhibited by many students in their class, thus improving\nthe quality of instruction. In this paper, we propose a new natural language\nprocessing-based framework to detect the common misconceptions among students'\ntextual responses to short-answer questions. We propose a probabilistic model\nfor students' textual responses involving misconceptions and experimentally\nvalidate it on a real-world student-response dataset. Experimental results show\nthat our proposed framework excels at classifying whether a response exhibits\none or more misconceptions. More importantly, it can also automatically detect\nthe common misconceptions exhibited across responses from multiple students to\nmultiple questions; this property is especially important at large scale, since\ninstructors will no longer need to manually specify all possible misconceptions\nthat students might exhibit.\n"", 'title': 'Data-Mining Textual Responses to Uncover Misconception Patterns'}]","Title: Data-Mining Textual Responses to Uncover Misconception Patterns

Abstract: An important, yet largely unstudied, problem in student data analysis is to detect misconceptions from students' responses to open-response questions. Misconception detection enables instructors to deliver more targeted feedback on the misconceptions exhibited by many students in their class, thus improving the quality of instruction. In this paper, we propose a new natural language processing-based framework to detect the common misconceptions among students' textual responses to short-answer questions. We propose a probabilistic model for students' textual responses involving misconceptions and experimentally validate it on a real-world student-response dataset. Experimental results show that our proposed framework excels at classifying whether a response exhibits one or more misconceptions. More importantly, it can also automatically detect the common misconceptions exhibited across responses from multiple students to multiple questions; this property is especially important at large scale, since instructors will no longer need to manually specify all possible misconceptions that students might exhibit."
What specific temperature-dependent phenomenon related to the crystalline electric field is observed in the DMFT theory below the lattice coherence temperature?,99ff5c0e,"[{'abstract': '  The temperature-dependent evolution of the Kondo lattice is a long-standing\ntopic of theoretical and experimental investigation and yet it lacks a truly\nmicroscopic description of the relation of the basic $f$-$d$ hybridization\nprocesses to the fundamental temperature scales of Kondo screening and\nFermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$\nhybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo\nlattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved\nphotoemission (ARPES) with sufficient detail to allow direct comparison to\nfirst principles dynamical mean field theory (DMFT) calculations containing\nfull realism of crystalline electric field states. The ARPES results, for two\northogonal (001) and (100) cleaved surfaces and three different $f$-$d$\nhybridization scenarios, with additional microscopic insight provided by DMFT,\nreveal $f$ participation in the Fermi surface at temperatures much higher than\nthe lattice coherence temperature, $T^*\\approx$ 45 K, commonly believed to be\nthe onset for such behavior. The identification of a $T$-dependent crystalline\nelectric field degeneracy crossover in the DMFT theory $below$ $T^*$ is\nspecifically highlighted.\n', 'title': 'Evolution of the Kondo lattice electronic structure above the transport coherence temperature'}]","Title: Evolution of the Kondo lattice electronic structure above the transport coherence temperature

Abstract:   The temperature-dependent evolution of the Kondo lattice is a long-standing
topic of theoretical and experimental investigation and yet it lacks a truly
microscopic description of the relation of the basic $f$-$d$ hybridization
processes to the fundamental temperature scales of Kondo screening and
Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$
hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo
lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved
photoemission (ARPES) with sufficient detail to allow direct comparison to
first principles dynamical mean field theory (DMFT) calculations containing
full realism of crystalline electric field states. The ARPES results, for two
orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$
hybridization scenarios, with additional microscopic insight provided by DMFT,
reveal $f$ participation in the Fermi surface at temperatures much higher than
the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be
the onset for such behavior. The identification of a $T$-dependent crystalline
electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is
specifically highlighted."
What is the potential benefit of  a fully probabilistic approach to hydrograph prediction for risk management,9be0f336,"[{'abstract': '  Dam breach models are commonly used to predict outflow hydrographs of\npotentially failing dams and are key ingredients for evaluating flood risk. In\nthis paper a new dam breach modeling framework is introduced that shall improve\nthe reliability of hydrograph predictions of homogeneous earthen embankment\ndams. Striving for a small number of parameters, the simplified physics-based\nmodel describes the processes of failing embankment dams by breach enlargement,\ndriven by progressive surface erosion. Therein the erosion rate of dam material\nis modeled by empirical sediment transport formulations. Embedding the model\ninto a Bayesian multilevel framework allows for quantitative analysis of\ndifferent categories of uncertainties. To this end, data available in\nliterature of observed peak discharge and final breach width of historical dam\nfailures was used to perform model inversion by applying Markov Chain Monte\nCarlo simulation. Prior knowledge is mainly based on non-informative\ndistribution functions. The resulting posterior distribution shows that the\nmain source of uncertainty is a correlated subset of parameters, consisting of\nthe residual error term and the epistemic term quantifying the breach erosion\nrate. The prediction intervals of peak discharge and final breach width are\ncongruent with values known from literature. To finally predict the outflow\nhydrograph for real case applications, an alternative residual model was\nformulated that assumes perfect data and a perfect model. The fully\nprobabilistic fashion of hydrograph prediction has the potential to improve the\nadequate risk management of downstream flooding.\n', 'title': 'Development of probabilistic dam breach model using Bayesian inference'}]","Title: Development of probabilistic dam breach model using Bayesian inference

Abstract: Dam breach models are commonly used to predict outflow hydrographs of potentially failing dams and are key ingredients for evaluating flood risk. In this paper a new dam breach modeling framework is introduced that shall improve the reliability of hydrograph predictions of homogeneous earthen embankment dams. Striving for a small number of parameters, the simplified physics-based model describes the processes of failing embankment dams by breach enlargement, driven by progressive surface erosion. Therein the erosion rate of dam material is modeled by empirical sediment transport formulations. Embedding the model into a Bayesian multilevel framework allows for quantitative analysis of different categories of uncertainties. To this end, data available in literature of observed peak discharge and final breach width of historical dam failures was used to perform model inversion by applying Markov Chain Monte Carlo simulation. Prior knowledge is mainly based on non-informative distribution functions. The resulting posterior distribution shows that the main source of uncertainty is a correlated subset of parameters, consisting of the residual error term and the epistemic term quantifying the breach erosion rate. The prediction intervals of peak discharge and final breach width are congruent with values known from literature. To finally predict the outflow hydrograph for real case applications, an alternative residual model was formulated that assumes perfect data and a perfect model. The fully probabilistic fashion of hydrograph prediction has the potential to improve the adequate risk management of downstream flooding.

The potential benefit of a fully probabilistic approach to hydrograph prediction for risk management is that it has the potential to improve the adequate risk management of downstream flooding."
What method combines belief propagation and linear response for Markov random field computation?,cfdce7e2,"[{'abstract': '  A susceptibility propagation that is constructed by combining a belief\npropagation and a linear response method is used for approximate computation\nfor Markov random fields. Herein, we formulate a new, improved susceptibility\npropagation by using the concept of a diagonal matching method that is based on\nmean-field approaches to inverse Ising problems. The proposed susceptibility\npropagation is robust for various network structures, and it is reduced to the\nordinary susceptibility propagation and to the adaptive\nThouless-Anderson-Palmer equation in special cases.\n', 'title': 'Susceptibility Propagation by Using Diagonal Consistency'}]","Based on the provided abstracts, I think I can help you find the answer.

The method that combines belief propagation and linear response for Markov random field computation is:

Title: Susceptibility Propagation by Using Diagonal Consistency

Abstract: A susceptibility propagation that is constructed by combining a belief propagation and a linear response method is used for approximate computation for Markov random fields. Herein, we formulate a new, improved susceptibility propagation by using the concept of a diagonal matching method that is based on mean-field approaches to inverse Ising problems. The proposed susceptibility propagation is robust for various network structures, and it is reduced to the ordinary susceptibility propagation and to the adaptive Thouless-Anderson-Palmer equation in special cases."
How does HDR improve the visual quality of digital content?,31c74a75,"[{'abstract': '  In this lecture note, we describe high dynamic range (HDR) imaging systems;\nsuch systems are able to represent luminances of much larger brightness and,\ntypically, also a larger range of colors than conventional standard dynamic\nrange (SDR) imaging systems. The larger luminance range greatly improve the\noverall quality of visual content, making it appears much more realistic and\nappealing to observers. HDR is one of the key technologies of the future\nimaging pipeline, which will change the way the digital visual content is\nrepresented and manipulated today.\n', 'title': 'High Dynamic Range Imaging Technology'}]","I think I can help you with that!

Title: High Dynamic Range Imaging Technology

Abstract: In this lecture note, we describe high dynamic range (HDR) imaging systems; such systems are able to represent luminances of much larger brightness and, typically, also a larger range of colors than conventional standard dynamic range (SDR) imaging systems. The larger luminance range greatly improve the overall quality of visual content, making it appears much more realistic and appealing to observers. HDR is one of the key technologies of the future imaging pipeline, which will change the way the digital visual content is represented and manipulated today."
how does cross-layer optimization improve data transfer in BANs operating near 2.4 GHz,398a5231,"[{'abstract': ""  In this paper, we study the performance of two cross-layer optimized dynamic\nrouting techniques for radio interference mitigation across multiple coexisting\nwireless body area networks (BANs), based on real-life measurements. At the\nnetwork layer, the best route is selected according to channel state\ninformation from the physical layer, associated with low duty cycle TDMA at the\nMAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel\ncooperative multi-path routing (CMR) incorporating 3-branch selection\ncombining) perform real-time and reliable data transfer across BANs operating\nnear the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'\nmixed-activities is used for analyzing the proposed cross-layer optimization.\nWe show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and\neven 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage\nprobability at a realistic signal-to-interference-plus-noise ratio (SINR).\nAcceptable packet delivery ratios (PDR) and spectral efficiencies are obtained\nfrom SPR and CMR with reasonably sensitive receivers across a range of TDMA low\nduty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The\ndistribution fits for received SINR through routing are also derived and\nvalidated with theoretical analysis.\n"", 'title': 'Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks'}]","Title: Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks

Abstract: In this paper, we study the performance of two cross-layer optimized dynamic routing techniques for radio interference mitigation across multiple coexisting wireless body area networks (BANs), based on real-life measurements. At the network layer, the best route is selected according to channel state information from the physical layer, associated with low duty cycle TDMA at the MAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel cooperative multi-path routing (CMR) incorporating 3-branch selection combining) perform real-time and reliable data transfer across BANs operating near the 2.4 GHz ISM band. An open-access experimental data set of 'everyday' mixed-activities is used for analyzing the proposed cross-layer optimization. We show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and even 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage probability at a realistic signal-to-interference-plus-noise ratio (SINR). Acceptable packet delivery ratios (PDR) and spectral efficiencies are obtained from SPR and CMR with reasonably sensitive receivers across a range of TDMA low duty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The distribution fits for received SINR through routing are also derived and validated with theoretical analysis."
What are the limitations of traditional core and truss decomposition algorithms in terms of scalability and approximation?,6cce0006,"[{'abstract': '  Finding the dense regions of a graph and relations among them is a\nfundamental problem in network analysis. Core and truss decompositions reveal\ndense subgraphs with hierarchical relations. The incremental nature of\nalgorithms for computing these decompositions and the need for global\ninformation at each step of the algorithm hinders scalable parallelization and\napproximations since the densest regions are not revealed until the end. In a\nprevious work, Lu et al. proposed to iteratively compute the $h$-indices of\nneighbor vertex degrees to obtain the core numbers and prove that the\nconvergence is obtained after a finite number of iterations. This work\ngeneralizes the iterative $h$-index computation for truss decomposition as well\nas nucleus decomposition which leverages higher-order structures to generalize\ncore and truss decompositions. In addition, we prove convergence bounds on the\nnumber of iterations. We present a framework of local algorithms to obtain the\ncore, truss, and nucleus decompositions. Our algorithms are local, parallel,\noffer high scalability, and enable approximations to explore time and quality\ntrade-offs. Our shared-memory implementation verifies the efficiency,\nscalability, and effectiveness of our local algorithms on real-world networks.\n', 'title': 'Local Algorithms for Hierarchical Dense Subgraph Discovery'}]","Title: Local Algorithms for Hierarchical Dense Subgraph Discovery

Abstract: ...The incremental nature of algorithms for computing these decompositions and the need for global information at each step of the algorithm hinders scalable parallelization and approximations since the densest regions are not revealed until the end. ...

The limitation mentioned is that traditional core and truss decomposition algorithms are not scalable and do not allow for approximations because they require global information at each step, and the densest regions are only revealed at the end."
What type of radioactive decay powers a macronova?,0205243c,"[{'abstract': ""  Macronovae (kilonovae) that arise in binary neutron star mergers are powered\nby radioactive beta decay of hundreds of $r$-process nuclides. We derive, using\nFermi's theory of beta decay, an analytic estimate of the nuclear heating rate.\nWe show that the heating rate evolves as a power law ranging between $t^{-6/5}$\nto $t^{-4/3}$. The overall magnitude of the heating rate is determined by the\nmean values of nuclear quantities, e.g., the nuclear matrix elements of beta\ndecay. These values are specified by using nuclear experimental data. We\ndiscuss the role of higher order beta transitions and the robustness of the\npower law. The robust and simple form of the heating rate suggests that\nobservations of the late-time bolometric light curve $\\propto t^{-\\frac{4}{3}}$\nwould be a direct evidence of a $r$-process driven macronova. Such observations\ncould also enable us to estimate the total amount of $r$-process nuclei\nproduced in the merger.\n"", 'title': ""Analytic heating rate of neutron star merger ejecta derived from Fermi's theory of beta decay""}]","Here is the answer:

Title: Analytic heating rate of neutron star merger ejecta derived from Fermi's theory of beta decay

Abstract: Macronovae (kilonovae) that arise in binary neutron star mergers are powered by radioactive beta decay of hundreds of $r$-process nuclides. We derive, using Fermi's theory of beta decay, an analytic estimate of the nuclear heating rate. We show that the heating rate evolves as a power law ranging between $t^{-6/5}$ to $t^{-4/3}$. The overall magnitude of the heating rate is determined by the mean values of nuclear quantities, e.g., the nuclear matrix elements of beta decay. These values are specified by using nuclear experimental data. We discuss the role of higher order beta transitions and the robustness of the power law. The robust and simple form of the heating rate suggests that observations of the late-time bolometric light curve $\propto t^{-\frac{4}{3}}$ would be a direct evidence of a $r$-process driven macronova. Such observations could also enable us to estimate the total amount of $r$-process nuclei produced in the merger."
How can a neural network be optimized to work efficiently with resistive memory crossbar arrays that have nonlinear I-V characteristics?,bc127825,"[{'abstract': '  Artificial Neural Network computation relies on intensive vector-matrix\nmultiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array\nshowed a feasibility of implementing such operations with high energy\nefficiency, thus there are many works on efficiently utilizing emerging NVM\ncrossbar array as analog vector-matrix multiplier. However, its nonlinear I-V\ncharacteristics restrain critical design parameters, such as the read voltage\nand weight range, resulting in substantial accuracy loss. In this paper,\ninstead of optimizing hardware parameters to a given neural network, we propose\na methodology of reconstructing a neural network itself optimized to resistive\nmemory crossbar arrays. To verify the validity of the proposed method, we\nsimulated various neural network with MNIST and CIFAR-10 dataset using two\ndifferent specific Resistive Random Access Memory (RRAM) model. Simulation\nresults show that our proposed neural network produces significantly higher\ninference accuracies than conventional neural network when the synapse devices\nhave nonlinear I-V characteristics.\n', 'title': 'Deep Neural Network Optimized to Resistive Memory with Nonlinear Current-Voltage Characteristics'}]","Title: Deep Neural Network Optimized to Resistive Memory with Nonlinear Current-Voltage Characteristics

Abstract: Artificial Neural Network computation relies on intensive vector-matrix multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array showed a feasibility of implementing such operations with high energy efficiency, thus there are many works on efficiently utilizing emerging NVM crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V characteristics restrain critical design parameters, such as the read voltage and weight range, resulting in substantial accuracy loss. In this paper, instead of optimizing hardware parameters to a given neural network, we propose a methodology of reconstructing a neural network itself optimized to resistive memory crossbar arrays. To verify the validity of the proposed method, we simulated various neural network with MNIST and CIFAR-10 dataset using two different specific Resistive Random Access Memory (RRAM) model. Simulation results show that our proposed neural network produces significantly higher inference accuracies than conventional neural network when the synapse devices have nonlinear I-V characteristics."
How do open-circuit switching and frequency tuning affect the performance of magneto-inductive passive relay networks? ,80ab56ef,"[{'abstract': '  We consider a wireless sensor network that uses inductive near-field coupling\nfor wireless powering or communication, or for both. The severely limited range\nof an inductively coupled source-destination pair can be improved using\nresonant relay devices, which are purely passive in nature. Utilization of such\nmagneto-inductive relays has only been studied for regular network topologies,\nallowing simplified assumptions on the mutual antenna couplings. In this work\nwe present an analysis of magneto-inductive passive relaying in arbitrarily\narranged networks. We find that the resulting channel has characteristics\nsimilar to multipath fading: the channel power gain is governed by a\nnon-coherent sum of phasors, resulting in increased frequency selectivity. We\npropose and study two strategies to increase the channel power gain of random\nrelay networks: i) deactivation of individual relays by open-circuit switching\nand ii) frequency tuning. The presented results show that both methods improve\nthe utilization of available passive relays, leading to reliable and\nsignificant performance gains.\n', 'title': 'Magneto-inductive Passive Relaying in Arbitrarily Arranged Networks'}]","I think I have found the relevant research paper for you!

Title: Magneto-inductive Passive Relaying in Arbitrarily Arranged Networks

Abstract: We consider a wireless sensor network that uses inductive near-field coupling for wireless powering or communication, or for both. The severely limited range of an inductively coupled source-destination pair can be improved using resonant relay devices, which are purely passive in nature. Utilization of such magneto-inductive relays has only been studied for regular network topologies, allowing simplified assumptions on the mutual antenna couplings. In this work we present an analysis of magneto-inductive passive relaying in arbitrarily arranged networks. We find that the resulting channel has characteristics similar to multipath fading: the channel power gain is governed by a non-coherent sum of phasors, resulting in increased frequency selectivity. We propose and study two strategies to increase the channel power gain of random relay networks: i) deactivation of individual relays by open-circuit switching and ii) frequency tuning. The presented results show that both methods improve the utilization of available passive relays, leading to reliable and significant performance gains."
Can simulating brain activity using a specific analytical model improve the performance of LSTM networks for forecasting future brain activity?,27621b0b,"[{'abstract': '  Many real-world data sets, especially in biology, are produced by highly\nmultivariate and nonlinear complex dynamical systems. In this paper, we focus\non brain imaging data, including both calcium imaging and functional MRI data.\nStandard vector-autoregressive models are limited by their linearity\nassumptions, while nonlinear general-purpose, large-scale temporal models, such\nas LSTM networks, typically require large amounts of training data, not always\nreadily available in biological applications; furthermore, such models have\nlimited interpretability. We introduce here a novel approach for learning a\nnonlinear differential equation model aimed at capturing brain dynamics.\nSpecifically, we propose a variable-projection optimization approach to\nestimate the parameters of the multivariate (coupled) van der Pol oscillator,\nand demonstrate that such a model can accurately represent nonlinear dynamics\nof the brain data. Furthermore, in order to improve the predictive accuracy\nwhen forecasting future brain-activity time series, we use this analytical\nmodel as an unlimited source of simulated data for pretraining LSTM; such\nmodel-specific data augmentation approach consistently improves LSTM\nperformance on both calcium and fMRI imaging data.\n', 'title': 'Learning Nonlinear Brain Dynamics: van der Pol Meets LSTM'}]","Title: Learning Nonlinear Brain Dynamics: van der Pol Meets LSTM

Abstract: Many real-world data sets, especially in biology, are produced by highly multivariate and nonlinear complex dynamical systems. In this paper, we focus on brain imaging data, including both calcium imaging and functional MRI data. Standard vector-autoregressive models are limited by their linearity assumptions, while nonlinear general-purpose, large-scale temporal models, such as LSTM networks, typically require large amounts of training data, not always readily available in biological applications; furthermore, such models have limited interpretability. We introduce here a novel approach for learning a nonlinear differential equation model aimed at capturing brain dynamics. Specifically, we propose a variable-projection optimization approach to estimate the parameters of the multivariate (coupled) van der Pol oscillator, and demonstrate that such a model can accurately represent nonlinear dynamics of the brain data. Furthermore, in order to improve the predictive accuracy when forecasting future brain-activity time series, we use this analytical model as an unlimited source of simulated data for pretraining LSTM; such model-specific data augmentation approach consistently improves LSTM performance on both calcium and fMRI imaging data."
What are the challenges in labeling data for road defect identification?,c7adb59b,"[{'abstract': '  Road networks in cities are massive and is a critical component of mobility.\nFast response to defects, that can occur not only due to regular wear and tear\nbut also because of extreme events like storms, is essential. Hence there is a\nneed for an automated system that is quick, scalable and cost-effective for\ngathering information about defects. We propose a system for city-scale road\naudit, using some of the most recent developments in deep learning and semantic\nsegmentation. For building and benchmarking the system, we curated a dataset\nwhich has annotations required for road defects. However, many of the labels\nrequired for road audit have high ambiguity which we overcome by proposing a\nlabel hierarchy. We also propose a multi-step deep learning model that segments\nthe road, subdivide the road further into defects, tags the frame for each\ndefect and finally localizes the defects on a map gathered using GPS. We\nanalyze and evaluate the models on image tagging as well as segmentation at\ndifferent levels of the label hierarchy.\n', 'title': 'City-Scale Road Audit System using Deep Learning'}]","Title: City-Scale Road Audit System using Deep Learning

Abstract:   Road networks in cities are massive and is a critical component of mobility.
Fast response to defects, that can occur not only due to regular wear and tear
but also because of extreme events like storms, is essential. Hence there is a
need for an automated system that is quick, scalable and cost-effective for
gathering information about defects. We propose a system for city-scale road
audit, using some of the most recent developments in deep learning and semantic
segmentation. For building and benchmarking the system, we curated a dataset
which has annotations required for road defects. However, many of the labels
required for road audit have high ambiguity which we overcome by proposing a
label hierarchy. We also propose a multi-step deep learning model that segments
the road, subdivide the road further into defects, tags the frame for each
defect and finally localizes the defects on a map gathered using GPS. We
analyze and evaluate the models on image tagging as well as segmentation at
different levels of the label hierarchy.

The challenge is mentioned as ""many of the labels required for road audit have high ambiguity"" which the authors overcome by proposing a label hierarchy."
The Datura family has an unusual size distribution compared to larger asteroid families. What is the proposed explanation for this difference?,7b1156fb,"[{'abstract': '  Young asteroid families are unique sources of information about fragmentation\nphysics and the structure of their parent bodies, since their physical\nproperties have not changed much since their birth. Families have different\nproperties such as age, size, taxonomy, collision severity and others, and\nunderstanding the effect of those properties on our observations of the\nsize-frequency distribution (SFD) of family fragments can give us important\ninsights into the hypervelocity collision processes at scales we cannot achieve\nin our laboratories. Here we take as an example the very young Datura family,\nwith a small 8-km parent body, and compare its size distribution to other\nfamilies, with both large and small parent bodies, and created by both\ncatastrophic and cratering formation events. We conclude that most likely\nexplanation for the shallower size distribution compared to larger families is\na more pronounced observational bias because of its small size. Its size\ndistribution is perfectly normal when its parent body size is taken into\naccount. We also discuss some other possibilities. In addition, we study\nanother common feature: an offset or ""bump"" in the distribution occurring for a\nfew of the larger elements. We hypothesize that it can be explained by a newly\ndescribed regime of cratering, ""spall cratering"", which controls the majority\nof impact craters on the surface of small asteroids like Datura.\n', 'title': 'Interpretations of family size distributions: The Datura example'}]","Title: Interpretations of family size distributions: The Datura example

Abstract: Young asteroid families are unique sources of information about fragmentation 
physics and the structure of their parent bodies, since their physical 
properties have not changed much since their birth. Families have different 
properties such as age, size, taxonomy, collision severity and others, and 
understanding the effect of those properties on our observations of the 
size-frequency distribution (SFD) of family fragments can give us important 
insights into the hypervelocity collision processes at scales we cannot achieve 
in our laboratories. Here we take as an example the very young Datura family, 
with a small 8-km parent body, and compare its size distribution to other 
families, with both large and small parent bodies, and created by both 
catastrophic and cratering formation events. We conclude that most likely 
explanation for the shallower size distribution compared to larger families is 
a more pronounced observational bias because of its small size. Its size 
distribution is perfectly normal when its parent body size is taken into 
account. We also discuss some other possibilities. In addition, we study 
another common feature: an offset or ""bump"" in the distribution occurring for a 
few of the larger elements. We hypothesize that it can be explained by a newly 
described regime of cratering, ""spall cratering"", which controls the majority 
of impact craters on the surface of small asteroids like Datura."
What is the impact of noisy channels on shapelet learning in multivariate time series?,43c821e2,"[{'abstract': '  Time series shapelets are discriminative sub-sequences and their similarity\nto time series can be used for time series classification. Initial shapelet\nextraction algorithms searched shapelets by complete enumeration of all\npossible data sub-sequences. Research on shapelets for univariate time series\nproposed a mechanism called shapelet learning which parameterizes the shapelets\nand learns them jointly with a prediction model in an optimization procedure.\nTrivial extension of this method to multivariate time series does not yield\nvery good results due to the presence of noisy channels which lead to\noverfitting. In this paper we propose a shapelet learning scheme for\nmultivariate time series in which we introduce channel masks to discount noisy\nchannels and serve as an implicit regularization.\n', 'title': 'Channel masking for multivariate time series shapelets'}]","Title: Channel masking for multivariate time series shapelets

Abstract: Time series shapelets are discriminative sub-sequences and their similarity to time series can be used for time series classification. Initial shapelet extraction algorithms searched shapelets by complete enumeration of all possible data sub-sequences. Research on shapelets for univariate time series proposed a mechanism called shapelet learning which parameterizes the shapelets and learns them jointly with a prediction model in an optimization procedure. Trivial extension of this method to multivariate time series does not yield very good results due to the presence of noisy channels which lead to overfitting. In this paper we propose a shapelet learning scheme for multivariate time series in which we introduce channel masks to discount noisy channels and serve as an implicit regularization."
"How do relativistic field theories, including the asymmetry between field sources and bosonic fields, impact the viability of the contingent absorber?",995404ab,"[{'abstract': ""  In view of a resurgence of concern about the measurement problem, it is\npointed out that the Relativistic Transactional Interpretation (RTI) remedies\nissues previously considered as drawbacks or refutations of the original TI.\nSpecifically, once one takes into account relativistic processes that are not\nrepresentable at the non-relativistic level (such as particle creation and\nannihilation, and virtual propagation), absorption is quantitatively defined in\nunambiguous physical terms. In addition, specifics of the relativistic\ntransactional model demonstrate that the Maudlin `contingent absorber'\nchallenge to the original TI cannot even be mounted: basic features of\nestablished relativistic field theories (in particular, the asymmetry between\nfield sources and the bosonic fields, and the fact that slow-moving bound\nstates, such as atoms, are not offer waves) dictate that the `slow-moving offer\nwave' required for the challenge scenario cannot exist. It is concluded that\nissues previously considered obstacles for TI are no longer legitimately viewed\nas such, and that reconsideration of the transactional picture is warranted in\nconnection with solving the measurement problem.\n"", 'title': 'On the Status of the Measurement Problem: Recalling the Relativistic Transactional Interpretation'}]","I think I can help you with that!

Title: On the Status of the Measurement Problem: Recalling the Relativistic Transactional Interpretation

Abstract:   In view of a resurgence of concern about the measurement problem, it is
pointed out that the Relativistic Transactional Interpretation (RTI) remedies
issues previously considered as drawbacks or refutations of the original TI.
Specifically, once one takes into account relativistic processes that are not
representable at the non-relativistic level (such as particle creation and
annihilation, and virtual propagation), absorption is quantitatively defined in
unambiguous physical terms. In addition, specifics of the relativistic
transactional model demonstrate that the Maudlin `contingent absorber'
challenge to the original TI cannot even be mounted: basic features of
established relativistic field theories (in particular, the asymmetry between
field sources and the bosonic fields, and the fact that slow-moving bound
states, such as atoms, are not offer waves) dictate that the `slow-moving offer
wave' required for the challenge scenario cannot exist. It is concluded that
issues previously considered obstacles for TI are no longer legitimately viewed
as such, and that reconsideration of the transactional picture is warranted in
connection with solving the measurement problem.

This paper addresses the impact of relativistic field theories, including the asymmetry between field sources and bosonic fields, on the contingent absorber challenge, and concludes that these features prevent the challenge scenario from existing."
What are the scaling laws derived for energy in turbulent dynamos?,412f5254,"[{'abstract': '  Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both\nwith and without a significant mean field are obtained. Exact bounds are\nderived for the total energy which conform with the Kolmogorov phenomenology of\nturbulence. Best fits to numerical data show the same functional dependences as\nthe inequalities obtained from optimum theory.\n', 'title': 'Scaling laws and bounds for the turbulent G.O. Roberts dynamo'}]","Title: Scaling laws and bounds for the turbulent G.O. Roberts dynamo

Abstract: Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both with and without a significant mean field are obtained. Exact bounds are derived for the total energy which conform with the Kolmogorov phenomenology of turbulence. Best fits to numerical data show the same functional dependences as the inequalities obtained from optimum theory."
How does this meta-algorithm improve the speed of optimization in large networks?,4d057a8f,"[{'abstract': '  The life of the modern world essentially depends on the work of the large\nartificial homogeneous networks, such as wired and wireless communication\nsystems, networks of roads and pipelines. The support of their effective\ncontinuous functioning requires automatic screening and permanent optimization\nwith processing of the huge amount of data by high-performance distributed\nsystems. We propose new meta-algorithm of large homogeneous network analysis,\nits decomposition into alternative sets of loosely connected subnets, and\nparallel optimization of the most independent elements. This algorithm is based\non a network-specific correlation function, Simulated Annealing technique, and\nis adapted to work in the computer cluster. On the example of large wireless\nnetwork, we show that proposed algorithm essentially increases speed of\nparallel optimization. The elaborated general approach can be used for analysis\nand optimization of the wide range of networks, including such specific types\nas artificial neural networks or organized in networks physiological systems of\nliving organisms.\n', 'title': 'Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks'}]","Title: Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks

Abstract: The life of the modern world essentially depends on the work of the large
artificial homogeneous networks, such as wired and wireless communication
systems, networks of roads and pipelines. The support of their effective
continuous functioning requires automatic screening and permanent optimization
with processing of the huge amount of data by high-performance distributed
systems. We propose new meta-algorithm of large homogeneous network analysis,
its decomposition into alternative sets of loosely connected subnets, and
parallel optimization of the most independent elements. This algorithm is based
on a network-specific correlation function, Simulated Annealing technique, and
is adapted to work in the computer cluster. On the example of large wireless
network, we show that proposed algorithm essentially increases speed of
parallel optimization. The elaborated general approach can be used for analysis
and optimization of the wide range of networks, including such specific types
as artificial neural networks or organized in networks physiological systems of
living organisms.

This meta-algorithm improves the speed of optimization in large networks by decomposing the network into alternative sets of loosely connected subnets and parallel optimizing the most independent elements using a network-specific correlation function and Simulated Annealing technique in a computer cluster."
What is the rank of the Waring decomposition of symmetrized matrix multiplication tensor sM_langle 3 angle ?,b92da3a4,"[{'abstract': '  The recent discovery that the exponent of matrix multiplication is determined\nby the rank of the symmetrized matrix multiplication tensor has invigorated\ninterest in better understanding symmetrized matrix multiplication. I present\nan explicit rank 18 Waring decomposition of $sM_{\\langle 3\\rangle}$ and\ndescribe its symmetry group.\n', 'title': 'A rank 18 Waring decomposition of $sM_{\\langle 3\\rangle}$ with 432 symmetries'}]","Title: A rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ with 432 symmetries

Abstract: The recent discovery that the exponent of matrix multiplication is determined
by the rank of the symmetrized matrix multiplication tensor has invigorated
interest in better understanding symmetrized matrix multiplication. I present
an explicit rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ and
describe its symmetry group."
Compare the performance of packet relaying and time-translating gateways for multi-hop time synchronization in wireless sensor networks,4eb215f9,"[{'abstract': '  We present the multi-hop extensions of the recently proposed energy-efficient\ntime synchronization scheme for wireless sensor networks, which is based on the\nasynchronous source clock frequency recovery and reversed two-way message\nexchanges. We consider two hierarchical extensions based on packet relaying and\ntime-translating gateways, respectively, and analyze their performance with\nrespect to the number of layers and the delay variations through simulations.\nThe simulation results demonstrate that the time synchronization performance of\nthe packet relaying, which has lower complexity, is close to that of\ntime-translating gateways.\n', 'title': 'Multi-Hop Extensions of Energy-Efficient Wireless Sensor Network Time Synchronization'}]","Title: Multi-Hop Extensions of Energy-Efficient Wireless Sensor Network Time Synchronization

Abstract:   We present the multi-hop extensions of the recently proposed energy-efficient
time synchronization scheme for wireless sensor networks, which is based on the
asynchronous source clock frequency recovery and reversed two-way message
exchanges. We consider two hierarchical extensions based on packet relaying and
time-translating gateways, respectively, and analyze their performance with
respect to the number of layers and the delay variations through simulations.
The simulation results demonstrate that the time synchronization performance of
the packet relaying, which has lower complexity, is close to that of
time-translating gateways."
"Besides liquid water, what other atmospheric or surface characteristics might indicate habitability?",d7249ecc,"[{'abstract': '  A habitable exoplanet is a world that can maintain stable liquid water on its\nsurface. Techniques and approaches to characterizing such worlds are essential,\nas performing a census of Earth-like planets that may or may not have life will\ninform our understanding of how frequently life originates and is sustained on\nworlds other than our own. Observational techniques like high contrast imaging\nand transit spectroscopy can reveal key indicators of habitability for\nexoplanets. Both polarization measurements and specular reflectance from oceans\n(also known as ""glint"") can provide direct evidence for surface liquid water,\nwhile constraining surface pressure and temperature (from moderate resolution\nspectra) can indicate liquid water stability. Indirect evidence for\nhabitability can come from a variety of sources, including observations of\nvariability due to weather, surface mapping studies, and/or measurements of\nwater vapor or cloud profiles that indicate condensation near a surface.\nApproaches to making the types of measurements that indicate habitability are\ndiverse, and have different considerations for the required wavelength range,\nspectral resolution, maximum noise levels, stellar host temperature, and\nobserving geometry.\n', 'title': 'Characterizing Exoplanet Habitability'}]","Based on the provided abstracts, I found an answer to your question.

Title: Characterizing Exoplanet Habitability

Abstract: A habitable exoplanet is a world that can maintain stable liquid water on its surface. Techniques and approaches to characterizing such worlds are essential, as performing a census of Earth-like planets that may or may not have life will inform our understanding of how frequently life originates and is sustained on worlds other than our own. Observational techniques like high contrast imaging and transit spectroscopy can reveal key indicators of habitability for exoplanets. Both polarization measurements and specular reflectance from oceans (also known as ""glint"") can provide direct evidence for surface liquid water, while constraining surface pressure and temperature (from moderate resolution spectra) can indicate liquid water stability. Indirect evidence for habitability can come from a variety of sources, including observations of variability due to weather, surface mapping studies, and/or measurements of water vapor or cloud profiles that indicate condensation near a surface.

According to this abstract, besides liquid water, other atmospheric or surface characteristics that might indicate habitability include:

* Variability due to weather
* Surface mapping studies
* Measurements of water vapor or cloud profiles that indicate condensation near a surface
* Constrained surface pressure and temperature
* Polarization measurements
* Specular reflectance from oceans (glint)"
Are there methods to improve the learning of feature interactions in DNNs?,7340fd5b,"[{'abstract': '  Feature engineering has been the key to the success of many prediction\nmodels. However, the process is non-trivial and often requires manual feature\nengineering or exhaustive searching. DNNs are able to automatically learn\nfeature interactions; however, they generate all the interactions implicitly,\nand are not necessarily efficient in learning all types of cross features. In\nthis paper, we propose the Deep & Cross Network (DCN) which keeps the benefits\nof a DNN model, and beyond that, it introduces a novel cross network that is\nmore efficient in learning certain bounded-degree feature interactions. In\nparticular, DCN explicitly applies feature crossing at each layer, requires no\nmanual feature engineering, and adds negligible extra complexity to the DNN\nmodel. Our experimental results have demonstrated its superiority over the\nstate-of-art algorithms on the CTR prediction dataset and dense classification\ndataset, in terms of both model accuracy and memory usage.\n', 'title': 'Deep & Cross Network for Ad Click Predictions'}]","Yes, there is a method to improve the learning of feature interactions in DNNs.

Title: Deep & Cross Network for Ad Click Predictions

Abstract: Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage."
What type of magnetic ordering is predicted to be unstable at a specific ratio of couplings in Schwinger boson theory? ,5987d4a6,"[{'abstract': '  We investigate the effect of dimensional crossover in the ground state of the\nantiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular\nlattice that interpolates between the regime of weakly coupled Haldane chains\n($J^{\\prime}\\! \\!\\ll\\!\\! J$) and the isotropic triangular lattice\n($J^{\\prime}\\!\\!=\\!\\!J$). We use the density-matrix renormalization group\n(DMRG) and Schwinger boson theory performed at the Gaussian correction level\nabove the saddle-point solution. Our DMRG results show an abrupt transition\nbetween decoupled spin chains and the spirally ordered regime at\n$(J^{\\prime}/J)_c\\sim 0.42$, signaled by the sudden closing of the spin gap.\nComing from the magnetically ordered side, the computation of the spin\nstiffness within Schwinger boson theory predicts the instability of the spiral\nmagnetic order toward a magnetically disordered phase with one-dimensional\nfeatures at $(J^{\\prime}/J)_c \\sim 0.43$. The agreement of these complementary\nmethods, along with the strong difference found between the intra- and the\ninterchain DMRG short spin-spin correlations; for sufficiently large values of\nthe interchain coupling, suggests that the interplay between the quantum\nfluctuations and the dimensional crossover effects gives rise to the\none-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.\n', 'title': 'One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice'}]","Title: One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice

Abstract: We investigate the effect of dimensional crossover in the ground state of the
antiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular
lattice that interpolates between the regime of weakly coupled Haldane chains
($J^{\prime}\! \!\ll\!\! J$) and the isotropic triangular lattice
($J^{\prime}\!\!=\!\!J$). We use the density-matrix renormalization group
(DMRG) and Schwinger boson theory performed at the Gaussian correction level
above the saddle-point solution. Our DMRG results show an abrupt transition
between decoupled spin chains and the spirally ordered regime at
$(J^{\prime}/J)_c\sim 0.42$, signaled by the sudden closing of the spin gap.
Coming from the magnetically ordered side, the computation of the spin
stiffness within Schwinger boson theory predicts the instability of the spiral
magnetic order toward a magnetically disordered phase with one-dimensional
features at $(J^{\prime}/J)_c \sim 0.43$. The agreement of these complementary
methods, along with the strong difference found between the intra- and the
interchain DMRG short spin-spin correlations; for sufficiently large values of
the interchain coupling, suggests that the interplay between the quantum
fluctuations and the dimensional crossover effects gives rise to the
one-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian."
How does the behavior of an order parameter in a continuous-time Markov chain with multiple wells evolve over time?,046844be,"[{'abstract': '  We consider continuous-time Markov chains which display a family of wells at\nthe same depth. We provide sufficient conditions which entail the convergence\nof the finite-dimensional distributions of the order parameter to the ones of a\nfinite state Markov chain. We also show that the state of the process can be\nrepresented as a time-dependent convex combination of metastable states, each\nof which is supported on one well.\n', 'title': 'Metastable Markov chains: from the convergence of the trace to the convergence of the finite-dimensional distributions'}]","I think I can help you with that!

Title: Metastable Markov chains: from the convergence of the trace to the convergence of the finite-dimensional distributions

Abstract:   We consider continuous-time Markov chains which display a family of wells at
the same depth. We provide sufficient conditions which entail the convergence
of the finite-dimensional distributions of the order parameter to the ones of a
finite state Markov chain. We also show that the state of the process can be
represented as a time-dependent convex combination of metastable states, each
of which is supported on one well.

This research paper discusses the behavior of an order parameter in a continuous-time Markov chain with multiple wells over time, stating that the state of the process can be represented as a time-dependent convex combination of metastable states, each supported on one well."
"What types of risk preferences, including the extremes, can be incorporated within a risk-sensitive MPC framework?",2414c58d,"[{'abstract': '  In this paper we present a framework for risk-sensitive model predictive\ncontrol (MPC) of linear systems affected by stochastic multiplicative\nuncertainty. Our key innovation is to consider a time-consistent, dynamic risk\nevaluation of the cumulative cost as the objective function to be minimized.\nThis framework is axiomatically justified in terms of time-consistency of risk\nassessments, is amenable to dynamic optimization, and is unifying in the sense\nthat it captures a full range of risk preferences from risk-neutral (i.e.,\nexpectation) to worst case. Within this framework, we propose and analyze an\nonline risk-sensitive MPC algorithm that is provably stabilizing. Furthermore,\nby exploiting the dual representation of time-consistent, dynamic risk\nmeasures, we cast the computation of the MPC control law as a convex\noptimization problem amenable to real-time implementation. Simulation results\nare presented and discussed.\n', 'title': 'A Framework for Time-Consistent, Risk-Sensitive Model Predictive Control: Theory and Algorithms'}]","Title: A Framework for Time-Consistent, Risk-Sensitive Model Predictive Control: Theory and Algorithms

Abstract: In this paper we present a framework for risk-sensitive model predictive control (MPC) of linear systems affected by stochastic multiplicative uncertainty. Our key innovation is to consider a time-consistent, dynamic risk evaluation of the cumulative cost as the objective function to be minimized. This framework is axiomatically justified in terms of time-consistency of risk assessments, is amenable to dynamic optimization, and is unifying in the sense that it captures a full range of risk preferences from risk-neutral (i.e., expectation) to worst case. Within this framework, we propose and analyze an online risk-sensitive MPC algorithm that is provably stabilizing. Furthermore, by exploiting the dual representation of time-consistent, dynamic risk measures, we cast the computation of the MPC control law as a convex optimization problem amenable to real-time implementation. Simulation results are presented and discussed."
How do agent-based models explain the relationship between social cohesion and tax evasion?,761c47ce,"[{'abstract': '  This paper presents a simple agent-based model of an economic system,\npopulated by agents playing different games according to their different view\nabout social cohesion and tax payment. After a first set of simulations,\ncorrectly replicating results of existing literature, a wider analysis is\npresented in order to study the effects of a dynamic-adaptation rule, in which\ncitizens may possibly decide to modify their individual tax compliance\naccording to individual criteria, such as, the strength of their ethical\ncommitment, the satisfaction gained by consumption of the public good and the\nperceived opinion of neighbors. Results show the presence of thresholds levels\nin the composition of society - between taxpayers and evaders - which explain\nthe extent of damages deriving from tax evasion.\n', 'title': 'A Game of Tax Evasion: evidences from an agent-based model'}]","I think I can help you with that!

Title: A Game of Tax Evasion: evidences from an agent-based model

Abstract: This paper presents a simple agent-based model of an economic system, populated by agents playing different games according to their different view about social cohesion and tax payment. After a first set of simulations, correctly replicating results of existing literature, a wider analysis is presented in order to study the effects of a dynamic-adaptation rule, in which citizens may possibly decide to modify their individual tax compliance according to individual criteria, such as, the strength of their ethical commitment, the satisfaction gained by consumption of the public good and the perceived opinion of neighbors. Results show the presence of thresholds levels in the composition of society - between taxpayers and evaders - which explain the extent of damages deriving from tax evasion."
how do cyclic amalgamations of free groups and orderable groups relate to isolated left orders,55c0c007,"[{'abstract': '  We show that certain orderable groups admit no isolated left orders. The\ngroups we consider are cyclic amalgamations of a free group with a general\norderable group, the HNN extensions of free groups over cyclic subgroups, and a\nparticular class of one-relator groups. In order to prove the results about\norders, we develop perturbation techniques for actions of these groups on the\nline.\n', 'title': 'Spaces of orders of some one-relator groups'}]","Title: Spaces of orders of some one-relator groups

Abstract: We show that certain orderable groups admit no isolated left orders. The groups we consider are cyclic amalgamations of a free group with a general orderable group, the HNN extensions of free groups over cyclic subgroups, and a particular class of one-relator groups. In order to prove the results about orders, we develop perturbation techniques for actions of these groups on the line."
Are there known hardness results for approximating the range of parameters where a matrix has the Restricted Isometry Property?,12f63884,"[{'abstract': '  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts\nas an approximate isometry when restricted to sparse vectors. Previous work has\nshown it to be NP-hard to determine whether a matrix possess this property, but\nonly in a narrow range of parameters. In this work, we show that it is NP-hard\nto make this determination for any accuracy parameter, even when we restrict\nourselves to instances which are either RIP or far from being RIP. This result\nimplies that it is NP-hard to approximate the range of parameters for which a\nmatrix possesses the Restricted Isometry Property with accuracy better than\nsome constant. Ours is the first work to prove such a claim without any\nadditional assumptions.\n', 'title': 'Approximately certifying the restricted isometry property is hard'}]","I think I can help you with that!

Title: Approximately certifying the restricted isometry property is hard

Abstract: A matrix is said to possess the Restricted Isometry Property (RIP) if it acts as an approximate isometry when restricted to sparse vectors. Previous work has shown it to be NP-hard to determine whether a matrix possess this property, but only in a narrow range of parameters. In this work, we show that it is NP-hard to make this determination for any accuracy parameter, even when we restrict ourselves to instances which are either RIP or far from being RIP. This result implies that it is NP-hard to approximate the range of parameters for which a matrix possesses the Restricted Isometry Property with accuracy better than some constant. Ours is the first work to prove such a claim without any additional assumptions."
What geometric object is studied over these moduli spaces of quadratic differentials?,c2bbd5a8,"[{'abstract': ""  We study special circle bundles over two elementary moduli spaces of\nmeromorphic quadratic differentials with real periods denoted by $\\mathcal\nQ_0^{\\mathbb R}(-7)$ and $\\mathcal Q^{\\mathbb R}_0([-3]^2)$. The space\n$\\mathcal Q_0^{\\mathbb R}(-7)$ is the moduli space of meromorphic quadratic\ndifferentials on the Riemann sphere with one pole of order 7 with real periods;\nit appears naturally in the study of a neighbourhood of the Witten's cycle\n$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic\ndifferentials of $\\mathcal M_{g,n}$. The space $\\mathcal Q^{\\mathbb\nR}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the\nRiemann sphere with two poles of order at most 3 with real periods; it appears\nin description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the\ncombinatorial model. The application of the formalism of the Bergman\ntau-function to the combinatorial model (with the goal of computing\nanalytically Poincare dual cycles to certain combinations of tautological\nclasses) requires the study of special sections of circle bundles over\n$\\mathcal Q_0^{\\mathbb R}(-7)$ and $\\mathcal Q^{\\mathbb R}_0([-3]^2)$; in the\ncase of the space $\\mathcal Q_0^{\\mathbb R}(-7)$ a section of this circle\nbundle is given by the argument of the modular discriminant. We study the\nspaces $\\mathcal Q_0^{\\mathbb R}(-7)$ and $\\mathcal Q^{\\mathbb R}_0([-3]^2)$,\nalso called the spaces of Boutroux curves, in detail, together with\ncorresponding circle bundles.\n"", 'title': 'Discriminant circle bundles over local models of Strebel graphs and Boutroux curves'}]","Title: Discriminant circle bundles over local models of Strebel graphs and Boutroux curves

Abstract: We study special circle bundles over two elementary moduli spaces of meromorphic quadratic differentials with real periods denoted by $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$. The space $\mathcal Q_0^{\mathbb R}(-7)$ is the moduli space of meromorphic quadratic differentials on the Riemann sphere with one pole of order 7 with real periods; it appears naturally in the study of a neighbourhood of the Witten's cycle $W_1$ in the combinatorial model based on Jenkins-Strebel quadratic differentials of $\mathcal M_{g,n}$. The space $\mathcal Q^{\mathbb R}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the Riemann sphere with two poles of order at most 3 with real periods; it appears in description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the combinatorial model. The application of the formalism of the Bergman tau-function to the combinatorial model (with the goal of computing analytically Poincare dual cycles to certain combinations of tautological classes) requires the study of special sections of circle bundles over $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$; in the case of the space $\mathcal Q_0^{\mathbb R}(-7)$ a section of this circle bundle is given by the argument of the modular discriminant. We study the spaces $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$, also called the spaces of Boutroux curves, in detail, together with corresponding circle bundles.

The geometric object studied over these moduli spaces is a Riemann sphere."
What are the Karush-Kuhn-Tucker conditions and how are they used to analyze support vector machine behavior?,7e9e7015,"[{'abstract': '  The support vector machine (SVM) is a powerful and widely used classification\nalgorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide\nrigorous mathematical proof for new insights into the behavior of SVM. These\ninsights provide perhaps unexpected relationships between SVM and two other\nlinear classifiers: the mean difference and the maximal data piling direction.\nFor example, we show that in many cases SVM can be viewed as a cropped version\nof these classifiers. By carefully exploring these connections we show how SVM\ntuning behavior is affected by characteristics including: balanced vs.\nunbalanced classes, low vs. high dimension, separable vs. non-separable data.\nThese results provide further insights into tuning SVM via cross-validation by\nexplaining observed pathological behavior and motivating improved\ncross-validation methodology. Finally, we also provide new results on the\ngeometry of complete data piling directions in high dimensional space.\n', 'title': 'Geometric Insights into Support Vector Machine Behavior using the KKT Conditions'}]","Title: Geometric Insights into Support Vector Machine Behavior using the KKT Conditions

Abstract: The support vector machine (SVM) is a powerful and widely used classification algorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide rigorous mathematical proof for new insights into the behavior of SVM. These insights provide perhaps unexpected relationships between SVM and two other linear classifiers: the mean difference and the maximal data piling direction. For example, we show that in many cases SVM can be viewed as a cropped version of these classifiers. By carefully exploring these connections we show how SVM tuning behavior is affected by characteristics including: balanced vs. unbalanced classes, low vs. high dimension, separable vs. non-separable data. These results provide further insights into tuning SVM via cross-validation by explaining observed pathological behavior and motivating improved cross-validation methodology. Finally, we also provide new results on the geometry of complete data piling directions in high dimensional space."
What factors influenced the decisions of Mozilla release managers regarding patch uplift requests?,bd3013b7,"[{'abstract': '  In rapid release development processes, patches that fix critical issues, or\nimplement high-value features are often promoted directly from the development\nchannel to a stabilization channel, potentially skipping one or more\nstabilization channels. This practice is called patch uplift. Patch uplift is\nrisky, because patches that are rushed through the stabilization phase can end\nup introducing regressions in the code. This paper examines patch uplift\noperations at Mozilla, with the aim to identify the characteristics of uplifted\npatches that introduce regressions. Through statistical and manual analyses, we\nquantitatively and qualitatively investigate the reasons behind patch uplift\ndecisions and the characteristics of uplifted patches that introduced\nregressions. Additionally, we interviewed three Mozilla release managers to\nunderstand organizational factors that affect patch uplift decisions and\noutcomes. Results show that most patches are uplifted because of a wrong\nfunctionality or a crash. Uplifted patches that lead to faults tend to have\nlarger patch size, and most of the faults are due to semantic or memory errors\nin the patches. Also, release managers are more inclined to accept patch uplift\nrequests that concern certain specific components, and-or that are submitted by\ncertain specific developers.\n', 'title': 'Is It Safe to Uplift This Patch? An Empirical Study on Mozilla Firefox'}]","Title: Is It Safe to Uplift This Patch? An Empirical Study on Mozilla Firefox

Abstract:   In rapid release development processes, patches that fix critical issues, or
implement high-value features are often promoted directly from the development
channel to a stabilization channel, potentially skipping one or more
stabilization channels. This practice is called patch uplift. Patch uplift is
risky, because patches that are rushed through the stabilization phase can end
up introducing regressions in the code. This paper examines patch uplift
operations at Mozilla, with the aim to identify the characteristics of uplifted
patches that introduce regressions. Through statistical and manual analyses, we
quantitatively and qualitatively investigate the reasons behind patch uplift
decisions and the characteristics of uplifted patches that introduced
regressions. Additionally, we interviewed three Mozilla release managers to
understand organizational factors that affect patch uplift decisions and
outcomes. Results show that most patches are uplifted because of a wrong
functionality or a crash. Uplifted patches that lead to faults tend to have
larger patch size, and most of the faults are due to semantic or memory errors
in the patches. Also, release managers are more inclined to accept patch uplift
requests that concern certain specific components, and-or that are submitted by
certain specific developers."
What challenge arises when applying the vortex method to flows around boundaries?,737f0bb5,"[{'abstract': '  The vortex method is a common numerical and theoretical approach used to\nimplement the motion of an ideal flow, in which the vorticity is approximated\nby a sum of point vortices, so that the Euler equations read as a system of\nordinary differential equations. Such a method is well justified in the full\nplane, thanks to the explicit representation formulas of Biot and Savart. In an\nexterior domain, we also replace the impermeable boundary by a collection of\npoint vortices generating the circulation around the obstacle. The density of\nthese point vortices is chosen in order that the flow remains tangent at\nmidpoints between adjacent vortices. In this work, we provide a rigorous\njustification for this method in exterior domains. One of the main mathematical\ndifficulties being that the Biot-Savart kernel defines a singular integral\noperator when restricted to a curve. For simplicity and clarity, we only treat\nthe case of the unit disk in the plane approximated by a uniformly distributed\nmesh of point vortices. The complete and general version of our work is\navailable in [arXiv:1707.01458].\n', 'title': 'The vortex method for 2D ideal flows in the exterior of a disk'}]","I think I can help you with that!

The challenge that arises when applying the vortex method to flows around boundaries is that ""the Biot-Savart kernel defines a singular integral operator when restricted to a curve.""

Here's the relevant paper:

Title: The vortex method for 2D ideal flows in the exterior of a disk

Abstract: The vortex method is a common numerical and theoretical approach used to implement the motion of an ideal flow, in which the vorticity is approximated by a sum of point vortices, so that the Euler equations read as a system of ordinary differential equations. Such a method is well justified in the full plane, thanks to the explicit representation formulas of Biot and Savart. In an exterior domain, we also replace the impermeable boundary by a collection of point vortices generating the circulation around the obstacle. The density of these point vortices is chosen in order that the flow remains tangent at midpoints between adjacent vortices. In this work, we provide a rigorous justification for this method in exterior domains. One of the main mathematical difficulties being that the Biot-Savart kernel defines a singular integral operator when restricted to a curve. For simplicity and clarity, we only treat the case of the unit disk in the plane approximated by a uniformly distributed mesh of point vortices. The complete and general version of our work is available in [arXiv:1707.01458]."
What is the  maximum magnitude measured for miras in the Large Magellanic Cloud?,b228f454,"[{'abstract': '  We study the near-infrared properties of 690 Mira candidates in the central\nregion of the Large Magellanic Cloud, based on time-series observations at\nJHKs. We use densely-sampled I-band observations from the OGLE project to\ngenerate template light curves in the near infrared and derive robust mean\nmagnitudes at those wavelengths. We obtain near-infrared Period-Luminosity\nrelations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We\nstudy the Period-Luminosity-Color relations and the color excesses of\nCarbon-rich Miras, which show evidence for a substantially different reddening\nlaw.\n', 'title': 'Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras'}]","Title: Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras

Abstract: We study the near-infrared properties of 690 Mira candidates in the central region of the Large Magellanic Cloud, based on time-series observations at JHKs. We use densely-sampled I-band observations from the OGLE project to generate template light curves in the near infrared and derive robust mean magnitudes at those wavelengths. We obtain near-infrared Period-Luminosity relations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We study the Period-Luminosity-Color relations and the color excesses of Carbon-rich Miras, which show evidence for a substantially different reddening law.

The abstract does not provide a specific maximum magnitude measured for Miras in the Large Magellanic Cloud. It only mentions that they derive robust mean magnitudes at JHKs wavelengths."
How does a control-function approach contribute to reducing bias in instrumental variable estimation after applying shrinkage?,681e4606,"[{'abstract': '  The two-stage least-squares (2SLS) estimator is known to be biased when its\nfirst-stage fit is poor. I show that better first-stage prediction can\nalleviate this bias. In a two-stage linear regression model with Normal noise,\nI consider shrinkage in the estimation of the first-stage instrumental variable\ncoefficients. For at least four instrumental variables and a single endogenous\nregressor, I establish that the standard 2SLS estimator is dominated with\nrespect to bias. The dominating IV estimator applies James-Stein type shrinkage\nin a first-stage high-dimensional Normal-means problem followed by a\ncontrol-function approach in the second stage. It preserves invariances of the\nstructural instrumental variable equations.\n', 'title': 'Bias Reduction in Instrumental Variable Estimation through First-Stage Shrinkage'}]","Title: Bias Reduction in Instrumental Variable Estimation through First-Stage Shrinkage

Abstract: The two-stage least-squares (2SLS) estimator is known to be biased when its
first-stage fit is poor. I show that better first-stage prediction can
alleviate this bias. In a two-stage linear regression model with Normal noise,
I consider shrinkage in the estimation of the first-stage instrumental variable
coefficients. For at least four instrumental variables and a single endogenous
regressor, I establish that the standard 2SLS estimator is dominated with
respect to bias. The dominating IV estimator applies James-Stein type shrinkage
in a first-stage high-dimensional Normal-means problem followed by a
control-function approach in the second stage. It preserves invariances of the
structural instrumental variable equations."
What novel pose representation is used in this structure-aware regression approach?,e722db9d,"[{'abstract': '  Regression based methods are not performing as well as detection based\nmethods for human pose estimation. A central problem is that the structural\ninformation in the pose is not well exploited in the previous regression\nmethods. In this work, we propose a structure-aware regression approach. It\nadopts a reparameterized pose representation using bones instead of joints. It\nexploits the joint connection structure to define a compositional loss function\nthat encodes the long range interactions in the pose. It is simple, effective,\nand general for both 2D and 3D pose estimation in a unified setting.\nComprehensive evaluation validates the effectiveness of our approach. It\nsignificantly advances the state-of-the-art on Human3.6M and is competitive\nwith state-of-the-art results on MPII.\n', 'title': 'Compositional Human Pose Regression'}]","Title: Compositional Human Pose Regression

Abstract: Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M and is competitive with state-of-the-art results on MPII."
Which data categories were found to significantly improve the accuracy of a hospital readmission prediction model?,1e20a9f2,"[{'abstract': '  Identification of patients at high risk for readmission could help reduce\nmorbidity and mortality as well as healthcare costs. Most of the existing\nstudies on readmission prediction did not compare the contribution of data\ncategories. In this study we analyzed relative contribution of 90,101 variables\nacross 398,884 admission records corresponding to 163,468 patients, including\npatient demographics, historical hospitalization information, discharge\ndisposition, diagnoses, procedures, medications and laboratory test results. We\nestablished an interpretable readmission prediction model based on Logistic\nRegression in scikit-learn, and added the available variables to the model one\nby one in order to analyze the influences of individual data categories on\nreadmission prediction accuracy. Diagnosis related groups (c-statistic\nincrement of 0.0933) and discharge disposition (c-statistic increment of\n0.0269) were the strongest contributors to model accuracy. Additionally, we\nalso identified the top ten contributing variables in every data category.\n', 'title': 'Contribution of Data Categories to Readmission Prediction Accuracy'}]","Title: Contribution of Data Categories to Readmission Prediction Accuracy

Abstract: Identification of patients at high risk for readmission could help reduce morbidity and mortality as well as healthcare costs. Most of the existing studies on readmission prediction did not compare the contribution of data categories. In this study we analyzed relative contribution of 90,101 variables across 398,884 admission records corresponding to 163,468 patients, including patient demographics, historical hospitalization information, discharge disposition, diagnoses, procedures, medications and laboratory test results. We established an interpretable readmission prediction model based on Logistic Regression in scikit-learn, and added the available variables to the model one by one in order to analyze the influences of individual data categories on readmission prediction accuracy. Diagnosis related groups (c-statistic increment of 0.0933) and discharge disposition (c-statistic increment of 0.0269) were the strongest contributors to model accuracy. Additionally, we also identified the top ten contributing variables in every data category.

The data categories that were found to significantly improve the accuracy of the hospital readmission prediction model are diagnosis related groups and discharge disposition."
What are the error upper bounds for spectral regularization methods in reproducing kernel decision rules?,81b3345e,"[{'abstract': '  We obtain a Bernstein-type inequality for sums of Banach-valued random\nvariables satisfying a weak dependence assumption of general type and under\ncertain smoothness assumptions of the underlying Banach norm. We use this\ninequality in order to investigate in the asymptotical regime the error upper\nbounds for the broad family of spectral regularization methods for reproducing\nkernel decision rules, when trained on a sample coming from a $\\tau-$mixing\nprocess.\n', 'title': 'Concentration of weakly dependent Banach-valued sums and applications to statistical learning methods'}]","I think I can help you with that!

Title: Concentration of weakly dependent Banach-valued sums and applications to statistical learning methods

Abstract: We obtain a Bernstein-type inequality for sums of Banach-valued random variables satisfying a weak dependence assumption of general type and under certain smoothness assumptions of the underlying Banach norm. We use this inequality in order to investigate in the asymptotical regime the error upper bounds for the broad family of spectral regularization methods for reproducing kernel decision rules, when trained on a sample coming from a $\tau-$mixing process.

Let me know if this is what you were looking for!"
What is the effect of patches for Meltdown and Spectre vulnerabilities on the performance of high performance computing applications?,58a935ae,"[{'abstract': '  In this work we examine how the updates addressing Meltdown and Spectre\nvulnerabilities impact the performance of HPC applications. To study this we\nuse the application kernel module of XDMoD to test the performance before and\nafter the application of the vulnerability patches. We tested the performance\ndifference for multiple application and benchmarks including: NWChem, NAMD,\nHPCC, IOR, MDTest and IMB. The results show that although some specific\nfunctions can have performance decreased by as much as 74%, the majority of\nindividual metrics indicates little to no decrease in performance. The\nreal-world applications show a 2-3% decrease in performance for single node\njobs and a 5-11% decrease for parallel multi node jobs.\n', 'title': 'Effect of Meltdown and Spectre Patches on the Performance of HPC Applications'}]","I think I can help you with that!

The effect of patches for Meltdown and Spectre vulnerabilities on the performance of high-performance computing applications is discussed in the paper:

Title: Effect of Meltdown and Spectre Patches on the Performance of HPC Applications

Abstract: In this work we examine how the updates addressing Meltdown and Spectre vulnerabilities impact the performance of HPC applications. To study this we use the application kernel module of XDMoD to test the performance before and after the application of the vulnerability patches. We tested the performance difference for multiple application and benchmarks including: NWChem, NAMD, HPCC, IOR, MDTest and IMB. The results show that although some specific functions can have performance decreased by as much as 74%, the majority of individual metrics indicates little to no decrease in performance. The real-world applications show a 2-3% decrease in performance for single node jobs and a 5-11% decrease for parallel multi node jobs."
What is the Period-Luminosity relation for oxygen rich miras in the Large Magellanic Cloud?,b228f454,"[{'abstract': '  We study the near-infrared properties of 690 Mira candidates in the central\nregion of the Large Magellanic Cloud, based on time-series observations at\nJHKs. We use densely-sampled I-band observations from the OGLE project to\ngenerate template light curves in the near infrared and derive robust mean\nmagnitudes at those wavelengths. We obtain near-infrared Period-Luminosity\nrelations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We\nstudy the Period-Luminosity-Color relations and the color excesses of\nCarbon-rich Miras, which show evidence for a substantially different reddening\nlaw.\n', 'title': 'Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras'}]","Title: Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras

Abstract:   We study the near-infrared properties of 690 Mira candidates in the central
region of the Large Magellanic Cloud, based on time-series observations at
JHKs. We use densely-sampled I-band observations from the OGLE project to
generate template light curves in the near infrared and derive robust mean
magnitudes at those wavelengths. We obtain near-infrared Period-Luminosity
relations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We
study the Period-Luminosity-Color relations and the color excesses of
Carbon-rich Miras, which show evidence for a substantially different reddening
law.

The answer is: The Period-Luminosity relation for oxygen-rich Miras in the Large Magellanic Cloud has a scatter as low as 0.12 mag at Ks."
How can Machine Learning be used to enhance network management in cellular networks?,fcf8573a,"[{'abstract': '  In this paper, we provide an analysis of self-organized network management,\nwith an end-to-end perspective of the network. Self-organization as applied to\ncellular networks is usually referred to Self-organizing Networks (SONs), and\nit is a key driver for improving Operations, Administration, and Maintenance\n(OAM) activities. SON aims at reducing the cost of installation and management\nof 4G and future 5G networks, by simplifying operational tasks through the\ncapability to configure, optimize and heal itself. To satisfy 5G network\nmanagement requirements, this autonomous management vision has to be extended\nto the end to end network. In literature and also in some instances of products\navailable in the market, Machine Learning (ML) has been identified as the key\ntool to implement autonomous adaptability and take advantage of experience when\nmaking decisions. In this paper, we survey how network management can\nsignificantly benefit from ML solutions. We review and provide the basic\nconcepts and taxonomy for SON, network management and ML. We analyse the\navailable state of the art in the literature, standardization, and in the\nmarket. We pay special attention to 3rd Generation Partnership Project (3GPP)\nevolution in the area of network management and to the data that can be\nextracted from 3GPP networks, in order to gain knowledge and experience in how\nthe network is working, and improve network performance in a proactive way.\nFinally, we go through the main challenges associated with this line of\nresearch, in both 4G and in what 5G is getting designed, while identifying new\ndirections for research.\n', 'title': 'From 4G to 5G: Self-organized Network Management meets Machine Learning'}]","Title: From 4G to 5G: Self-organized Network Management meets Machine Learning

Abstract: In this paper, we provide an analysis of self-organized network management, with an end-to-end perspective of the network. Self-organization as applied to cellular networks is usually referred to Self-organizing Networks (SONs), and it is a key driver for improving Operations, Administration, and Maintenance (OAM) activities. SON aims at reducing the cost of installation and management of 4G and future 5G networks, by simplifying operational tasks through the capability to configure, optimize and heal itself. To satisfy 5G network management requirements, this autonomous management vision has to be extended to the end to end network. In literature and also in some instances of products available in the market, Machine Learning (ML) has been identified as the key tool to implement autonomous adaptability and take advantage of experience when making decisions. In this paper, we survey how network management can significantly benefit from ML solutions. We review and provide the basic concepts and taxonomy for SON, network management and ML. We analyse the available state of the art in the literature, standardization, and in the market. We pay special attention to 3rd Generation Partnership Project (3GPP) evolution in the area of network management and to the data that can be extracted from 3GPP networks, in order to gain knowledge and experience in how the network is working, and improve network performance in a proactive way. Finally, we go through the main challenges associated with this line of research, in both 4G and in what 5G is getting designed, while identifying new directions for research."
Under what environmental conditions is a purely exploitative strategy the best?,26cf94e8,"[{'abstract': '  We study a minimal model for the growth of a phenotypically heterogeneous\npopulation of cells subject to a fluctuating environment in which they can\nreplicate (by exploiting available resources) and modify their phenotype within\na given landscape (thereby exploring novel configurations). The model displays\nan exploration-exploitation trade-off whose specifics depend on the statistics\nof the environment. Most notably, the phenotypic distribution corresponding to\nmaximum population fitness (i.e. growth rate) requires a non-zero exploration\nrate when the magnitude of environmental fluctuations changes randomly over\ntime, while a purely exploitative strategy turns out to be optimal in two-state\nenvironments, independently of the statistics of switching times. We obtain\nanalytical insight into the limiting cases of very fast and very slow\nexploration rates by directly linking population growth to the features of the\nenvironment.\n', 'title': 'Exploration-exploitation tradeoffs dictate the optimal distributions of phenotypes for populations subject to fitness fluctuations'}]","Title: Exploration-exploitation tradeoffs dictate the optimal distributions of phenotypes for populations subject to fitness fluctuations

Abstract: We study a minimal model for the growth of a phenotypically heterogeneous population of cells subject to a fluctuating environment in which they can replicate (by exploiting available resources) and modify their phenotype within a given landscape (thereby exploring novel configurations). The model displays an exploration-exploitation trade-off whose specifics depend on the statistics of the environment. Most notably, the phenotypic distribution corresponding to maximum population fitness (i.e. growth rate) requires a non-zero exploration rate when the magnitude of environmental fluctuations changes randomly over time, while a purely exploitative strategy turns out to be optimal in two-state environments, independently of the statistics of switching times."
Can you explain how the absence of certain data levels during training can impact model performance?,d874dd13,"[{'abstract': '  One advantage of decision tree based methods like random forests is their\nability to natively handle categorical predictors without having to first\ntransform them (e.g., by using feature engineering techniques). However, in\nthis paper, we show how this capability can lead to an inherent ""absent levels""\nproblem for decision tree based methods that has never been thoroughly\ndiscussed, and whose consequences have never been carefully explored. This\nproblem occurs whenever there is an indeterminacy over how to handle an\nobservation that has reached a categorical split which was determined when the\nobservation in question\'s level was absent during training. Although these\nincidents may appear to be innocuous, by using Leo Breiman and Adele Cutler\'s\nrandom forests FORTRAN code and the randomForest R package (Liaw and Wiener,\n2002) as motivating case studies, we examine how overlooking the absent levels\nproblem can systematically bias a model. Furthermore, by using three real data\nexamples, we illustrate how absent levels can dramatically alter a model\'s\nperformance in practice, and we empirically demonstrate how some simple\nheuristics can be used to help mitigate the effects of the absent levels\nproblem until a more robust theoretical solution is found.\n', 'title': 'Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem'}]","I think I can help you with that!

Title: Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem

Abstract: One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent ""absent levels"" problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found."
Are there any existing formulations of dissipative hydrodynamics that this research complements or extends?,9230c951,"[{'abstract': ""  We construct a Schwinger-Keldysh effective field theory for relativistic\nhydrodynamics for charged matter in a thermal background using a superspace\nformalism. Superspace allows us to efficiently impose the symmetries of the\nproblem and to obtain a simple expression for the effective action. We show\nthat the theory we obtain is compatible with the Kubo-Martin-Schwinger\ncondition, which in turn implies that Green's functions obey the\nfluctuation-dissipation theorem. Our approach complements and extends existing\nformulations found in the literature.\n"", 'title': 'Dissipative hydrodynamics in superspace'}]","Based on the provided context, I found that the research on ""Dissipative hydrodynamics in superspace"" complements and extends existing formulations found in the literature."
"What is core2vec, and how does it differ from existing methods like node2vec, DeepWalk, and LINE?",fe90413a,"[{'abstract': '  Recent advances in the field of network representation learning are mostly\nattributed to the application of the skip-gram model in the context of graphs.\nState-of-the-art analogues of skip-gram model in graphs define a notion of\nneighbourhood and aim to find the vector representation for a node, which\nmaximizes the likelihood of preserving this neighborhood.\nIn this paper, we take a drastic departure from the existing notion of\nneighbourhood of a node by utilizing the idea of coreness. More specifically,\nwe utilize the well-established idea that nodes with similar core numbers play\nequivalent roles in the network and hence induce a novel and an organic notion\nof neighbourhood. Based on this idea, we propose core2vec, a new algorithmic\nframework for learning low dimensional continuous feature mapping for a node.\nConsequently, the nodes having similar core numbers are relatively closer in\nthe vector space that we learn.\nWe further demonstrate the effectiveness of core2vec by comparing word\nsimilarity scores obtained by our method where the node representations are\ndrawn from standard word association graphs against scores computed by other\nstate-of-the-art network representation techniques like node2vec, DeepWalk and\nLINE. Our results always outperform these existing methods\n', 'title': 'Core2Vec: A core-preserving feature learning framework for networks'}]","Title: Core2Vec: A core-preserving feature learning framework for networks

Abstract: Recent advances in the field of network representation learning are mostly attributed to the application of the skip-gram model in the context of graphs. State-of-the-art analogues of skip-gram model in graphs define a notion of neighbourhood and aim to find the vector representation for a node, which maximizes the likelihood of preserving this neighborhood. In this paper, we take a drastic departure from the existing notion of neighbourhood of a node by utilizing the idea of coreness. More specifically, we utilize the well-established idea that nodes with similar core numbers play equivalent roles in the network and hence induce a novel and an organic notion of neighbourhood. Based on this idea, we propose core2vec, a new algorithmic framework for learning low dimensional continuous feature mapping for a node. Consequently, the nodes having similar core numbers are relatively closer in the vector space that we learn. We further demonstrate the effectiveness of core2vec by comparing word similarity scores obtained by our method where the node representations are drawn from standard word association graphs against scores computed by other state-of-the-art network representation techniques like node2vec, DeepWalk and LINE. Our results always outperform these existing methods."
How does the ABC posterior change as the amount of data increases?,1bab02e5,"[{'abstract': '  We present an informal review of recent work on the asymptotics of\nApproximate Bayesian Computation (ABC). In particular we focus on how does the\nABC posterior, or point estimates obtained by ABC, behave in the limit as we\nhave more data? The results we review show that ABC can perform well in terms\nof point estimation, but standard implementations will over-estimate the\nuncertainty about the parameters. If we use the regression correction of\nBeaumont et al. then ABC can also accurately quantify this uncertainty. The\ntheoretical results also have practical implications for how to implement ABC.\n', 'title': 'Asymptotics of ABC'}]","Title: Asymptotics of ABC

Abstract: We present an informal review of recent work on the asymptotics of Approximate Bayesian Computation (ABC). In particular we focus on how does the ABC posterior, or point estimates obtained by ABC, behave in the limit as we have more data? The results we review show that ABC can perform well in terms of point estimation, but standard implementations will over-estimate the uncertainty about the parameters. If we use the regression correction of Beaumont et al. then ABC can also accurately quantify this uncertainty. The theoretical results also have practical implications for how to implement ABC."
What makes the sparse coding objective developed in this paper suitable for optimization despite its non-convexity?,1fb8bf53,"[{'abstract': '  A variety of representation learning approaches have been investigated for\nreinforcement learning; much less attention, however, has been given to\ninvestigating the utility of sparse coding. Outside of reinforcement learning,\nsparse coding representations have been widely used, with non-convex objectives\nthat result in discriminative representations. In this work, we develop a\nsupervised sparse coding objective for policy evaluation. Despite the\nnon-convexity of this objective, we prove that all local minima are global\nminima, making the approach amenable to simple optimization strategies. We\nempirically show that it is key to use a supervised objective, rather than the\nmore straightforward unsupervised sparse coding approach. We compare the\nlearned representations to a canonical fixed sparse representation, called\ntile-coding, demonstrating that the sparse coding representation outperforms a\nwide variety of tilecoding representations.\n', 'title': 'Learning Sparse Representations in Reinforcement Learning with Sparse Coding'}]","Title: Learning Sparse Representations in Reinforcement Learning with Sparse Coding

Abstract: A variety of representation learning approaches have been investigated for reinforcement learning; much less attention, however, has been given to investigating the utility of sparse coding. Outside of reinforcement learning, sparse coding representations have been widely used, with non-convex objectives that result in discriminative representations. In this work, we develop a supervised sparse coding objective for policy evaluation. Despite the non-convexity of this objective, we prove that all local minima are global minima, making the approach amenable to simple optimization strategies. We empirically show that it is key to use a supervised objective, rather than the more straightforward unsupervised sparse coding approach. We compare the learned representations to a canonical fixed sparse representation, called tile-coding, demonstrating that the sparse coding representation outperforms a wide variety of tilecoding representations.

The paper states that ""Despite the non-convexity of this objective, we prove that all local minima are global minima, making the approach amenable to simple optimization strategies."" This means that the sparse coding objective is suitable for optimization despite its non-convexity because all local minima are global minima, making it possible to find the optimal solution using simple optimization methods."
What causes the larger negative magnetoresistance in a Ni80Fe20 and p-silicon bilayer compared to Ni80Fe20 on an oxide?,b60f20ec,"[{'abstract': '  In this work, we present an experimental study of spin mediated enhanced\nnegative magnetoresistance in Ni80Fe20 (50 nm)/p-Si (350 nm) bilayer. The\nresistance measurement shows a reduction of ~2.5% for the bilayer specimen as\ncompared to 1.3% for Ni80Fe20 (50 nm) on oxide specimen for an out-of-plane\napplied magnetic field of 3T. In the Ni80Fe20-only film, the negative\nmagnetoresistance behavior is attributed to anisotropic magnetoresistance. We\npropose that spin polarization due to spin-Hall effect is the underlying cause\nof the enhanced negative magnetoresistance observed in the bilayer. Silicon has\nweak spin orbit coupling so spin Hall magnetoresistance measurement is not\nfeasible. We use V2{\\omega} and V3{\\omega} measurement as a function of\nmagnetic field and angular rotation of magnetic field in direction normal to\nelectric current to elucidate the spin-Hall effect. The angular rotation of\nmagnetic field shows a sinusoidal behavior for both V2{\\omega} and V3{\\omega},\nwhich is attributed to the spin phonon interactions resulting from the\nspin-Hall effect mediated spin polarization. We propose that the spin\npolarization leads to a decrease in hole-phonon scattering resulting in\nenhanced negative magnetoresistance.\n', 'title': 'Spin mediated enhanced negative magnetoresistance in Ni80Fe20 and p-silicon bilayer'}]","Title: Spin mediated enhanced negative magnetoresistance in Ni80Fe20 and p-silicon bilayer

Abstract: In this work, we present an experimental study of spin mediated enhanced negative magnetoresistance in Ni80Fe20 (50 nm)/p-Si (350 nm) bilayer. The resistance measurement shows a reduction of ~2.5% for the bilayer specimen as compared to 1.3% for Ni80Fe20 (50 nm) on oxide specimen for an out-of-plane applied magnetic field of 3T. In the Ni80Fe20-only film, the negative magnetoresistance behavior is attributed to anisotropic magnetoresistance. We propose that spin polarization due to spin-Hall effect is the underlying cause of the enhanced negative magnetoresistance observed in the bilayer. Silicon has weak spin orbit coupling so spin Hall magnetoresistance measurement is not feasible. We use V2{\omega} and V3{\omega} measurement as a function of magnetic field and angular rotation of magnetic field in direction normal to electric current to elucidate the spin-Hall effect. The angular rotation of magnetic field shows a sinusoidal behavior for both V2{\omega} and V3{\omega}, which is attributed to the spin phonon interactions resulting from the spin-Hall effect mediated spin polarization. We propose that the spin polarization leads to a decrease in hole-phonon scattering resulting in enhanced negative magnetoresistance.

According to this research, the larger negative magnetoresistance in the Ni80Fe20 and p-silicon bilayer is due to the spin polarization caused by the spin-Hall effect, which leads to a decrease in hole-phonon scattering."
What are the advantages of using Gaussian process regression compared to traditional nudged elastic band calculations for finding minimum energy paths?,f33a9f77,"[{'abstract': '  The calculation of minimum energy paths for transitions such as atomic and/or\nspin re-arrangements is an important task in many contexts and can often be\nused to determine the mechanism and rate of transitions. An important challenge\nis to reduce the computational effort in such calculations, especially when ab\ninitio or electron density functional calculations are used to evaluate the\nenergy since they can require large computational effort. Gaussian process\nregression is used here to reduce significantly the number of energy\nevaluations needed to find minimum energy paths of atomic rearrangements. By\nusing results of previous calculations to construct an approximate energy\nsurface and then converge to the minimum energy path on that surface in each\nGaussian process iteration, the number of energy evaluations is reduced\nsignificantly as compared with regular nudged elastic band calculations. For a\ntest problem involving rearrangements of a heptamer island on a crystal\nsurface, the number of energy evaluations is reduced to less than a fifth. The\nscaling of the computational effort with the number of degrees of freedom as\nwell as various possible further improvements to this approach are discussed.\n', 'title': 'Minimum energy path calculations with Gaussian process regression'}]","I think I can help you with that!

Title: Minimum energy path calculations with Gaussian process regression

Abstract: The calculation of minimum energy paths for transitions such as atomic and/or spin re-arrangements is an important task in many contexts and can often be used to determine the mechanism and rate of transitions. An important challenge is to reduce the computational effort in such calculations, especially when ab initio or electron density functional calculations are used to evaluate the energy since they can require large computational effort. Gaussian process regression is used here to reduce significantly the number of energy evaluations needed to find minimum energy paths of atomic rearrangements. By using results of previous calculations to construct an approximate energy surface and then converge to the minimum energy path on that surface in each Gaussian process iteration, the number of energy evaluations is reduced significantly as compared with regular nudged elastic band calculations. For a test problem involving rearrangements of a heptamer island on a crystal surface, the number of energy evaluations is reduced to less than a fifth. The scaling of the computational effort with the number of degrees of freedom as well as various possible further improvements to this approach are discussed.

According to this paper, the advantages of using Gaussian process regression compared to traditional nudged elastic band calculations for finding minimum energy paths are:

* Reduced computational effort: Gaussian process regression reduces the number of energy evaluations needed to find minimum energy paths.
* Significant reduction in energy evaluations: The number of energy evaluations is reduced to less than a fifth compared to regular nudged elastic band calculations.

Let me know if you have any further questions!"
How do artificial learning systems address the challenge of catastrophic forgetting during lifelong learning?,5daa856c,"[{'abstract': ""  Humans can learn in a continuous manner. Old rarely utilized knowledge can be\noverwritten by new incoming information while important, frequently used\nknowledge is prevented from being erased. In artificial learning systems,\nlifelong learning so far has focused mainly on accumulating knowledge over\ntasks and overcoming catastrophic forgetting. In this paper, we argue that,\ngiven the limited model capacity and the unlimited new information to be\nlearned, knowledge has to be preserved or erased selectively. Inspired by\nneuroplasticity, we propose a novel approach for lifelong learning, coined\nMemory Aware Synapses (MAS). It computes the importance of the parameters of a\nneural network in an unsupervised and online manner. Given a new sample which\nis fed to the network, MAS accumulates an importance measure for each parameter\nof the network, based on how sensitive the predicted output function is to a\nchange in this parameter. When learning a new task, changes to important\nparameters can then be penalized, effectively preventing important knowledge\nrelated to previous tasks from being overwritten. Further, we show an\ninteresting connection between a local version of our method and Hebb's\nrule,which is a model for the learning process in the brain. We test our method\non a sequence of object recognition tasks and on the challenging problem of\nlearning an embedding for predicting $<$subject, predicate, object$>$ triplets.\nWe show state-of-the-art performance and, for the first time, the ability to\nadapt the importance of the parameters based on unlabeled data towards what the\nnetwork needs (not) to forget, which may vary depending on test conditions.\n"", 'title': 'Memory Aware Synapses: Learning what (not) to forget'}]","Title: Memory Aware Synapses: Learning what (not) to forget

Abstract:   Humans can learn in a continuous manner. Old rarely utilized knowledge can be
overwritten by new incoming information while important, frequently used
knowledge is prevented from being erased. In artificial learning systems,
lifelong learning so far has focused mainly on accumulating knowledge over
tasks and overcoming catastrophic forgetting. In this paper, we argue that,
given the limited model capacity and the unlimited new information to be
learned, knowledge has to be preserved or erased selectively. Inspired by
neuroplasticity, we propose a novel approach for lifelong learning, coined
Memory Aware Synapses (MAS). It computes the importance of the parameters of a
neural network in an unsupervised and online manner. Given a new sample which
is fed to the network, MAS accumulates an importance measure for each parameter
of the network, based on how sensitive the predicted output function is to a
change in this parameter. When learning a new task, changes to important
parameters can then be penalized, effectively preventing important knowledge
related to previous tasks from being overwritten. Further, we show an
interesting connection between a local version of our method and Hebb's
rule,which is a model for the learning process in the brain. We test our method
on a sequence of object recognition tasks and on the challenging problem of
learning an embedding for predicting $<$subject, predicate, object$>$ triplets.
We show state-of-the-art performance and, for the first time, the ability to
adapt the importance of the parameters based on unlabeled data towards what the
network needs (not) to forget, which may vary depending on test conditions."
What are the implications of a flat energy landscape in neural networks?,481fc13e,"[{'abstract': '  Training neural networks involves finding minima of a high-dimensional\nnon-convex loss function. Knowledge of the structure of this energy landscape\nis sparse. Relaxing from linear interpolations, we construct continuous paths\nbetween minima of recent neural network architectures on CIFAR10 and CIFAR100.\nSurprisingly, the paths are essentially flat in both the training and test\nlandscapes. This implies that neural networks have enough capacity for\nstructural changes, or that these changes are small between minima. Also, each\nminimum has at least one vanishing Hessian eigenvalue in addition to those\nresulting from trivial invariance.\n', 'title': 'Essentially No Barriers in Neural Network Energy Landscape'}]","Title: Essentially No Barriers in Neural Network Energy Landscape

Abstract: Training neural networks involves finding minima of a high-dimensional non-convex loss function. Knowledge of the structure of this energy landscape is sparse. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that neural networks have enough capacity for structural changes, or that these changes are small between minima. Also, each minimum has at least one vanishing Hessian eigenvalue in addition to those resulting from trivial invariance.

The implication of a flat energy landscape in neural networks is that neural networks have enough capacity for structural changes, or that these changes are small between minima."
How do non-equal wave vectors in beam splitters affect atom interferometer recoil diagrams?,f790353b,"[{'abstract': '  It is shown that using beam splitters with non-equal wave vectors results in\na new recoil diagram which is qualitatively different from the well-known\ndiagram associated with the Mach-Zehnder atom interferometer. We predict a new\nasymmetric Mach-Zehnder atom interferometer (AMZAI) and study it when one uses\na Raman beam splitter. The main feature is that the phase of AMZAI contains a\nquantum part proportional to the recoil frequency. A response sensitive only to\nthe quantum phase was found. A new technique to measure the recoil frequency\nand fine structure constant is proposed and studied outside of the Raman-Nath\napproximation.\n', 'title': 'Asymmetric Mach-Zehnder atom interferometers'}]","I think I can help you with that!

Title: Asymmetric Mach-Zehnder atom interferometers

Abstract: It is shown that using beam splitters with non-equal wave vectors results in a new recoil diagram which is qualitatively different from the well-known diagram associated with the Mach-Zehnder atom interferometer. We predict a new asymmetric Mach-Zehnder atom interferometer (AMZAI) and study it when one uses a Raman beam splitter. The main feature is that the phase of AMZAI contains a quantum part proportional to the recoil frequency. A response sensitive only to the quantum phase was found. A new technique to measure the recoil frequency and fine structure constant is proposed and studied outside of the Raman-Nath approximation.

This paper seems to be directly addressing your question, so I hope this is the one you were looking for!"
What type of neural networks are used to learn the relationship between visual data and robot movement?,70416061,"[{'abstract': '  We present bilateral teleoperation system for task learning and robot motion\ngeneration. Our system includes a bilateral teleoperation platform and a deep\nlearning software. The deep learning software refers to human demonstration\nusing the bilateral teleoperation platform to collect visual images and robotic\nencoder values. It leverages the datasets of images and robotic encoder\ninformation to learn about the inter-modal correspondence between visual images\nand robot motion. In detail, the deep learning software uses a combination of\nDeep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent\nNeural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor\nangles, to learn motion taught be human teleoperation. The learnt models are\nused to predict new motion trajectories for similar tasks. Experimental results\nshow that our system has the adaptivity to generate motion for similar scooping\ntasks. Detailed analysis is performed based on failure cases of the\nexperimental results. Some insights about the cans and cannots of the system\nare summarized.\n', 'title': 'Deep Learning Scooping Motion using Bilateral Teleoperations'}]","Here is the answer to your question:

Title: Deep Learning Scooping Motion using Bilateral Teleoperations

Abstract: We present bilateral teleoperation system for task learning and robot motion generation. Our system includes a bilateral teleoperation platform and a deep learning software. The deep learning software refers to human demonstration using the bilateral teleoperation platform to collect visual images and robotic encoder values. It leverages the datasets of images and robotic encoder information to learn about the inter-modal correspondence between visual images and robot motion. In detail, the deep learning software uses a combination of Deep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent Neural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor angles, to learn motion taught be human teleoperation. The learnt models are used to predict new motion trajectories for similar tasks. Experimental results show that our system has the adaptivity to generate motion for similar scooping tasks. Detailed analysis is performed based on failure cases of the experimental results. Some insights about the cans and cannots of the system are summarized."
What is Ensemble Sampling and how does it relate to Thompson Sampling?,49981aba,"[{'abstract': '  Thompson sampling has emerged as an effective heuristic for a broad range of\nonline decision problems. In its basic form, the algorithm requires computing\nand sampling from a posterior distribution over models, which is tractable only\nfor simple special cases. This paper develops ensemble sampling, which aims to\napproximate Thompson sampling while maintaining tractability even in the face\nof complex models such as neural networks. Ensemble sampling dramatically\nexpands on the range of applications for which Thompson sampling is viable. We\nestablish a theoretical basis that supports the approach and present\ncomputational results that offer further insight.\n', 'title': 'Ensemble Sampling'}]","I think I can help you with that!

Title: Ensemble Sampling

Abstract: Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.

It appears that Ensemble Sampling is an approach that approximates Thompson Sampling, allowing it to be tractable even with complex models like neural networks."
How can computational techniques based on an emergent paradigm analyze urban activity patterns from positioning data?,30533fe4,"[{'abstract': '  Positioning data offer a remarkable source of information to analyze crowds\nurban dynamics. However, discovering urban activity patterns from the emergent\nbehavior of crowds involves complex system modeling. An alternative approach is\nto adopt computational techniques belonging to the emergent paradigm, which\nenables self-organization of data and allows adaptive analysis. Specifically,\nour approach is based on stigmergy. By using stigmergy each sample position is\nassociated with a digital pheromone deposit, which progressively evaporates and\naggregates with other deposits according to their spatiotemporal proximity.\nBased on this principle, we exploit positioning data to identify high density\nareas (hotspots) and characterize their activity over time. This\ncharacterization allows the comparison of dynamics occurring in different days,\nproviding a similarity measure exploitable by clustering techniques. Thus, we\ncluster days according to their activity behavior, discovering unexpected urban\nactivity patterns. As a case study, we analyze taxi traces in New York City\nduring 2015.\n', 'title': 'Stigmergy-based modeling to discover urban activity patterns from positioning data'}]","Title: Stigmergy-based modeling to discover urban activity patterns from positioning data

Abstract: Positioning data offer a remarkable source of information to analyze crowds urban dynamics. However, discovering urban activity patterns from the emergent behavior of crowds involves complex system modeling. An alternative approach is to adopt computational techniques belonging to the emergent paradigm, which enables self-organization of data and allows adaptive analysis. Specifically, our approach is based on stigmergy. By using stigmergy each sample position is associated with a digital pheromone deposit, which progressively evaporates and aggregates with other deposits according to their spatiotemporal proximity. Based on this principle, we exploit positioning data to identify high density areas (hotspots) and characterize their activity over time. This characterization allows the comparison of dynamics occurring in different days, providing a similarity measure exploitable by clustering techniques. Thus, we cluster days according to their activity behavior, discovering unexpected urban activity patterns. As a case study, we analyze taxi traces in New York City during 2015."
What is the retrospective higher-order Markov process (RHOMP) and how does it address the drawbacks of traditional Markov chains?,1a4e3129,"[{'abstract': '  Users form information trails as they browse the web, checkin with a\ngeolocation, rate items, or consume media. A common problem is to predict what\na user might do next for the purposes of guidance, recommendation, or\nprefetching. First-order and higher-order Markov chains have been widely used\nmethods to study such sequences of data. First-order Markov chains are easy to\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\nin contrast, have too many parameters and suffer from overfitting the training\ndata. Fitting these parameters with regularization and smoothing only offers\nmild improvements. In this paper we propose the retrospective higher-order\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\nis a special case of a higher-order Markov chain where the transitions depend\nretrospectively on a single history state instead of an arbitrary combination\nof history states. There are two immediate computational advantages: the number\nof parameters is linear in the order of the Markov chain and the model can be\nfit to large state spaces. Furthermore, by providing a specific structure to\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\nutilizing history states without risks of overfitting the data. We demonstrate\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\nmethod on various real application datasets spanning geolocation data, review\nsequences, and business locations. The RHOMP model uniformly outperforms\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\nfactorizations in terms of prediction accuracy.\n', 'title': 'Retrospective Higher-Order Markov Processes for User Trails'}]","Title: Retrospective Higher-Order Markov Processes for User Trails

Abstract: Users form information trails as they browse the web, checkin with a geolocation, rate items, or consume media. A common problem is to predict what a user might do next for the purposes of guidance, recommendation, or prefetching. First-order and higher-order Markov chains have been widely used methods to study such sequences of data. First-order Markov chains are easy to estimate, but lack accuracy when history matters. Higher-order Markov chains, in contrast, have too many parameters and suffer from overfitting the training data. Fitting these parameters with regularization and smoothing only offers mild improvements. In this paper we propose the retrospective higher-order Markov process (RHOMP) as a low-parameter model for such sequences. This model is a special case of a higher-order Markov chain where the transitions depend retrospectively on a single history state instead of an arbitrary combination of history states. There are two immediate computational advantages: the number of parameters is linear in the order of the Markov chain and the model can be fit to large state spaces. Furthermore, by providing a specific structure to the higher-order chain, RHOMPs improve the model accuracy by efficiently utilizing history states without risks of overfitting the data. We demonstrate how to estimate a RHOMP from data and we demonstrate the effectiveness of our method on various real application datasets spanning geolocation data, review sequences, and business locations. The RHOMP model uniformly outperforms higher-order Markov chains, Kneser-Ney regularization, and tensor factorizations in terms of prediction accuracy."
